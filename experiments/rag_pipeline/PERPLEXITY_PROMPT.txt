# Perplexity용 프롬프트 (보고서 작성 요청)

아래 내용을 Perplexity에 복사-붙여넣기하여 전문적인 보고서를 생성하세요.

---

다음 RAG 챗봇 최적화 프로젝트에 대한 전문적인 기술 보고서를 작성해주세요. 최신 연구 논문과 best practice를 참고하여 각 최적화 기법의 이론적 근거를 강화해주세요.

## 프로젝트 개요

**목적**: Git & Python 개발 학습 도우미 RAG 챗봇 성능 최적화
**기간**: 2025-11-19 (1일 집중 최적화)
**초기 성능**: Faithfulness 45%, Answer Correctness 44% (RAGAS 평가)
**목표 성능**: Faithfulness 93%+, Answer Correctness 75%+

## 구현한 최적화 기법

### 1. Enhanced RAG Pipeline

**Hybrid Search (Dense + Sparse + RRF)**
- Dense Search: BGE-M3 임베딩 (의미적 유사도)
- Sparse Search: Lexical weights (BM25-like 키워드 매칭)
- Reciprocal Rank Fusion: 두 검색 결과 통합 (k=60)

**참고할 논문**:
- "Reciprocal Rank Fusion outperforms Condorcet" (SIGIR 2009)
- BGE-M3 관련 최신 연구

**Context Quality Filter (Self-RAG Style)**
- GPT-4o-mini로 각 컨텍스트를 RELEVANT/PARTIAL/IRRELEVANT로 분류
- IRRELEVANT 문서는 최종 LLM에 전달하지 않음
- 환각(hallucination) 감소 목적

**참고할 논문**:
- "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-reflection" (Asai et al., 2023, arXiv:2310.11511)

**Two-Stage Reranking**
- Stage 1: BAAI/bge-reranker-v2-m3 (빠른 필터링)
- Stage 2: BAAI/bge-reranker-large (정밀 순위)

### 2. 데이터 기반 Hyperparameter 최적화

**문서 분포 분석**:
- 총 12,796 chunks (Git/Python만)
- 통계: Mean=829 chars, Median=899 chars, P75=969, P95=1012

**최적화 결과**:
- chunk_size: 1024 → 900 (Median 기반, 50%의 문서 최적화)
- chunk_overlap: 150 → 180 (20% 표준 비율)
- rerank_top_k: 5 → 10 (초기 검색의 20%, 이상적 비율)

**참고할 자료**:
- LangChain/LlamaIndex의 chunking best practices
- RAG 시스템의 top-k 파라미터 최적화 연구

### 3. System Prompt 최적화

**적용한 LLM 프롬프팅 이론**:

1. **Constitutional AI** (Anthropic 2022, arXiv:2212.08073)
   - "검색된 문서에만 기반하여 답변" 원칙 명시
   - 환각 방지를 위한 Constitutional Rules

2. **Few-shot Prompting** (Brown et al. 2020, arXiv:2005.14165)
   - 3개의 완벽한 예시로 원하는 답변 스타일 학습
   - 100줄의 지침보다 3개의 예시가 더 효과적

3. **Context Grounding** (Self-RAG 2023)
   - RELEVANT 문서만 사용
   - 내부적으로 문서 평가, 사용자에게는 최종 답변만

4. **Chain-of-Thought** (Wei et al. 2022, arXiv:2201.11903)
   - 복잡한 질문 내부 분해
   - 최종 답변은 간결하게

5. **Minimal Citation Strategy**
   - 답변 끝에 한 번만 출처 표시
   - 가독성 향상

**결과**:
- 답변 길이: 15-20줄 → 5-8줄 (70% 감소)
- 톤: 형식적 → 자연스러운 대화형
- 출처: "근거 1, 근거 2" → 실제 파일명

## 예상 성능 개선

| Metric | Before | Target | Improvement |
|--------|--------|--------|-------------|
| Faithfulness | 45% | 93%+ | +106% |
| Answer Correctness | 44% | 75%+ | +70% |
| Context Precision | 65% | 80%+ | +23% |
| Context Recall | 68% | 85%+ | +25% |

## 기술 스택

- Embedding: BAAI/bge-m3 (Dense + Sparse)
- Vector DB: ChromaDB (12,796 chunks)
- Reranking: bge-reranker-v2-m3 + bge-reranker-large
- LLM: GPT-4.1 (답변 생성) + GPT-4o-mini (컨텍스트 평가)

## 요청 사항

다음 내용을 포함한 전문적인 보고서를 작성해주세요:

1. **각 최적화 기법의 이론적 배경**
   - 최신 논문 및 연구 인용
   - 왜 이 방법이 효과적인지 설명

2. **Hybrid Search 상세 분석**
   - Dense vs Sparse search의 장단점
   - RRF의 수학적 원리 및 효과
   - 관련 최신 연구 (2023-2024)

3. **Self-RAG 및 Context Quality Filter**
   - Self-RAG 논문 상세 분석
   - LLM을 평가자로 사용하는 최신 트렌드
   - Constitutional AI와의 연관성

4. **Chunking 및 Hyperparameter 최적화**
   - RAG 시스템의 chunking best practices
   - Top-k 파라미터 선택 이론 (sqrt(N), 3-5% rule 등)
   - 데이터 기반 최적화의 중요성

5. **LLM Prompting 최신 연구**
   - Constitutional AI, Few-shot, Chain-of-Thought 상세 분석
   - 2024년 최신 프롬프팅 기법
   - RAG 시스템에 특화된 프롬프트 설계

6. **성능 예측 근거**
   - 각 최적화가 RAGAS 지표에 미치는 영향 분석
   - 유사 사례 연구 (case studies)
   - 예상 개선율의 이론적 근거

7. **향후 연구 방향**
   - GraphRAG, CRAG 등 최신 RAG 아키텍처
   - Agentic RAG, Multi-hop reasoning
   - 2024-2025 RAG 연구 트렌드

**보고서 스타일**:
- 학술적이고 전문적인 톤
- 최신 논문(2023-2024) 및 arXiv 인용
- 각 기법의 수학적/이론적 배경 포함
- 실무 적용 가능한 인사이트 제공
- 한국어로 작성

**참고할 주요 논문**:
1. Self-RAG (arXiv:2310.11511, 2023)
2. Constitutional AI (arXiv:2212.08073, 2022)
3. Chain-of-Thought (arXiv:2201.11903, 2022)
4. Few-shot Learning (arXiv:2005.14165, 2020)
5. BGE-M3 (최신 임베딩 모델)
6. RAG 관련 최신 survey 논문 (2024)

최신 연구 동향을 반영하여, 이 프로젝트의 각 최적화 기법이 현재 RAG 연구의 best practice와 어떻게 부합하는지 분석해주세요.
