"""
LangGraph RAG ë…¸ë“œ í•¨ìˆ˜

ì´ ëª¨ë“ˆì€ LangGraph ì›Œí¬í”Œë¡œìš°ì˜ ê° ë…¸ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
ê° ë…¸ë“œëŠ” RAGStateë¥¼ ìž…ë ¥ë°›ì•„ ìˆ˜ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

ì£¼ìš” ë…¸ë“œ:
- query_router: ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…
- hybrid_retrieve: Hybrid Search (Dense + Sparse + RRF)
- rerank_stage1: 1ì°¨ Reranking (BGE-reranker-v2-m3)
- rerank_stage2: 2ì°¨ Reranking (BGE-reranker-large)
- grade_documents: ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
- transform_query: ì¿¼ë¦¬ ìž¬ìž‘ì„±
- generate: ë‹µë³€ ìƒì„±
- hallucination_check: í™˜ê° ê²€ì¦
- answer_grading: ë‹µë³€ í’ˆì§ˆ í‰ê°€
- web_search: ì›¹ ê²€ìƒ‰ fallback
- load_user_context: ì‚¬ìš©ìž ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ê°œì¸í™”)
- personalize_response: ë‹µë³€ ê°œì¸í™”
- suggest_related_questions: ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ
"""

import asyncio
import logging
import re
import time
from typing import Dict, List, Tuple

import chromadb
from FlagEmbedding import BGEM3FlagModel, FlagReranker
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from openai import AsyncOpenAI, OpenAI

from .config import get_config
from .state import (
    RAGState,
    add_to_history,
    IntentClassification,
    IntentType,
    DocumentRelevance,
    RelevanceType,
    RewrittenQuery,
    QueryRewriteAction,
    HallucinationGrade,
    HallucinationType,
    UsefulnessGrade,
    UsefulnessType,
)
from .tools import get_web_search_tool

logger = logging.getLogger(__name__)


# ========== ì „ì—­ ë¦¬ì†ŒìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´) ==========

class RAGResources:
    """
    RAG ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ (ì‹±ê¸€í†¤)

    LangGraph ë…¸ë“œë“¤ì´ ê³µìœ í•˜ëŠ” ë¦¬ì†ŒìŠ¤:
    - ìž„ë² ë”© ëª¨ë¸
    - Reranker ëª¨ë¸ë“¤
    - ChromaDB ì»¬ë ‰ì…˜
    - LLM í´ë¼ì´ì–¸íŠ¸
    """

    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        """ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)"""
        if self._initialized:
            return

        logger.info("Initializing RAG resources...")
        config = get_config()

        # ìž„ë² ë”© ëª¨ë¸
        logger.info(f"Loading embedding model: {config.embedding_model}")
        self.embedding_model = BGEM3FlagModel(
            config.embedding_model,
            use_fp16=True,
            device=config.embedding_device,
        )
        self.embedding_batch_size = config.embedding_batch_size

        # Reranker Stage 1
        logger.info(f"Loading reranker stage 1: {config.reranker_stage1_model}")
        self.reranker_stage1 = FlagReranker(
            config.reranker_stage1_model,
            use_fp16=True,
            device=config.reranker_stage1_device,
        )

        # Reranker Stage 2
        logger.info(f"Loading reranker stage 2: {config.reranker_stage2_model}")
        self.reranker_stage2 = FlagReranker(
            config.reranker_stage2_model,
            use_fp16=True,
            device=config.reranker_stage2_device,
        )

        # ChromaDB
        logger.info(f"Connecting to ChromaDB at {config.chroma_db_path}")
        client = chromadb.PersistentClient(path=str(config.chroma_db_path))
        self.collection = client.get_collection("rag_chunks")
        logger.info(f"Collection loaded: {self.collection.count()} documents")

        # LLM í´ë¼ì´ì–¸íŠ¸ (ë™ê¸°/ë¹„ë™ê¸°)
        self.llm_client = OpenAI()
        self.async_llm_client = AsyncOpenAI()

        # LangChain LLM í´ë¼ì´ì–¸íŠ¸ (structured outputìš©)
        self.langchain_llm = ChatOpenAI(
            model=config.llm_model,
            temperature=config.llm_temperature,
        )
        self.langchain_llm_fast = ChatOpenAI(
            model=config.context_quality_model,
            temperature=0,
        )

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt_path = config.artifacts_dir.parent / config.config["llm"]["system_prompt_path"]
        self.system_prompt = (
            system_prompt_path.read_text(encoding="utf-8")
            if system_prompt_path.exists()
            else ""
        )

        # ì›¹ ê²€ìƒ‰ ë„êµ¬
        self.web_search_tool = get_web_search_tool()

        self._initialized = True
        logger.info("âœ“ RAG resources initialized")


def get_resources() -> RAGResources:
    """ì „ì—­ RAG ë¦¬ì†ŒìŠ¤ ë°˜í™˜"""
    return RAGResources()

# ========== ë…¸ë“œ 0: Intent Classifier ==========


def intent_classifier_node(state: RAGState) -> RAGState:
    """
    ì§ˆë¬¸ ì˜ë„ë¥¼ ë¶„ë¥˜í•´ in_scopeê°€ ì•„ë‹ˆë©´ ì´ˆê¸°ì— ì¢…ë£Œì‹œí‚¨ë‹¤.

    Categories:
    - IN_SCOPE: ê°œë°œ/í”„ë¡œê·¸ëž˜ë°/í•™ìŠµ ê´€ë ¨
    - GREETING: ì¸ì‚¬/ê°ì‚¬ ë“±
    - CHITCHAT: ìž¡ë‹´/ìš”ì²­(ì•„ì´ìŠ¤í¬ë¦¼ ì‚¬ì¤˜ ë“±)
    - NONSENSICAL: ë¬´ì˜ë¯¸/ìŠ¤íŒ¸
    """
    logger.info("[Intent] ì§ˆë¬¸ ì˜ë„ ë¶„ë¥˜ ì‹œìž‘")
    resources = get_resources()
    config = get_config()

    question = state["question"]
    intent = "unknown"

    prompt = f"""ë‹¤ìŒ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì´ ê°œë°œ/í”„ë¡œê·¸ëž˜ë°/í•™ìŠµ ê´€ë ¨ì¸ì§€ ë¶„ë¥˜í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ì•„ëž˜ ì¤‘ í•˜ë‚˜ì˜ ë¼ë²¨ë§Œ ë‹µë³€:
- IN_SCOPE: ê°œë°œ, í”„ë¡œê·¸ëž˜ë°, ì†Œí”„íŠ¸ì›¨ì–´ í•™ìŠµ/ë””ë²„ê¹…/ë„êµ¬ ì‚¬ìš©
- GREETING: ì¸ì‚¬, ê°ì‚¬, ì•ˆë¶€
- CHITCHAT: ìž¡ë‹´/ì‚¬ì ìš”ì²­ (ì˜ˆ: ì•„ì´ìŠ¤í¬ë¦¼ ì‚¬ì¤˜, ë…¸ëž˜ ì¶”ì²œ)
- NONSENSICAL: ë¬´ì˜ë¯¸/ìŠ¤íŒ¸/ì˜ë¯¸ ì—†ëŠ” ìž…ë ¥

ì§ˆë¬¸: {question}

ì •ë‹µ ë¼ë²¨ í•œ ë‹¨ì–´ë§Œ:"""

    try:
        response = resources.llm_client.chat.completions.create(
            model=config.context_quality_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=4,
        )
        label = response.choices[0].message.content.strip().upper()
        if "IN_SCOPE" in label:
            intent = "in_scope"
        elif "GREETING" in label:
            intent = "greeting"
        elif "CHITCHAT" in label:
            intent = "chitchat"
        elif "NON" in label:
            intent = "nonsensical"
        else:
            intent = "unknown"
    except Exception as e:
        logger.warning(f"[Intent] ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ in_scopeë¡œ ì²˜ë¦¬")
        intent = "in_scope"

    state["intent"] = intent

    # in_scopeê°€ ì•„ë‹ˆë©´ ë°”ë¡œ ì§§ì€ ë©”ì‹œì§€ í›„ ì¢…ë£Œ
    if intent != "in_scope":
        reply_map = {
            "greeting": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ê°œë°œÂ·í•™ìŠµ ë„ìš°ë¯¸ì˜ˆìš”. ê¶ê¸ˆí•œ ê°œë°œ/í”„ë¡œê·¸ëž˜ë° ì§ˆë¬¸ì„ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”.",
            "chitchat": "ì €ëŠ” ê°œë°œÂ·í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ì— ì§‘ì¤‘í•˜ê³  ìžˆì–´ìš”. ì½”ë“œë‚˜ ì—ëŸ¬, í•™ìŠµ ì£¼ì œë¥¼ ë§ì”€í•´ ì£¼ì„¸ìš”!",
            "nonsensical": "ì§€ê¸ˆ ìž…ë ¥ìœ¼ë¡œëŠ” ë„ì›€ì„ ë“œë¦¬ê¸° ì–´ë ¤ì›Œìš”. ê°œë°œ/í”„ë¡œê·¸ëž˜ë° ê´€ë ¨ ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”.",
        }
        state["generation"] = reply_map.get(
            intent,
            "ê°œë°œÂ·í•™ìŠµ ê´€ë ¨ ì§ˆë¬¸ì„ ì•Œë ¤ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ê²Œìš”.",
        )

    return add_to_history(state, "intent_classifier")


# ========== ë…¸ë“œ 1: Query Router ==========

def query_router_node(state: RAGState) -> RAGState:
    """
    ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ… ê²°ì •

    Args:
        state (RAGState): í˜„ìž¬ ìƒíƒœ

    Returns:
        RAGState: ë¼ìš°íŒ… ê²°ì •ì´ ì¶”ê°€ëœ ìƒíƒœ

    ë¼ìš°íŒ… ì „ëžµ:
    - "vectorstore": ë²¡í„° ê²€ìƒ‰ (ê¸°ë³¸)
    - "websearch": ì›¹ ê²€ìƒ‰ (ìµœì‹  ì •ë³´ í•„ìš”)
    - "direct": LLMë§Œ ì‚¬ìš© (ê²€ìƒ‰ ë¶ˆí•„ìš”)

    í˜„ìž¬ êµ¬í˜„: ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¼ìš°íŒ…
    í–¥í›„ ê°œì„ : LLM ê¸°ë°˜ ë¶„ë¥˜
    """
    logger.info(f"[QueryRouter] ì§ˆë¬¸ ë¶„ì„: {state['question'][:100]}")

    question = state["question"].lower()

    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¼ìš°íŒ…
    # TODO: LLM ê¸°ë°˜ ë¶„ë¥˜ë¡œ ê°œì„ 
    if any(
        keyword in question
        for keyword in ["ìµœê·¼", "í˜„ìž¬", "2024", "2025", "ë‰´ìŠ¤", "íŠ¸ë Œë“œ"]
    ):
        route = "websearch"
        logger.info("[QueryRouter] â†’ ì›¹ ê²€ìƒ‰ (ìµœì‹  ì •ë³´)")
    elif any(
        keyword in question
        for keyword in ["ì•ˆë…•", "hello", "hi", "ê°ì‚¬", "ê³ ë§ˆì›Œ"]
    ):
        route = "direct"
        logger.info("[QueryRouter] â†’ ì§ì ‘ ë‹µë³€ (ê²€ìƒ‰ ë¶ˆí•„ìš”)")
    else:
        route = "vectorstore"
        logger.info("[QueryRouter] â†’ ë²¡í„° ê²€ìƒ‰ (ê¸°ë³¸)")

    state["route"] = route
    return add_to_history(state, "query_router")


# ========== ë…¸ë“œ 2: Hybrid Retrieve ==========

def hybrid_retrieve_node(state: RAGState) -> RAGState:
    """
    Hybrid Search (Dense + Sparse + RRF Fusion)

    Args:
        state (RAGState): í˜„ìž¬ ìƒíƒœ

    Returns:
        RAGState: ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ

    ê²€ìƒ‰ ë‹¨ê³„:
    1. ì¿¼ë¦¬ ì¸ì½”ë”© (Dense + Sparse)
    2. Dense ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜)
    3. Sparse ê²€ìƒ‰ (í‚¤ì›Œë“œ ê¸°ë°˜)
    4. RRF Fusion (ë‘ ê²°ê³¼ ê²°í•©)
    """
    logger.info("[HybridRetrieve] ê²€ìƒ‰ ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]

    # Step 1: ì¿¼ë¦¬ ì¸ì½”ë”©
    query_encoding = resources.embedding_model.encode(
        [question],
        batch_size=1,
        max_length=1024,
        return_dense=True,
        return_sparse=True,
        return_colbert_vecs=False,
    )
    query_dense = query_encoding["dense_vecs"][0].tolist()
    query_sparse = query_encoding["lexical_weights"][0]

    # Step 2: Dense ê²€ìƒ‰
    dense_top_k = config.hybrid_dense_top_k
    dense_results = resources.collection.query(
        query_embeddings=[query_dense],
        n_results=dense_top_k,
        include=["documents", "metadatas"],
    )
    dense_docs = dense_results["documents"][0]
    dense_ids = dense_results["ids"][0]
    dense_metas = dense_results["metadatas"][0]

    if not dense_docs:
        logger.warning("[HybridRetrieve] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        state["documents"] = []
        state["metadatas"] = []
        return add_to_history(state, "hybrid_retrieve")

    # Step 3: Sparse ê²€ìƒ‰
    sparse_top_k = config.hybrid_sparse_top_k
    dense_scored = [
        (dense_ids[i], dense_docs[i], 1.0 / (i + 1)) for i in range(len(dense_docs))
    ]
    sparse_scored = _sparse_search(
        resources, query_sparse, dense_docs, dense_ids, top_k=sparse_top_k
    )

    # Step 4: RRF Fusion
    rrf_k = config.config["retrieval"]["rrf_k"]
    fused_docs = _reciprocal_rank_fusion(dense_scored, sparse_scored, k=rrf_k)

    # ë©”íƒ€ë°ì´í„° ë§¤í•‘ (O(1) ì¡°íšŒ)
    doc_to_meta = {}
    for i, doc in enumerate(dense_docs):
        meta = dense_metas[i].copy()
        meta["chunk_id"] = dense_ids[i]
        doc_to_meta[doc] = meta

    fused_metadatas = [
        doc_to_meta.get(doc, {"domain": "unknown", "chunk_id": "unknown"})
        for doc in fused_docs
    ]

    elapsed = time.time() - start_time
    logger.info(
        f"[HybridRetrieve] {len(fused_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ ({elapsed:.2f}s)"
    )

    state["documents"] = fused_docs
    state["metadatas"] = fused_metadatas
    return add_to_history(state, "hybrid_retrieve")


def _sparse_search(
    resources: RAGResources,
    query_sparse_vector: Dict,
    documents: List[str],
    doc_ids: List[str],
    top_k: int = 50,
) -> List[Tuple[str, str, float]]:
    """
    Sparse ê²€ìƒ‰ (BGE-M3 lexical weights ì‚¬ìš©)

    Args:
        resources: RAG ë¦¬ì†ŒìŠ¤
        query_sparse_vector: ì¿¼ë¦¬ sparse vector
        documents: í›„ë³´ ë¬¸ì„œë“¤
        doc_ids: ë¬¸ì„œ IDë“¤
        top_k: ìƒìœ„ kê°œ ë°˜í™˜

    Returns:
        List[Tuple[str, str, float]]: (doc_id, doc_text, score)
    """
    # ë¬¸ì„œ ì¸ì½”ë”© (sparseë§Œ)
    doc_encodings = resources.embedding_model.encode(
        documents,
        batch_size=resources.embedding_batch_size,
        max_length=1024,
        return_dense=False,
        return_sparse=True,
        return_colbert_vecs=False,
    )

    # Sparse score ê³„ì‚° (inner product)
    scores = []
    for i, doc_sparse in enumerate(doc_encodings["lexical_weights"]):
        score = 0.0
        for term, query_weight in query_sparse_vector.items():
            if term in doc_sparse:
                score += query_weight * doc_sparse[term]
        scores.append((doc_ids[i], documents[i], score))

    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    scores.sort(key=lambda x: x[2], reverse=True)
    return scores[:top_k]


def _reciprocal_rank_fusion(
    dense_results: List[Tuple],
    sparse_results: List[Tuple],
    k: int = 60,
) -> List[str]:
    """
    Reciprocal Rank Fusion

    Args:
        dense_results: Dense ê²€ìƒ‰ ê²°ê³¼ [(doc_id, doc_text, score), ...]
        sparse_results: Sparse ê²€ìƒ‰ ê²°ê³¼ [(doc_id, doc_text, score), ...]
        k: RRF ìƒìˆ˜ (ê¸°ë³¸: 60)

    Returns:
        List[str]: Fusionëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

    RRF ê³µì‹:
        score(d) = Î£ 1 / (k + rank_i(d))
    """
    rrf_scores = {}
    doc_texts = {}

    # Dense ìˆœìœ„ ì¶”ê°€
    for rank, (doc_id, doc_text, _) in enumerate(dense_results):
        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1.0 / (k + rank + 1)
        doc_texts[doc_id] = doc_text

    # Sparse ìˆœìœ„ ì¶”ê°€
    for rank, (doc_id, doc_text, _) in enumerate(sparse_results):
        rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + 1.0 / (k + rank + 1)
        doc_texts[doc_id] = doc_text

    # RRF ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
    fused_docs = [doc_texts[doc_id] for doc_id in sorted_ids]

    logger.debug(
        f"[RRF] Dense {len(dense_results)} + Sparse {len(sparse_results)} "
        f"â†’ {len(fused_docs)} unique docs"
    )
    return fused_docs


# ========== ë…¸ë“œ 3: Rerank Stage 1 ==========

def rerank_stage1_node(state: RAGState) -> RAGState:
    """
    1ì°¨ Reranking (BGE-reranker-v2-m3)

    Args:
        state (RAGState): í˜„ìž¬ ìƒíƒœ

    Returns:
        RAGState: 1ì°¨ reranking ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ

    ì „ëžµ:
    - Hybrid search ê²°ê³¼ ì¤‘ ìƒìœ„ 25ê°œë¥¼ reranking
    - ë¹ ë¥¸ ëª¨ë¸ë¡œ ì´ˆê¸° í•„í„°ë§
    """
    logger.info("[Rerank Stage 1] ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]
    documents = state["documents"]
    metadatas = state["metadatas"]

    if not documents:
        logger.warning("[Rerank Stage 1] ë¬¸ì„œ ì—†ìŒ")
        state["reranked_documents"] = []
        state["reranked_metadatas"] = []
        return add_to_history(state, "rerank_stage1")

    # ìƒìœ„ 25ê°œë§Œ reranking (ì„±ëŠ¥ ìµœì í™”)
    rerank_input_k = min(25, len(documents))
    docs_to_rerank = documents[:rerank_input_k]
    metas_to_rerank = metadatas[:rerank_input_k]

    # Reranking
    reranked_docs = _rerank(
        question, docs_to_rerank, resources.reranker_stage1, rerank_input_k
    )

    # ë©”íƒ€ë°ì´í„° ë§¤í•‘
    doc_to_meta = {doc: meta for doc, meta in zip(docs_to_rerank, metas_to_rerank)}
    reranked_metas = [
        doc_to_meta.get(doc, {"domain": "unknown"}) for doc in reranked_docs
    ]

    elapsed = time.time() - start_time
    logger.info(
        f"[Rerank Stage 1] {len(reranked_docs)}ê°œ ë¬¸ì„œ reranking ì™„ë£Œ ({elapsed:.2f}s)"
    )

    state["reranked_documents"] = reranked_docs
    state["reranked_metadatas"] = reranked_metas
    return add_to_history(state, "rerank_stage1")


# ========== ë…¸ë“œ 4: Rerank Stage 2 ==========

def rerank_stage2_node(state: RAGState) -> RAGState:
    """
    2ì°¨ Reranking (BGE-reranker-large)

    Args:
        state (RAGState): í˜„ìž¬ ìƒíƒœ

    Returns:
        RAGState: 2ì°¨ reranking ê²°ê³¼ê°€ ì¶”ê°€ëœ ìƒíƒœ

    ì „ëžµ:
    - 1ì°¨ reranking ê²°ê³¼ë¥¼ ë” ê°•ë ¥í•œ ëª¨ë¸ë¡œ ìž¬í‰ê°€
    - ìµœì¢… top_kê°œ ì„ íƒ (ê¸°ë³¸: 10ê°œ)
    """
    logger.info("[Rerank Stage 2] ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]
    documents = state["reranked_documents"]
    metadatas = state["reranked_metadatas"]

    if not documents:
        logger.warning("[Rerank Stage 2] ë¬¸ì„œ ì—†ìŒ")
        state["final_documents"] = []
        state["final_metadatas"] = []
        return add_to_history(state, "rerank_stage2")

    # ìµœì¢… top_k ì„ íƒ
    final_k = config.rerank_top_k
    reranked_docs = _rerank(question, documents, resources.reranker_stage2, final_k)

    # ë©”íƒ€ë°ì´í„° ë§¤í•‘
    doc_to_meta = {doc: meta for doc, meta in zip(documents, metadatas)}
    reranked_metas = [
        doc_to_meta.get(doc, {"domain": "unknown"}) for doc in reranked_docs
    ]

    elapsed = time.time() - start_time
    logger.info(
        f"[Rerank Stage 2] {len(reranked_docs)}ê°œ ìµœì¢… ë¬¸ì„œ ì„ íƒ ({elapsed:.2f}s)"
    )

    state["final_documents"] = reranked_docs
    state["final_metadatas"] = reranked_metas
    return add_to_history(state, "rerank_stage2")


def _rerank(
    query: str, documents: List[str], reranker: FlagReranker, top_k: int
) -> List[str]:
    """
    ë¬¸ì„œ Reranking

    Args:
        query: ì¿¼ë¦¬
        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        reranker: Reranker ëª¨ë¸
        top_k: ìƒìœ„ kê°œ ë°˜í™˜

    Returns:
        List[str]: Rerankingëœ ë¬¸ì„œë“¤
    """
    if not documents:
        return []

    pairs = [[query, doc] for doc in documents]
    scores = reranker.compute_score(pairs, normalize=True)

    # ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬
    if isinstance(scores, (int, float)):
        scores = [scores]

    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
    scored_docs = list(zip(documents, scores))
    scored_docs.sort(key=lambda x: x[1], reverse=True)

    return [doc for doc, _ in scored_docs[:top_k]]


# ========== ê³„ì† (nodes_part2.pyë¡œ ë¶„í• ) ==========
# ë‹¤ìŒ ë…¸ë“œë“¤:
# - grade_documents_node
# - transform_query_node
# - generate_node
# - hallucination_check_node
# - answer_grading_node
# - web_search_node
# nodes.py ê³„ì† - ë…¸ë“œ 5~10

# ========== ë…¸ë“œ 5: Grade Documents ==========

def grade_documents_node(state):
    """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ (Corrective RAG)"""
    logger.info("[GradeDocuments] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    question = state["question"]
    documents = state["final_documents"]

    if not documents:
        logger.warning("[GradeDocuments] ë¬¸ì„œ ì—†ìŒ")
        state["document_relevance"] = "not_relevant"
        return add_to_history(state, "grade_documents")

    # ë³‘ë ¬ í‰ê°€ (ë¹„ë™ê¸°)
    results = asyncio.run(
        _evaluate_documents_async(resources.async_llm_client, question, documents)
    )

    # ê²°ê³¼ ì§‘ê³„
    relevant_count = sum(
        1 for label in results if "RELEVANT" in label or "PARTIAL" in label
    )
    relevance_ratio = relevant_count / len(results)

    if relevance_ratio >= 0.5:
        state["document_relevance"] = "relevant"
        logger.info(
            f"[GradeDocuments] ë¬¸ì„œ ê´€ë ¨ì„±: RELEVANT ({relevant_count}/{len(results)})"
        )
    else:
        state["document_relevance"] = "not_relevant"
        logger.info(
            f"[GradeDocuments] ë¬¸ì„œ ê´€ë ¨ì„±: NOT RELEVANT ({relevant_count}/{len(results)})"
        )
        state["web_search_needed"] = True

    elapsed = time.time() - start_time
    logger.info(f"[GradeDocuments] í‰ê°€ ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "grade_documents")


async def _evaluate_documents_async(async_client, question, documents):
    """ë¬¸ì„œ ê´€ë ¨ì„± ë³‘ë ¬ í‰ê°€ (ë¹„ë™ê¸°)"""
    config = get_config()

    tasks = [
        _evaluate_single_document(
            async_client, question, doc, config.context_quality_model
        )
        for doc in documents
    ]

    results = await asyncio.gather(*tasks)
    return results


async def _evaluate_single_document(async_client, question, document, model):
    """ë‹¨ì¼ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
    doc_preview = document[:800] if len(document) > 800 else document

    prompt = f"""ì§ˆë¬¸: {question}

ë¬¸ì„œ: {doc_preview}

ì´ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆê¹Œ?

- RELEVANT: ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•  ìˆ˜ ìžˆëŠ” ì •ë³´ í¬í•¨
- PARTIAL: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ ì¼ë¶€ í¬í•¨
- IRRELEVANT: ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ìŒ

ë‹¨ì–´ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš” (RELEVANT, PARTIAL, IRRELEVANT):"""

    try:
        response = await async_client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10,
        )
        label = response.choices[0].message.content.strip().upper()
        return label
    except Exception as e:
        logger.warning(f"ë¬¸ì„œ í‰ê°€ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ PARTIAL ì‚¬ìš©")
        return "PARTIAL"


# ========== ë…¸ë“œ 6: Transform Query ==========

def transform_query_node(state):
    """ì¿¼ë¦¬ ìž¬ìž‘ì„± (Query Transformation)"""
    logger.info("[TransformQuery] ì¿¼ë¦¬ ìž¬ìž‘ì„± ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]

    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ì´ê³  ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ ìž¬ìž‘ì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ìž¬ìž‘ì„± ì§€ì¹¨:
- í•µì‹¬ í‚¤ì›Œë“œ ê°•ì¡°
- êµ¬ì²´ì ì¸ ìš©ì–´ ì‚¬ìš©
- ê²€ìƒ‰ì— ë„ì›€ì´ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€

ìž¬ìž‘ì„±ëœ ì§ˆë¬¸:"""

    try:
        response = resources.llm_client.chat.completions.create(
            model=config.context_quality_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=100,
        )
        transformed = response.choices[0].message.content.strip()
        logger.info(f"[TransformQuery] ì›ë³¸: {question}")
        logger.info(f"[TransformQuery] ìž¬ìž‘ì„±: {transformed}")

        state["transformed_query"] = transformed
        state["question"] = transformed  # ìž¬ìž‘ì„±ëœ ì¿¼ë¦¬ë¡œ êµì²´

    except Exception as e:
        logger.error(f"[TransformQuery] ì‹¤íŒ¨: {e}, ì›ë³¸ ì¿¼ë¦¬ ìœ ì§€")
        state["transformed_query"] = question

    elapsed = time.time() - start_time
    logger.info(f"[TransformQuery] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "transform_query")


# ========== ë…¸ë“œ 7: Generate ==========

def generate_node(state):
    """ë‹µë³€ ìƒì„± (LLM)"""
    logger.info("[Generate] ë‹µë³€ ìƒì„± ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]
    documents = state["final_documents"]
    metadatas = state["final_metadatas"]

    if not documents:
        logger.warning("[Generate] ë¬¸ì„œ ì—†ìŒ")
        state["generation"] = "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•´ë³´ì‹œê² ì–´ìš”?"
        return add_to_history(state, "generate")

    # ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    context_block = "\n\n".join(
        f"[ë¬¸ì„œ {i+1}] {meta.get('domain', 'unknown')}\n{doc}"
        for i, (doc, meta) in enumerate(zip(documents, metadatas))
    )

    # LLM í˜¸ì¶œ
    messages = [
        {"role": "system", "content": resources.system_prompt},
        {
            "role": "user",
            "content": f"ì§ˆë¬¸: {question}\n\nì»¨í…ìŠ¤íŠ¸:\n{context_block}\n\nê·œì¹™: ë³¸ë¬¸ì— íˆ´/ì¶œì²˜ëª…(tavily, websearch ë“±)ì„ ë„£ì§€ ë§ê³ , ì¶œì²˜ëŠ” ë§ˆì§€ë§‰ 'ðŸ“š ì°¸ê³ ' ì„¹ì…˜ì—ë§Œ í‘œê¸°í•˜ì„¸ìš”.",
        },
    ]

    try:
        response = resources.llm_client.chat.completions.create(
            model=config.llm_model,
            messages=messages,
            temperature=config.llm_temperature,
            max_tokens=config.llm_max_tokens,
            top_p=config.config["llm"].get("top_p", 0.9),
        )
        answer_text = response.choices[0].message.content

        # ê¸°ì¡´ ì¶œì²˜ ì œê±° ë° íˆ´ëª… ì •ë¦¬
        answer_text = _clean_tool_mentions(_strip_existing_sources(answer_text))

        # URL ì¶œì²˜ ì¶”ê°€
        source_urls = []
        for meta in metadatas:
            url = meta.get("url", "unknown")
            if url != "unknown" and url not in source_urls:
                source_urls.append(url)

        if source_urls:
            sources_section = "\n\nðŸ“š ì°¸ê³ :\n" + "\n".join(
                f"- {url}" for url in source_urls
            )
            answer = answer_text + sources_section
        else:
            answer = answer_text

        state["generation"] = answer
        logger.info("[Generate] ë‹µë³€ ìƒì„± ì™„ë£Œ")

    except Exception as e:
        logger.error(f"[Generate] ì‹¤íŒ¨: {e}")
        state["generation"] = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    elapsed = time.time() - start_time
    logger.info(f"[Generate] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "generate")


def _strip_existing_sources(answer_text: str) -> str:
    """ê¸°ì¡´ ì¶œì²˜ ì„¹ì…˜ ì œê±°"""
    marker = "ðŸ“š ì°¸ê³ "
    if marker in answer_text:
        return answer_text.split(marker)[0].rstrip()
    return answer_text


def _clean_tool_mentions(answer_text: str) -> str:
    """
    ë³¸ë¬¸ì—ì„œ tavily/websearch ë“± íˆ´ ì´ë¦„ì„ ì œê±°í•´ ë‹µë³€ì„ ìžì—°ìŠ¤ëŸ½ê²Œ ë§Œë“ ë‹¤.
    """
    cleaned = answer_text
    for token in ["tavily", "websearch", "web search", "Tavily", "WebSearch"]:
        cleaned = re.sub(rf"\(?\b{re.escape(token)}\b\)?", "", cleaned, flags=re.IGNORECASE)
    # Collapse double spaces left by removals
    cleaned = re.sub(r"\s{2,}", " ", cleaned)
    return cleaned.strip()


# ========== ë…¸ë“œ 8: Hallucination Check ==========

def hallucination_check_node(state):
    """í™˜ê° ê²€ì¦ (Self-RAG)"""
    logger.info("[HallucinationCheck] í™˜ê° ê²€ì¦ ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    generation = state["generation"]
    documents = state["final_documents"]

    if not documents:
        logger.warning("[HallucinationCheck] ë¬¸ì„œ ì—†ìŒ, ê²€ì¦ ìŠ¤í‚µ")
        state["hallucination_grade"] = "not_sure"
        return add_to_history(state, "hallucination_check")

    # ì¶œì²˜ ì œê±°í•œ ë‹µë³€ë§Œ ê²€ì¦
    answer_only = _clean_tool_mentions(_strip_existing_sources(generation))

    # ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ truncate)
    context_preview = "\n\n".join(documents[:3])
    if len(context_preview) > 2000:
        context_preview = context_preview[:2000] + "..."

    prompt = f"""ë‹¤ìŒ ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹µë³€:
{answer_only}

ì œê³µëœ ë¬¸ì„œ:
{context_preview}

ë‹µë³€ì˜ ëª¨ë“  ì£¼ìž¥ì´ ë¬¸ì„œì—ì„œ í™•ì¸ë©ë‹ˆê¹Œ?

- SUPPORTED: ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì´ ë¬¸ì„œì— ê·¼ê±°í•¨
- NOT_SUPPORTED: ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì´ í¬í•¨ë¨
- NOT_SURE: íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€

ë‹¨ì–´ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš” (SUPPORTED, NOT_SUPPORTED, NOT_SURE):"""

    try:
        response = resources.llm_client.chat.completions.create(
            model=config.context_quality_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=20,
        )
        label = response.choices[0].message.content.strip().upper()

        if "SUPPORTED" in label and "NOT" not in label:
            state["hallucination_grade"] = "supported"
            logger.info("[HallucinationCheck] ê²°ê³¼: SUPPORTED (ë¬¸ì„œì— ê·¼ê±°í•¨)")
        elif "NOT_SUPPORTED" in label:
            state["hallucination_grade"] = "not_supported"
            logger.warning("[HallucinationCheck] ê²°ê³¼: NOT SUPPORTED (í™˜ê° ë°œê²¬)")
            state["web_search_needed"] = True
        else:
            state["hallucination_grade"] = "not_sure"
            logger.info("[HallucinationCheck] ê²°ê³¼: NOT SURE")

    except Exception as e:
        logger.error(f"[HallucinationCheck] ì‹¤íŒ¨: {e}")
        state["hallucination_grade"] = "not_sure"

    elapsed = time.time() - start_time
    logger.info(f"[HallucinationCheck] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "hallucination_check")


# ========== ë…¸ë“œ 9: Answer Grading ==========

def answer_grading_node(state):
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€ (Self-RAG)"""
    logger.info("[AnswerGrading] ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()
    config = get_config()

    question = state["question"]
    generation = state["generation"]

    # ì¶œì²˜ ì œê±°í•œ ë‹µë³€ë§Œ í‰ê°€
    answer_only = _clean_tool_mentions(_strip_existing_sources(generation))

    prompt = f"""ë‹¤ìŒ ë‹µë³€ì´ ì§ˆë¬¸ì— ìœ ìš©í•œì§€ íŒë‹¨í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€: {answer_only}

ì´ ë‹µë³€ì´ ì§ˆë¬¸ì— ì¶©ë¶„ížˆ ë‹µë³€í•©ë‹ˆê¹Œ?

- USEFUL: ì§ˆë¬¸ì— ì¶©ë¶„ížˆ ë‹µë³€í•¨
- NOT_USEFUL: ì§ˆë¬¸ì— ë‹µë³€í•˜ì§€ ëª»í•¨

ë‹¨ì–´ í•˜ë‚˜ë§Œ ë‹µë³€í•˜ì„¸ìš” (USEFUL, NOT_USEFUL):"""

    try:
        response = resources.llm_client.chat.completions.create(
            model=config.context_quality_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=20,
        )
        label = response.choices[0].message.content.strip().upper()

        if "USEFUL" in label and "NOT" not in label:
            state["answer_usefulness"] = "useful"
            logger.info("[AnswerGrading] ê²°ê³¼: USEFUL")
        else:
            state["answer_usefulness"] = "not_useful"
            logger.warning("[AnswerGrading] ê²°ê³¼: NOT USEFUL")
            state["web_search_needed"] = True

    except Exception as e:
        logger.error(f"[AnswerGrading] ì‹¤íŒ¨: {e}")
        state["answer_usefulness"] = "useful"  # ì‹¤íŒ¨ ì‹œ ê¸ì •ìœ¼ë¡œ ê°€ì •

    elapsed = time.time() - start_time
    logger.info(f"[AnswerGrading] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "answer_grading")


# ========== ë…¸ë“œ 10: Web Search ==========

def web_search_node(state):
    """ì›¹ ê²€ìƒ‰ fallback (Corrective RAG)"""
    logger.info("[WebSearch] ì›¹ ê²€ìƒ‰ ì‹œìž‘")
    start_time = time.time()

    web_search_tool = get_web_search_tool()
    question = state["question"]

    if not web_search_tool.enabled:
        logger.warning("[WebSearch] ì›¹ ê²€ìƒ‰ ë¹„í™œì„±í™”ë¨")
        state["final_documents"] = []
        state["final_metadatas"] = []
        return add_to_history(state, "web_search")

    # ì›¹ ê²€ìƒ‰ ì‹¤í–‰
    documents, metadatas = web_search_tool.search_with_metadata(question)

    state["final_documents"] = documents
    state["final_metadatas"] = metadatas

    elapsed = time.time() - start_time
    logger.info(f"[WebSearch] {len(documents)}ê°œ ê²°ê³¼ ê²€ìƒ‰ ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "web_search")


# ========== ê°œì¸í™” ë° ì§ˆë¬¸ ì¶”ì²œ ë…¸ë“œ ==========

# ========== ë…¸ë“œ 11: Load User Context (ê°œì¸í™”) ==========

def load_user_context_node(state: RAGState) -> RAGState:
    """
    ì‚¬ìš©ìž ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ (ê°œì¸í™” - ê°„ì†Œí™” ë²„ì „)

    Djangoì—ì„œ ì „ë‹¬ë°›ì€ user_contextë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ìž¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ
    ì‚¬ìš©ìž í•™ìŠµ ëª©í‘œ ë° ê´€ì‹¬ì‚¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    Note: ë©˜í† ë‹˜ì˜ ì›ë³¸ê³¼ ë‹¤ë¥´ê²Œ DB ì¿¼ë¦¬ë¥¼ í•˜ì§€ ì•Šê³ ,
          Djangoì—ì„œ ì´ë¯¸ ì „ë‹¬ë°›ì€ user_contextë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        state (RAGState): í˜„ìž¬ ìƒíƒœ

    Returns:
        RAGState: ê°œì¸í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì¶”ê°€ëœ ìƒíƒœ
    """
    logger.info("[LoadUserContext] ì‚¬ìš©ìž ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì‹œìž‘")
    start_time = time.time()

    user_id = state.get("user_id", "")
    user_context = state.get("user_context", {})
    question = state["question"]

    if not user_id or not user_context:
        logger.info("[LoadUserContext] user_id ë˜ëŠ” user_context ì—†ìŒ, ê°œì¸í™” ìŠ¤í‚µ")
        return add_to_history(state, "load_user_context")

    try:
        # Djangoì—ì„œ ì „ë‹¬ë°›ì€ user_context ì‚¬ìš©
        # user_context êµ¬ì¡°: {
        #     "learning_goals": "Python ë§ˆìŠ¤í„°í•˜ê¸°, Django í•™ìŠµ",
        #     "interested_topics": "ì›¹ ê°œë°œ, API ì„¤ê³„, ë°ì´í„°ë² ì´ìŠ¤",
        # }
        learning_goals = user_context.get("learning_goals", "")
        interested_topics = user_context.get("interested_topics", "")

        if not learning_goals and not interested_topics:
            logger.info("[LoadUserContext] ì‚¬ìš©ìž í•™ìŠµ ë°ì´í„° ì—†ìŒ")
            return add_to_history(state, "load_user_context")

        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = _extract_keywords(question)

        # ê´€ë ¨ í•­ëª© ì°¾ê¸°
        related_items = []
        forgotten_items = []

        # learning_goals ë¶„ì„
        if learning_goals:
            goals_list = [g.strip() for g in learning_goals.split(",")]
            for goal in goals_list:
                if _is_related_to_question(goal, question_keywords, question):
                    related_items.append({
                        "type": "learning_goal",
                        "content": goal,
                    })
                    # ì§ˆë¬¸ì— ì§ì ‘ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ê²½ìš° ìƒê¸° í›„ë³´
                    if not _is_mentioned_in_question(goal, question):
                        forgotten_items.append({
                            "type": "learning_goal",
                            "content": goal,
                        })

        # interested_topics ë¶„ì„
        if interested_topics:
            topics_list = [t.strip() for t in interested_topics.split(",")]
            for topic in topics_list:
                if _is_related_to_question(topic, question_keywords, question):
                    related_items.append({
                        "type": "interested_topic",
                        "content": topic,
                    })
                    # ì§ˆë¬¸ì— ì§ì ‘ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ê²½ìš° ìƒê¸° í›„ë³´
                    if not _is_mentioned_in_question(topic, question):
                        forgotten_items.append({
                            "type": "interested_topic",
                            "content": topic,
                        })

        state["related_selections"] = related_items
        state["forgotten_candidates"] = forgotten_items

        logger.info(
            f"[LoadUserContext] ë¡œë“œ ì™„ë£Œ - "
            f"ê´€ë ¨: {len(related_items)}, "
            f"ìƒê¸° í›„ë³´: {len(forgotten_items)}"
        )

    except Exception as e:
        logger.error(f"[LoadUserContext] ì‹¤íŒ¨: {e}")
        state["related_selections"] = []
        state["forgotten_candidates"] = []

    elapsed = time.time() - start_time
    logger.info(f"[LoadUserContext] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "load_user_context")


def _extract_keywords(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ

    Args:
        text: ìž…ë ¥ í…ìŠ¤íŠ¸

    Returns:
        List[str]: ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡
    """
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê³µë°± ê¸°ì¤€ ë¶„ë¦¬ + ë¶ˆìš©ì–´ ì œê±°)
    stopwords = {"ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", "ì™€", "ê³¼", "í•˜ê³ ", "ìžˆ", "ì—†", "ìˆ˜", "ë”", "ë“±"}

    words = text.lower().replace("?", "").replace(".", "").split()
    keywords = [w for w in words if len(w) > 1 and w not in stopwords]

    return keywords


def _is_related_to_question(item: str, keywords: List[str], question: str) -> bool:
    """
    í•­ëª©ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìžˆëŠ”ì§€ íŒë‹¨

    Args:
        item: ì‚¬ìš©ìž í•™ìŠµ ëª©í‘œ ë˜ëŠ” ê´€ì‹¬ì‚¬
        keywords: ì§ˆë¬¸ í‚¤ì›Œë“œ ëª©ë¡
        question: ì›ë³¸ ì§ˆë¬¸

    Returns:
        bool: ê´€ë ¨ ì—¬ë¶€
    """
    item_lower = item.lower()
    question_lower = question.lower()

    # ì§ì ‘ ì–¸ê¸‰ëœ ê²½ìš°
    if item_lower in question_lower:
        return True

    # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ê´€ë ¨ ìžˆìŒ
    for keyword in keywords:
        if keyword in item_lower:
            return True

    return False


def _is_mentioned_in_question(item: str, question: str) -> bool:
    """
    í•­ëª©ì´ ì§ˆë¬¸ì— ì§ì ‘ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸

    Args:
        item: ì‚¬ìš©ìž í•™ìŠµ ëª©í‘œ ë˜ëŠ” ê´€ì‹¬ì‚¬
        question: ì›ë³¸ ì§ˆë¬¸

    Returns:
        bool: ì–¸ê¸‰ ì—¬ë¶€
    """
    item_lower = item.lower()
    question_lower = question.lower()

    return item_lower in question_lower


# ========== ë…¸ë“œ 12: Personalize Response (ê°œì¸í™”) ==========

def personalize_response_node(state: RAGState) -> RAGState:
    """
    ë‹µë³€ ê°œì¸í™” (ìƒê¸° ë©”ì‹œì§€ ì£¼ìž…)

    ìƒì„±ëœ ë‹µë³€ì— ì‚¬ìš©ìžê°€ ìžŠì—ˆì„ ìˆ˜ ìžˆëŠ” ê³¼ê±° í•™ìŠµ ëª©í‘œë‚˜ ê´€ì‹¬ì‚¬ë¥¼
    ìƒê¸°ì‹œí‚¤ëŠ” ë©”ì‹œì§€ë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        state (RAGState): í˜„ìž¬ ìƒíƒœ

    Returns:
        RAGState: ê°œì¸í™”ëœ ë‹µë³€ì´ í¬í•¨ëœ ìƒíƒœ
    """
    logger.info("[PersonalizeResponse] ë‹µë³€ ê°œì¸í™” ì‹œìž‘")
    start_time = time.time()

    generation = state["generation"]
    forgotten_candidates = state.get("forgotten_candidates", [])

    if not forgotten_candidates:
        logger.info("[PersonalizeResponse] ìƒê¸°í•  ë‚´ìš© ì—†ìŒ, ìŠ¤í‚µ")
        state["reminder_added"] = False
        return add_to_history(state, "personalize_response")

    try:
        # ìƒê¸° ë©”ì‹œì§€ ìƒì„± (ìµœëŒ€ 2ê°œ í•­ëª©ë§Œ)
        reminder_items = forgotten_candidates[:2]
        reminder_parts = []

        for item in reminder_items:
            item_type = item.get("type", "")
            content = item.get("content", "")

            if content:
                if item_type == "learning_goal":
                    reminder_parts.append(f"'{content}' í•™ìŠµ ëª©í‘œ")
                else:
                    reminder_parts.append(f"'{content}'")

        if reminder_parts:
            # ìžì—°ìŠ¤ëŸ¬ìš´ ìƒê¸° ë©”ì‹œì§€ êµ¬ì„±
            if len(reminder_parts) == 1:
                items_text = reminder_parts[0]
            else:
                items_text = f"{reminder_parts[0]}ê³¼(ì™€) {reminder_parts[1]}"

            reminder_message = (
                f"\n\nðŸ’¡ **ì°¸ê³ **: ì´ì „ì— ê´€ì‹¬ì„ ë³´ì´ì…¨ë˜ {items_text}ë„ "
                f"í•¨ê»˜ í™•ì¸í•´ë³´ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
            )

            # ì¶œì²˜ ì„¹ì…˜ ì•žì— ì‚½ìž…
            if "ðŸ“š ì°¸ê³ :" in generation:
                parts = generation.split("ðŸ“š ì°¸ê³ :")
                personalized_generation = parts[0].rstrip() + reminder_message + "\n\nðŸ“š ì°¸ê³ :" + parts[1]
            else:
                personalized_generation = generation + reminder_message

            state["generation"] = personalized_generation
            state["reminder_added"] = True

            logger.info(f"[PersonalizeResponse] ìƒê¸° ë©”ì‹œì§€ ì¶”ê°€: {items_text}")
        else:
            state["reminder_added"] = False

    except Exception as e:
        logger.error(f"[PersonalizeResponse] ì‹¤íŒ¨: {e}")
        state["reminder_added"] = False

    elapsed = time.time() - start_time
    logger.info(f"[PersonalizeResponse] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "personalize_response")


# ========== ë…¸ë“œ 13: Suggest Related Questions ==========

def suggest_related_questions_node(state: RAGState) -> RAGState:
    """
    ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ

    í˜„ìž¬ ì§ˆë¬¸ê³¼ ë‹µë³€, ê·¸ë¦¬ê³  ì‚¬ìš©ìž ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ
    ì‚¬ìš©ìžê°€ ë‹¤ìŒì— í•  ìˆ˜ ìžˆëŠ” ê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.

    Args:
        state (RAGState): í˜„ìž¬ ìƒíƒœ

    Returns:
        RAGState: ê´€ë ¨ ì§ˆë¬¸ ëª©ë¡ì´ í¬í•¨ëœ ìƒíƒœ
    """
    logger.info("[SuggestQuestions] ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ ì‹œìž‘")
    start_time = time.time()

    resources = get_resources()

    question = state["question"]
    generation = state["generation"]
    user_context = state.get("user_context", {})

    # ì‚¬ìš©ìž ì»¨í…ìŠ¤íŠ¸ ìš”ì•½
    context_summary = ""
    if user_context:
        learning_goals = user_context.get("learning_goals", "")
        interested_topics = user_context.get("interested_topics", "")
        if learning_goals:
            context_summary += f"í•™ìŠµ ëª©í‘œ: {learning_goals}\n"
        if interested_topics:
            context_summary += f"ê´€ì‹¬ ì£¼ì œ: {interested_topics}\n"

    # ë‹µë³€ì—ì„œ ì¶œì²˜ ì œê±°
    answer_only = _strip_existing_sources(generation)
    if len(answer_only) > 500:
        answer_only = answer_only[:500] + "..."

    system_prompt = """ë‹¹ì‹ ì€ í•™ìŠµ ë„ìš°ë¯¸ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ í˜„ìž¬ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ë³´ê³ ,
ìžì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆ ìˆ˜ ìžˆëŠ” ê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ì¶”ì²œí•˜ì„¸ìš”.

ì¶”ì²œ ì§ˆë¬¸ì€:
- í˜„ìž¬ ì§ˆë¬¸ì—ì„œ ìžì—°ìŠ¤ëŸ½ê²Œ íŒŒìƒë˜ëŠ” ë‚´ìš©
- ë” ê¹Šì´ ìžˆëŠ” ì´í•´ë¥¼ ë•ëŠ” ë‚´ìš©
- ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ë‚´ìš©
- ì‚¬ìš©ìžì˜ í•™ìŠµ ëª©í‘œ/ê´€ì‹¬ì‚¬ì™€ ê´€ë ¨ëœ ë‚´ìš©

ê° ì§ˆë¬¸ì€ í•œ ì¤„ë¡œ ìž‘ì„±í•˜ê³ , ë²ˆí˜¸ë‚˜ ë¶ˆë¦¿ ì—†ì´ ì¤„ë°”ê¿ˆìœ¼ë¡œë§Œ êµ¬ë¶„í•˜ì„¸ìš”."""

    user_prompt = f"""í˜„ìž¬ ì§ˆë¬¸: {question}

ë‹µë³€ ìš”ì•½: {answer_only}"""

    if context_summary:
        user_prompt += f"\n\nì‚¬ìš©ìž ì •ë³´:\n{context_summary}"

    user_prompt += "\n\nê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš” (ê° ì§ˆë¬¸ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„):"

    try:
        response = resources.llm_client.chat.completions.create(
            model=resources.langchain_llm_fast.model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.7,
            max_tokens=200,
        )

        suggestions_text = response.choices[0].message.content.strip()

        # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
        suggestions = [
            line.strip().lstrip("0123456789.-) ").strip()
            for line in suggestions_text.split("\n")
            if line.strip() and len(line.strip()) > 10
        ]

        # ìµœëŒ€ 3ê°œë§Œ
        suggestions = suggestions[:3]

        state["related_questions"] = suggestions
        logger.info(f"[SuggestQuestions] {len(suggestions)}ê°œ ì§ˆë¬¸ ì¶”ì²œ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"[SuggestQuestions] ì‹¤íŒ¨: {e}")
        state["related_questions"] = []

    elapsed = time.time() - start_time
    logger.info(f"[SuggestQuestions] ì™„ë£Œ ({elapsed:.2f}s)")

    return add_to_history(state, "suggest_related_questions")
