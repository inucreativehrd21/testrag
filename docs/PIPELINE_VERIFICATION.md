# RAG íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë¦¬í¬íŠ¸ (í™•ì¥ í¬ë¡¤ë§ 1,001ê°œ ëŒ€ì‘)

## âœ… ê²€ì¦ ì™„ë£Œ í•­ëª©

### 1. **data_prep.py** - URL ë©”íƒ€ë°ì´í„° í¬í•¨ âœ“

**í™•ì¸ ì‚¬í•­:**
- âœ… `_load_documents_with_metadata()` ë©”ì„œë“œê°€ pages.jsonì—ì„œ URLì„ ì½ìŒ
- âœ… ê° ì²­í¬ì— URLì„ í¬í•¨í•˜ì—¬ ì €ì¥ (78ë²ˆ ì¤„)
- âœ… chunks.parquetì— 'url' ì»¬ëŸ¼ ì €ì¥

**ì½”ë“œ:**
```python
chunk_rows.append({
    "domain": domain,
    "chunk_id": f"{domain}_{len(chunk_rows)}",
    "text": chunk,
    "length": len(chunk),
    "url": chunk_url  # â† URL ì¶”ê°€!
})
```

**ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬:**
- ë©”ëª¨ë¦¬ ì‚¬ìš©: 1,001ê°œ ë¬¸ì„œ â†’ ì˜ˆìƒ 7,000-10,000 ì²­í¬ â†’ ì•½ 500MB-1GB RAM
- ì²˜ë¦¬ ì‹œê°„: 2-3ë¶„ ì˜ˆìƒ
- **ë¬¸ì œ ì—†ìŒ** âœ“

---

### 2. **index_builder.py** - URL ë©”íƒ€ë°ì´í„° ChromaDB ì €ì¥ âœ“

**í™•ì¸ ì‚¬í•­:**
- âœ… chunks.parquetì—ì„œ 'url' ì»¬ëŸ¼ ì½ìŒ
- âœ… ChromaDB metadataì— URL í¬í•¨ (90-93ë²ˆ ì¤„)

**ì½”ë“œ:**
```python
metadata_columns = ["domain", "length"]
if "url" in batch.columns:
    metadata_columns.append("url")  # â† URL ì¶”ê°€
metadatas = batch[metadata_columns].to_dict(orient="records")
```

**ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬:**
- ë°°ì¹˜ í¬ê¸°: 512 ì²­í¬/ë°°ì¹˜ (72ë²ˆ ì¤„)
- ì˜ˆìƒ ë°°ì¹˜ ìˆ˜: 7,000 ì²­í¬ Ã· 512 = ~14 ë°°ì¹˜
- GPU ì¸ë±ì‹± ì‹œê°„: **5-7ë¶„** ì˜ˆìƒ
- CPU ì¸ë±ì‹± ì‹œê°„: 15-20ë¶„ ì˜ˆìƒ
- **ë¬¸ì œ ì—†ìŒ** âœ“

---

### 3. **answerer_v2_optimized.py** - URL ì¶œì²˜ í‘œì‹œ âœ“

**í™•ì¸ ì‚¬í•­:**
- âœ… retrieve() ë©”ì„œë“œê°€ metadatas ë°˜í™˜
- âœ… answer() ë©”ì„œë“œê°€ URL ì¶œì²˜ ì¶”ê°€ (397-408ë²ˆ ì¤„)
- âœ… ì¤‘ë³µ URL ì œê±°

**ì½”ë“œ:**
```python
# Add source URLs at the end
source_urls = []
for meta in metadatas:
    url = meta.get('url', 'unknown')
    if url != 'unknown' and url not in source_urls:
        source_urls.append(url)

if source_urls:
    sources_section = "\n\nğŸ“š ì°¸ê³ :\n" + "\n".join(f"- {url}" for url in source_urls)
    answer = answer_text + sources_section
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ë‹µë³€ ë‚´ìš©...

ğŸ“š ì°¸ê³ :
- https://www.atlassian.com/git/tutorials/rewriting-history/git-rebase
- https://git-scm.com/book/ko/v2/Git-ë¸Œëœì¹˜-Rebase-í•˜ê¸°
- https://docs.gitlab.com/ee/topics/git/git_rebase.html
```

**ë¬¸ì œ ì—†ìŒ** âœ“

---

### 4. **config/enhanced.yaml** - ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„¤ì • âœ“

**í™•ì¸ ì‚¬í•­:**
- âœ… chunk_size: 900 (ë°ì´í„° ê¸°ë°˜ ìµœì í™”)
- âœ… batch_size: 32 (ì ì ˆí•œ í¬ê¸°)
- âœ… hybrid search top_k: 50+50 (ì¶©ë¶„í•œ í›„ë³´)
- âœ… rerank_top_k: 10 (ìµœì¢… ì»¨í…ìŠ¤íŠ¸)

**ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬:**
- 7,000-10,000 ì²­í¬ ì¸ë±ì‹±: **ë¬¸ì œ ì—†ìŒ**
- ê²€ìƒ‰ ì†ë„: 50+50 í›„ë³´ ê²€ìƒ‰ â†’ 10ê°œ rerank â†’ ë§¤ìš° ë¹ ë¦„
- **ë¬¸ì œ ì—†ìŒ** âœ“

---

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° ê°œì„  ì œì•ˆ

### 1. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**

**í˜„ì¬ ìƒíƒœ:**
- data_prep: ~500MB-1GB RAM
- index_builder: ~2-4GB RAM (GPU), ~4-8GB RAM (CPU)
- ChromaDB í¬ê¸°: ~3-4GB ë””ìŠ¤í¬

**ê¶Œì¥ì‚¬í•­:**
- ìµœì†Œ 16GB RAM ê¶Œì¥
- GPU ì‚¬ìš© ì‹œ ìµœì†Œ 8GB VRAM ê¶Œì¥

---

### 2. **index_builder.py ë°°ì¹˜ í¬ê¸°**

**í˜„ì¬ ì„¤ì •:**
```python
batch_size = 512  # ChromaDB ë°°ì¹˜ í¬ê¸° (72ë²ˆ ì¤„)
```

**ë¬¸ì œì :**
- 7,000-10,000 ì²­í¬ ì¸ë±ì‹± ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±

**ê°œì„  ì œì•ˆ:**
ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”:

```python
# index_builder.py 72ë²ˆ ì¤„
batch_size = 256  # 512 â†’ 256ìœ¼ë¡œ ì¤„ì„ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)
```

---

### 3. **í¬ë¡¤ëŸ¬ content_extractor ê²€ì¦ í•„ìš”**

**í™•ì¸ í•„ìš”:**
í™•ì¥ í¬ë¡¤ëŸ¬ê°€ ì‹¤ì œë¡œ URLì„ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œí•˜ëŠ”ì§€ ê²€ì¦ í•„ìš”

**ê²€ì¦ ë°©ë²•:**
```bash
# 1. í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ (ì†Œê·œëª¨)
cd c:\develop1\test\crawler
python -c "
from scrapers.git_scraper_extended import GitDocsScraperExtended
scraper = GitDocsScraperExtended()
# í…ŒìŠ¤íŠ¸: atlassianë§Œ í¬ë¡¤ë§
from config.settings_extended import TARGET_URLS
config = TARGET_URLS['git']['atlassian']
docs = scraper._scrape_source('atlassian', config)
print(f'ìˆ˜ì§‘: {len(docs)}ê°œ')
print(f'URL ìƒ˜í”Œ: {docs[0].get(\"url\", \"NO URL\")}')
"

# 2. URL íƒœê¹… í™•ì¸
python -c "
import json
with open('data/raw/git/pages.json') as f:
    docs = json.load(f)
urls_found = sum(1 for d in docs if d.get('url', 'unknown') != 'unknown')
print(f'ì´ ë¬¸ì„œ: {len(docs)}ê°œ')
print(f'URL íƒœê¹…: {urls_found}/{len(docs)} ({urls_found/len(docs)*100:.1f}%)')
print(f'ìƒ˜í”Œ URL: {docs[0].get(\"url\", \"NO URL\")}')
"
```

---

### 4. **ChromaDB ì„¤ì • ìµœì í™”**

**í˜„ì¬:** ê¸°ë³¸ ì„¤ì • ì‚¬ìš©

**ê°œì„  ì œì•ˆ:**
ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ìµœì í™”ëœ ChromaDB ì„¤ì • ì¶”ê°€

**ìˆ˜ì • íŒŒì¼:** `index_builder.py`

```python
# 59ë²ˆ ì¤„ ìˆ˜ì •
# Before:
client = chromadb.PersistentClient(path=str(self.chroma_path))

# After:
client = chromadb.PersistentClient(
    path=str(self.chroma_path),
    settings=chromadb.Settings(
        anonymized_telemetry=False,
        allow_reset=True,
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”
        chroma_db_impl="duckdb+parquet",  # ë” ë¹ ë¥¸ ë°±ì—”ë“œ
        chroma_server_cors_allow_origins=["*"]
    )
)
```

---

## ğŸ“Š ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ (1,001ê°œ ë¬¸ì„œ)

| ë‹¨ê³„ | CPU | GPU (RTX 4090) |
|------|-----|----------------|
| **1. í¬ë¡¤ë§** | 40-50ë¶„ | 40-50ë¶„ |
| **2. data_prep** | 2-3ë¶„ | 2-3ë¶„ |
| **3. index_builder** | 15-20ë¶„ | **5-7ë¶„** |
| **4. ë‹µë³€ ìƒì„±** | 5-10ì´ˆ/ì§ˆë¬¸ | 3-5ì´ˆ/ì§ˆë¬¸ |
| **ì´í•©** | ~60ë¶„ | **~50ë¶„** |

---

## ğŸ” ì‹¤í–‰ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í¬ë¡¤ë§ ì „:
- [ ] ìµœì†Œ 5GB ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„ í™•ì¸
- [ ] ì¸í„°ë„· ì—°ê²° ì•ˆì •ì„± í™•ì¸
- [ ] `crawler/config/settings_extended.py` ë¡œë“œ í™•ì¸

### ë°ì´í„° ì¤€ë¹„ ì „:
- [ ] `data/raw/git/pages.json` ì¡´ì¬ í™•ì¸
- [ ] `data/raw/python/pages.json` ì¡´ì¬ í™•ì¸
- [ ] ê° pages.jsonì— URL íƒœê¹… í™•ì¸

### ì¸ë±ì‹± ì „:
- [ ] `artifacts/chunks.parquet` ì¡´ì¬ í™•ì¸
- [ ] chunks.parquetì— 'url' ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
- [ ] ìµœì†Œ 16GB RAM í™•ì¸
- [ ] GPU ì‚¬ìš© ì‹œ ìµœì†Œ 8GB VRAM í™•ì¸

### ë‹µë³€ ìƒì„± ì „:
- [ ] `artifacts/chroma_db/` ì¡´ì¬ í™•ì¸
- [ ] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸

---

## ğŸ¯ ê²°ë¡ 

### âœ… ì¤€ë¹„ ì™„ë£Œ
- data_prep.py: URL ë©”íƒ€ë°ì´í„° í¬í•¨ âœ“
- index_builder.py: URL ë©”íƒ€ë°ì´í„° ì €ì¥ âœ“
- answerer_v2_optimized.py: URL ì¶œì²˜ í‘œì‹œ âœ“
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥ âœ“

### âš ï¸ í™•ì¸ í•„ìš”
1. í¬ë¡¤ëŸ¬ content_extractor.pyê°€ URLì„ ì˜¬ë°”ë¥´ê²Œ ì¶”ì¶œí•˜ëŠ”ì§€ ê²€ì¦
2. ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ index_builder.py ë°°ì¹˜ í¬ê¸° ì¡°ì •
3. ChromaDB ì„¤ì • ìµœì í™” (ì„ íƒì‚¬í•­)

### ğŸš€ ë‹¤ìŒ ë‹¨ê³„
1. í™•ì¥ í¬ë¡¤ë§ ì‹¤í–‰ (`run_crawl_extended.py`)
2. URL íƒœê¹… í™•ì¸ (ìœ„ ê²€ì¦ ë°©ë²• ì‚¬ìš©)
3. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (data_prep â†’ index_builder)
4. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ìœ¼ë¡œ URL ì¶œì²˜ í™•ì¸

---

**ì‘ì„±ì¼:** 2025-11-28
**ë²„ì „:** Pipeline Verification v1
