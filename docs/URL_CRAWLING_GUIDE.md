# ğŸ”— URL ì¶œì²˜ íƒœê¹… ì™„ë£Œ ê°€ì´ë“œ

## âœ… ì™„ë£Œëœ ì‘ì—…

URL ì¶œì²˜ê°€ ì œëŒ€ë¡œ í‘œì‹œë˜ë„ë¡ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

### 1. **í¬ë¡¤ëŸ¬ êµ¬ì¶•** âœ…
- ìœ„ì¹˜: `c:\develop1\test\crawler\`
- Git 180ë¬¸ì„œ + Python 272ë¬¸ì„œ í¬ë¡¤ë§
- ê° ë¬¸ì„œë§ˆë‹¤ URL íƒœê¹… í¬í•¨

### 2. **ë°ì´í„° ì¤€ë¹„ íŒŒì´í”„ë¼ì¸ ìˆ˜ì •** âœ…
- `experiments/rag_pipeline/data_prep.py`: URL metadata í¬í•¨
- `experiments/rag_pipeline/index_builder.py`: URLì„ ChromaDBì— ì €ì¥

### 3. **ë‹µë³€ ìƒì„± ì‹œ URL í‘œì‹œ** âœ…
- `experiments/rag_pipeline/answerer_v2_optimized.py`: ë‹µë³€ ëì— URL ì¶œì²˜ ì¶”ê°€

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### **1ë‹¨ê³„: í¬ë¡¤ë§ (Git + Python ë¬¸ì„œ)**

```bash
cd c:\develop1\test\crawler

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install beautifulsoup4 lxml requests tqdm

# í¬ë¡¤ë§ ì‹¤í–‰
python run_crawl.py
```

**ì„ íƒ ì˜µì…˜:**
- `1`: Gitë§Œ í¬ë¡¤ë§ (180ë¬¸ì„œ)
- `2`: Pythonë§Œ í¬ë¡¤ë§ (272ë¬¸ì„œ)
- `3`: ë‘˜ ë‹¤ í¬ë¡¤ë§ **(ê¶Œì¥)**

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- Git: ì•½ 10-15ë¶„ (rate limiting 2ì´ˆ/í˜ì´ì§€)
- Python: ì•½ 15-20ë¶„
- **ì´ ì•½ 30ë¶„**

**ê²°ê³¼ë¬¼:**
- `data/raw/git/pages.json` (URL í¬í•¨)
- `data/raw/python/pages.json` (URL í¬í•¨)

---

### **2ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ (ì²­í‚¹)**

```bash
cd c:\develop1\test\experiments\rag_pipeline

# ì²­í‚¹ ì‹¤í–‰ (URL metadata í¬í•¨)
python data_prep.py --config config/enhanced.yaml
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:** 1-2ë¶„

**ê²°ê³¼ë¬¼:**
- `artifacts/chunks.parquet` (URL ì»¬ëŸ¼ í¬í•¨)

**ê²€ì¦:**
```python
import pandas as pd
df = pd.read_parquet("artifacts/chunks.parquet")
print(df.columns)  # ['domain', 'chunk_id', 'text', 'length', 'url']
print(df['url'].head())  # URL í™•ì¸
```

---

### **3ë‹¨ê³„: ë²¡í„° ì¸ë±ì‹± (URL metadata í¬í•¨)**

```bash
# ChromaDB ì¸ë±ì‹± (URL metadata í¬í•¨)
python index_builder.py --config config/enhanced.yaml
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- CPU: 10-15ë¶„
- GPU (RTX 4090): **3-5ë¶„**

**ê²°ê³¼ë¬¼:**
- `artifacts/chroma_db/` (URLì´ metadataì— í¬í•¨ë¨)

---

### **4ë‹¨ê³„: í…ŒìŠ¤íŠ¸ (URL ì¶œì²˜ í™•ì¸)**

```bash
# ì§ˆë¬¸í•˜ê³  URL ì¶œì²˜ í™•ì¸
python answerer_v2_optimized.py "Pythonì—ì„œ ì–•ì€ ë³µì‚¬ì™€ ê¹Šì€ ë³µì‚¬ì˜ ì°¨ì´ëŠ”?" --config config/enhanced.yaml
```

**ì˜ˆìƒ ì¶œë ¥:**

```
================================================================================
ë‹µë³€:
================================================================================
ì–•ì€ ë³µì‚¬(shallow copy)ëŠ” ê°ì²´ì˜ ìµœìƒìœ„ ë ˆë²¨ë§Œ ë³µì‚¬í•˜ê³  ë‚´ë¶€ ê°ì²´ëŠ” ì°¸ì¡°ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.
ê¹Šì€ ë³µì‚¬(deep copy)ëŠ” ê°ì²´ì™€ ê·¸ ì•ˆì˜ ëª¨ë“  ì¤‘ì²©ëœ ê°ì²´ê¹Œì§€ ì¬ê·€ì ìœ¼ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë¦¬ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œ:
- ì–•ì€ ë³µì‚¬: ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ëŠ” ì›ë³¸ê³¼ ê°™ì€ ê°ì²´ë¥¼ ê°€ë¦¬í‚´
- ê¹Šì€ ë³µì‚¬: ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ë„ ì™„ì „íˆ ìƒˆë¡œìš´ ê°ì²´ë¡œ ìƒì„±

Pythonì—ì„œëŠ” copy.copy()ë¡œ ì–•ì€ ë³µì‚¬, copy.deepcopy()ë¡œ ê¹Šì€ ë³µì‚¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ğŸ“š ì°¸ê³ :
- https://realpython.com/python-shallow-deep-copy
- https://www.programiz.com/python-programming/shallow-deep-copy
- https://docs.python.org/3/library/copy.html
================================================================================
```

âœ… **URLì´ ì œëŒ€ë¡œ í‘œì‹œë©ë‹ˆë‹¤!**

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
c:\develop1\test\
â”œâ”€â”€ crawler/                      # ìƒˆë¡œ ì¶”ê°€í•œ í¬ë¡¤ëŸ¬
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py          # Git 180 + Python 272 ë¬¸ì„œ ì„¤ì •
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ base_scraper.py      # HTTP ìš”ì²­ + Rate limiting
â”‚   â”‚   â”œâ”€â”€ content_extractor.py # HTML â†’ êµ¬ì¡°í™”ëœ JSON
â”‚   â”‚   â”œâ”€â”€ git_scraper.py       # Git í¬ë¡¤ëŸ¬ (URL í¬í•¨)
â”‚   â”‚   â””â”€â”€ python_scraper.py    # Python í¬ë¡¤ëŸ¬ (URL í¬í•¨)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ retry_handler.py
â”‚   â””â”€â”€ run_crawl.py             # í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â”œâ”€â”€ git/
â”‚       â”‚   â”œâ”€â”€ pages.json       # â† URL í¬í•¨!
â”‚       â”‚   â””â”€â”€ metadata.json
â”‚       â””â”€â”€ python/
â”‚           â”œâ”€â”€ pages.json       # â† URL í¬í•¨!
â”‚           â””â”€â”€ metadata.json
â”‚
â””â”€â”€ experiments/rag_pipeline/
    â”œâ”€â”€ data_prep.py             # â† URL metadata í¬í•¨ ìˆ˜ì •
    â”œâ”€â”€ index_builder.py         # â† URL metadata ì €ì¥ ìˆ˜ì •
    â”œâ”€â”€ answerer_v2_optimized.py # â† URL ì¶œì²˜ í‘œì‹œ ìˆ˜ì •
    â””â”€â”€ artifacts/
        â”œâ”€â”€ chunks.parquet       # url ì»¬ëŸ¼ í¬í•¨
        â””â”€â”€ chroma_db/           # url metadata í¬í•¨
```

---

## ğŸ” ì£¼ìš” ë³€ê²½ ì‚¬í•­

### **1. `data_prep.py` ë³€ê²½**

**Before:**
```python
{
    "domain": "python",
    "chunk_id": "python_123",
    "text": "...",
    "length": 500
}
```

**After:**
```python
{
    "domain": "python",
    "chunk_id": "python_123",
    "text": "...",
    "length": 500,
    "url": "https://realpython.com/python-shallow-deep-copy"  # â† ì¶”ê°€!
}
```

### **2. `index_builder.py` ë³€ê²½**

**Before:**
```python
metadatas = batch[["domain", "length"]].to_dict(orient="records")
```

**After:**
```python
metadata_columns = ["domain", "length"]
if "url" in batch.columns:
    metadata_columns.append("url")  # â† URL ì¶”ê°€!
metadatas = batch[metadata_columns].to_dict(orient="records")
```

### **3. `answerer_v2_optimized.py` ë³€ê²½**

**Before:**
```python
return answer  # ë‹µë³€ë§Œ ë°˜í™˜
```

**After:**
```python
# URL ì¶œì²˜ ì¶”ê°€
source_urls = []
for meta in metadatas:
    url = meta.get('url', 'unknown')
    if url != 'unknown' and url not in source_urls:
        source_urls.append(url)

if source_urls:
    sources_section = "\n\nğŸ“š ì°¸ê³ :\n" + "\n".join(f"- {url}" for url in source_urls)
    answer = answer_text + sources_section

return answer  # ë‹µë³€ + URL ì¶œì²˜
```

---

## ğŸ¯ í¬ë¡¤ë§ ëŒ€ìƒ ë¬¸ì„œ ìˆ˜

### **Git (ì´ 180ë¬¸ì„œ)**
- Atlassian: 54ë¬¸ì„œ
- Pro Git (í•œêµ­ì–´): 36ë¬¸ì„œ
- GitHub Docs: 17ë¬¸ì„œ
- W3Schools: 18ë¬¸ì„œ
- Git Reference: 35ë¬¸ì„œ

### **Python (ì´ 272ë¬¸ì„œ)**
- Real Python: 45ë¬¸ì„œ
- Official Tutorial: 15ë¬¸ì„œ
- W3Schools: 27ë¬¸ì„œ
- Official Library: 28ë¬¸ì„œ
- Official HOWTOs: 18ë¬¸ì„œ
- GeeksforGeeks: 30ë¬¸ì„œ
- Programiz: 26ë¬¸ì„œ
- PyMOTW: 45ë¬¸ì„œ
- Official Advanced: 38ë¬¸ì„œ

**ì´ 452ë¬¸ì„œ** (Git 180 + Python 272)

---

## ğŸ› ï¸ Troubleshooting

### **Q1: "pages.json not found" ì—ëŸ¬**
â†’ 1ë‹¨ê³„(í¬ë¡¤ë§)ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.

### **Q2: URLì´ "unknown"ìœ¼ë¡œ í‘œì‹œë¨**
â†’ í¬ë¡¤ë§ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ê³ , 2-3ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì¬ì‹¤í–‰í•˜ì„¸ìš”.

### **Q3: í¬ë¡¤ë§ì´ ë„ˆë¬´ ëŠë¦¼**
â†’ `crawler/config/settings.py`ì—ì„œ `request_delay`ë¥¼ 1.0ì´ˆë¡œ ì¤„ì´ì„¸ìš” (í•˜ì§€ë§Œ 429 ì—ëŸ¬ ìœ„í—˜).

### **Q4: ì¼ë¶€ í˜ì´ì§€ê°€ í¬ë¡¤ë§ ì‹¤íŒ¨**
â†’ ì •ìƒì…ë‹ˆë‹¤. 404 ì—ëŸ¬ë‚˜ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ìë™ìœ¼ë¡œ ìŠ¤í‚µë˜ë©°, ë¡œê·¸ì— ê¸°ë¡ë©ë‹ˆë‹¤.

---

## âœ… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **1ë‹¨ê³„**: `data/raw/git/pages.json`ê³¼ `data/raw/python/pages.json` ìƒì„± í™•ì¸
- [ ] **2ë‹¨ê³„**: `artifacts/chunks.parquet`ì— `url` ì»¬ëŸ¼ í¬í•¨ í™•ì¸
- [ ] **3ë‹¨ê³„**: ChromaDB ì¸ë±ì‹± ì™„ë£Œ (`artifacts/chroma_db/` í´ë” ìƒì„±)
- [ ] **4ë‹¨ê³„**: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì—ì„œ **URL ì¶œì²˜**ê°€ ë‹µë³€ ëì— í‘œì‹œë˜ëŠ”ì§€ í™•ì¸

---

## ğŸ‰ ì™„ë£Œ!

ì´ì œ RAG ì±—ë´‡ì´ ë‹µë³€í•  ë•Œ **ì‹¤ì œ í¬ë¡¤ë§í•œ URL ì¶œì²˜**ë¥¼ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤!

**ì˜ˆì‹œ:**

**ì§ˆë¬¸:** "git rebaseëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

**ë‹µë³€:**
```
git rebaseëŠ” í•œ ë¸Œëœì¹˜ì˜ ë³€ê²½ì‚¬í•­ì„ ë‹¤ë¥¸ ë¸Œëœì¹˜ ìœ„ì— ì¬ì ìš©í•˜ëŠ” ëª…ë ¹ì…ë‹ˆë‹¤.
mergeì™€ ë‹¬ë¦¬ ì„ í˜•ì ì¸ ì»¤ë°‹ íˆìŠ¤í† ë¦¬ë¥¼ ë§Œë“¤ì–´ í”„ë¡œì íŠ¸ íˆìŠ¤í† ë¦¬ë¥¼ ê¹”ë”í•˜ê²Œ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“š ì°¸ê³ :
- https://www.atlassian.com/git/tutorials/rewriting-history/git-rebase
- https://git-scm.com/book/ko/v2/Git-ë¸Œëœì¹˜-Rebase-í•˜ê¸°
- https://git-scm.com/docs/git-rebase
```

**"ê·¼ê±° 1, ê·¼ê±° 2" â†’ ì‹¤ì œ URLë¡œ ë³€ê²½ ì™„ë£Œ!** âœ…
