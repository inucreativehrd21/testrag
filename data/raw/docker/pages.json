[
  {
    "title": "Get started",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference",
    "sections": [
      {
        "header": "",
        "content": "If you're new to Docker, this section guides you through the essential resources to get started.\n\nFollow the guides to help you get started and learn how Docker can optimize your development workflows.\n\nFor more advanced concepts and scenarios in Docker, see Guides.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 266
        }
      },
      {
        "header": "Foundations of Docker",
        "content": "Install Docker and jump into discovering what Docker is.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 56
        }
      },
      {
        "header": "Get Docker",
        "content": "Choose the best installation path for your setup.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 49
        }
      },
      {
        "header": "What is Docker?",
        "content": "Learn about the Docker platform.\n\nLearn the foundational concepts and workflows of Docker.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 90
        }
      },
      {
        "header": "Introduction",
        "content": "Get started with the basics and the benefits of containerizing your applications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 81
        }
      },
      {
        "header": "Docker concepts",
        "content": "Gain a better understanding of foundational Docker concepts.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 60
        }
      },
      {
        "header": "Docker workshop",
        "content": "Get guided through a 45-minute workshop to learn about Docker.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 62
        }
      }
    ],
    "url": "https://docs.docker.com/get-started/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "What is Docker?",
    "summary": "Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker's methodologies for shipping, testing, and deploying code, you can significantly reduce the delay between writing code and running it in production. Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security lets you run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you don't need to rely on what's installed on the host. You can share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.",
    "sections": [
      {
        "header": "The Docker platform",
        "content": "Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security lets you run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you don't need to rely on what's installed on the host. You can share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.\n\nDocker provides tooling and a platform to manage the lifecycle of your containers:\n\nDevelop your application and its supporting components using containers. The container becomes the unit for distributing and testing your application. When you're ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 955
        }
      },
      {
        "header": "What can I use Docker for?",
        "content": "Fast, consistent delivery of your applications\n\nDocker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows.\n\nConsider the following example scenario:\n\nYour developers write code locally and share their work with their colleagues using Docker containers. They use Docker to push their applications into a test environment and run automated and manual tests. When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation. When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.\n\nResponsive deployment and scaling\n\nDocker's container-based platform allows for highly portable workloads. Docker containers can run on a developer's local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.\n\nDocker's portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time.\n\nRunning more workloads on the same hardware\n\nDocker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your server capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1675
        }
      },
      {
        "header": "Fast, consistent delivery of your applications",
        "content": "Docker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows.\n\nConsider the following example scenario:\n\nYour developers write code locally and share their work with their colleagues using Docker containers. They use Docker to push their applications into a test environment and run automated and manual tests. When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation. When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 790
        }
      },
      {
        "header": "Responsive deployment and scaling",
        "content": "Docker's container-based platform allows for highly portable workloads. Docker containers can run on a developer's local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.\n\nDocker's portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 426
        }
      },
      {
        "header": "Running more workloads on the same hardware",
        "content": "Docker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your server capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 327
        }
      },
      {
        "header": "Docker architecture",
        "content": "Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.\n\nThe Docker daemon\n\nThe Docker daemon ( dockerd ) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.\n\nThe Docker client\n\nThe Docker client ( docker ) is the primary way that many Docker users interact with Docker. When you use commands such as docker run , the client sends these commands to dockerd , which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.\n\nDocker Desktop\n\nDocker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices. Docker Desktop includes the Docker daemon ( dockerd ), the Docker client ( docker ), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop .\n\nDocker registries\n\nA Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker looks for images on Docker Hub by default. You can even run your own private registry.\n\nWhen you use the docker pull or docker run commands, Docker pulls the required images from your configured registry. When you use the docker push command, Docker pushes your image to your configured registry.\n\nDocker objects\n\nWhen you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.\n\nImages\n\nAn image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.\n\nYou might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.\n\nContainers\n\nA container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.\n\nBy default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container's network, storage, or other underlying subsystems are from other containers or from the host machine.\n\nA container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that aren't stored in persistent storage disappear.\n\nExample docker run command\n\nThe following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash .\n\n$ docker run -i -t ubuntu /bin/bash\n\nWhen you run this command, the following happens (assuming you are using the default registry configuration):\n\nIf you don't have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually. Docker creates a new container, as though you had run a docker container create command manually. Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem. Docker creates a network interface to connect the container to the default network, since you didn't specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine's network connection. Docker starts the container and executes /bin/bash . Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while Docker logs the output to your terminal. When you run exit to terminate the /bin/bash command, the container stops but isn't removed. You can start it again or remove it.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 5118
        }
      },
      {
        "header": "The Docker daemon",
        "content": "The Docker daemon ( dockerd ) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 215
        }
      },
      {
        "header": "The Docker client",
        "content": "The Docker client ( docker ) is the primary way that many Docker users interact with Docker. When you use commands such as docker run , the client sends these commands to dockerd , which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 305
        }
      },
      {
        "header": "Docker Desktop",
        "content": "Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices. Docker Desktop includes the Docker daemon ( dockerd ), the Docker client ( docker ), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 374
        }
      },
      {
        "header": "Docker registries",
        "content": "A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker looks for images on Docker Hub by default. You can even run your own private registry.\n\nWhen you use the docker pull or docker run commands, Docker pulls the required images from your configured registry. When you use the docker push command, Docker pushes your image to your configured registry.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 400
        }
      },
      {
        "header": "Docker objects",
        "content": "When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.\n\nImages\n\nAn image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.\n\nYou might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.\n\nContainers\n\nA container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.\n\nBy default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container's network, storage, or other underlying subsystems are from other containers or from the host machine.\n\nA container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that aren't stored in persistent storage disappear.\n\nExample docker run command\n\nThe following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash .\n\n$ docker run -i -t ubuntu /bin/bash\n\nWhen you run this command, the following happens (assuming you are using the default registry configuration):\n\nIf you don't have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually. Docker creates a new container, as though you had run a docker container create command manually. Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem. Docker creates a network interface to connect the container to the default network, since you didn't specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine's network connection. Docker starts the container and executes /bin/bash . Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while Docker logs the output to your terminal. When you run exit to terminate the /bin/bash command, the container stops but isn't removed. You can start it again or remove it.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3213
        }
      },
      {
        "header": "Images",
        "content": "An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.\n\nYou might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 892
        }
      },
      {
        "header": "Containers",
        "content": "A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.\n\nBy default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container's network, storage, or other underlying subsystems are from other containers or from the host machine.\n\nA container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that aren't stored in persistent storage disappear.\n\nExample docker run command\n\nThe following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash .\n\n$ docker run -i -t ubuntu /bin/bash\n\nWhen you run this command, the following happens (assuming you are using the default registry configuration):\n\nIf you don't have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually. Docker creates a new container, as though you had run a docker container create command manually. Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem. Docker creates a network interface to connect the container to the default network, since you didn't specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine's network connection. Docker starts the container and executes /bin/bash . Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while Docker logs the output to your terminal. When you run exit to terminate the /bin/bash command, the container stops but isn't removed. You can start it again or remove it.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2124
        }
      },
      {
        "header": "The underlying technology",
        "content": "Docker is written in the Go programming language and takes advantage of several features of the Linux kernel to deliver its functionality. Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container.\n\nThese namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 467
        }
      },
      {
        "header": "Next steps",
        "content": "Install Docker Get started with Docker",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 38
        }
      }
    ],
    "url": "https://docs.docker.com/get-started/docker-overview/",
    "doc_type": "docker",
    "total_sections": 15
  },
  {
    "title": "What is a container?",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference",
    "sections": [
      {
        "header": "Explanation",
        "content": "Imagine you're developing a killer web app that has three main components - a React frontend, a Python API, and a PostgreSQL database. If you wanted to work on this project, you'd have to install Node, Python, and PostgreSQL.\n\nHow do you make sure you have the same versions as the other developers on your team? Or your CI/CD system? Or what's used in production?\n\nHow do you ensure the version of Python (or Node or the database) your app needs isn't affected by what's already on your machine? How do you manage potential conflicts?\n\nWhat is a container? Simply put, containers are isolated processes for each of your app's components. Each component - the frontend React app, the Python API engine, and the database - runs in its own isolated environment, completely isolated from everything else on your machine.\n\nHere's what makes them awesome. Containers are:\n\n• Self-contained. Each container has everything it needs to function with no reliance on any pre-installed dependencies on the host machine.\n• Isolated. Since containers are run in isolation, they have minimal influence on the host and other containers, increasing the security of your applications.\n• Independent. Each container is independently managed. Deleting one container won't affect any others.\n• Portable. Containers can run anywhere! The container that runs on your development machine will work the same way in a data center or anywhere in the cloud!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1430
        }
      },
      {
        "header": "Containers versus virtual machines (VMs)",
        "content": "Without getting too deep, a VM is an entire operating system with its own kernel, hardware drivers, programs, and applications. Spinning up a VM only to isolate a single application is a lot of overhead.\n\nA container is simply an isolated process with all of the files it needs to run. If you run multiple containers, they all share the same kernel, allowing you to run more applications on less infrastructure.\n\nUsing VMs and containers together\n\nQuite often, you will see containers and VMs used together. As an example, in a cloud environment, the provisioned machines are typically VMs. However, instead of provisioning one machine to run one application, a VM with a container runtime can run multiple containerized applications, increasing resource utilization and reducing costs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 786
        }
      },
      {
        "header": "Try it out",
        "content": "In this hands-on, you will see how to run a Docker container using the Docker Desktop GUI.\n\nUse the following instructions to run a container.\n\nOpen Docker Desktop and select the Search field on the top navigation bar.\n\nSpecify welcome-to-docker in the search input and then select the Pull button.\n\nOnce the image is successfully pulled, select the Run button.\n\nExpand the Optional settings.\n\nIn the Container name, specify welcome-to-docker.\n\nIn the Host port, specify 8080.\n\nSelect Run to start your container.\n\nCongratulations! You just ran your first container! ð\n\n• Open Docker Desktop and select the Search field on the top navigation bar.\n• Specify welcome-to-docker in the search input and then select the Pull button.\n• Once the image is successfully pulled, select the Run button.\n• Expand the Optional settings.\n• In the Container name, specify welcome-to-docker.\n• In the Host port, specify 8080.\n• Select Run to start your container.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 950
        }
      },
      {
        "header": "View your container",
        "content": "You can view all of your containers by going to the Containers view of the Docker Desktop Dashboard.\n\nThis container runs a web server that displays a simple website. When working with more complex projects, you'll run different parts in different containers. For example, you might run a different container for the frontend, backend, and database.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 349
        }
      },
      {
        "header": "Access the frontend",
        "content": "When you launched the container, you exposed one of the container's ports onto your machine. Think of this as creating configuration to let you to connect through the isolated environment of the container.\n\nFor this container, the frontend is accessible on port 8080. To open the website, select the link in the Port(s) column of your container or visit http://localhost:8080 in your browser.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 392
        }
      },
      {
        "header": "Explore your container",
        "content": "Docker Desktop lets you explore and interact with different aspects of your container. Try it out yourself.\n\nGo to the Containers view in the Docker Desktop Dashboard.\n\nSelect your container.\n\nSelect the Files tab to explore your container's isolated file system.\n\n• Go to the Containers view in the Docker Desktop Dashboard.\n• Select your container.\n• Select the Files tab to explore your container's isolated file system.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 423
        }
      },
      {
        "header": "Stop your container",
        "content": "The docker/welcome-to-docker container continues to run until you stop it.\n\nGo to the Containers view in the Docker Desktop Dashboard.\n\nLocate the container you'd like to stop.\n\nSelect the Stop action in the Actions column.\n\nFollow the instructions to run a container using the CLI:\n\nOpen your CLI terminal and start a container by using the docker run command:\n\nThe output from this command is the full container ID.\n\nCongratulations! You just fired up your first container! ð\n\n• Go to the Containers view in the Docker Desktop Dashboard.\n• Locate the container you'd like to stop.\n• Select the Stop action in the Actions column.\n\n• Open your CLI terminal and start a container by using the docker run command:$ docker run -d -p 8080:80 docker/welcome-to-docker The output from this command is the full container ID.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d -p 8080:80 docker/welcome-to-docker\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 820
        }
      },
      {
        "header": "View your running containers",
        "content": "You can verify if the container is up and running by using the docker ps command:\n\nYou will see output like the following:\n\nThis container runs a web server that displays a simple website. When working with more complex projects, you'll run different parts in different containers. For example, a different container for the frontend, backend, and database.\n\nThe docker ps command will show you only running containers. To view stopped containers, add the -a flag to list all containers: docker ps -a\n\n[Admonition] The docker ps command will show you only running containers. To view stopped containers, add the -a flag to list all containers: docker ps -a",
        "code_examples": [
          "```\ndocker ps\n```",
          "```\nCONTAINER ID   IMAGE                      COMMAND                  CREATED          STATUS          PORTS                      NAMESa1f7a4bb3a27   docker/welcome-to-docker   \"/docker-entrypoint.â¦\"   11 seconds ago   Up 11 seconds   0.0.0.0:8080->80/tcp       gracious_keldysh\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 656
        }
      },
      {
        "header": "Access the frontend",
        "content": "When you launched the container, you exposed one of the container's ports onto your machine. Think of this as creating configuration to let you to connect through the isolated environment of the container.\n\nFor this container, the frontend is accessible on port 8080. To open the website, select the link in the Port(s) column of your container or visit http://localhost:8080 in your browser.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 392
        }
      },
      {
        "header": "Stop your container",
        "content": "The docker/welcome-to-docker container continues to run until you stop it. You can stop a container using the docker stop command.\n\nRun docker ps to get the ID of the container\n\nProvide the container ID or name to the docker stop command:\n\nWhen referencing containers by ID, you don't need to provide the full ID. You only need to provide enough of the ID to make it unique. As an example, the previous container could be stopped by running the following command:\n\n• Run docker ps to get the ID of the container\n• Provide the container ID or name to the docker stop command:docker stop <the-container-id>\n\n[Admonition] When referencing containers by ID, you don't need to provide the full ID. You only need to provide enough of the ID to make it unique. As an example, the previous container could be stopped by running the following command:docker stop a1f",
        "code_examples": [
          "```\ndocker stop <the-container-id>\n```",
          "```\ndocker stop a1f\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 857
        }
      },
      {
        "header": "Additional resources",
        "content": "The following links provide additional guidance into containers:\n\n• Running a container\n• Overview of container\n• Why Docker?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 125
        }
      },
      {
        "header": "Next steps",
        "content": "Now that you have learned the basics of a Docker container, it's time to learn about Docker images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 99
        }
      }
    ],
    "url": "https://docs.docker.com/guides/docker-concepts/the-basics/what-is-a-container/",
    "doc_type": "docker",
    "total_sections": 12
  },
  {
    "title": "What is an image?",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference",
    "sections": [
      {
        "header": "Explanation",
        "content": "Seeing as a container is an isolated process, where does it get its files and configuration? How do you share those environments?\n\nThat's where container images come in. A container image is a standardized package that includes all of the files, binaries, libraries, and configurations to run a container.\n\nFor a PostgreSQL image, that image will package the database binaries, config files, and other dependencies. For a Python web app, it'll include the Python runtime, your app code, and all of its dependencies.\n\nThere are two important principles of images:\n\nImages are immutable. Once an image is created, it can't be modified. You can only make a new image or add changes on top of it.\n\nContainer images are composed of layers. Each layer represents a set of file system changes that add, remove, or modify files.\n\nThese two principles let you to extend or add to existing images. For example, if you are building a Python app, you can start from the Python image and add additional layers to install your app's dependencies and add your code. This lets you focus on your app, rather than Python itself.\n\n• Images are immutable. Once an image is created, it can't be modified. You can only make a new image or add changes on top of it.\n• Container images are composed of layers. Each layer represents a set of file system changes that add, remove, or modify files.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1371
        }
      },
      {
        "header": "Finding images",
        "content": "Docker Hub is the default global marketplace for storing and distributing images. It has over 100,000 images created by developers that you can run locally. You can search for Docker Hub images and run them directly from Docker Desktop.\n\nDocker Hub provides a variety of Docker-supported and endorsed images known as Docker Trusted Content. These provide fully managed services or great starters for your own images. These include:\n\nFor example, Redis and Memcached are a few popular ready-to-go Docker Official Images. You can download these images and have these services up and running in a matter of seconds. There are also base images, like the Node.js Docker image, that you can use as a starting point and add your own files and configurations.\n\n• Docker Official Images - a curated set of Docker repositories, serve as the starting point for the majority of users, and are some of the most secure on Docker Hub\n• Docker Verified Publishers - high-quality images from commercial publishers verified by Docker\n• Docker-Sponsored Open Source - images published and maintained by open-source projects sponsored by Docker through Docker's open source program",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1161
        }
      },
      {
        "header": "Try it out",
        "content": "In this hands-on, you will learn how to search and pull a container image using the Docker Desktop GUI.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 103
        }
      },
      {
        "header": "Search for and download an image",
        "content": "Open the Docker Desktop Dashboard and select the Images view in the left-hand navigation menu.\n\nSelect the Search images to run button. If you don't see it, select the global search bar at the top of the screen.\n\nIn the Search field, enter \"welcome-to-docker\". Once the search has completed, select the docker/welcome-to-docker image.\n\nSelect Pull to download the image.\n\n• Open the Docker Desktop Dashboard and select the Images view in the left-hand navigation menu.\n• Select the Search images to run button. If you don't see it, select the global search bar at the top of the screen.\n• In the Search field, enter \"welcome-to-docker\". Once the search has completed, select the docker/welcome-to-docker image.\n• Select Pull to download the image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 747
        }
      },
      {
        "header": "Learn about the image",
        "content": "Once you have an image downloaded, you can learn quite a few details about the image either through the GUI or the CLI.\n\nIn the Docker Desktop Dashboard, select the Images view.\n\nSelect the docker/welcome-to-docker image to open details about the image.\n\nThe image details page presents you with information regarding the layers of the image, the packages and libraries installed in the image, and any discovered vulnerabilities.\n\nFollow the instructions to search and pull a Docker image using CLI to view its layers.\n\n• In the Docker Desktop Dashboard, select the Images view.\n• Select the docker/welcome-to-docker image to open details about the image.\n• The image details page presents you with information regarding the layers of the image, the packages and libraries installed in the image, and any discovered vulnerabilities.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 832
        }
      },
      {
        "header": "Search for and download an image",
        "content": "Open a terminal and search for images using the docker search command:\n\nYou will see output like the following:\n\nThis output shows you information about relevant images available on Docker Hub.\n\nPull the image using the docker pull command.\n\nYou will see output like the following:\n\nEach of line represents a different downloaded layer of the image. Remember that each layer is a set of filesystem changes and provides functionality of the image.\n\n• Open a terminal and search for images using the docker search command:docker search docker/welcome-to-docker You will see output like the following:NAME DESCRIPTION STARS OFFICIAL docker/welcome-to-docker Docker image for new users getting started wâ¦ 20 This output shows you information about relevant images available on Docker Hub.\n• Pull the image using the docker pull command.docker pull docker/welcome-to-docker You will see output like the following:Using default tag: latest latest: Pulling from docker/welcome-to-docker 579b34f0a95b: Download complete d11a451e6399: Download complete 1c2214f9937c: Download complete b42a2f288f4d: Download complete 54b19e12c655: Download complete 1fb28e078240: Download complete 94be7e780731: Download complete 89578ce72c35: Download complete Digest: sha256:eedaff45e3c78538087bdd9dc7afafac7e110061bbdd836af4104b10f10ab693 Status: Downloaded newer image for docker/welcome-to-docker:latest docker.io/docker/welcome-to-docker:latest Each of line represents a different downloaded layer of the image. Remember that each layer is a set of filesystem changes and provides functionality of the image.",
        "code_examples": [
          "```\ndocker search docker/welcome-to-docker\n```",
          "```\nNAME                       DESCRIPTION                                     STARS     OFFICIALdocker/welcome-to-docker   Docker image for new users getting started wâ¦   20\n```",
          "```\ndocker pull docker/welcome-to-docker\n```",
          "```\nUsing default tag: latestlatest: Pulling from docker/welcome-to-docker579b34f0a95b: Download completed11a451e6399: Download complete1c2214f9937c: Download completeb42a2f288f4d: Download complete54b19e12c655: Download complete1fb28e078240: Download complete94be7e780731: Download complete89578ce72c35: Download completeDigest: sha256:eedaff45e3c78538087bdd9dc7afafac7e110061bbdd836af4104b10f10ab693Status: Downloaded newer image for docker/welcome-to-docker:latestdocker.io/docker/welcome-to-docker:latest\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1590
        }
      },
      {
        "header": "Learn about the image",
        "content": "List your downloaded images using the docker image ls command:\n\nYou will see output like the following:\n\nThe command shows a list of Docker images currently available on your system. The docker/welcome-to-docker has a total size of approximately 29.7MB.\n\nThe image size represented here reflects the uncompressed size of the image, not the download size of the layers.\n\nList the image's layers using the docker image history command:\n\nYou will see output like the following:\n\nThis output shows you all of the layers, their sizes, and the command used to create the layer.\n\nViewing the full command\n\nIf you add the --no-trunc flag to the command, you will see the full command. Note that, since the output is in a table-like format, longer commands will cause the output to be very difficult to navigate.\n\nIn this walkthrough, you searched and pulled a Docker image. In addition to pulling a Docker image, you also learned about the layers of a Docker Image.\n\n• List your downloaded images using the docker image ls command:docker image ls You will see output like the following:REPOSITORY TAG IMAGE ID CREATED SIZE docker/welcome-to-docker latest eedaff45e3c7 4 months ago 29.7MB The command shows a list of Docker images currently available on your system. The docker/welcome-to-docker has a total size of approximately 29.7MB.Image sizeThe image size represented here reflects the uncompressed size of the image, not the download size of the layers.\n• List the image's layers using the docker image history command:docker image history docker/welcome-to-docker You will see output like the following:IMAGE CREATED CREATED BY SIZE COMMENT 648f93a1ba7d 4 months ago COPY /app/build /usr/share/nginx/html # builâ¦ 1.6MB buildkit.dockerfile.v0 <missing> 5 months ago /bin/sh -c #(nop) CMD [\"nginx\" \"-g\" \"daemonâ¦ 0B <missing> 5 months ago /bin/sh -c #(nop) STOPSIGNAL SIGQUIT 0B <missing> 5 months ago /bin/sh -c #(nop) EXPOSE 80 0B <missing> 5 months ago /bin/sh -c #(nop) ENTRYPOINT [\"/docker-entrâ¦ 0B <missing> 5 months ago /bin/sh -c #(nop) COPY file:9e3b2b63db9f8fc7â¦ 4.62kB <missing> 5 months ago /bin/sh -c #(nop) COPY file:57846632accc8975â¦ 3.02kB <missing> 5 months ago /bin/sh -c #(nop) COPY file:3b1b9915b7dd898aâ¦ 298B <missing> 5 months ago /bin/sh -c #(nop) COPY file:caec368f5a54f70aâ¦ 2.12kB <missing> 5 months ago /bin/sh -c #(nop) COPY file:01e75c6dd0ce317dâ¦ 1.62kB <missing> 5 months ago /bin/sh -c set -x && addgroup -g 101 -S â¦ 9.7MB <missing> 5 months ago /bin/sh -c #(nop) ENV PKG_RELEASE=1 0B <missing> 5 months ago /bin/sh -c #(nop) ENV NGINX_VERSION=1.25.3 0B <missing> 5 months ago /bin/sh -c #(nop) LABEL maintainer=NGINX Doâ¦ 0B <missing> 5 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 5 months ago /bin/sh -c #(nop) ADD file:ff3112828967e8004â¦ 7.66MB This output shows you all of the layers, their sizes, and the command used to create the layer.Viewing the full commandIf you add the --no-trunc flag to the command, you will see the full command. Note that, since the output is in a table-like format, longer commands will cause the output to be very difficult to navigate.",
        "code_examples": [
          "```\ndocker image ls\n```",
          "```\nREPOSITORY                 TAG       IMAGE ID       CREATED        SIZEdocker/welcome-to-docker   latest    eedaff45e3c7   4 months ago   29.7MB\n```",
          "```\ndocker image history docker/welcome-to-docker\n```",
          "```\nIMAGE          CREATED        CREATED BY                                      SIZE      COMMENT648f93a1ba7d   4 months ago   COPY /app/build /usr/share/nginx/html # builâ¦   1.6MB     buildkit.dockerfile.v0<missing>      5 months ago   /bin/sh -c #(nop)  CMD [\"nginx\" \"-g\" \"daemonâ¦   0B<missing>      5 months ago   /bin/sh -c #(nop)  STOPSIGNAL SIGQUIT           0B<missing>      5 months ago   /bin/sh -c #(nop)  EXPOSE 80                    0B<missing>      5 months ago   /bin/sh -c #(nop)  ENTRYPOINT [\"/docker-entrâ¦   0B<missing>      5 months ago   /bin/sh -c #(nop) COPY file:9e3b2b63db9f8fc7â¦   4.62kB<missing>      5 months ago   /bin/sh -c #(nop) COPY file:57846632accc8975â¦   3.02kB<missing>      5 months ago   /bin/sh -c #(nop) COPY file:3b1b9915b7dd898aâ¦   298B<missing>      5 months ago   /bin/sh -c #(nop) COPY file:caec368f5a54f70aâ¦   2.12kB<missing>      5 months ago   /bin/sh -c #(nop) COPY file:01e75c6dd0ce317dâ¦   1.62kB<missing>      5 months ago   /bin/sh -c set -x     && addgroup -g 101 -S â¦   9.7MB<missing>      5 months ago   /bin/sh -c #(nop)  ENV PKG_RELEASE=1            0B<missing>      5 months ago   /bin/sh -c #(nop)  ENV NGINX_VERSION=1.25.3     0B<missing>      5 months ago   /bin/sh -c #(nop)  LABEL maintainer=NGINX Doâ¦   0B<missing>      5 months ago   /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B<missing>      5 months ago   /bin/sh -c #(nop) ADD file:ff3112828967e8004â¦   7.66MB\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 3132
        }
      },
      {
        "header": "Additional resources",
        "content": "The following resources will help you learn more about exploring, finding, and building images:\n\n• Docker trusted content\n• Explore the Image view in Docker Desktop\n• Docker Build overview",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 188
        }
      },
      {
        "header": "Next steps",
        "content": "Now that you have learned the basics of images, it's time to learn about distributing images through registries.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 112
        }
      }
    ],
    "url": "https://docs.docker.com/guides/docker-concepts/the-basics/what-is-an-image/",
    "doc_type": "docker",
    "total_sections": 9
  },
  {
    "title": "What is a registry?",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference",
    "sections": [
      {
        "header": "Explanation",
        "content": "Now that you know what a container image is and how it works, you might wonder - where do you store these images?\n\nWell, you can store your container images on your computer system, but what if you want to share them with your friends or use them on another machine? That's where the image registry comes in.\n\nAn image registry is a centralized location for storing and sharing your container images. It can be either public or private. Docker Hub is a public registry that anyone can use and is the default registry.\n\nWhile Docker Hub is a popular option, there are many other available container registries available today, including Amazon Elastic Container Registry (ECR), Azure Container Registry (ACR), and Google Container Registry (GCR). You can even run your private registry on your local system or inside your organization. For example, Harbor, JFrog Artifactory, GitLab Container registry etc.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 905
        }
      },
      {
        "header": "Registry vs. repository",
        "content": "While you're working with registries, you might hear the terms registry and repository as if they're interchangeable. Even though they're related, they're not quite the same thing.\n\nA registry is a centralized location that stores and manages container images, whereas a repository is a collection of related container images within a registry. Think of it as a folder where you organize your images based on projects. Each repository contains one or more container images.\n\nThe following diagram shows the relationship between a registry, repositories, and images.\n\nYou can create one private repository and unlimited public repositories using the free version of Docker Hub. For more information, visit the Docker Hub subscription page.\n\n[Admonition] You can create one private repository and unlimited public repositories using the free version of Docker Hub. For more information, visit the Docker Hub subscription page.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 924
        }
      },
      {
        "header": "Try it out",
        "content": "In this hands-on, you will learn how to build and push a Docker image to the Docker Hub repository.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 99
        }
      },
      {
        "header": "Sign up for a free Docker account",
        "content": "If you haven't created one yet, head over to the Docker Hub page to sign up for a new Docker account.\n\nYou can use your Google or GitHub account to authenticate.\n\n• If you haven't created one yet, head over to the Docker Hub page to sign up for a new Docker account. You can use your Google or GitHub account to authenticate.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 325
        }
      },
      {
        "header": "Create your first repository",
        "content": "Sign in to Docker Hub.\n\nSelect the Create repository button in the top-right corner.\n\nSelect your namespace (most likely your username) and enter docker-quickstart as the repository name.\n\nSet the visibility to Public.\n\nSelect the Create button to create the repository.\n\nThat's it. You've successfully created your first repository. ð\n\nThis repository is empty right now. You'll now fix this by pushing an image to it.\n\n• Sign in to Docker Hub.\n• Select the Create repository button in the top-right corner.\n• Select your namespace (most likely your username) and enter docker-quickstart as the repository name.\n• Set the visibility to Public.\n• Select the Create button to create the repository.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 700
        }
      },
      {
        "header": "Sign in with Docker Desktop",
        "content": "• Download and install Docker Desktop, if not already installed.\n• In the Docker Desktop GUI, select the Sign in button in the top-right corner",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 143
        }
      },
      {
        "header": "Clone sample Node.js code",
        "content": "In order to create an image, you first need a project. To get you started quickly, you'll use a sample Node.js project found at github.com/dockersamples/helloworld-demo-node. This repository contains a pre-built Dockerfile necessary for building a Docker image.\n\nDon't worry about the specifics of the Dockerfile, as you'll learn about that in later sections.\n\nClone the GitHub repository using the following command:\n\nNavigate into the newly created directory.\n\nRun the following command to build a Docker image, swapping out YOUR_DOCKER_USERNAME with your username.\n\nMake sure you include the dot (.) at the end of the docker build command. This tells Docker where to find the Dockerfile.\n\nRun the following command to list the newly created Docker image:\n\nYou will see output like the following:\n\nStart a container to test the image by running the following command (swap out the username with your own username):\n\nYou can verify if the container is working by visiting http://localhost:8080 with your browser.\n\nUse the docker tag command to tag the Docker image. Docker tags allow you to label and version your images.\n\nFinally, it's time to push the newly built image to your Docker Hub repository by using the docker push command:\n\nOpen Docker Hub and navigate to your repository. Navigate to the Tags section and see your newly pushed image.\n\nIn this walkthrough, you signed up for a Docker account, created your first Docker Hub repository, and built, tagged, and pushed a container image to your Docker Hub repository.\n\n• Clone the GitHub repository using the following command:git clone https://github.com/dockersamples/helloworld-demo-node\n• Navigate into the newly created directory.cd helloworld-demo-node\n• Run the following command to build a Docker image, swapping out YOUR_DOCKER_USERNAME with your username.docker build -t <YOUR_DOCKER_USERNAME>/docker-quickstart . NoteMake sure you include the dot (.) at the end of the docker build command. This tells Docker where to find the Dockerfile.\n• Run the following command to list the newly created Docker image:docker images You will see output like the following:REPOSITORY TAG IMAGE ID CREATED SIZE <YOUR_DOCKER_USERNAME>/docker-quickstart latest 476de364f70e 2 minutes ago 170MB\n• Start a container to test the image by running the following command (swap out the username with your own username):docker run -d -p 8080:8080 <YOUR_DOCKER_USERNAME>/docker-quickstart You can verify if the container is working by visiting http://localhost:8080 with your browser.\n• Use the docker tag command to tag the Docker image. Docker tags allow you to label and version your images.docker tag <YOUR_DOCKER_USERNAME>/docker-quickstart <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0\n• Finally, it's time to push the newly built image to your Docker Hub repository by using the docker push command:docker push <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0\n• Open Docker Hub and navigate to your repository. Navigate to the Tags section and see your newly pushed image.\n\n[Admonition] Make sure you include the dot (.) at the end of the docker build command. This tells Docker where to find the Dockerfile.",
        "code_examples": [
          "```\ncd helloworld-demo-node\n```",
          "```\ndocker build -t <YOUR_DOCKER_USERNAME>/docker-quickstart .\n```",
          "```\ndocker images\n```",
          "```\nREPOSITORY                                 TAG       IMAGE ID       CREATED         SIZE<YOUR_DOCKER_USERNAME>/docker-quickstart   latest    476de364f70e   2 minutes ago   170MB\n```",
          "```\ndocker run -d -p 8080:8080 <YOUR_DOCKER_USERNAME>/docker-quickstart\n```",
          "```\ndocker tag <YOUR_DOCKER_USERNAME>/docker-quickstart <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0\n```",
          "```\ndocker push <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0\n```"
        ],
        "usage_examples": [
          "```\ngit clone https://github.com/dockersamples/helloworld-demo-node\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3155
        }
      },
      {
        "header": "Additional resources",
        "content": "• Docker Hub Quickstart\n• Manage Docker Hub Repositories",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 56
        }
      },
      {
        "header": "Next steps",
        "content": "Now that you understand the basics of containers and images, you're ready to learn about Docker Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 104
        }
      }
    ],
    "url": "https://docs.docker.com/guides/docker-concepts/the-basics/what-is-a-registry/",
    "doc_type": "docker",
    "total_sections": 9
  },
  {
    "title": "Understanding the image layers",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference",
    "sections": [
      {
        "header": "Explanation",
        "content": "As you learned in What is an image?, container images are composed of layers. And each of these layers, once created, are immutable. But, what does that actually mean? And how are those layers used to create the filesystem a container can use?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 243
        }
      },
      {
        "header": "Image layers",
        "content": "Each layer in an image contains a set of filesystem changes - additions, deletions, or modifications. Letâs look at a theoretical image:\n\nThis example might look like:\n\nThis is beneficial because it allows layers to be reused between images. For example, imagine you wanted to create another Python application. Due to layering, you can leverage the same Python base. This will make builds faster and reduce the amount of storage and bandwidth required to distribute the images. The image layering might look similar to the following:\n\nLayers let you extend images of others by reusing their base layers, allowing you to add only the data that your application needs.\n\n• The first layer adds basic commands and a package manager, such as apt.\n• The second layer installs a Python runtime and pip for dependency management.\n• The third layer copies in an applicationâs specific requirements.txt file.\n• The fourth layer installs that applicationâs specific dependencies.\n• The fifth layer copies in the actual source code of the application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1046
        }
      },
      {
        "header": "Stacking the layers",
        "content": "Layering is made possible by content-addressable storage and union filesystems. While this will get technical, hereâs how it works:\n\nWhen the union filesystem is created, in addition to the image layers, a directory is created specifically for the running container. This allows the container to make filesystem changes while allowing the original image layers to remain untouched. This enables you to run multiple containers from the same underlying image.\n\n• After each layer is downloaded, it is extracted into its own directory on the host filesystem.\n• When you run a container from an image, a union filesystem is created where layers are stacked on top of each other, creating a new and unified view.\n• When the container starts, its root directory is set to the location of this unified directory, using chroot.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 821
        }
      },
      {
        "header": "Try it out",
        "content": "In this hands-on guide, you will create new image layers manually using the docker container commit command. Note that youâll rarely create images this way, as youâll normally use a Dockerfile. But, it makes it easier to understand how itâs all working.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 259
        }
      },
      {
        "header": "Create a base image",
        "content": "In this first step, you will create your own base image that you will then use for the following steps.\n\nDownload and install Docker Desktop.\n\nIn a terminal, run the following command to start a new container:\n\nOnce the image has been downloaded and the container has started, you should see a new shell prompt. This is running inside your container. It will look similar to the following (the container ID will vary):\n\nInside the container, run the following command to install Node.js:\n\nWhen this command runs, it downloads and installs Node inside the container. In the context of the union filesystem, these filesystem changes occur within the directory unique to this container.\n\nValidate if Node is installed by running the following command:\n\nYou should then see a âHello world!â appear in the console.\n\nNow that you have Node installed, youâre ready to save the changes youâve made as a new image layer, from which you can start new containers or build new images. To do so, you will use the docker container commit command. Run the following command in a new terminal:\n\nView the layers of your image using the docker image history command:\n\nYou will see output similar to the following:\n\nNote the âAdd nodeâ comment on the top line. This layer contains the Node.js install you just made.\n\nTo prove your image has Node installed, you can start a new container using this new image:\n\nWith that, you should get a âHello againâ output in the terminal, showing Node was installed and working.\n\nNow that youâre done creating your base image, you can remove that container:\n\nBase image definition\n\nA base image is a foundation for building other images. It's possible to use any images as a base image. However, some images are intentionally created as building blocks, providing a foundation or starting point for an application.\n\nIn this example, you probably wonât deploy this node-base image, as it doesnât actually do anything yet. But itâs a base you can use for other builds.\n\n• Download and install Docker Desktop.\n• In a terminal, run the following command to start a new container:$ docker run --name=base-container -ti ubuntu Once the image has been downloaded and the container has started, you should see a new shell prompt. This is running inside your container. It will look similar to the following (the container ID will vary):root@d8c5ca119fcd:/#\n• Inside the container, run the following command to install Node.js:$ apt update && apt install -y nodejs When this command runs, it downloads and installs Node inside the container. In the context of the union filesystem, these filesystem changes occur within the directory unique to this container.\n• Validate if Node is installed by running the following command:$ node -e 'console.log(\"Hello world!\")' You should then see a âHello world!â appear in the console.\n• Now that you have Node installed, youâre ready to save the changes youâve made as a new image layer, from which you can start new containers or build new images. To do so, you will use the docker container commit command. Run the following command in a new terminal:$ docker container commit -m \"Add node\" base-container node-base\n• View the layers of your image using the docker image history command:$ docker image history node-base You will see output similar to the following:IMAGE CREATED CREATED BY SIZE COMMENT d5c1fca2cdc4 10 seconds ago /bin/bash 126MB Add node 2b7cc08dcdbb 5 weeks ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B <missing> 5 weeks ago /bin/sh -c #(nop) ADD file:07cdbabf782942af0â¦ 69.2MB <missing> 5 weeks ago /bin/sh -c #(nop) LABEL org.opencontainers.â¦ 0B <missing> 5 weeks ago /bin/sh -c #(nop) LABEL org.opencontainers.â¦ 0B <missing> 5 weeks ago /bin/sh -c #(nop) ARG LAUNCHPAD_BUILD_ARCH 0B <missing> 5 weeks ago /bin/sh -c #(nop) ARG RELEASE 0B Note the âAdd nodeâ comment on the top line. This layer contains the Node.js install you just made.\n• To prove your image has Node installed, you can start a new container using this new image:$ docker run node-base node -e \"console.log('Hello again')\" With that, you should get a âHello againâ output in the terminal, showing Node was installed and working.\n• Now that youâre done creating your base image, you can remove that container:$ docker rm -f base-container",
        "code_examples": [
          "```\nroot@d8c5ca119fcd:/#\n```",
          "```\nIMAGE          CREATED          CREATED BY                                      SIZE      COMMENTd5c1fca2cdc4   10 seconds ago   /bin/bash                                       126MB     Add node2b7cc08dcdbb   5 weeks ago      /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B<missing>      5 weeks ago      /bin/sh -c #(nop) ADD file:07cdbabf782942af0â¦   69.2MB<missing>      5 weeks ago      /bin/sh -c #(nop)  LABEL org.opencontainers.â¦   0B<missing>      5 weeks ago      /bin/sh -c #(nop)  LABEL org.opencontainers.â¦   0B<missing>      5 weeks ago      /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B<missing>      5 weeks ago      /bin/sh -c #(nop)  ARG RELEASE                  0B\n```"
        ],
        "usage_examples": [
          "```\n$docker run --name=base-container -ti ubuntu\n```",
          "```\n$apt update&&apt install -y nodejs\n```",
          "```\n$node -e'console.log(\"Hello world!\")'\n```",
          "```\n$docker container commit -m\"Add node\"base-container node-base\n```",
          "```\n$docker imagehistorynode-base\n```",
          "```\n$docker run node-base node -e\"console.log('Hello again')\"\n```",
          "```\n$docker rm -f base-container\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 4324
        }
      },
      {
        "header": "Build an app image",
        "content": "Now that you have a base image, you can extend that image to build additional images.\n\nStart a new container using the newly created node-base image:\n\nInside of this container, run the following command to create a Node program:\n\nTo run this Node program, you can use the following command and see the message printed on the screen:\n\nIn another terminal, run the following command to save this containerâs changes as a new image:\n\nThis command not only creates a new image named sample-app, but also adds additional configuration to the image to set the default command when starting a container. In this case, you are setting it to automatically run node app.js.\n\nIn a terminal outside of the container, run the following command to view the updated layers:\n\nYouâll then see output that looks like the following. Note the top layer comment has âAdd appâ and the next layer has âAdd nodeâ:\n\nFinally, start a new container using the brand new image. Since you specified the default command, you can use the following command:\n\nYou should see your greeting appear in the terminal, coming from your Node program.\n\nNow that youâre done with your containers, you can remove them using the following command:\n\n• Start a new container using the newly created node-base image:$ docker run --name=app-container -ti node-base\n• Inside of this container, run the following command to create a Node program:$ echo 'console.log(\"Hello from an app\")' > app.js To run this Node program, you can use the following command and see the message printed on the screen:$ node app.js\n• In another terminal, run the following command to save this containerâs changes as a new image:$ docker container commit -c \"CMD node app.js\" -m \"Add app\" app-container sample-app This command not only creates a new image named sample-app, but also adds additional configuration to the image to set the default command when starting a container. In this case, you are setting it to automatically run node app.js.\n• In a terminal outside of the container, run the following command to view the updated layers:$ docker image history sample-app Youâll then see output that looks like the following. Note the top layer comment has âAdd appâ and the next layer has âAdd nodeâ:IMAGE CREATED CREATED BY SIZE COMMENT c1502e2ec875 About a minute ago /bin/bash 33B Add app 5310da79c50a 4 minutes ago /bin/bash 126MB Add node 2b7cc08dcdbb 5 weeks ago /bin/sh -c #(nop) CMD [\"/bin/bash\"] 0B <missing> 5 weeks ago /bin/sh -c #(nop) ADD file:07cdbabf782942af0â¦ 69.2MB <missing> 5 weeks ago /bin/sh -c #(nop) LABEL org.opencontainers.â¦ 0B <missing> 5 weeks ago /bin/sh -c #(nop) LABEL org.opencontainers.â¦ 0B <missing> 5 weeks ago /bin/sh -c #(nop) ARG LAUNCHPAD_BUILD_ARCH 0B <missing> 5 weeks ago /bin/sh -c #(nop) ARG RELEASE 0B\n• Finally, start a new container using the brand new image. Since you specified the default command, you can use the following command:$ docker run sample-app You should see your greeting appear in the terminal, coming from your Node program.\n• Now that youâre done with your containers, you can remove them using the following command:$ docker rm -f app-container",
        "code_examples": [
          "```\nIMAGE          CREATED              CREATED BY                                      SIZE      COMMENTc1502e2ec875   About a minute ago   /bin/bash                                       33B       Add app5310da79c50a   4 minutes ago        /bin/bash                                       126MB     Add node2b7cc08dcdbb   5 weeks ago          /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B<missing>      5 weeks ago          /bin/sh -c #(nop) ADD file:07cdbabf782942af0â¦   69.2MB<missing>      5 weeks ago          /bin/sh -c #(nop)  LABEL org.opencontainers.â¦   0B<missing>      5 weeks ago          /bin/sh -c #(nop)  LABEL org.opencontainers.â¦   0B<missing>      5 weeks ago          /bin/sh -c #(nop)  ARG LAUNCHPAD_BUILD_ARCH     0B<missing>      5 weeks ago          /bin/sh -c #(nop)  ARG RELEASE                  0B\n```"
        ],
        "usage_examples": [
          "```\n$docker run --name=app-container -ti node-base\n```",
          "```\n$echo'console.log(\"Hello from an app\")'> app.js\n```",
          "```\n$node app.js\n```",
          "```\n$docker container commit -c\"CMD node app.js\"-m\"Add app\"app-container sample-app\n```",
          "```\n$docker imagehistorysample-app\n```",
          "```\n$docker run sample-app\n```",
          "```\n$docker rm -f app-container\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 3179
        }
      },
      {
        "header": "Additional resources",
        "content": "If youâd like to dive deeper into the things you learned, check out the following resources:\n\n• docker image history\n• docker container commit",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 144
        }
      },
      {
        "header": "Next steps",
        "content": "As hinted earlier, most image builds donât use docker container commit. Instead, youâll use a Dockerfile which automates these steps for you.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 145
        }
      }
    ],
    "url": "https://docs.docker.com/guides/docker-concepts/building-images/understanding-image-layers/",
    "doc_type": "docker",
    "total_sections": 8
  },
  {
    "title": "Writing a Dockerfile",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference",
    "sections": [
      {
        "header": "Explanation",
        "content": "A Dockerfile is a text-based document that's used to create a container image. It provides instructions to the image builder on the commands to run, files to copy, startup command, and more.\n\nAs an example, the following Dockerfile would produce a ready-to-run Python application:",
        "code_examples": [],
        "usage_examples": [
          "```\nFROMpython:3.13WORKDIR/usr/local/app# Install the application dependenciesCOPYrequirements.txt ./RUNpip install --no-cache-dir -r requirements.txt# Copy in the source codeCOPYsrc ./srcEXPOSE8080# Setup an app user so the container doesn't run as the root userRUNuseradd appUSERappCMD[\"uvicorn\",\"app.main:app\",\"--host\",\"0.0.0.0\",\"--port\",\"8080\"]\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 280
        }
      },
      {
        "header": "Common instructions",
        "content": "Some of the most common instructions in a Dockerfile include:\n\nTo read through all of the instructions or go into greater detail, check out the Dockerfile reference.\n\n• FROM <image> - this specifies the base image that the build will extend.\n• WORKDIR <path> - this instruction specifies the \"working directory\" or the path in the image where files will be copied and commands will be executed.\n• COPY <host-path> <image-path> - this instruction tells the builder to copy files from the host and put them into the container image.\n• RUN <command> - this instruction tells the builder to run the specified command.\n• ENV <name> <value> - this instruction sets an environment variable that a running container will use.\n• EXPOSE <port-number> - this instruction sets configuration on the image that indicates a port the image would like to expose.\n• USER <user-or-uid> - this instruction sets the default user for all subsequent instructions.\n• CMD [\"<command>\", \"<arg1>\"] - this instruction sets the default command a container using this image will run.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1053
        }
      },
      {
        "header": "Try it out",
        "content": "Just as you saw with the previous example, a Dockerfile typically follows these steps:\n\nIn this quick hands-on guide, you'll write a Dockerfile that builds a simple Node.js application. If you're not familiar with JavaScript-based applications, don't worry. It isn't necessary for following along with this guide.\n\n• Determine your base image\n• Install application dependencies\n• Copy in any relevant source code and/or binaries\n• Configure the final image",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 456
        }
      },
      {
        "header": "Set up",
        "content": "Download this ZIP file and extract the contents into a directory on your machine.\n\nIf you'd rather not download a ZIP file, clone the https://github.com/docker/getting-started-todo-app project and checkout the build-image-from-scratch branch.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 242
        }
      },
      {
        "header": "Creating the Dockerfile",
        "content": "Now that you have the project, youâre ready to create the Dockerfile.\n\nDownload and install Docker Desktop.\n\nExamine the project.\n\nExplore the contents of getting-started-todo-app/app/. You'll notice that a Dockerfile already exists. It is a simple text file that you can open in any text or code editor.\n\nDelete the existing Dockerfile.\n\nFor this exercise, you'll pretend you're starting from scratch and will create a new Dockerfile.\n\nCreate a file named Dockerfile in the getting-started-todo-app/app/ folder.\n\nDockerfile file extensions\n\nIt's important to note that the Dockerfile has no file extension. Some editors will automatically add an extension to the file (or complain it doesn't have one).\n\nIn the Dockerfile, define your base image by adding the following line:\n\nNow, define the working directory by using the WORKDIR instruction. This will specify where future commands will run and the directory files will be copied inside the container image.\n\nCopy all of the files from your project on your machine into the container image by using the COPY instruction:\n\nInstall the app's dependencies by using the yarn CLI and package manager. To do so, run a command using the RUN instruction:\n\nFinally, specify the default command to run by using the CMD instruction:\n\nAnd with that, you should have the following Dockerfile:\n\nThis Dockerfile isn't production-ready yet\n\nIt's important to note that this Dockerfile is not following all of the best practices yet (by design). It will build the app, but the builds won't be as fast, or the images as secure, as they could be.\n\nKeep reading to learn more about how to make the image maximize the build cache, run as a non-root user, and multi-stage builds.\n\nContainerize new projects quickly with docker init\n\nThe docker init command will analyze your project and quickly create a Dockerfile, a compose.yaml, and a .dockerignore, helping you get up and going. Since you're learning about Dockerfiles specifically here, you won't use it now. But, learn more about it here.\n\n• Download and install Docker Desktop.\n• Examine the project.Explore the contents of getting-started-todo-app/app/. You'll notice that a Dockerfile already exists. It is a simple text file that you can open in any text or code editor.\n• Delete the existing Dockerfile.For this exercise, you'll pretend you're starting from scratch and will create a new Dockerfile.\n• Create a file named Dockerfile in the getting-started-todo-app/app/ folder.Dockerfile file extensionsIt's important to note that the Dockerfile has no file extension. Some editors will automatically add an extension to the file (or complain it doesn't have one).\n• In the Dockerfile, define your base image by adding the following line:FROM node:22-alpine\n• Now, define the working directory by using the WORKDIR instruction. This will specify where future commands will run and the directory files will be copied inside the container image.WORKDIR /app\n• Copy all of the files from your project on your machine into the container image by using the COPY instruction:COPY . .\n• Install the app's dependencies by using the yarn CLI and package manager. To do so, run a command using the RUN instruction:RUN yarn install --production\n• Finally, specify the default command to run by using the CMD instruction:CMD [\"node\", \"./src/index.js\"]And with that, you should have the following Dockerfile:FROM node:22-alpineWORKDIR /appCOPY . .RUN yarn install --productionCMD [\"node\", \"./src/index.js\"]",
        "code_examples": [
          "```\nFROMnode:22-alpine\n```",
          "```\nWORKDIR/app\n```",
          "```\nCOPY. .\n```",
          "```\nRUNyarn install --production\n```",
          "```\nCMD[\"node\",\"./src/index.js\"]\n```",
          "```\nFROMnode:22-alpineWORKDIR/appCOPY. .RUNyarn install --productionCMD[\"node\",\"./src/index.js\"]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 3488
        }
      },
      {
        "header": "Additional resources",
        "content": "To learn more about writing a Dockerfile, visit the following resources:\n\n• Dockerfile reference\n• Dockerfile best practices\n• Base images\n• Getting started with Docker Init",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 173
        }
      },
      {
        "header": "Next steps",
        "content": "Now that you have created a Dockerfile and learned the basics, it's time to learn about building, tagging, and pushing the images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 130
        }
      }
    ],
    "url": "https://docs.docker.com/guides/docker-concepts/building-images/writing-a-dockerfile/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "Publishing and exposing ports",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Get started GuidesManualsReference",
    "sections": [
      {
        "header": "Explanation",
        "content": "If you've been following the guides so far, you understand that containers provide isolated processes for each component of your application. Each component - a React frontend, a Python API, and a Postgres database - runs in its own sandbox environment, completely isolated from everything else on your host machine. This isolation is great for security and managing dependencies, but it also means you canât access them directly. For example, you canât access the web app in your browser.\n\nThatâs where port publishing comes in.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 535
        }
      },
      {
        "header": "Publishing ports",
        "content": "Publishing a port provides the ability to break through a little bit of networking isolation by setting up a forwarding rule. As an example, you can indicate that requests on your hostâs port 8080 should be forwarded to the containerâs port 80. Publishing ports happens during container creation using the -p (or --publish) flag with docker run. The syntax is:\n\nFor example, to publish the container's port 80 to host port 8080:\n\nNow, any traffic sent to port 8080 on your host machine will be forwarded to port 80 within the container.\n\nWhen a port is published, it's published to all network interfaces by default. This means any traffic that reaches your machine can access the published application. Be mindful of publishing databases or any sensitive information. Learn more about published ports here.\n\n• HOST_PORT: The port number on your host machine where you want to receive traffic\n• CONTAINER_PORT: The port number within the container that's listening for connections\n\n[Admonition] When a port is published, it's published to all network interfaces by default. This means any traffic that reaches your machine can access the published application. Be mindful of publishing databases or any sensitive information. Learn more about published ports here.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d -p HOST_PORT:CONTAINER_PORT nginx\n```",
          "```\n$docker run -d -p 8080:80 nginx\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1268
        }
      },
      {
        "header": "Publishing to ephemeral ports",
        "content": "At times, you may want to simply publish the port but donât care which host port is used. In these cases, you can let Docker pick the port for you. To do so, simply omit the HOST_PORT configuration.\n\nFor example, the following command will publish the containerâs port 80 onto an ephemeral port on the host:\n\nOnce the container is running, using docker ps will show you the port that was chosen:\n\nIn this example, the app is exposed on the host at port 54772.",
        "code_examples": [
          "```\ndocker psCONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS          PORTS                    NAMESa527355c9c53   nginx         \"/docker-entrypoint.â¦\"   4 seconds ago    Up 3 seconds    0.0.0.0:54772->80/tcp    romantic_williamson\n```"
        ],
        "usage_examples": [
          "```\n$docker run -p80nginx\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 463
        }
      },
      {
        "header": "Publishing all ports",
        "content": "When creating a container image, the EXPOSE instruction is used to indicate the packaged application will use the specified port. These ports aren't published by default.\n\nWith the -P or --publish-all flag, you can automatically publish all exposed ports to ephemeral ports. This is quite useful when youâre trying to avoid port conflicts in development or testing environments.\n\nFor example, the following command will publish all of the exposed ports configured by the image:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -P nginx\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 479
        }
      },
      {
        "header": "Try it out",
        "content": "In this hands-on guide, you'll learn how to publish container ports using both the CLI and Docker Compose for deploying a web application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 138
        }
      },
      {
        "header": "Use the Docker CLI",
        "content": "In this step, you will run a container and publish its port using the Docker CLI.\n\nDownload and install Docker Desktop.\n\nIn a terminal, run the following command to start a new container:\n\nThe first 8080 refers to the host port. This is the port on your local machine that will be used to access the application running inside the container. The second 80 refers to the container port. This is the port that the application inside the container listens on for incoming connections. Hence, the command binds to port 8080 of the host to port 80 on the container system.\n\nVerify the published port by going to the Containers view of the Docker Desktop Dashboard.\n\nOpen the website by either selecting the link in the Port(s) column of your container or visiting http://localhost:8080 in your browser.\n\n• Download and install Docker Desktop.\n• In a terminal, run the following command to start a new container:$ docker run -d -p 8080:80 docker/welcome-to-docker The first 8080 refers to the host port. This is the port on your local machine that will be used to access the application running inside the container. The second 80 refers to the container port. This is the port that the application inside the container listens on for incoming connections. Hence, the command binds to port 8080 of the host to port 80 on the container system.\n• Verify the published port by going to the Containers view of the Docker Desktop Dashboard.\n• Open the website by either selecting the link in the Port(s) column of your container or visiting http://localhost:8080 in your browser.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d -p 8080:80 docker/welcome-to-docker\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1568
        }
      },
      {
        "header": "Use Docker Compose",
        "content": "This example will launch the same application using Docker Compose:\n\nCreate a new directory and inside that directory, create a compose.yaml file with the following contents:\n\nThe ports configuration accepts a few different forms of syntax for the port definition. In this case, youâre using the same HOST_PORT:CONTAINER_PORT used in the docker run command.\n\nOpen a terminal and navigate to the directory you created in the previous step.\n\nUse the docker compose up command to start the application.\n\nOpen your browser to http://localhost:8080.\n\n• Create a new directory and inside that directory, create a compose.yaml file with the following contents:services:app:image:docker/welcome-to-dockerports:- 8080:80The ports configuration accepts a few different forms of syntax for the port definition. In this case, youâre using the same HOST_PORT:CONTAINER_PORT used in the docker run command.\n• Open a terminal and navigate to the directory you created in the previous step.\n• Use the docker compose up command to start the application.\n• Open your browser to http://localhost:8080.",
        "code_examples": [
          "```\nservices:app:image:docker/welcome-to-dockerports:-8080:80\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1086
        }
      },
      {
        "header": "Additional resources",
        "content": "If youâd like to dive in deeper on this topic, be sure to check out the following resources:\n\n• docker container port CLI reference\n• Published ports",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 151
        }
      },
      {
        "header": "Next steps",
        "content": "Now that you understand how to publish and expose ports, you're ready to learn how to override the container defaults using the docker run command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 147
        }
      }
    ],
    "url": "https://docs.docker.com/guides/docker-concepts/running-containers/publishing-ports/",
    "doc_type": "docker",
    "total_sections": 9
  },
  {
    "title": "Python language-specific guide",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Guides Get startedManualsReferencePython language-specific guideThis guide explains how to containerize Python applications using Docker. Python 20 minutes1Containerize your app2Develop your app3Linting and typing4Automate your builds with GitHub Actions5Test your deploymentÂ« Back to all guides\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Guides Get startedManualsReferencePython language-specific guideThis gui",
    "sections": [
      {
        "header": "",
        "content": "This guide is a community contribution. Docker would like to thank Esteban Maya and Igor Aleksandrov for their contribution to this guide.\n\nThe Python language-specific guide teaches you how to containerize a Python application using Docker. In this guide, youâll learn how to:\n\nStart by containerizing an existing Python application.\n\n• Get started\n\n• Containerize and run a Python application\n• Set up a local environment to develop a Python application using containers\n• Lint, format, typing and best practices\n• Configure a CI/CD pipeline for a containerized Python application using GitHub Actions\n• Deploy your containerized Python application locally to Kubernetes to test and debug your deployment",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 708
        }
      },
      {
        "header": "Modules",
        "content": "Learn how to containerize a Python application.\n\nLearn how to develop your Python application locally.\n\nLearn how to set up linting, formatting and type checking for your Python application.\n\nLearn how to configure CI/CD using GitHub Actions for your Python application.\n\nLearn how to develop locally using Kubernetes\n\n• Containerize your appLearn how to containerize a Python application.\n• Develop your appLearn how to develop your Python application locally.\n• Linting and typingLearn how to set up linting, formatting and type checking for your Python application.\n• Automate your builds with GitHub ActionsLearn how to configure CI/CD using GitHub Actions for your Python application.\n• Test your deploymentLearn how to develop locally using Kubernetes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 757
        }
      }
    ],
    "url": "https://docs.docker.com/language/python/",
    "doc_type": "docker",
    "total_sections": 2
  },
  {
    "title": "Node.js language-specific guide",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Guides Get startedManualsReferenceNode.js language-specific guideThis guide explains how to containerize Node.js applications using Docker. JavaScript 20 minutes1Containerize your app2Develop your app3Run your tests4Configure CI/CD5Test your deploymentÂ« Back to all guides\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Guides Get startedManualsReferenceNode.js language-specific guideThis guide explains how to con",
    "sections": [
      {
        "header": "",
        "content": "The Node.js language-specific guide teaches you how to containerize a Node.js application using Docker. In this guide, youâll learn how to:\n\nStart by containerizing an existing Node.js application.\n\n• Get started\n\n• Containerize and run a Node.js application\n• Set up a local environment to develop a Node.js application using containers\n• Run tests for a Node.js application using containers\n• Configure a CI/CD pipeline for a containerized Node.js application using GitHub Actions\n• Deploy your containerized Node.js application locally to Kubernetes to test and debug your deployment",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 588
        }
      },
      {
        "header": "Modules",
        "content": "Learn how to containerize a Node.js application.\n\nLearn how to develop your Node.js application locally using containers.\n\nLearn how to run your Node.js tests in a container.\n\nLearn how to configure CI/CD using GitHub Actions for your Node.js application.\n\nLearn how to deploy locally to test and debug your Kubernetes deployment\n\n• Containerize your appLearn how to containerize a Node.js application.\n• Develop your appLearn how to develop your Node.js application locally using containers.\n• Run your testsLearn how to run your Node.js tests in a container.\n• Configure CI/CDLearn how to configure CI/CD using GitHub Actions for your Node.js application.\n• Test your deploymentLearn how to deploy locally to test and debug your Kubernetes deployment",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 752
        }
      }
    ],
    "url": "https://docs.docker.com/language/nodejs/",
    "doc_type": "docker",
    "total_sections": 2
  },
  {
    "title": "Java language-specific guide",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Guides Get startedManualsReferenceJava language-specific guideThis guide demonstrates how to containerize Java applications using Docker. Java 20 minutes1Containerize your app2Develop your app3Run your tests4Configure CI/CD5Test your deploymentÂ« Back to all guides\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Guides Get startedManualsReferenceJava language-specific guideThis guide demonstrates how to containeri",
    "sections": [
      {
        "header": "",
        "content": "The Java getting started guide teaches you how to create a containerized Spring Boot application using Docker. In this module, youâll learn how to:\n\nAfter completing the Java getting started modules, you should be able to containerize your own Java application based on the examples and instructions provided in this guide.\n\nGet started containerizing your first Java app.\n\n• Get started\n\n• Containerize and run a Spring Boot application with Maven\n• Set up a local development environment to connect a database to the container, configure a debugger, and use Compose Watch for live reload\n• Run your unit tests inside a container\n• Configure a CI/CD pipeline for your application using GitHub Actions\n• Deploy your containerized application locally to Kubernetes to test and debug your deployment",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 799
        }
      },
      {
        "header": "Modules",
        "content": "Learn how to containerize a Java application.\n\nLearn how to develop your application locally.\n\nHow to build and run your Java tests\n\nLearn how to Configure CI/CD for your Java application\n\nLearn how to develop locally using Kubernetes\n\n• Containerize your appLearn how to containerize a Java application.\n• Develop your appLearn how to develop your application locally.\n• Run your testsHow to build and run your Java tests\n• Configure CI/CDLearn how to Configure CI/CD for your Java application\n• Test your deploymentLearn how to develop locally using Kubernetes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 562
        }
      }
    ],
    "url": "https://docs.docker.com/language/java/",
    "doc_type": "docker",
    "total_sections": 2
  },
  {
    "title": "docker",
    "summary": "Depending on your Docker system configuration, you may be required to preface each docker command with sudo . To avoid having to use sudo with the docker command, your system administrator can create a Unix group called docker and add users to it. For more information about installing Docker or sudo configuration, refer to the installation instructions for your operating system.",
    "sections": [
      {
        "header": "Description",
        "content": "Depending on your Docker system configuration, you may be required to preface each docker command with sudo . To avoid having to use sudo with the docker command, your system administrator can create a Unix group called docker and add users to it.\n\nFor more information about installing Docker or sudo configuration, refer to the installation instructions for your operating system.\n\nDisplay help text\n\nTo list the help on any command just execute the command, followed by the --help option.\n\n$ docker run --help Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Create and run a new container from an image Options: --add-host value Add a custom host-to-IP mapping (host:ip) (default []) -a, --attach value Attach to STDIN, STDOUT or STDERR (default []) <...>\n\nEnvironment variables\n\nThe following list of environment variables are supported by the docker command line:\n\nVariable Description DOCKER_API_VERSION Override the negotiated API version to use for debugging (e.g. 1.19 ) DOCKER_CERT_PATH Location of your authentication keys. This variable is used both by the docker CLI and the dockerd daemon DOCKER_CONFIG The location of your client configuration files. DOCKER_CONTENT_TRUST_SERVER The URL of the Notary server to use. Defaults to the same URL as the registry. DOCKER_CONTENT_TRUST When set Docker uses notary to sign and verify images. Equates to --disable-content-trust=false for build, create, pull, push, run. DOCKER_CONTEXT Name of the docker context to use (overrides DOCKER_HOST env var and default context set with docker context use ) DOCKER_CUSTOM_HEADERS (Experimental) Configure custom HTTP headers to be sent by the client. Headers must be provided as a comma-separated list of name=value pairs. This is the equivalent to the HttpHeaders field in the configuration file. DOCKER_DEFAULT_PLATFORM Default platform for commands that take the --platform flag. DOCKER_HIDE_LEGACY_COMMANDS When set, Docker hides \"legacy\" top-level commands (such as docker rm , and docker pull ) in docker help output, and only Management commands per object-type (e.g., docker container ) are printed. This may become the default in a future release. DOCKER_HOST Daemon socket to connect to. DOCKER_TLS Enable TLS for connections made by the docker CLI (equivalent of the --tls command-line option). Set to a non-empty value to enable TLS. Note that TLS is enabled automatically if any of the other TLS options are set. DOCKER_TLS_VERIFY When set Docker uses TLS and verifies the remote. This variable is used both by the docker CLI and the dockerd daemon BUILDKIT_PROGRESS Set type of progress output ( auto , plain , tty , rawjson ) when building with BuildKit backend . Use plain to show container output (default auto ).\n\nBecause Docker is developed using Go, you can also use any environment variables used by the Go runtime. In particular, you may find these useful:\n\nVariable Description HTTP_PROXY Proxy URL for HTTP requests unless overridden by NoProxy. HTTPS_PROXY Proxy URL for HTTPS requests unless overridden by NoProxy. NO_PROXY Comma-separated values specifying hosts that should be excluded from proxying.\n\nSee the Go specification for details on these variables.\n\nOption types\n\nSingle character command line options can be combined, so rather than typing docker run -i -t --name test busybox sh , you can write docker run -it --name test busybox sh .\n\nBoolean\n\nBoolean options take the form -d=false . The value you see in the help text is the default value which is set if you do not specify that flag. If you specify a Boolean flag without a value, this will set the flag to true , irrespective of the default value.\n\nFor example, running docker run -d will set the value to true , so your container will run in \"detached\" mode, in the background.\n\nOptions which default to true (e.g., docker build --rm=true ) can only be set to the non-default value by explicitly setting them to false :\n\n$ docker build --rm = false .\n\nMulti\n\nYou can specify options like -a=[] multiple times in a single command line, for example in these commands:\n\n$ docker run -a stdin -a stdout -i -t ubuntu /bin/bash $ docker run -a stdin -a stdout -a stderr ubuntu /bin/ls\n\nSometimes, multiple options can call for a more complex value string as for -v :\n\n$ docker run -v /host:/container example/mysql\n\nNote Do not use the -t and -a stderr options together due to limitations in the pty implementation. All stderr in pty mode simply goes to stdout .\n\nStrings and Integers\n\nOptions like --name=\"\" expect a string, and they can only be specified once. Options like -c=0 expect an integer, and they can only be specified once.\n\nConfiguration files\n\nBy default, the Docker command line stores its configuration files in a directory called .docker within your $HOME directory.\n\nDocker manages most of the files in the configuration directory and you shouldn't modify them. However, you can modify the config.json file to control certain aspects of how the docker command behaves.\n\nYou can modify the docker command behavior using environment variables or command-line options. You can also use options within config.json to modify some of the same behavior. If an environment variable and the --config flag are set, the flag takes precedent over the environment variable. Command line options override environment variables and environment variables override properties you specify in a config.json file.\n\nChange the .docker directory\n\nTo specify a different directory, use the DOCKER_CONFIG environment variable or the --config command line option. If both are specified, then the --config option overrides the DOCKER_CONFIG environment variable. The example below overrides the docker ps command using a config.json file located in the ~/testconfigs/ directory.\n\n$ docker --config ~/testconfigs/ ps\n\nThis flag only applies to whatever command is being ran. For persistent configuration, you can set the DOCKER_CONFIG environment variable in your shell (e.g. ~/.profile or ~/.bashrc ). The example below sets the new directory to be HOME/newdir/.docker .\n\n$ echo export DOCKER_CONFIG = $HOME /newdir/.docker > ~/.profile\n\nDocker CLI configuration file ( config.json ) properties\n\nUse the Docker CLI configuration to customize settings for the docker CLI. The configuration file uses JSON formatting, and properties:\n\nBy default, configuration file is stored in ~/.docker/config.json . Refer to the change the .docker directory section to use a different location.\n\nWarning The configuration file and other files inside the ~/.docker configuration directory may contain sensitive information, such as authentication information for proxies or, depending on your credential store, credentials for your image registries. Review your configuration file's content before sharing with others, and prevent committing the file to version control.\n\nCustomize the default output format for commands\n\nThese fields lets you customize the default output format for some commands if no --format flag is provided.\n\nProperty Description configFormat Custom default format for docker config ls output. See docker config ls for a list of supported formatting directives. imagesFormat Custom default format for docker images / docker image ls output. See docker images for a list of supported formatting directives. networksFormat Custom default format for docker network ls output. See docker network ls for a list of supported formatting directives. nodesFormat Custom default format for docker node ls output. See docker node ls for a list of supported formatting directives. pluginsFormat Custom default format for docker plugin ls output. See docker plugin ls for a list of supported formatting directives. psFormat Custom default format for docker ps / docker container ps output. See docker ps for a list of supported formatting directives. secretFormat Custom default format for docker secret ls output. See docker secret ls for a list of supported formatting directives. serviceInspectFormat Custom default format for docker service inspect output. See docker service inspect for a list of supported formatting directives. servicesFormat Custom default format for docker service ls output. See docker service ls for a list of supported formatting directives. statsFormat Custom default format for docker stats output. See docker stats for a list of supported formatting directives. tasksFormat Custom default format for docker stack ps output. See docker stack ps for a list of supported formatting directives. volumesFormat Custom default format for docker volume ls output. See docker volume ls for a list of supported formatting directives.\n\nCustom HTTP headers\n\nThe property HttpHeaders specifies a set of headers to include in all messages sent from the Docker client to the daemon. Docker doesn't try to interpret or understand these headers; it simply puts them into the messages. Docker does not allow these headers to change any headers it sets for itself.\n\nAlternatively, use the DOCKER_CUSTOM_HEADERS environment variable , which is available in v27.1 and higher. This environment-variable is experimental, and its exact behavior may change.\n\nCredential store options\n\nThe property credsStore specifies an external binary to serve as the default credential store. When this property is set, docker login will attempt to store credentials in the binary specified by docker-credential-<value> which is visible on $PATH . If this property isn't set, credentials are stored in the auths property of the CLI configuration file. For more information, see the Credential stores section in the docker login documentation\n\nThe property credHelpers specifies a set of credential helpers to use preferentially over credsStore or auths when storing and retrieving credentials for specific registries. If this property is set, the binary docker-credential-<value> will be used when storing or retrieving credentials for a specific registry. For more information, see the Credential helpers section in the docker login documentation\n\nAutomatic proxy configuration for containers\n\nThe property proxies specifies proxy environment variables to be automatically set on containers, and set as --build-arg on containers used during docker build . A \"default\" set of proxies can be configured, and will be used for any Docker daemon that the client connects to, or a configuration per host (Docker daemon), for example, https://docker-daemon1.example.com . The following properties can be set for each environment:\n\nProperty Description httpProxy Default value of HTTP_PROXY and http_proxy for containers, and as --build-arg on docker build httpsProxy Default value of HTTPS_PROXY and https_proxy for containers, and as --build-arg on docker build ftpProxy Default value of FTP_PROXY and ftp_proxy for containers, and as --build-arg on docker build noProxy Default value of NO_PROXY and no_proxy for containers, and as --build-arg on docker build allProxy Default value of ALL_PROXY and all_proxy for containers, and as --build-arg on docker build\n\nThese settings are used to configure proxy settings for containers only, and not used as proxy settings for the docker CLI or the dockerd daemon. Refer to the environment variables and HTTP/HTTPS proxy sections for configuring proxy settings for the CLI and daemon.\n\nWarning Proxy settings may contain sensitive information (for example, if the proxy requires authentication). Environment variables are stored as plain text in the container's configuration, and as such can be inspected through the remote API or committed to an image when using docker commit .\n\nDefault key-sequence to detach from containers\n\nOnce attached to a container, users detach from it and leave it running using the using CTRL-p CTRL-q key sequence. This detach key sequence is customizable using the detachKeys property. Specify a <sequence> value for the property. The format of the <sequence> is a comma-separated list of either a letter [a-Z], or the ctrl- combined with any of the following:\n\na-z (a single lowercase alpha character ) @ (at sign) [ (left bracket) \\\\ (two backward slashes) _ (underscore) ^ (caret)\n\nYour customization applies to all containers started in with your Docker client. Users can override your custom or the default key sequence on a per-container basis. To do this, the user specifies the --detach-keys flag with the docker attach , docker exec , docker run or docker start command.\n\nCLI plugin options\n\nThe property plugins contains settings specific to CLI plugins. The key is the plugin name, while the value is a further map of options, which are specific to that plugin.\n\nSample configuration file\n\nFollowing is a sample config.json file to illustrate the format used for various fields:\n\n{ \"HttpHeaders\" : { \"MyHeader\" : \"MyValue\" }, \"psFormat\" : \"table {{.ID}}\\\\t{{.Image}}\\\\t{{.Command}}\\\\t{{.Labels}}\" , \"imagesFormat\" : \"table {{.ID}}\\\\t{{.Repository}}\\\\t{{.Tag}}\\\\t{{.CreatedAt}}\" , \"pluginsFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.Enabled}}\" , \"statsFormat\" : \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\" , \"servicesFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.Mode}}\" , \"secretFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.CreatedAt}}\\t{{.UpdatedAt}}\" , \"configFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.CreatedAt}}\\t{{.UpdatedAt}}\" , \"serviceInspectFormat\" : \"pretty\" , \"nodesFormat\" : \"table {{.ID}}\\t{{.Hostname}}\\t{{.Availability}}\" , \"detachKeys\" : \"ctrl-e,e\" , \"credsStore\" : \"secretservice\" , \"credHelpers\" : { \"awesomereg.example.org\" : \"hip-star\" , \"unicorn.example.com\" : \"vcbait\" }, \"plugins\" : { \"plugin1\" : { \"option\" : \"value\" }, \"plugin2\" : { \"anotheroption\" : \"anothervalue\" , \"athirdoption\" : \"athirdvalue\" } }, \"proxies\" : { \"default\" : { \"httpProxy\" : \"http://user:pass@example.com:3128\" , \"httpsProxy\" : \"https://my-proxy.example.com:3129\" , \"noProxy\" : \"intra.mycorp.example.com\" , \"ftpProxy\" : \"http://user:pass@example.com:3128\" , \"allProxy\" : \"socks://example.com:1234\" }, \"https://manager1.mycorp.example.com:2377\" : { \"httpProxy\" : \"http://user:pass@example.com:3128\" , \"httpsProxy\" : \"https://my-proxy.example.com:3129\" } } }\n\nExperimental features\n\nExperimental features provide early access to future product functionality. These features are intended for testing and feedback, and they may change between releases without warning or can be removed from a future release.\n\nStarting with Docker 20.10, experimental CLI features are enabled by default, and require no configuration to enable them.\n\nNotary\n\nIf using your own notary server and a self-signed certificate or an internal Certificate Authority, you need to place the certificate at tls/<registry_url>/ca.crt in your Docker config directory.\n\nAlternatively you can trust the certificate globally by adding it to your system's list of root Certificate Authorities.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 35,
          "content_length": 14798
        }
      },
      {
        "header": "Display help text",
        "content": "To list the help on any command just execute the command, followed by the --help option.\n\n$ docker run --help Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...] Create and run a new container from an image Options: --add-host value Add a custom host-to-IP mapping (host:ip) (default []) -a, --attach value Attach to STDIN, STDOUT or STDERR (default []) <...>",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 360
        }
      },
      {
        "header": "Environment variables",
        "content": "The following list of environment variables are supported by the docker command line:\n\nVariable Description DOCKER_API_VERSION Override the negotiated API version to use for debugging (e.g. 1.19 ) DOCKER_CERT_PATH Location of your authentication keys. This variable is used both by the docker CLI and the dockerd daemon DOCKER_CONFIG The location of your client configuration files. DOCKER_CONTENT_TRUST_SERVER The URL of the Notary server to use. Defaults to the same URL as the registry. DOCKER_CONTENT_TRUST When set Docker uses notary to sign and verify images. Equates to --disable-content-trust=false for build, create, pull, push, run. DOCKER_CONTEXT Name of the docker context to use (overrides DOCKER_HOST env var and default context set with docker context use ) DOCKER_CUSTOM_HEADERS (Experimental) Configure custom HTTP headers to be sent by the client. Headers must be provided as a comma-separated list of name=value pairs. This is the equivalent to the HttpHeaders field in the configuration file. DOCKER_DEFAULT_PLATFORM Default platform for commands that take the --platform flag. DOCKER_HIDE_LEGACY_COMMANDS When set, Docker hides \"legacy\" top-level commands (such as docker rm , and docker pull ) in docker help output, and only Management commands per object-type (e.g., docker container ) are printed. This may become the default in a future release. DOCKER_HOST Daemon socket to connect to. DOCKER_TLS Enable TLS for connections made by the docker CLI (equivalent of the --tls command-line option). Set to a non-empty value to enable TLS. Note that TLS is enabled automatically if any of the other TLS options are set. DOCKER_TLS_VERIFY When set Docker uses TLS and verifies the remote. This variable is used both by the docker CLI and the dockerd daemon BUILDKIT_PROGRESS Set type of progress output ( auto , plain , tty , rawjson ) when building with BuildKit backend . Use plain to show container output (default auto ).\n\nBecause Docker is developed using Go, you can also use any environment variables used by the Go runtime. In particular, you may find these useful:\n\nVariable Description HTTP_PROXY Proxy URL for HTTP requests unless overridden by NoProxy. HTTPS_PROXY Proxy URL for HTTPS requests unless overridden by NoProxy. NO_PROXY Comma-separated values specifying hosts that should be excluded from proxying.\n\nSee the Go specification for details on these variables.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 2401
        }
      },
      {
        "header": "Option types",
        "content": "Single character command line options can be combined, so rather than typing docker run -i -t --name test busybox sh , you can write docker run -it --name test busybox sh .\n\nBoolean\n\nBoolean options take the form -d=false . The value you see in the help text is the default value which is set if you do not specify that flag. If you specify a Boolean flag without a value, this will set the flag to true , irrespective of the default value.\n\nFor example, running docker run -d will set the value to true , so your container will run in \"detached\" mode, in the background.\n\nOptions which default to true (e.g., docker build --rm=true ) can only be set to the non-default value by explicitly setting them to false :\n\n$ docker build --rm = false .\n\nMulti\n\nYou can specify options like -a=[] multiple times in a single command line, for example in these commands:\n\n$ docker run -a stdin -a stdout -i -t ubuntu /bin/bash $ docker run -a stdin -a stdout -a stderr ubuntu /bin/ls\n\nSometimes, multiple options can call for a more complex value string as for -v :\n\n$ docker run -v /host:/container example/mysql\n\nNote Do not use the -t and -a stderr options together due to limitations in the pty implementation. All stderr in pty mode simply goes to stdout .\n\nStrings and Integers\n\nOptions like --name=\"\" expect a string, and they can only be specified once. Options like -c=0 expect an integer, and they can only be specified once.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1424
        }
      },
      {
        "header": "Boolean",
        "content": "Boolean options take the form -d=false . The value you see in the help text is the default value which is set if you do not specify that flag. If you specify a Boolean flag without a value, this will set the flag to true , irrespective of the default value.\n\nFor example, running docker run -d will set the value to true , so your container will run in \"detached\" mode, in the background.\n\nOptions which default to true (e.g., docker build --rm=true ) can only be set to the non-default value by explicitly setting them to false :\n\n$ docker build --rm = false .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 561
        }
      },
      {
        "header": "Multi",
        "content": "You can specify options like -a=[] multiple times in a single command line, for example in these commands:\n\n$ docker run -a stdin -a stdout -i -t ubuntu /bin/bash $ docker run -a stdin -a stdout -a stderr ubuntu /bin/ls\n\nSometimes, multiple options can call for a more complex value string as for -v :\n\n$ docker run -v /host:/container example/mysql\n\nNote Do not use the -t and -a stderr options together due to limitations in the pty implementation. All stderr in pty mode simply goes to stdout .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 497
        }
      },
      {
        "header": "Strings and Integers",
        "content": "Options like --name=\"\" expect a string, and they can only be specified once. Options like -c=0 expect an integer, and they can only be specified once.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 150
        }
      },
      {
        "header": "Configuration files",
        "content": "By default, the Docker command line stores its configuration files in a directory called .docker within your $HOME directory.\n\nDocker manages most of the files in the configuration directory and you shouldn't modify them. However, you can modify the config.json file to control certain aspects of how the docker command behaves.\n\nYou can modify the docker command behavior using environment variables or command-line options. You can also use options within config.json to modify some of the same behavior. If an environment variable and the --config flag are set, the flag takes precedent over the environment variable. Command line options override environment variables and environment variables override properties you specify in a config.json file.\n\nChange the .docker directory\n\nTo specify a different directory, use the DOCKER_CONFIG environment variable or the --config command line option. If both are specified, then the --config option overrides the DOCKER_CONFIG environment variable. The example below overrides the docker ps command using a config.json file located in the ~/testconfigs/ directory.\n\n$ docker --config ~/testconfigs/ ps\n\nThis flag only applies to whatever command is being ran. For persistent configuration, you can set the DOCKER_CONFIG environment variable in your shell (e.g. ~/.profile or ~/.bashrc ). The example below sets the new directory to be HOME/newdir/.docker .\n\n$ echo export DOCKER_CONFIG = $HOME /newdir/.docker > ~/.profile",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1470
        }
      },
      {
        "header": "Change the .docker directory",
        "content": "To specify a different directory, use the DOCKER_CONFIG environment variable or the --config command line option. If both are specified, then the --config option overrides the DOCKER_CONFIG environment variable. The example below overrides the docker ps command using a config.json file located in the ~/testconfigs/ directory.\n\n$ docker --config ~/testconfigs/ ps\n\nThis flag only applies to whatever command is being ran. For persistent configuration, you can set the DOCKER_CONFIG environment variable in your shell (e.g. ~/.profile or ~/.bashrc ). The example below sets the new directory to be HOME/newdir/.docker .\n\n$ echo export DOCKER_CONFIG = $HOME /newdir/.docker > ~/.profile",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 685
        }
      },
      {
        "header": "Docker CLI configuration file ( config.json ) properties",
        "content": "Use the Docker CLI configuration to customize settings for the docker CLI. The configuration file uses JSON formatting, and properties:\n\nBy default, configuration file is stored in ~/.docker/config.json . Refer to the change the .docker directory section to use a different location.\n\nWarning The configuration file and other files inside the ~/.docker configuration directory may contain sensitive information, such as authentication information for proxies or, depending on your credential store, credentials for your image registries. Review your configuration file's content before sharing with others, and prevent committing the file to version control.\n\nCustomize the default output format for commands\n\nThese fields lets you customize the default output format for some commands if no --format flag is provided.\n\nProperty Description configFormat Custom default format for docker config ls output. See docker config ls for a list of supported formatting directives. imagesFormat Custom default format for docker images / docker image ls output. See docker images for a list of supported formatting directives. networksFormat Custom default format for docker network ls output. See docker network ls for a list of supported formatting directives. nodesFormat Custom default format for docker node ls output. See docker node ls for a list of supported formatting directives. pluginsFormat Custom default format for docker plugin ls output. See docker plugin ls for a list of supported formatting directives. psFormat Custom default format for docker ps / docker container ps output. See docker ps for a list of supported formatting directives. secretFormat Custom default format for docker secret ls output. See docker secret ls for a list of supported formatting directives. serviceInspectFormat Custom default format for docker service inspect output. See docker service inspect for a list of supported formatting directives. servicesFormat Custom default format for docker service ls output. See docker service ls for a list of supported formatting directives. statsFormat Custom default format for docker stats output. See docker stats for a list of supported formatting directives. tasksFormat Custom default format for docker stack ps output. See docker stack ps for a list of supported formatting directives. volumesFormat Custom default format for docker volume ls output. See docker volume ls for a list of supported formatting directives.\n\nCustom HTTP headers\n\nThe property HttpHeaders specifies a set of headers to include in all messages sent from the Docker client to the daemon. Docker doesn't try to interpret or understand these headers; it simply puts them into the messages. Docker does not allow these headers to change any headers it sets for itself.\n\nAlternatively, use the DOCKER_CUSTOM_HEADERS environment variable , which is available in v27.1 and higher. This environment-variable is experimental, and its exact behavior may change.\n\nCredential store options\n\nThe property credsStore specifies an external binary to serve as the default credential store. When this property is set, docker login will attempt to store credentials in the binary specified by docker-credential-<value> which is visible on $PATH . If this property isn't set, credentials are stored in the auths property of the CLI configuration file. For more information, see the Credential stores section in the docker login documentation\n\nThe property credHelpers specifies a set of credential helpers to use preferentially over credsStore or auths when storing and retrieving credentials for specific registries. If this property is set, the binary docker-credential-<value> will be used when storing or retrieving credentials for a specific registry. For more information, see the Credential helpers section in the docker login documentation\n\nAutomatic proxy configuration for containers\n\nThe property proxies specifies proxy environment variables to be automatically set on containers, and set as --build-arg on containers used during docker build . A \"default\" set of proxies can be configured, and will be used for any Docker daemon that the client connects to, or a configuration per host (Docker daemon), for example, https://docker-daemon1.example.com . The following properties can be set for each environment:\n\nProperty Description httpProxy Default value of HTTP_PROXY and http_proxy for containers, and as --build-arg on docker build httpsProxy Default value of HTTPS_PROXY and https_proxy for containers, and as --build-arg on docker build ftpProxy Default value of FTP_PROXY and ftp_proxy for containers, and as --build-arg on docker build noProxy Default value of NO_PROXY and no_proxy for containers, and as --build-arg on docker build allProxy Default value of ALL_PROXY and all_proxy for containers, and as --build-arg on docker build\n\nThese settings are used to configure proxy settings for containers only, and not used as proxy settings for the docker CLI or the dockerd daemon. Refer to the environment variables and HTTP/HTTPS proxy sections for configuring proxy settings for the CLI and daemon.\n\nWarning Proxy settings may contain sensitive information (for example, if the proxy requires authentication). Environment variables are stored as plain text in the container's configuration, and as such can be inspected through the remote API or committed to an image when using docker commit .\n\nDefault key-sequence to detach from containers\n\nOnce attached to a container, users detach from it and leave it running using the using CTRL-p CTRL-q key sequence. This detach key sequence is customizable using the detachKeys property. Specify a <sequence> value for the property. The format of the <sequence> is a comma-separated list of either a letter [a-Z], or the ctrl- combined with any of the following:\n\na-z (a single lowercase alpha character ) @ (at sign) [ (left bracket) \\\\ (two backward slashes) _ (underscore) ^ (caret)\n\nYour customization applies to all containers started in with your Docker client. Users can override your custom or the default key sequence on a per-container basis. To do this, the user specifies the --detach-keys flag with the docker attach , docker exec , docker run or docker start command.\n\nCLI plugin options\n\nThe property plugins contains settings specific to CLI plugins. The key is the plugin name, while the value is a further map of options, which are specific to that plugin.\n\nSample configuration file\n\nFollowing is a sample config.json file to illustrate the format used for various fields:\n\n{ \"HttpHeaders\" : { \"MyHeader\" : \"MyValue\" }, \"psFormat\" : \"table {{.ID}}\\\\t{{.Image}}\\\\t{{.Command}}\\\\t{{.Labels}}\" , \"imagesFormat\" : \"table {{.ID}}\\\\t{{.Repository}}\\\\t{{.Tag}}\\\\t{{.CreatedAt}}\" , \"pluginsFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.Enabled}}\" , \"statsFormat\" : \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\" , \"servicesFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.Mode}}\" , \"secretFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.CreatedAt}}\\t{{.UpdatedAt}}\" , \"configFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.CreatedAt}}\\t{{.UpdatedAt}}\" , \"serviceInspectFormat\" : \"pretty\" , \"nodesFormat\" : \"table {{.ID}}\\t{{.Hostname}}\\t{{.Availability}}\" , \"detachKeys\" : \"ctrl-e,e\" , \"credsStore\" : \"secretservice\" , \"credHelpers\" : { \"awesomereg.example.org\" : \"hip-star\" , \"unicorn.example.com\" : \"vcbait\" }, \"plugins\" : { \"plugin1\" : { \"option\" : \"value\" }, \"plugin2\" : { \"anotheroption\" : \"anothervalue\" , \"athirdoption\" : \"athirdvalue\" } }, \"proxies\" : { \"default\" : { \"httpProxy\" : \"http://user:pass@example.com:3128\" , \"httpsProxy\" : \"https://my-proxy.example.com:3129\" , \"noProxy\" : \"intra.mycorp.example.com\" , \"ftpProxy\" : \"http://user:pass@example.com:3128\" , \"allProxy\" : \"socks://example.com:1234\" }, \"https://manager1.mycorp.example.com:2377\" : { \"httpProxy\" : \"http://user:pass@example.com:3128\" , \"httpsProxy\" : \"https://my-proxy.example.com:3129\" } } }\n\nExperimental features\n\nExperimental features provide early access to future product functionality. These features are intended for testing and feedback, and they may change between releases without warning or can be removed from a future release.\n\nStarting with Docker 20.10, experimental CLI features are enabled by default, and require no configuration to enable them.\n\nNotary\n\nIf using your own notary server and a self-signed certificate or an internal Certificate Authority, you need to place the certificate at tls/<registry_url>/ca.crt in your Docker config directory.\n\nAlternatively you can trust the certificate globally by adding it to your system's list of root Certificate Authorities.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 8616
        }
      },
      {
        "header": "Customize the default output format for commands",
        "content": "These fields lets you customize the default output format for some commands if no --format flag is provided.\n\nProperty Description configFormat Custom default format for docker config ls output. See docker config ls for a list of supported formatting directives. imagesFormat Custom default format for docker images / docker image ls output. See docker images for a list of supported formatting directives. networksFormat Custom default format for docker network ls output. See docker network ls for a list of supported formatting directives. nodesFormat Custom default format for docker node ls output. See docker node ls for a list of supported formatting directives. pluginsFormat Custom default format for docker plugin ls output. See docker plugin ls for a list of supported formatting directives. psFormat Custom default format for docker ps / docker container ps output. See docker ps for a list of supported formatting directives. secretFormat Custom default format for docker secret ls output. See docker secret ls for a list of supported formatting directives. serviceInspectFormat Custom default format for docker service inspect output. See docker service inspect for a list of supported formatting directives. servicesFormat Custom default format for docker service ls output. See docker service ls for a list of supported formatting directives. statsFormat Custom default format for docker stats output. See docker stats for a list of supported formatting directives. tasksFormat Custom default format for docker stack ps output. See docker stack ps for a list of supported formatting directives. volumesFormat Custom default format for docker volume ls output. See docker volume ls for a list of supported formatting directives.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1743
        }
      },
      {
        "header": "Custom HTTP headers",
        "content": "The property HttpHeaders specifies a set of headers to include in all messages sent from the Docker client to the daemon. Docker doesn't try to interpret or understand these headers; it simply puts them into the messages. Docker does not allow these headers to change any headers it sets for itself.\n\nAlternatively, use the DOCKER_CUSTOM_HEADERS environment variable , which is available in v27.1 and higher. This environment-variable is experimental, and its exact behavior may change.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 486
        }
      },
      {
        "header": "Credential store options",
        "content": "The property credsStore specifies an external binary to serve as the default credential store. When this property is set, docker login will attempt to store credentials in the binary specified by docker-credential-<value> which is visible on $PATH . If this property isn't set, credentials are stored in the auths property of the CLI configuration file. For more information, see the Credential stores section in the docker login documentation\n\nThe property credHelpers specifies a set of credential helpers to use preferentially over credsStore or auths when storing and retrieving credentials for specific registries. If this property is set, the binary docker-credential-<value> will be used when storing or retrieving credentials for a specific registry. For more information, see the Credential helpers section in the docker login documentation",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 849
        }
      },
      {
        "header": "Automatic proxy configuration for containers",
        "content": "The property proxies specifies proxy environment variables to be automatically set on containers, and set as --build-arg on containers used during docker build . A \"default\" set of proxies can be configured, and will be used for any Docker daemon that the client connects to, or a configuration per host (Docker daemon), for example, https://docker-daemon1.example.com . The following properties can be set for each environment:\n\nProperty Description httpProxy Default value of HTTP_PROXY and http_proxy for containers, and as --build-arg on docker build httpsProxy Default value of HTTPS_PROXY and https_proxy for containers, and as --build-arg on docker build ftpProxy Default value of FTP_PROXY and ftp_proxy for containers, and as --build-arg on docker build noProxy Default value of NO_PROXY and no_proxy for containers, and as --build-arg on docker build allProxy Default value of ALL_PROXY and all_proxy for containers, and as --build-arg on docker build\n\nThese settings are used to configure proxy settings for containers only, and not used as proxy settings for the docker CLI or the dockerd daemon. Refer to the environment variables and HTTP/HTTPS proxy sections for configuring proxy settings for the CLI and daemon.\n\nWarning Proxy settings may contain sensitive information (for example, if the proxy requires authentication). Environment variables are stored as plain text in the container's configuration, and as such can be inspected through the remote API or committed to an image when using docker commit .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1524
        }
      },
      {
        "header": "Default key-sequence to detach from containers",
        "content": "Once attached to a container, users detach from it and leave it running using the using CTRL-p CTRL-q key sequence. This detach key sequence is customizable using the detachKeys property. Specify a <sequence> value for the property. The format of the <sequence> is a comma-separated list of either a letter [a-Z], or the ctrl- combined with any of the following:\n\na-z (a single lowercase alpha character ) @ (at sign) [ (left bracket) \\\\ (two backward slashes) _ (underscore) ^ (caret)\n\nYour customization applies to all containers started in with your Docker client. Users can override your custom or the default key sequence on a per-container basis. To do this, the user specifies the --detach-keys flag with the docker attach , docker exec , docker run or docker start command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 781
        }
      },
      {
        "header": "CLI plugin options",
        "content": "The property plugins contains settings specific to CLI plugins. The key is the plugin name, while the value is a further map of options, which are specific to that plugin.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 171
        }
      },
      {
        "header": "Sample configuration file",
        "content": "Following is a sample config.json file to illustrate the format used for various fields:\n\n{ \"HttpHeaders\" : { \"MyHeader\" : \"MyValue\" }, \"psFormat\" : \"table {{.ID}}\\\\t{{.Image}}\\\\t{{.Command}}\\\\t{{.Labels}}\" , \"imagesFormat\" : \"table {{.ID}}\\\\t{{.Repository}}\\\\t{{.Tag}}\\\\t{{.CreatedAt}}\" , \"pluginsFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.Enabled}}\" , \"statsFormat\" : \"table {{.Container}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\" , \"servicesFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.Mode}}\" , \"secretFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.CreatedAt}}\\t{{.UpdatedAt}}\" , \"configFormat\" : \"table {{.ID}}\\t{{.Name}}\\t{{.CreatedAt}}\\t{{.UpdatedAt}}\" , \"serviceInspectFormat\" : \"pretty\" , \"nodesFormat\" : \"table {{.ID}}\\t{{.Hostname}}\\t{{.Availability}}\" , \"detachKeys\" : \"ctrl-e,e\" , \"credsStore\" : \"secretservice\" , \"credHelpers\" : { \"awesomereg.example.org\" : \"hip-star\" , \"unicorn.example.com\" : \"vcbait\" }, \"plugins\" : { \"plugin1\" : { \"option\" : \"value\" }, \"plugin2\" : { \"anotheroption\" : \"anothervalue\" , \"athirdoption\" : \"athirdvalue\" } }, \"proxies\" : { \"default\" : { \"httpProxy\" : \"http://user:pass@example.com:3128\" , \"httpsProxy\" : \"https://my-proxy.example.com:3129\" , \"noProxy\" : \"intra.mycorp.example.com\" , \"ftpProxy\" : \"http://user:pass@example.com:3128\" , \"allProxy\" : \"socks://example.com:1234\" }, \"https://manager1.mycorp.example.com:2377\" : { \"httpProxy\" : \"http://user:pass@example.com:3128\" , \"httpsProxy\" : \"https://my-proxy.example.com:3129\" } } }",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1453
        }
      },
      {
        "header": "Experimental features",
        "content": "Experimental features provide early access to future product functionality. These features are intended for testing and feedback, and they may change between releases without warning or can be removed from a future release.\n\nStarting with Docker 20.10, experimental CLI features are enabled by default, and require no configuration to enable them.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 347
        }
      },
      {
        "header": "Notary",
        "content": "If using your own notary server and a self-signed certificate or an internal Certificate Authority, you need to place the certificate at tls/<registry_url>/ca.crt in your Docker config directory.\n\nAlternatively you can trust the certificate globally by adding it to your system's list of root Certificate Authorities.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 317
        }
      },
      {
        "header": "Options",
        "content": "Option Default Description --config /root/.docker Location of client config files -c, --context Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with docker context use ) -D, --debug Enable debug mode -H, --host Daemon socket to connect to -l, --log-level info Set the logging level ( debug , info , warn , error , fatal ) --tls Use TLS; implied by --tlsverify --tlscacert /root/.docker/ca.pem Trust certs signed only by this CA --tlscert /root/.docker/cert.pem Path to TLS certificate file --tlskey /root/.docker/key.pem Path to TLS key file --tlsverify Use TLS and verify the remote",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 642
        }
      },
      {
        "header": "Examples",
        "content": "Specify daemon host (-H, --host)\n\nYou can use the -H , --host flag to specify a socket to use when you invoke a docker command. You can use the following protocols:\n\nScheme Description Example unix://[<path>] Unix socket (Linux only) unix:///var/run/docker.sock tcp://[<IP or host>[:port]] TCP connection tcp://174.17.0.1:2376 ssh://[username@]<IP or host>[:port] SSH connection ssh://user@192.168.64.5 npipe://[<name>] Named pipe (Windows only) npipe:////./pipe/docker_engine\n\nIf you don't specify the -H flag, and you're not using a custom context , commands use the following default sockets:\n\nunix:///var/run/docker.sock on macOS and Linux npipe:////./pipe/docker_engine on Windows\n\nTo achieve a similar effect without having to specify the -H flag for every command, you could also create a context , or alternatively, use the DOCKER_HOST environment variable .\n\nFor more information about the -H flag, see Daemon socket option .\n\nUsing TCP sockets\n\nThe following example shows how to invoke docker ps over TCP, to a remote daemon with IP address 174.17.0.1 , listening on port 2376 :\n\n$ docker -H tcp://174.17.0.1:2376 ps\n\nNote By convention, the Docker daemon uses port 2376 for secure TLS connections, and port 2375 for insecure, non-TLS connections.\n\nUsing SSH sockets\n\nWhen you use SSH invoke a command on a remote daemon, the request gets forwarded to the /var/run/docker.sock Unix socket on the SSH host.\n\n$ docker -H ssh://user@192.168.64.5 ps\n\nYou can optionally specify the location of the socket by appending a path component to the end of the SSH address.\n\n$ docker -H ssh://user@192.168.64.5/var/run/docker.sock ps",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1632
        }
      },
      {
        "header": "Specify daemon host (-H, --host)",
        "content": "You can use the -H , --host flag to specify a socket to use when you invoke a docker command. You can use the following protocols:\n\nScheme Description Example unix://[<path>] Unix socket (Linux only) unix:///var/run/docker.sock tcp://[<IP or host>[:port]] TCP connection tcp://174.17.0.1:2376 ssh://[username@]<IP or host>[:port] SSH connection ssh://user@192.168.64.5 npipe://[<name>] Named pipe (Windows only) npipe:////./pipe/docker_engine\n\nIf you don't specify the -H flag, and you're not using a custom context , commands use the following default sockets:\n\nunix:///var/run/docker.sock on macOS and Linux npipe:////./pipe/docker_engine on Windows\n\nTo achieve a similar effect without having to specify the -H flag for every command, you could also create a context , or alternatively, use the DOCKER_HOST environment variable .\n\nFor more information about the -H flag, see Daemon socket option .\n\nUsing TCP sockets\n\nThe following example shows how to invoke docker ps over TCP, to a remote daemon with IP address 174.17.0.1 , listening on port 2376 :\n\n$ docker -H tcp://174.17.0.1:2376 ps\n\nNote By convention, the Docker daemon uses port 2376 for secure TLS connections, and port 2375 for insecure, non-TLS connections.\n\nUsing SSH sockets\n\nWhen you use SSH invoke a command on a remote daemon, the request gets forwarded to the /var/run/docker.sock Unix socket on the SSH host.\n\n$ docker -H ssh://user@192.168.64.5 ps\n\nYou can optionally specify the location of the socket by appending a path component to the end of the SSH address.\n\n$ docker -H ssh://user@192.168.64.5/var/run/docker.sock ps",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1598
        }
      },
      {
        "header": "Using TCP sockets",
        "content": "The following example shows how to invoke docker ps over TCP, to a remote daemon with IP address 174.17.0.1 , listening on port 2376 :\n\n$ docker -H tcp://174.17.0.1:2376 ps\n\nNote By convention, the Docker daemon uses port 2376 for secure TLS connections, and port 2375 for insecure, non-TLS connections.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 303
        }
      },
      {
        "header": "Using SSH sockets",
        "content": "When you use SSH invoke a command on a remote daemon, the request gets forwarded to the /var/run/docker.sock Unix socket on the SSH host.\n\n$ docker -H ssh://user@192.168.64.5 ps\n\nYou can optionally specify the location of the socket by appending a path component to the end of the SSH address.\n\n$ docker -H ssh://user@192.168.64.5/var/run/docker.sock ps",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 353
        }
      },
      {
        "header": "Subcommands",
        "content": "Command Description docker build (legacy builder) Build an image from a Dockerfile docker builder Manage builds docker buildx Docker Buildx docker checkpoint Manage checkpoints docker compose Docker Compose docker config Manage Swarm configs docker container Manage containers docker context Manage contexts docker debug Get a shell into any container or image. An alternative to debugging with `docker exec`. docker desktop (Beta) Docker Desktop docker image Manage images docker init Creates Docker-related starter files for your project docker inspect Return low-level information on Docker objects docker login Authenticate to a registry docker logout Log out from a registry docker manifest Manage Docker image manifests and manifest lists docker mcp docker model Docker Model Runner docker network Manage networks docker node Manage Swarm nodes docker offload Control Docker Offload from the CLI docker plugin Manage plugins docker sandbox Docker Sandbox docker scout Command line tool for Docker Scout docker search Search Docker Hub for images docker secret Manage Swarm secrets docker service Manage Swarm services docker stack Manage Swarm stacks docker swarm Manage Swarm docker system Manage Docker docker trust Manage trust on Docker images docker version Show the Docker version information docker volume Manage volumes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 1333
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/",
    "doc_type": "docker",
    "total_sections": 25
  },
  {
    "title": "docker buildx build",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Start a build\nUsage | docker buildx build [OPTIONS] PATH | URL | -\nAliasesAn alias is a short or memorable alternative for a longer command. | docker build docker builder build docker image build docker buildx b",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 240
        }
      },
      {
        "header": "Description",
        "content": "The docker buildx build command starts a build using BuildKit.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 62
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n--add-host | Add a custom host-to-IP mapping (format: host:ip)\n--allow | Allow extra privileged entitlement (e.g., network.host, security.insecure, device)\n--annotation | Add annotation to the image\n--attest | Attestation parameters (format: type=sbom,generator=image)\n--build-arg | Set build-time variables\n--build-context | Additional build contexts (e.g., name=path)\n--cache-from | External cache sources (e.g., user/app:cache, type=local,src=path/to/dir)\n--cache-to | Cache export destinations (e.g., user/app:cache, type=local,dest=path/to/dir)\n--call | build | Set method for evaluating build (check, outline, targets)\n--cgroup-parent | Set the parent cgroup for the RUN instructions during build\n--check | Shorthand for --call=check\n-f, --file | Name of the Dockerfile (default: PATH/Dockerfile)\n--iidfile | Write the image ID to a file\n--label | Set metadata for an image\n--load | Shorthand for --output=type=docker\n--metadata-file | Write build result metadata to a file\n--network | Set the networking mode for the RUN instructions during build\n--no-cache | Do not use cache when building the image\n--no-cache-filter | Do not cache specified stages\n-o, --output | Output destination (format: type=local,dest=path)\n--platform | Set target platform for build\n--progress | auto | Set type of progress output (auto, none, plain, quiet, rawjson, tty). Use plain to show container output\n--provenance | Shorthand for --attest=type=provenance\n--pull | Always attempt to pull all referenced images\n--push | Shorthand for --output=type=registry\n-q, --quiet | Suppress the build output and print image ID on success\n--sbom | Shorthand for --attest=type=sbom\n--secret | Secret to expose to the build (format: id=mysecret[,src=/local/secret])\n--shm-size | Shared memory size for build containers\n--ssh | SSH agent socket or keys to expose to the build (format: default|<id>[=<socket>|<key>[,<key>]])\n-t, --tag | Image identifier (format: [registry/]repository[:tag])\n--target | Set the target build stage to build\n--ulimit | Ulimit options",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 2083
        }
      },
      {
        "header": "Add entries to container hosts file (--add-host)",
        "content": "You can add other hosts into a build container's /etc/hosts file by using one or more --add-host flags. This example adds static addresses for hosts named my-hostname and my_hostname_v6:\n\nIf you need your build to connect to services running on the host, you can use the special host-gateway value for --add-host. In the following example, build containers resolve host.docker.internal to the host's gateway IP.\n\nYou can wrap an IPv6 address in square brackets. = and : are both valid separators. Both formats in the following example are valid:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker buildx build --add-hostmy_hostname=8.8.8.8 --add-hostmy_hostname_v6=2001:4860:4860::8888 .\n```",
          "```\n$docker buildx build --add-host host.docker.internal=host-gateway .\n```",
          "```\n$docker buildx build --add-host my-hostname:10.180.0.1 --add-host my-hostname_v6=[2001:4860:4860::8888].\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 545
        }
      },
      {
        "header": "Create annotations (--annotation)",
        "content": "Add OCI annotations to the image index, manifest, or descriptor. The following example adds the foo=bar annotation to the image manifests:\n\nYou can optionally add a type prefix to specify the level of the annotation. By default, the image manifest is annotated. The following example adds the foo=bar annotation the image index instead of the manifests:\n\nYou can specify multiple types, separated by a comma (,) to add the annotation to multiple image components. The following example adds the foo=bar annotation to image index, descriptors, manifests:\n\nYou can also specify a platform qualifier in square brackets ([os/arch]) in the type prefix, to apply the annotation to a subset of manifests with the matching platform. The following example adds the foo=bar annotation only to the manifest with the linux/amd64 platform:\n\nWildcards are not supported in the platform qualifier; you can't specify a type prefix like manifest[linux/*] to add annotations only to manifests which has linux as the OS platform.\n\nFor more information about annotations, see Annotations.",
        "code_examples": [
          "```\n--annotation=\"key=value\"--annotation=\"[type:]key=value\"\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build -t TAG --annotation\"foo=bar\"--push .\n```",
          "```\n$docker buildx build -t TAG --annotation\"index:foo=bar\"--push .\n```",
          "```\n$docker buildx build -t TAG --annotation\"index,manifest,manifest-descriptor:foo=bar\"--push .\n```",
          "```\n$docker buildx build -t TAG --annotation\"manifest[linux/amd64]:foo=bar\"--push .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1068
        }
      },
      {
        "header": "Create attestations (--attest)",
        "content": "Create image attestations. BuildKit currently supports:\n\nsbom - Software Bill of Materials.\n\nUse --attest=type=sbom to generate an SBOM for an image at build-time. Alternatively, you can use the --sbom shorthand.\n\nFor more information, see here.\n\nprovenance - SLSA Provenance\n\nUse --attest=type=provenance to generate provenance for an image at build-time. Alternatively, you can use the --provenance shorthand.\n\nBy default, a minimal provenance attestation will be created for the build result, which will only be attached for images pushed to registries.\n\nFor more information, see here.\n\n• sbom - Software Bill of Materials.Use --attest=type=sbom to generate an SBOM for an image at build-time. Alternatively, you can use the --sbom shorthand.For more information, see here.\n• provenance - SLSA ProvenanceUse --attest=type=provenance to generate provenance for an image at build-time. Alternatively, you can use the --provenance shorthand.By default, a minimal provenance attestation will be created for the build result, which will only be attached for images pushed to registries.For more information, see here.",
        "code_examples": [
          "```\n--attest=type=sbom,...--attest=type=provenance,...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1116
        }
      },
      {
        "header": "Allow extra privileged entitlement (--allow)",
        "content": "Allow extra privileged entitlement. List of entitlements:\n\nFor entitlements to be enabled, the BuildKit daemon also needs to allow them with --allow-insecure-entitlement (see create --buildkitd-flags).\n\n• network.host - Allows executions with host networking.\n• security.insecure - Allows executions without sandbox. See related Dockerfile extensions.\n• device - Allows access to Container Device Interface (CDI) devices.--allow device - Grants access to all devices.--allow device=kind|name - Grants access to a specific device.--allow device=kind|name,alias=kind|name - Grants access to a specific device, with optional aliasing.\n\n• --allow device - Grants access to all devices.\n• --allow device=kind|name - Grants access to a specific device.\n• --allow device=kind|name,alias=kind|name - Grants access to a specific device, with optional aliasing.",
        "code_examples": [
          "```\n--allow=ENTITLEMENT\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx create --use --name insecure-builder --buildkitd-flags'--allow-insecure-entitlement security.insecure'$docker buildx build --allow security.insecure .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 851
        }
      },
      {
        "header": "Set build-time variables (--build-arg)",
        "content": "You can use ENV instructions in a Dockerfile to define variable values. These values persist in the built image. Often persistence isn't what you want. Users want to specify variables differently depending on which host they build an image on.\n\nA good example is http_proxy or source versions for pulling intermediate files. The ARG instruction lets Dockerfile authors define values that users can set at build-time using the --build-arg flag:\n\nThis flag allows you to pass the build-time variables that are accessed like regular environment variables in the RUN instruction of the Dockerfile. These values don't persist in the intermediate or final images like ENV values do. You must add --build-arg for each build argument.\n\nUsing this flag doesn't alter the output you see when the build process echoes theARG lines from the Dockerfile.\n\nFor detailed information on using ARG and ENV instructions, see the Dockerfile reference.\n\nYou can also use the --build-arg flag without a value, in which case the daemon propagates the value from the local environment into the Docker container it's building:\n\nThis example is similar to how docker run -e works. Refer to the docker run documentation for more information.\n\nThere are also useful built-in build arguments, such as:\n\nLearn more about the built-in build arguments in the Dockerfile reference docs.\n\n• BUILDKIT_CONTEXT_KEEP_GIT_DIR=<bool>: trigger git context to keep the .git directory\n• BUILDKIT_INLINE_CACHE=<bool>: inline cache metadata to image config or not\n• BUILDKIT_MULTI_PLATFORM=<bool>: opt into deterministic output regardless of multi-platform output or not",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker buildx build --build-argHTTP_PROXY=http://10.20.30.2:1234 --build-argFTP_PROXY=http://40.50.60.5:4567 .\n```",
          "```\n$exportHTTP_PROXY=http://10.20.30.2:1234$docker buildx build --build-arg HTTP_PROXY .\n```",
          "```\n$docker buildx build --build-argBUILDKIT_MULTI_PLATFORM=1.\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1625
        }
      },
      {
        "header": "Additional build contexts (--build-context)",
        "content": "Define additional build context with specified contents. In Dockerfile the context can be accessed when FROM name or --from=name is used. When Dockerfile defines a stage with the same name it is overwritten.\n\nExpose a secondary local source directory:\n\nUse the docker-image:// scheme.\n\nReplace alpine:latest with a pinned one:\n\nUse the oci-layout:/// scheme.\n\nSource an image from a local OCI layout compliant directory, either by tag, or by digest:\n\nThe OCI layout directory must be compliant with the OCI layout specification.\n\n• local source directory\n• local OCI layout compliant directory\n• container image",
        "code_examples": [
          "```\n--build-context=name=VALUE\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineCOPY--from=project myfile /\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --build-contextproject=path/to/project/source .#docker buildx build --build-contextproject=https://github.com/myuser/project.git .\n```",
          "```\n$docker buildx build --build-contextalpine=docker-image://alpine@sha256:0123456789 .\n```",
          "```\n$docker buildx build --build-contextfoo=oci-layout:///path/to/local/layout:<tag>$docker buildx build --build-contextfoo=oci-layout:///path/to/local/layout@sha256:<digest>\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineRUNapk add gitCOPY--from=foo myfile /FROMfoo\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 611
        }
      },
      {
        "header": "Override the configured builder instance (--builder)",
        "content": "Same as buildx --builder.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 25
        }
      },
      {
        "header": "Use an external cache source for a build (--cache-from)",
        "content": "Use an external cache source for a build. Supported types are:\n\nIf no type is specified, registry exporter is used with a specified reference.\n\nMore info about cache exporters and available attributes can be found in the Cache storage backends documentation\n\n• registry can import cache from a cache manifest or (special) image configuration on the registry.\n• local can import cache from local files previously exported with --cache-to.\n• gha can import cache from a previously exported cache with --cache-to in your GitHub repository.\n• s3 can import cache from a previously exported cache with --cache-to in your S3 bucket.\n• azblob can import cache from a previously exported cache with --cache-to in your Azure bucket.\n\n[Admonition] More info about cache exporters and available attributes can be found in the Cache storage backends documentation",
        "code_examples": [
          "```\n--cache-from=[NAME|type=TYPE[,KEY=VALUE]]\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --cache-from=user/app:cache .$docker buildx build --cache-from=user/app .$docker buildx build --cache-from=type=registry,ref=user/app .$docker buildx build --cache-from=type=local,src=path/to/cache .$docker buildx build --cache-from=type=gha .$docker buildx build --cache-from=type=s3,region=eu-west-1,bucket=mybucket .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 851
        }
      },
      {
        "header": "Export build cache to an external cache destination (--cache-to)",
        "content": "Export build cache to an external cache destination. Supported types are:\n\nMore info about cache exporters and available attributes can be found in the Cache storage backends documentation\n\n• registry exports build cache to a cache manifest in the registry.\n• local exports cache to a local directory on the client.\n• inline writes the cache metadata into the image configuration.\n• gha exports cache through the GitHub Actions Cache service API.\n• s3 exports cache to a S3 bucket.\n• azblob exports cache to an Azure bucket.\n\n[Admonition] More info about cache exporters and available attributes can be found in the Cache storage backends documentation",
        "code_examples": [
          "```\n--cache-to=[NAME|type=TYPE[,KEY=VALUE]]\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --cache-to=user/app:cache .$docker buildx build --cache-to=type=inline .$docker buildx build --cache-to=type=registry,ref=user/app .$docker buildx build --cache-to=type=local,dest=path/to/cache .$docker buildx build --cache-to=type=gha .$docker buildx build --cache-to=type=s3,region=eu-west-1,bucket=mybucket .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 652
        }
      },
      {
        "header": "Invoke a frontend method (--call)",
        "content": "BuildKit frontends can support alternative modes of executions for builds, using frontend methods. Frontend methods are a way to change or extend the behavior of a build invocation, which lets you, for example, inspect, validate, or generate alternative outputs from a build.\n\nThe --call flag for docker buildx build lets you specify the frontend method that you want to execute. If this flag is unspecified, it defaults to executing the build and evaluating build checks.\n\nFor Dockerfiles, the available methods are:\n\nNote that other frontends may implement these or other methods. To see the list of available methods for the frontend you're using, use --call=subrequests.describe.\n\nThe --call=targets and --call=outline methods include descriptions for build targets and arguments, if available. Descriptions are generated from comments in the Dockerfile. A comment on the line before a FROM instruction becomes the description of a build target, and a comment before an ARG instruction the description of a build argument. The comment must lead with the name of the stage or argument, for example:\n\nWhen you run docker buildx build --call=outline, the output includes the descriptions, as follows:\n\nFor more examples on how to write Dockerfile docstrings, check out the Dockerfile for Docker docs.\n\nThe check method evaluates build checks without executing the build. The --check flag is a convenient shorthand for --call=check. Use the check method to validate the build configuration before starting the build.\n\nUsing --check without specifying a target evaluates the entire Dockerfile. If you want to evaluate a specific target, use the --target flag.\n\nThe outline method prints the name of the specified target (or the default target, if --target isn't specified), and the build arguments that the target consumes, along with their default values, if set.\n\nThe following example shows the default target release and its build arguments:\n\nThis means that the release target is configurable using these build arguments:\n\nThe targets method lists all the build targets in the Dockerfile. These are the stages that you can build using the --target flag. It also indicates the default target, which is the target that will be built when you don't specify a target.\n\nCommand | Description\n--- | ---\nbuild (default) | Execute the build and evaluate build checks for the current build target.\ncheck | Evaluate build checks for the either the entire Dockerfile or the selected target, without executing a build.\noutline | Show the build arguments that you can set for a target, and their default values.\ntargets | List all the build targets in the Dockerfile.\nsubrequests.describe | List all the frontend methods that the current frontend supports.",
        "code_examples": [
          "```\n--call=[build|check|outline|targets]\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build -q --call=subrequests.describe .NAME                 VERSION DESCRIPTIONoutline              1.0.0   List all parameters current build target supportstargets              1.0.0   List all targets current build supportssubrequests.describe 1.0.0   List available subrequest types\n```",
          "```\n# syntax=docker/dockerfile:1# GO_VERSION sets the Go version for the buildARGGO_VERSION=1.22# base-builder is the base stage for building the projectFROMgolang:${GO_VERSION} AS base-builder\n```",
          "```\n$docker buildx build -q --call=outline .TARGET:      base-builderDESCRIPTION: is the base stage for building the projectBUILD ARG    VALUE   DESCRIPTIONGO_VERSION   1.22    sets the Go version for the build\n```",
          "```\n$docker buildx build -q --check https://github.com/docker/docs.gitWARNING: InvalidBaseImagePlatformBase image wjdp/htmltest:v0.17.0 was pulled with platform \"linux/amd64\", expected \"linux/arm64\" for current buildDockerfile:43--------------------41 |         \"#content/desktop/previous-versions/*.md\"42 |43 | >>> FROM wjdp/htmltest:v${HTMLTEST_VERSION} AS test44 |     WORKDIR /test45 |     COPY --from=build /out ./public--------------------\n```",
          "```\n$docker buildx build -q --call=outline https://github.com/docker/docs.gitTARGET:      releaseDESCRIPTION: is an empty scratch image with only compiled assetsBUILD ARG          VALUE     DESCRIPTIONGO_VERSION         1.22      sets the Go version for the base stageHUGO_VERSION       0.127.0HUGO_ENV                     sets the hugo.Environment (production, development, preview)DOCS_URL                     sets the base URL for the sitePAGEFIND_VERSION   1.1.0\n```",
          "```\n$docker buildx build\\--build-arg GO_VERSION=1.22 \\--build-arg HUGO_VERSION=0.127.0 \\--build-arg HUGO_ENV=production \\--build-arg DOCS_URL=https://example.com \\--build-arg PAGEFIND_VERSION=1.1.0 \\--target release https://github.com/docker/docs.git\n```",
          "```\n$docker buildx build -q --call=targets https://github.com/docker/docs.gitTARGET            DESCRIPTIONbase              is the base stage with build dependenciesnode              installs Node.js dependencieshugo              downloads and extracts the Hugo binarybuild-base        is the base stage for building the sitedev               is for local development with Docker Composebuild             creates production builds with Hugolint              lints markdown filestest              validates HTML output and checks for broken linksupdate-modules    downloads and vendors Hugo modulesvendor            is an empty stage with only vendored Hugo modulesbuild-upstream    builds an upstream project with a replacement modulevalidate-upstream validates HTML output for upstream buildsunused-media      checks for unused graphics and other mediapagefind          installs the Pagefind runtimeindex             generates a Pagefind indextest-go-redirects checks that the /go/ redirects are validrelease (default) is an empty scratch image with only compiled assets\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 13,
          "content_length": 2747
        }
      },
      {
        "header": "Use a custom parent cgroup (--cgroup-parent)",
        "content": "When you run docker buildx build with the --cgroup-parent option, the daemon runs the containers used in the build with the corresponding docker run flag.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 154
        }
      },
      {
        "header": "Specify a Dockerfile (-f, --file)",
        "content": "Specifies the filepath of the Dockerfile to use. If unspecified, a file named Dockerfile at the root of the build context is used by default.\n\nTo read a Dockerfile from stdin, you can use - as the argument for --file.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker buildx build -f <filepath> .\n```",
          "```\n$cat Dockerfile|docker buildx build -f - .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 217
        }
      },
      {
        "header": "Load the single-platform build result to docker images (--load)",
        "content": "Shorthand for --output=type=docker. Will automatically load the single-platform build result to docker images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 110
        }
      },
      {
        "header": "Write build result metadata to a file (--metadata-file)",
        "content": "To output build metadata such as the image digest, pass the --metadata-file flag. The metadata will be written as a JSON object to the specified file. The directory of the specified file must already exist and be writable.\n\nBuild record provenance (buildx.build.provenance) includes minimal provenance by default. Set the BUILDX_METADATA_PROVENANCE environment variable to customize this behavior:\n\nBuild warnings (buildx.build.warnings) are not included by default. Set the BUILDX_METADATA_WARNINGS environment variable to 1 or true to include them.\n\n• min sets minimal provenance (default).\n• max sets full provenance.\n• disabled, false or 0 doesn't set any provenance.\n\n[Admonition] Build record provenance (buildx.build.provenance) includes minimal provenance by default. Set the BUILDX_METADATA_PROVENANCE environment variable to customize this behavior:min sets minimal provenance (default).max sets full provenance.disabled, false or 0 doesn't set any provenance.\n\n[Admonition] Build warnings (buildx.build.warnings) are not included by default. Set the BUILDX_METADATA_WARNINGS environment variable to 1 or true to include them.",
        "code_examples": [
          "```\n{\"buildx.build.provenance\":{},\"buildx.build.ref\":\"mybuilder/mybuilder0/0fjb6ubs52xx3vygf6fgdl611\",\"buildx.build.warnings\":{},\"containerimage.config.digest\":\"sha256:2937f66a9722f7f4a2df583de2f8cb97fc9196059a410e7f00072fc918930e66\",\"containerimage.descriptor\":{\"annotations\":{\"config.digest\":\"sha256:2937f66a9722f7f4a2df583de2f8cb97fc9196059a410e7f00072fc918930e66\",\"org.opencontainers.image.created\":\"2022-02-08T21:28:03Z\"},\"digest\":\"sha256:19ffeab6f8bc9293ac2c3fdf94ebe28396254c993aea0b5a542cfb02e0883fa3\",\"mediaType\":\"application/vnd.oci.image.manifest.v1+json\",\"size\":506},\"containerimage.digest\":\"sha256:19ffeab6f8bc9293ac2c3fdf94ebe28396254c993aea0b5a542cfb02e0883fa3\"}\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --load --metadata-file metadata.json .$cat metadata.json\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1136
        }
      },
      {
        "header": "Set the networking mode for the RUN instructions during build (--network)",
        "content": "Available options for the networking mode are:\n\nFind more details in the Dockerfile reference.\n\n• default (default): Run in the default network.\n• none: Run with no network access.\n• host: Run in the hostâs network environment.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 229
        }
      },
      {
        "header": "Ignore build cache for specific stages (--no-cache-filter)",
        "content": "The --no-cache-filter lets you specify one or more stages of a multi-stage Dockerfile for which build cache should be ignored. To specify multiple stages, use a comma-separated syntax:\n\nFor example, the following Dockerfile contains four stages:\n\nTo ignore the cache for the install stage:\n\nTo ignore the cache the install and release stages:\n\nThe arguments for the --no-cache-filter flag must be names of stages.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMoven/bun:1 AS baseWORKDIR/appFROMbase AS installWORKDIR/temp/devRUN--mount=type=bind,source=package.json,target=package.json\\--mount=type=bind,source=bun.lockb,target=bun.lockb\\bun install --frozen-lockfileFROMbase AS testCOPY--from=install /temp/dev/node_modules node_modulesCOPY. .RUNbun testFROMbase AS releaseENVNODE_ENV=productionCOPY--from=install /temp/dev/node_modules node_modulesCOPY. .ENTRYPOINT[\"bun\",\"run\",\"index.js\"]\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --no-cache-filter stage1,stage2,stage3 .\n```",
          "```\n$docker buildx build --no-cache-filter install .\n```",
          "```\n$docker buildx build --no-cache-filter install,release .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 413
        }
      },
      {
        "header": "Set the export action for the build result (-o, --output)",
        "content": "Sets the export action for the build result. The default output, when using the docker build driver, is a container image exported to the local image store. The --output flag makes this step configurable allows export of results directly to the client's filesystem, an OCI image tarball, a registry, and more.\n\nBuildx with docker driver only supports the local, tarball, and image exporters. The docker-container driver supports all exporters.\n\nIf you only specify a filepath as the argument to --output, Buildx uses the local exporter. If the value is -, Buildx uses the tar exporter and writes the output to stdout.\n\nYou can export multiple outputs by repeating the flag.\n\nSupported exported types are:\n\nThe local export type writes all result files to a directory on the client. The new files will be owned by the current user. On multi-platform builds, all results will be put in subdirectories by their platform.\n\nFor more information, see Local and tar exporters.\n\nThe tar export type writes all result files as a single tarball on the client. On multi-platform builds all results will be put in subdirectories by their platform.\n\nFor more information, see Local and tar exporters.\n\nThe oci export type writes the result image or manifest list as an OCI image layout tarball on the client.\n\nFor more information, see OCI and Docker exporters.\n\nThe docker export type writes the single-platform result image as a Docker image specification tarball on the client. Tarballs created by this exporter are also OCI compatible.\n\nThe default image store in Docker Engine doesn't support loading multi-platform images. You can enable the containerd image store, or push multi-platform images is to directly push to a registry, see registry.\n\nFor more information, see OCI and Docker exporters.\n\nThe image exporter writes the build result as an image or a manifest list. When using docker driver the image will appear in docker images. Optionally, image can be automatically pushed to a registry by specifying attributes.\n\nFor more information, see Image and registry exporters.\n\nThe registry exporter is a shortcut for type=image,push=true.\n\nFor more information, see Image and registry exporters.\n\n• dest - destination directory where files will be written\n\n• dest - destination path where tarball will be written. â-â writes to stdout.\n\n• dest - destination path where tarball will be written. â-â writes to stdout.\n\n• dest - destination path where tarball will be written. If not specified, the tar will be loaded automatically to the local image store.\n• context - name for the Docker context where to import the result\n\n• name - name (references) for the new image.\n• push - Boolean to automatically push the image.",
        "code_examples": [
          "```\n-o, --output=[PATH,-,type=TYPE[,KEY=VALUE]\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build -o . .$docker buildx build -o outdir .$docker buildx build -o - . > out.tar$docker buildx build -otype=docker .$docker buildx build -otype=docker,dest=- . > myimage.tar$docker buildx build -t tonistiigi/foo -otype=registry\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 2725
        }
      },
      {
        "header": "Set the target platforms for the build (--platform)",
        "content": "Set the target platform for the build. All FROM commands inside the Dockerfile without their own --platform flag will pull base images for this platform and this value will also be the platform of the resulting image.\n\nThe default value is the platform of the BuildKit daemon where the build runs. The value takes the form of os/arch or os/arch/variant. For example, linux/amd64 or linux/arm/v7. Additionally, the --platform flag also supports a special local value, which tells BuildKit to use the platform of the BuildKit client that invokes the build.\n\nWhen using docker-container driver with buildx, this flag can accept multiple values as an input separated by a comma. With multiple values the result will be built for all of the specified platforms and joined together into a single manifest list.\n\nIf the Dockerfile needs to invoke the RUN command, the builder needs runtime support for the specified platform. In a clean setup, you can only execute RUN commands for your system architecture. If your kernel supports binfmt_misc launchers for secondary architectures, buildx will pick them up automatically. Docker Desktop releases come with binfmt_misc automatically configured for arm64 and arm architectures. You can see what runtime platforms your current builder instance supports by running docker buildx inspect --bootstrap.\n\nInside a Dockerfile, you can access the current platform value through TARGETPLATFORM build argument. Refer to the Dockerfile reference for the full description of automatic platform argument variants .\n\nYou can find the formatting definition for the platform specifier in the containerd source code.",
        "code_examples": [
          "```\n--platform=value[,value]\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --platform=linux/arm64 .$docker buildx build --platform=linux/amd64,linux/arm64,linux/arm/v7 .$docker buildx build --platform=darwin .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1641
        }
      },
      {
        "header": "Set type of progress output (--progress)",
        "content": "Set type of progress output. Supported values are:\n\nYou can also use the BUILDKIT_PROGRESS environment variable to set its value.\n\nThe following example uses plain output during the build:\n\nCheck also the BUILDKIT_COLORS environment variable for modifying the colors of the terminal output.\n\nThe rawjson output marshals the solve status events from BuildKit to JSON lines. This mode is designed to be read by an external program.\n\n• auto (default): Uses the tty mode if the client is a TTY, or plain otherwise\n• tty: An interactive stream of the output with color and redrawing\n• plain: Prints the raw build progress in a plaintext format\n• quiet: Suppress the build output and print image ID on success (same as --quiet)\n• rawjson: Prints the raw build progress as JSON lines\n\n[Admonition] You can also use the BUILDKIT_PROGRESS environment variable to set its value.\n\n[Admonition] Check also the BUILDKIT_COLORS environment variable for modifying the colors of the terminal output.",
        "code_examples": [
          "```\n--progress=VALUE\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --load --progress=plain .#1[internal]load build definition from Dockerfile#1transferring dockerfile: 227B 0.0sdone#1DONE 0.1s#2[internal]load .dockerignore#2transferring context: 129B 0.0sdone#2DONE 0.0s...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 983
        }
      },
      {
        "header": "Create provenance attestations (--provenance)",
        "content": "Shorthand for --attest=type=provenance, used to configure provenance attestations for the build result. For example, --provenance=mode=max can be used as an abbreviation for --attest=type=provenance,mode=max.\n\nAdditionally, --provenance can be used with Boolean values to enable or disable provenance attestations. For example, --provenance=false disables all provenance attestations, while --provenance=true enables all provenance attestations.\n\nBy default, a minimal provenance attestation will be created for the build result. Note that the default image store in Docker Engine doesn't support attestations. Provenance attestations only persist for images pushed directly to a registry if you use the default image store. Alternatively, you can switch to using the containerd image store.\n\nFor more information about provenance attestations, see here.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 854
        }
      },
      {
        "header": "Push the build result to a registry (--push)",
        "content": "Shorthand for --output=type=registry. Will automatically push the build result to registry.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 91
        }
      },
      {
        "header": "Create SBOM attestations (--sbom)",
        "content": "Shorthand for --attest=type=sbom, used to configure SBOM attestations for the build result. For example, --sbom=generator=<user>/<generator-image> can be used as an abbreviation for --attest=type=sbom,generator=<user>/<generator-image>.\n\nAdditionally, --sbom can be used with Boolean values to enable or disable SBOM attestations. For example, --sbom=false disables all SBOM attestations.\n\nNote that the default image store in Docker Engine doesn't support attestations. Provenance attestations only persist for images pushed directly to a registry if you use the default image store. Alternatively, you can switch to using the containerd image store.\n\nFor more information, see here.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 684
        }
      },
      {
        "header": "Secret to expose to the build (--secret)",
        "content": "Exposes secrets (authentication credentials, tokens) to the build. A secret can be mounted into the build using a RUN --mount=type=secret mount in the Dockerfile. For more information about how to use build secrets, see Build secrets.\n\nSupported types are:\n\nBuildx attempts to detect the type automatically if unset. If an environment variable with the same key as id is set, then Buildx uses type=env and the variable value becomes the secret. If no such environment variable is set, and type is not set, then Buildx falls back to type=file.\n\nSource a build secret from a file.\n\nIn the following example, type=file is automatically detected because no environment variable matching aws (the ID) is set.\n\nSource a build secret from an environment variable.\n\nIn the following example, type=env is automatically detected because an environment variable matching id is set.\n\nIn the following example, the build argument SECRET_TOKEN is set to contain the value of the environment variable API_KEY.\n\nYou can also specify the name of the environment variable with src or source:\n\nSpecifying the environment variable name with src or source, you are required to set type=env explicitly, or else Buildx assumes that the secret is type=file, and looks for a file with the name of src or source (in this case, a file named API_KEY relative to the location where the docker buildx build command was executed.\n\nKey | Description | Default\n--- | --- | ---\nid | ID of the secret. | N/A (this key is required)\nsrc, source | Filepath of the file containing the secret value (absolute or relative to current working directory). | id if unset.\n\nKey | Description | Default\n--- | --- | ---\nid | ID of the secret. | N/A (this key is required)\nenv, src, source | Environment variable to source the secret from. | id if unset.\n\n[Admonition] Specifying the environment variable name with src or source, you are required to set type=env explicitly, or else Buildx assumes that the secret is type=file, and looks for a file with the name of src or source (in this case, a file named API_KEY relative to the location where the docker buildx build command was executed.",
        "code_examples": [
          "```\n--secret=[type=TYPE[,KEY=VALUE]\n```",
          "```\n# syntax=docker/dockerfile:1FROMnode:alpineRUN--mount=type=bind,target=.\\--mount=type=secret,id=SECRET_TOKEN,env=SECRET_TOKEN\\yarn run test\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build --secret[type=file,]id=<ID>[,src=<FILEPATH>].\n```",
          "```\n$docker buildx build --secretid=aws,src=$HOME/.aws/credentials .\n```",
          "```\n# syntax=docker/dockerfile:1FROMpython:3RUNpip install awscliRUN--mount=type=secret,id=aws,target=/root/.aws/credentials\\aws s3 cp s3://... ...\n```",
          "```\n$docker buildx build --secret[type=env,]id=<ID>[,env=<VARIABLE>].\n```",
          "```\n$SECRET_TOKEN=token docker buildx build --secretid=SECRET_TOKEN .\n```",
          "```\n$API_KEY=token docker buildx build --secretid=SECRET_TOKEN,env=API_KEY .\n```",
          "```\n$API_KEY=token docker buildx build --secrettype=env,id=SECRET_TOKEN,src=API_KEY .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 10,
          "content_length": 2143
        }
      },
      {
        "header": "Shared memory size for build containers (--shm-size)",
        "content": "Sets the size of the shared memory allocated for build containers when using RUN instructions.\n\nThe format is <number><unit>. number must be greater than 0. Unit is optional and can be b (bytes), k (kilobytes), m (megabytes), or g (gigabytes). If you omit the unit, the system uses bytes.\n\nIn most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.\n\n[Admonition] In most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 757
        }
      },
      {
        "header": "SSH agent socket or keys to expose to the build (--ssh)",
        "content": "This can be useful when some commands in your Dockerfile need specific SSH authentication (e.g., cloning a private repository).\n\n--ssh exposes SSH agent socket or keys to the build and can be used with the RUN --mount=type=ssh mount.\n\nExample to access Gitlab using an SSH agent socket:",
        "code_examples": [
          "```\n--ssh=default|<id>[=<socket>|<key>[,<key>]]\n```"
        ],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1FROMalpineRUNapk add --no-cache openssh-clientRUNmkdir -p -m0700~/.ssh&&ssh-keyscan gitlab.com >> ~/.ssh/known_hostsRUN--mount=type=ssh ssh -q -T git@gitlab.com 2>&1|tee /hello# \"Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY\" should be printed here# with the type of build progress is defined as `plain`.\n```",
          "```\n$eval$(ssh-agent)$ssh-add ~/.ssh/id_rsa(Input your passphrase here)$docker buildx build --sshdefault=$SSH_AUTH_SOCK.\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 286
        }
      },
      {
        "header": "Tag an image (-t, --tag)",
        "content": "This examples builds in the same way as the previous example, but it then tags the resulting image. The repository name will be docker/apache and the tag 2.0.\n\nRead more about valid tags.\n\nYou can apply multiple tags to an image. For example, you can apply the latest tag to a newly built image and add another tag that references a specific version.\n\nFor example, to tag an image both as docker/fedora-jboss:latest and docker/fedora-jboss:v2.1, use the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker buildx build -t docker/apache:2.0 .\n```",
          "```\n$docker buildx build -t docker/fedora-jboss:latest -t docker/fedora-jboss:v2.1 .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 464
        }
      },
      {
        "header": "Specifying target build stage (--target)",
        "content": "When building a Dockerfile with multiple build stages, use the --target option to specify an intermediate build stage by name as a final stage for the resulting image. The builder skips commands after the target stage.",
        "code_examples": [
          "```\nFROMdebian AS build-env# ...FROMalpine AS production-env# ...\n```"
        ],
        "usage_examples": [
          "```\n$docker buildx build -t mybuildimage --target build-env .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 218
        }
      },
      {
        "header": "Set ulimits (--ulimit)",
        "content": "--ulimit overrides the default ulimits of build's containers when using RUN instructions and are specified with a soft and hard limit as such: <type>=<soft limit>[:<hard limit>], for example:\n\nIf you don't provide a hard limit, the soft limit is used for both values. If no ulimits are set, they're inherited from the default ulimits set on the daemon.\n\nIn most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.\n\n[Admonition] If you don't provide a hard limit, the soft limit is used for both values. If no ulimits are set, they're inherited from the default ulimits set on the daemon.\n\n[Admonition] In most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker buildx build --ulimitnofile=1024:1024 .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 995
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/image/build/",
    "doc_type": "docker",
    "total_sections": 31
  },
  {
    "title": "docker container run",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Create and run a new container from an image\nUsage | docker container run [OPTIONS] IMAGE [COMMAND] [ARG...]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker run",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 224
        }
      },
      {
        "header": "Description",
        "content": "The docker run command runs a command in a new container, pulling the image if needed and starting the container.\n\nYou can restart a stopped container with all its previous changes intact using docker start. Use docker ps -a to view a list of all containers, including those that are stopped.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 292
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n--add-host | Add a custom host-to-IP mapping (host:ip)\n--annotation | API 1.43+ Add an annotation to the container (passed through to the OCI runtime)\n-a, --attach | Attach to STDIN, STDOUT or STDERR\n--blkio-weight | Block IO (relative weight), between 10 and 1000, or 0 to disable (default 0)\n--blkio-weight-device | Block IO weight (relative device weight)\n--cap-add | Add Linux capabilities\n--cap-drop | Drop Linux capabilities\n--cgroup-parent | Optional parent cgroup for the container\n--cgroupns | API 1.41+ Cgroup namespace to use (host|private)'host': Run the container in the Docker host's cgroup namespace'private': Run the container in its own private cgroup namespace'': Use the cgroup namespace as configured by thedefault-cgroupns-mode option on the daemon (default)\n--cidfile | Write the container ID to the file\n--cpu-count | CPU count (Windows only)\n--cpu-percent | CPU percent (Windows only)\n--cpu-period | Limit CPU CFS (Completely Fair Scheduler) period\n--cpu-quota | Limit CPU CFS (Completely Fair Scheduler) quota\n--cpu-rt-period | API 1.25+ Limit CPU real-time period in microseconds\n--cpu-rt-runtime | API 1.25+ Limit CPU real-time runtime in microseconds\n-c, --cpu-shares | CPU shares (relative weight)\n--cpus | API 1.25+ Number of CPUs\n--cpuset-cpus | CPUs in which to allow execution (0-3, 0,1)\n--cpuset-mems | MEMs in which to allow execution (0-3, 0,1)\n-d, --detach | Run container in background and print container ID\n--detach-keys | Override the key sequence for detaching a container\n--device | Add a host device to the container\n--device-cgroup-rule | Add a rule to the cgroup allowed devices list\n--device-read-bps | Limit read rate (bytes per second) from a device\n--device-read-iops | Limit read rate (IO per second) from a device\n--device-write-bps | Limit write rate (bytes per second) to a device\n--device-write-iops | Limit write rate (IO per second) to a device\n--disable-content-trust | true | Skip image verification\n--dns | Set custom DNS servers\n--dns-option | Set DNS options\n--dns-search | Set custom DNS search domains\n--domainname | Container NIS domain name\n--entrypoint | Overwrite the default ENTRYPOINT of the image\n-e, --env | Set environment variables\n--env-file | Read in a file of environment variables\n--expose | Expose a port or a range of ports\n--gpus | API 1.40+ GPU devices to add to the container ('all' to pass all GPUs)\n--group-add | Add additional groups to join\n--health-cmd | Command to run to check health\n--health-interval | Time between running the check (ms|s|m|h) (default 0s)\n--health-retries | Consecutive failures needed to report unhealthy\n--health-start-interval | API 1.44+ Time between running the check during the start period (ms|s|m|h) (default 0s)\n--health-start-period | API 1.29+ Start period for the container to initialize before starting health-retries countdown (ms|s|m|h) (default 0s)\n--health-timeout | Maximum time to allow one check to run (ms|s|m|h) (default 0s)\n--help | Print usage\n-h, --hostname | Container host name\n--init | API 1.25+ Run an init inside the container that forwards signals and reaps processes\n-i, --interactive | Keep STDIN open even if not attached\n--io-maxbandwidth | Maximum IO bandwidth limit for the system drive (Windows only)\n--io-maxiops | Maximum IOps limit for the system drive (Windows only)\n--ip | IPv4 address (e.g., 172.30.100.104)\n--ip6 | IPv6 address (e.g., 2001:db8::33)\n--ipc | IPC mode to use\n--isolation | Container isolation technology\n--kernel-memory | Kernel memory limit\n-l, --label | Set meta data on a container\n--label-file | Read in a line delimited file of labels\n--link | Add link to another container\n--link-local-ip | Container IPv4/IPv6 link-local addresses\n--log-driver | Logging driver for the container\n--log-opt | Log driver options\n--mac-address | Container MAC address (e.g., 92:d0:c6:0a:29:33)\n-m, --memory | Memory limit\n--memory-reservation | Memory soft limit\n--memory-swap | Swap limit equal to memory plus swap: '-1' to enable unlimited swap\n--memory-swappiness | -1 | Tune container memory swappiness (0 to 100)\n--mount | Attach a filesystem mount to the container\n--name | Assign a name to the container\n--network | Connect a container to a network\n--network-alias | Add network-scoped alias for the container\n--no-healthcheck | Disable any container-specified HEALTHCHECK\n--oom-kill-disable | Disable OOM Killer\n--oom-score-adj | Tune host's OOM preferences (-1000 to 1000)\n--pid | PID namespace to use\n--pids-limit | Tune container pids limit (set -1 for unlimited)\n--platform | API 1.32+ Set platform if server is multi-platform capable\n--privileged | Give extended privileges to this container\n-p, --publish | Publish a container's port(s) to the host\n-P, --publish-all | Publish all exposed ports to random ports\n--pull | missing | Pull image before running (always, missing, never)\n-q, --quiet | Suppress the pull output\n--read-only | Mount the container's root filesystem as read only\n--restart | no | Restart policy to apply when a container exits\n--rm | Automatically remove the container and its associated anonymous volumes when it exits\n--runtime | Runtime to use for this container\n--security-opt | Security Options\n--shm-size | Size of /dev/shm\n--sig-proxy | true | Proxy received signals to the process\n--stop-signal | Signal to stop the container\n--stop-timeout | API 1.25+ Timeout (in seconds) to stop a container\n--storage-opt | Storage driver options for the container\n--sysctl | Sysctl options\n--tmpfs | Mount a tmpfs directory\n-t, --tty | Allocate a pseudo-TTY\n--ulimit | Ulimit options\n--use-api-socket | Bind mount Docker API socket and required auth\n-u, --user | Username or UID (format: <name|uid>[:<group|gid>])\n--userns | User namespace to use\n--uts | UTS namespace to use\n-v, --volume | Bind mount a volume\n--volume-driver | Optional volume driver for the container\n--volumes-from | Mount volumes from the specified container(s)\n-w, --workdir | Working directory inside the container",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 6021
        }
      },
      {
        "header": "Assign name (--name)",
        "content": "The --name flag lets you specify a custom identifier for a container. The following example runs a container named test using the nginx:alpine image in detached mode.\n\nYou can reference the container by name with other commands. For example, the following commands stop and remove a container named test:\n\nIf you don't specify a custom name using the --name flag, the daemon assigns a randomly generated name, such as vibrant_cannon, to the container. Using a custom-defined name provides the benefit of having an easy-to-remember ID for a container.\n\nMoreover, if you connect the container to a user-defined bridge network, other containers on the same network can refer to the container by name via DNS.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --nametest-d nginx:alpine4bed76d3ad428b889c56c1ecc2bf2ed95cb08256db22dc5ef5863e1d03252a19$docker psCONTAINER ID   IMAGE          COMMAND                  CREATED        STATUS                  PORTS     NAMES4bed76d3ad42   nginx:alpine   \"/docker-entrypoint.â¦\"   1 second ago   Up Less than a second   80/tcp    test\n```",
          "```\n$docker stoptesttest$docker rmtesttest\n```",
          "```\n$docker network create mynetcb79f45948d87e389e12013fa4d969689ed2c3316985dd832a43aaec9a0fe394$docker run --nametest--net mynet -d nginx:alpine58df6ecfbc2ad7c42d088ed028d367f9e22a5f834d7c74c66c0ab0485626c32a$docker run --net mynet busybox:latest pingtestPING test (172.18.0.2): 56 data bytes64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.073 ms64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.411 ms64 bytes from 172.18.0.2: seq=2 ttl=64 time=0.319 ms64 bytes from 172.18.0.2: seq=3 ttl=64 time=0.383 ms...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 705
        }
      },
      {
        "header": "Capture container ID (--cidfile)",
        "content": "To help with automation, you can have Docker write the container ID out to a file of your choosing. This is similar to how some programs might write out their process ID to a file (you might've seen them as PID files):\n\nThis creates a container and prints test to the console. The cidfile flag makes Docker attempt to create a new file and write the container ID to it. If the file exists already, Docker returns an error. Docker closes this file when docker run exits.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --cidfile /tmp/docker_test.cid ubuntuecho\"test\"\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 469
        }
      },
      {
        "header": "PID settings (--pid)",
        "content": "By default, all containers have the PID namespace enabled.\n\nPID namespace provides separation of processes. The PID Namespace removes the view of the system processes, and allows process ids to be reused including PID 1.\n\nIn certain cases you want your container to share the host's process namespace, allowing processes within the container to see all of the processes on the system. For example, you could build a container with debugging tools like strace or gdb, but want to use these tools when debugging processes within the container.\n\nTo run htop in a container that shares the process namespac of the host:\n\nRun an alpine container with the --pid=host option:\n\nInstall htop in the container:\n\nInvoke the htop command.\n\nJoining another container's PID namespace can be useful for debugging that container.\n\nStart a container running a Redis server:\n\nRun an Alpine container that attaches the --pid namespace to the my-nginx container:\n\nInstall strace in the Alpine container:\n\nAttach to process 1, the process ID of the my-nginx container:\n\n• Run an alpine container with the --pid=host option:$ docker run --rm -it --pid=host alpine\n• Install htop in the container:/ # apk add --quiet htop\n• Invoke the htop command./ # htop\n\n• Start a container running a Redis server:$ docker run --rm --name my-nginx -d nginx:alpine\n• Run an Alpine container that attaches the --pid namespace to the my-nginx container:$ docker run --rm -it --pid=container:my-nginx \\ --cap-add SYS_PTRACE \\ --security-opt seccomp=unconfined \\ alpine\n• Install strace in the Alpine container:/ # apk add strace\n• Attach to process 1, the process ID of the my-nginx container:/ # strace -p 1 strace: Process 1 attached",
        "code_examples": [
          "```\n--pid=\"\"  : Set the PID (Process) Namespace mode for the container,'container:<name|id>': joins another container's PID namespace'host': use the host's PID namespace inside the container\n```",
          "```\n/ # apk add --quiet htop\n```",
          "```\n/ # htop\n```",
          "```\n/ # apk add strace\n```",
          "```\n/ # strace -p 1strace: Process 1 attached\n```"
        ],
        "usage_examples": [
          "```\n$docker run --rm -it --pid=host alpine\n```",
          "```\n$docker run --rm --name my-nginx -d nginx:alpine\n```",
          "```\n$docker run --rm -it --pid=container:my-nginx\\--cap-add SYS_PTRACE \\--security-opt seccomp=unconfined \\alpine\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 1695
        }
      },
      {
        "header": "Disable namespace remapping for a container (--userns)",
        "content": "If you enable user namespaces on the daemon, all containers are started with user namespaces enabled by default. To disable user namespace remapping for a specific container, you can set the --userns flag to host.\n\nhost is the only valid value for the --userns flag.\n\nFor more information, refer to Isolate containers with a user namespace.",
        "code_examples": [
          "```\ndocker run --userns=host hello-world\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 340
        }
      },
      {
        "header": "UTS settings (--uts)",
        "content": "The UTS namespace is for setting the hostname and the domain that's visible to running processes in that namespace. By default, all containers, including those with --network=host, have their own UTS namespace. Setting --uts to host results in the container using the same UTS namespace as the host.\n\nDocker disallows combining the --hostname and --domainname flags with --uts=host. This is to prevent containers running in the host's UTS namespace from attempting to change the hosts' configuration.\n\nYou may wish to share the UTS namespace with the host if you would like the hostname of the container to change as the hostname of the host changes. A more advanced use case would be changing the host's hostname from a container.\n\n[Admonition] Docker disallows combining the --hostname and --domainname flags with --uts=host. This is to prevent containers running in the host's UTS namespace from attempting to change the hosts' configuration.",
        "code_examples": [
          "```\n--uts=\"\"  : Set the UTS namespace mode for the container'host': use the host's UTS namespace inside the container\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 945
        }
      },
      {
        "header": "IPC settings (--ipc)",
        "content": "The --ipc flag accepts the following values:\n\nIf not specified, daemon default is used, which can either be \"private\" or \"shareable\", depending on the daemon version and configuration.\n\nSystem V interprocess communication (IPC) namespaces provide separation of named shared memory segments, semaphores and message queues.\n\nShared memory segments are used to accelerate inter-process communication at memory speed, rather than through pipes or through the network stack. Shared memory is commonly used by databases and custom-built (typically C/OpenMPI, C++/using boost libraries) high performance applications for scientific computing and financial services industries. If these types of applications are broken into multiple containers, you might need to share the IPC mechanisms of the containers, using \"shareable\" mode for the main (i.e. \"donor\") container, and \"container:<donor-name-or-ID>\" for other containers.\n\nValue | Description\n--- | ---\n\"\" | Use daemon's default.\n\"none\" | Own private IPC namespace, with /dev/shm not mounted.\n\"private\" | Own private IPC namespace.\n\"shareable\" | Own private IPC namespace, with a possibility to share it with other containers.\n\"container:<name-or-ID>\" | Join another (\"shareable\") container's IPC namespace.\n\"host\" | Use the host system's IPC namespace.",
        "code_examples": [
          "```\n--ipc=\"MODE\"  : Set the IPC mode for the container\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 4,
          "content_length": 1300
        }
      },
      {
        "header": "Escalate container privileges (--privileged)",
        "content": "The --privileged flag gives the following capabilities to a container:\n\nIn other words, the container can then do almost everything that the host can do. This flag exists to allow special use-cases, like running Docker within Docker.\n\nUse the --privileged flag with caution. A container with --privileged is not a securely sandboxed process. Containers in this mode can get a root shell on the host and take control over the system.\n\nFor most use cases, this flag should not be the preferred solution. If your container requires escalated privileges, you should prefer to explicitly grant the necessary permissions, for example by adding individual kernel capabilities with --cap-add.\n\nFor more information, see Runtime privilege and Linux capabilities\n\nThe following example doesn't work, because by default, Docker drops most potentially dangerous kernel capabilities, including CAP_SYS_ADMIN (which is required to mount filesystems).\n\nIt works when you add the --privileged flag:\n\n• Enables all Linux kernel capabilities\n• Disables the default seccomp profile\n• Disables the default AppArmor profile\n• Disables the SELinux process label\n• Grants access to all host devices\n• Makes /sys read-write\n• Makes cgroups mounts read-write\n\n[Admonition] Use the --privileged flag with caution. A container with --privileged is not a securely sandboxed process. Containers in this mode can get a root shell on the host and take control over the system.For most use cases, this flag should not be the preferred solution. If your container requires escalated privileges, you should prefer to explicitly grant the necessary permissions, for example by adding individual kernel capabilities with --cap-add.For more information, see Runtime privilege and Linux capabilities",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -t -i --rm ubuntu bashroot@bc338942ef20:/# mount -t tmpfs none /mntmount: permission denied\n```",
          "```\n$docker run -t -i --privileged ubuntu bashroot@50e3f57e16e6:/# mount -t tmpfs none /mntroot@50e3f57e16e6:/# df -hFilesystem      Size  Used Avail Use% Mounted onnone            1.9G     0  1.9G   0% /mnt\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1761
        }
      },
      {
        "header": "Set working directory (-w, --workdir)",
        "content": "The -w option runs the command executed inside the directory specified, in this example, /path/to/dir/. If the path doesn't exist, Docker creates it inside the container.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -w /path/to/dir/ ubuntupwd\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 170
        }
      },
      {
        "header": "Set storage driver options per container (--storage-opt)",
        "content": "This (size) constraints the container filesystem size to 120G at creation time. This option is only available for the btrfs, overlay2, windowsfilter, and zfs storage drivers.\n\nFor the overlay2 storage driver, the size option is only available if the backing filesystem is xfs and mounted with the pquota mount option. Under these conditions, you can pass any size less than the backing filesystem size.\n\nFor the windowsfilter, btrfs, and zfs storage drivers, you cannot pass a size less than the Default BaseFS Size.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -it --storage-optsize=120G fedora /bin/bash\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 516
        }
      },
      {
        "header": "Mount tmpfs (--tmpfs)",
        "content": "The --tmpfs flag lets you create a tmpfs mount.\n\nThe options that you can pass to --tmpfs are identical to the Linux mount -t tmpfs -o command. The following example mounts an empty tmpfs into the container with the rw, noexec, nosuid, size=65536k options.\n\nFor more information, see tmpfs mounts.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d --tmpfs /run:rw,noexec,nosuid,size=65536k my_image\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 297
        }
      },
      {
        "header": "Mount volume (-v)",
        "content": "The example above mounts the current directory into the container at the same path using the -v flag, sets it as the working directory, and then runs the pwd command inside the container.\n\nAs of Docker Engine version 23, you can use relative paths on the host.\n\nThe example above mounts the content directory in the current directory into the container at the /content path using the -v flag, sets it as the working directory, and then runs the pwd command inside the container.\n\nWhen the host directory of a bind-mounted volume doesn't exist, Docker automatically creates this directory on the host for you. In the example above, Docker creates the /doesnt/exist folder before starting your container.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker  run  -v$(pwd):$(pwd)-w$(pwd)-i -t  ubuntupwd\n```",
          "```\n$docker  run  -v ./content:/content -w /content -i -t  ubuntupwd\n```",
          "```\n$docker run -v /doesnt/exist:/foo -w /foo -i -t ubuntu bash\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 702
        }
      },
      {
        "header": "Mount volume read-only (--read-only)",
        "content": "You can use volumes in combination with the --read-only flag to control where a container writes files. The --read-only flag mounts the container's root filesystem as read only prohibiting writes to locations other than the specified volumes for the container.\n\nBy bind-mounting the Docker Unix socket and statically linked Docker binary (refer to get the Linux binary), you give the container the full access to create and manipulate the host's Docker daemon.\n\nOn Windows, you must specify the paths using Windows-style path semantics.\n\nThe following examples fails when using Windows-based containers, as the destination of a volume or bind mount inside the container must be one of: a non-existing or empty directory; or a drive other than C:. Further, the source of a bind mount must be a local directory, not a file.\n\nFor in-depth information about volumes, refer to manage data in containers",
        "code_examples": [
          "```\nPSC:\\>dockerrun-vc:\\foo:c:\\destmicrosoft/nanoservercmd/s/ctypec:\\dest\\somefile.txtContentsoffilePSC:\\>dockerrun-vc:\\foo:d:microsoft/nanoservercmd/s/ctyped:\\somefile.txtContentsoffile\n```",
          "```\nnetusez:\\\\remotemachine\\sharedockerrun-vz:\\foo:c:\\dest...dockerrun-v\\\\uncpath\\to\\directory:c:\\dest...dockerrun-vc:\\foo\\somefile.txt:c:\\dest...dockerrun-vc:\\foo:c:...dockerrun-vc:\\foo:c:\\existing-directory-with-contents...\n```"
        ],
        "usage_examples": [
          "```\n$docker run --read-only -v /icanwrite busybox touch /icanwrite/here\n```",
          "```\n$docker run -t -i -v /var/run/docker.sock:/var/run/docker.sock -v /path/to/static-docker-binary:/usr/bin/docker busybox sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 897
        }
      },
      {
        "header": "Add bind mounts or volumes using the --mount flag",
        "content": "The --mount flag allows you to mount volumes, host-directories, and tmpfs mounts in a container.\n\nThe --mount flag supports most options supported by the -v or the --volume flag, but uses a different syntax. For in-depth information on the --mount flag, and a comparison between --volume and --mount, refer to Bind mounts.\n\nEven though there is no plan to deprecate --volume, usage of --mount is recommended.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --read-only --mounttype=volume,target=/icanwrite busybox touch /icanwrite/here\n```",
          "```\n$docker run -t -i --mounttype=bind,src=/data,dst=/data busybox sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 408
        }
      },
      {
        "header": "Publish or expose port (-p, --expose)",
        "content": "This binds port 8080 of the container to TCP port 80 on 127.0.0.1 of the host. You can also specify udp and sctp ports. The Networking overview page explains in detail how to publish ports with Docker.\n\nIf you don't specify an IP address (i.e., -p 80:80 instead of -p 127.0.0.1:80:80) when publishing a container's ports, Docker publishes the port on all interfaces (address 0.0.0.0) by default. These ports are externally accessible. This also applies if you configured UFW to block this specific port, as Docker manages its own iptables rules. Read more\n\nThis exposes port 80 of the container without publishing the port to the host system's interfaces.\n\n[Admonition] If you don't specify an IP address (i.e., -p 80:80 instead of -p 127.0.0.1:80:80) when publishing a container's ports, Docker publishes the port on all interfaces (address 0.0.0.0) by default. These ports are externally accessible. This also applies if you configured UFW to block this specific port, as Docker manages its own iptables rules. Read more",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -p 127.0.0.1:80:8080/tcp nginx:alpine\n```",
          "```\n$docker run --expose80nginx:alpine\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1022
        }
      },
      {
        "header": "Publish all exposed ports (-P, --publish-all)",
        "content": "The -P, or --publish-all, flag publishes all the exposed ports to the host. Docker binds each exposed port to a random port on the host.\n\nThe -P flag only publishes port numbers that are explicitly flagged as exposed, either using the Dockerfile EXPOSE instruction or the --expose flag for the docker run command.\n\nThe range of ports are within an ephemeral port range defined by /proc/sys/net/ipv4/ip_local_port_range. Use the -p flag to explicitly map a single port or range of ports.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -P nginx:alpine\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 486
        }
      },
      {
        "header": "Set the pull policy (--pull)",
        "content": "Use the --pull flag to set the image pull policy when creating (and running) the container.\n\nThe --pull flag can take one of these values:\n\nWhen creating (and running) a container from an image, the daemon checks if the image exists in the local image cache. If the image is missing, an error is returned to the CLI, allowing it to initiate a pull.\n\nThe default (missing) is to only pull the image if it's not present in the daemon's image cache. This default allows you to run images that only exist locally (for example, images you built from a Dockerfile, but that have not been pushed to a registry), and reduces networking.\n\nThe always option always initiates a pull before creating the container. This option makes sure the image is up-to-date, and prevents you from using outdated images, but may not be suitable in situations where you want to test a locally built image before pushing (as pulling the image overwrites the existing image in the image cache).\n\nThe never option disables (implicit) pulling images when creating containers, and only uses images that are available in the image cache. If the specified image is not found, an error is produced, and the container is not created. This option is useful in situations where networking is not available, or to prevent images from being pulled implicitly when creating containers.\n\nThe following example shows docker run with the --pull=never option set, which produces en error as the image is missing in the image-cache:\n\nValue | Description\n--- | ---\nmissing (default) | Pull the image if it was not found in the image cache, or use the cached image otherwise.\nnever | Do not pull the image, even if it's missing, and produce an error if the image does not exist in the image cache.\nalways | Always perform a pull before creating the container.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --pull=never hello-worlddocker: Error response from daemon: No such image: hello-world:latest.\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 7,
          "content_length": 1812
        }
      },
      {
        "header": "Set environment variables (-e, --env, --env-file)",
        "content": "Use the -e, --env, and --env-file flags to set simple (non-array) environment variables in the container you're running, or overwrite variables defined in the Dockerfile of the image you're running.\n\nYou can define the variable and its value when running the container:\n\nYou can also use variables exported to your local environment:\n\nWhen running the command, the Docker CLI client checks the value the variable has in your local environment and passes it to the container. If no = is provided and that variable isn't exported in your local environment, the variable is unset in the container.\n\nYou can also load the environment variables from a file. This file should use the syntax <variable>=value (which sets the variable to the given value) or <variable> (which takes the value from the local environment), and # for comments. Lines beginning with # are treated as line comments and are ignored, whereas a # appearing anywhere else in a line is treated as part of the variable value.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -e MYVAR1 --envMYVAR2=foo --env-file ./env.list ubuntu bash\n```",
          "```\n$docker run --envVAR1=value1 --envVAR2=value2 ubuntu env|grep VARVAR1=value1VAR2=value2\n```",
          "```\nexport VAR1=value1export VAR2=value2$docker run --env VAR1 --env VAR2 ubuntu env|grep VARVAR1=value1VAR2=value2\n```",
          "```\n$cat env.list#This is a commentVAR1=value1VAR2=value2USER$docker run --env-file env.list ubuntu env|grep -E'VAR|USER'VAR1=value1VAR2=value2USER=jonzeolla\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 989
        }
      },
      {
        "header": "Set metadata on container (-l, --label, --label-file)",
        "content": "A label is a key=value pair that applies metadata to a container. To label a container with two labels:\n\nThe my-label key doesn't specify a value so the label defaults to an empty string (\"\"). To add multiple labels, repeat the label flag (-l or --label).\n\nThe key=value must be unique to avoid overwriting the label value. If you specify labels with identical keys but different values, each subsequent value overwrites the previous. Docker uses the last key=value you supply.\n\nUse the --label-file flag to load multiple labels from a file. Delimit each label in the file with an EOL mark. The example below loads labels from a labels file in the current directory:\n\nThe label-file format is similar to the format for loading environment variables. (Unlike environment variables, labels are not visible to processes running inside a container.) The following example shows a label-file format:\n\nYou can load multiple label-files by supplying multiple --label-file flags.\n\nFor additional information on working with labels, see Labels.",
        "code_examples": [
          "```\ncom.example.label1=\"a label\"#this is a commentcom.example.label2=another\\ labelcom.example.label3\n```"
        ],
        "usage_examples": [
          "```\n$docker run -l my-label --label com.example.foo=bar ubuntu bash\n```",
          "```\n$docker run --label-file ./labels ubuntu bash\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1035
        }
      },
      {
        "header": "Connect a container to a network (--network)",
        "content": "To start a container and connect it to a network, use the --network option.\n\nIf you want to add a running container to a network use the docker network connect subcommand.\n\nYou can connect multiple containers to the same network. Once connected, the containers can communicate using only another container's IP address or name. For overlay networks or custom plugins that support multi-host connectivity, containers connected to the same multi-host network but launched from different Engines can also communicate in this way.\n\nThe default bridge network only allows containers to communicate with each other using internal IP addresses. User-created bridge networks provide DNS resolution between containers using container names.\n\nYou can disconnect a container from a network using the docker network disconnect command.\n\nThe following commands create a network named my-net and add a busybox container to the my-net network.\n\nYou can also choose the IP addresses for the container with --ip and --ip6 flags when you start the container on a user-defined network. To assign a static IP to containers, you must specify subnet block for the network.\n\nTo connect the container to more than one network, repeat the --network option.\n\nTo specify options when connecting to more than one network, use the extended syntax for the --network flag. Comma-separated options that can be specified in the extended --network syntax are:\n\nsysctl settings that start with net.ipv4., net.ipv6. or net.mpls. can be set per-interface using driver-opt label com.docker.network.endpoint.sysctls. The interface name must be the string IFNAME.\n\nTo set more than one sysctl for an interface, quote the whole driver-opt field, remembering to escape the quotes for the shell if necessary. For example, if the interface to my-net is given name eth0, the following example sets sysctls net.ipv4.conf.eth0.log_martians=1 and net.ipv4.conf.eth0.forwarding=0, and assigns the IPv4 address 192.0.2.42.\n\nNetwork drivers may restrict the sysctl settings that can be modified and, to protect the operation of the network, new restrictions may be added in the future.\n\nFor more information on connecting a container to a network when using the run command, see the Docker network overview.\n\nOption | Top-level Equivalent | Description\n--- | --- | ---\nname | The name of the network (mandatory)\nalias | --network-alias | Add network-scoped alias for the container\nip | --ip | IPv4 address (e.g., 172.30.100.104)\nip6 | --ip6 | IPv6 address (e.g., 2001:db8::33)\nmac-address | --mac-address | Container MAC address (e.g., 92:d0:c6:0a:29:33)\nlink-local-ip | --link-local-ip | Container IPv4/IPv6 link-local addresses\ndriver-opt | docker network connect --driver-opt | Network driver options\ngw-priority | Highest gw-priority provides the default gateway. Accepts positive and negative values.\n\n[Admonition] The default bridge network only allows containers to communicate with each other using internal IP addresses. User-created bridge networks provide DNS resolution between containers using container names.\n\n[Admonition] Network drivers may restrict the sysctl settings that can be modified and, to protect the operation of the network, new restrictions may be added in the future.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker network create my-net$docker run -itd --network=my-net busybox\n```",
          "```\n$docker network create --subnet 192.0.2.0/24 my-net$docker run -itd --network=my-net --ip=192.0.2.69 busybox\n```",
          "```\n$docker network create --subnet 192.0.2.0/24 my-net1$docker network create --subnet 192.0.3.0/24 my-net2$docker run -itd --network=my-net1 --network=my-net2 busybox\n```",
          "```\n$docker network create --subnet 192.0.2.0/24 my-net1$docker network create --subnet 192.0.3.0/24 my-net2$docker run -itd --network=name=my-net1,ip=192.0.2.42 --network=name=my-net2,ip=192.0.3.42 busybox\n```",
          "```\n$docker network create --subnet 192.0.2.0/24 my-net$docker run -itd --network=name=my-net,\\\"driver-opt=com.docker.network.endpoint.sysctls=net.ipv4.conf.IFNAME.log_martians=1,net.ipv4.conf.IFNAME.forwarding=0\\\",ip=192.0.2.42 busybox\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 13,
          "content_length": 3247
        }
      },
      {
        "header": "Mount volumes from container (--volumes-from)",
        "content": "The --volumes-from flag mounts all the defined volumes from the referenced containers. You can specify more than one container by repetitions of the --volumes-from argument. The container ID may be optionally suffixed with :ro or :rw to mount the volumes in read-only or read-write mode, respectively. By default, Docker mounts the volumes in the same mode (read write or read only) as the reference container.\n\nLabeling systems like SELinux require placing proper labels on volume content mounted into a container. Without a label, the security system might prevent the processes running inside the container from using the content. By default, Docker does not change the labels set by the OS.\n\nTo change the label in the container context, you can add either of two suffixes :z or :Z to the volume mount. These suffixes tell Docker to relabel file objects on the shared volumes. The z option tells Docker that two containers share the volume content. As a result, Docker labels the content with a shared content label. Shared volume labels allow all containers to read/write content. The Z option tells Docker to label the content with a private unshared label. Only the current container can use a private volume.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --volumes-from 777f7dc92da7 --volumes-from ba8c0c54f0f2:ro -i -t ubuntupwd\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1216
        }
      },
      {
        "header": "Detached mode (-d, --detach)",
        "content": "The --detach (or -d) flag starts a container as a background process that doesn't occupy your terminal window. By design, containers started in detached mode exit when the root process used to run the container exits, unless you also specify the --rm option. If you use -d with --rm, the container is removed when it exits or when the daemon exits, whichever happens first.\n\nDon't pass a service x start command to a detached container. For example, this command attempts to start the nginx service.\n\nThis succeeds in starting the nginx service inside the container. However, it fails the detached container paradigm in that, the root process (service nginx start) returns and the detached container stops as designed. As a result, the nginx service starts but can't be used. Instead, to start a process such as the nginx web server do the following:\n\nTo do input/output with a detached container use network connections or shared volumes. These are required because the container is no longer listening to the command line where docker run was run.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d -p 80:80 my_image service nginx start\n```",
          "```\n$docker run -d -p 80:80 my_image nginx -g'daemon off;'\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1049
        }
      },
      {
        "header": "Override the detach sequence (--detach-keys)",
        "content": "Use the --detach-keys option to override the Docker key sequence for detach. This is useful if the Docker default sequence conflicts with key sequence you use for other applications. There are two ways to define your own detach key sequence, as a per-container override or as a configuration property on your entire configuration.\n\nTo override the sequence for an individual container, use the --detach-keys=\"<sequence>\" flag with the docker attach command. The format of the <sequence> is either a letter [a-Z], or the ctrl- combined with any of the following:\n\nThese a, ctrl-a, X, or ctrl-\\\\ values are all examples of valid key sequences. To configure a different configuration default key sequence for all containers, see Configuration file section.\n\n• a-z (a single lowercase alpha character )\n• @ (at sign)\n• [ (left bracket)\n• \\\\ (two backward slashes)\n• _ (underscore)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 876
        }
      },
      {
        "header": "Add host device to container (--device)",
        "content": "It's often necessary to directly expose devices to a container. The --device option enables that. For example, adding a specific block storage device or loop device or audio device to an otherwise unprivileged container (without the --privileged flag) and have the application directly access it.\n\nBy default, the container is able to read, write and mknod these devices. This can be overridden using a third :rwm set of options to each --device flag. If the container is running in privileged mode, then Docker ignores the specified permissions.\n\nThe --device option cannot be safely used with ephemeral devices. You shouldn't add block devices that may be removed to untrusted containers with --device.\n\nFor Windows, the format of the string passed to the --device option is in the form of --device=<IdType>/<Id>. Beginning with Windows Server 2019 and Windows 10 October 2018 Update, Windows only supports an IdType of class and the Id as a device interface class GUID. Refer to the table defined in the Windows container docs for a list of container-supported device interface class GUIDs.\n\nIf you specify this option for a process-isolated Windows container, Docker makes all devices that implement the requested device interface class GUID available in the container. For example, the command below makes all COM ports on the host visible in the container.\n\nThe --device option is only supported on process-isolated Windows containers, and produces an error if the container isolation is hyperv.\n\nContainer Device Interface (CDI) is a standardized mechanism for container runtimes to create containers which are able to interact with third party devices.\n\nCDI is currently only supported for Linux containers and is enabled by default since Docker Engine 28.3.0.\n\nWith CDI, device configurations are declaratively defined using a JSON or YAML file. In addition to enabling the container to interact with the device node, it also lets you specify additional configuration for the device, such as environment variables, host mounts (such as shared objects), and executable hooks.\n\nYou can reference a CDI device with the --device flag using the fully-qualified name of the device, as shown in the following example:\n\nThis starts an ubuntu container with access to the specified CDI device, vendor.com/class=device-name, assuming that:\n\n• A valid CDI specification (JSON or YAML file) for the requested device is available on the system running the daemon, in one of the configured CDI specification directories.\n• The CDI feature has been enabled in the daemon; see Enable CDI devices.\n\n[Admonition] The --device option cannot be safely used with ephemeral devices. You shouldn't add block devices that may be removed to untrusted containers with --device.\n\n[Admonition] The --device option is only supported on process-isolated Windows containers, and produces an error if the container isolation is hyperv.",
        "code_examples": [
          "```\nPSC:\\>dockerrun--device=class/86E0D1E0-8089-11D0-9CE4-08003E301F73mcr.microsoft.com/windows/servercore:ltsc2019\n```"
        ],
        "usage_examples": [
          "```\n$docker run -it --rm\\--device=/dev/sdc:/dev/xvdc \\--device=/dev/sdd \\--device=/dev/zero:/dev/foobar \\ubuntu ls -l /dev/{xvdc,sdd,foobar}brw-rw---- 1 root disk 8, 2 Feb  9 16:05 /dev/xvdcbrw-rw---- 1 root disk 8, 3 Feb  9 16:05 /dev/sddcrw-rw-rw- 1 root root 1, 5 Feb  9 16:05 /dev/foobar\n```",
          "```\n$docker run --device=/dev/sda:/dev/xvdc --rm -it ubuntu fdisk  /dev/xvdcCommand (m for help): q$docker run --device=/dev/sda:/dev/xvdc:r --rm -it ubuntu fdisk  /dev/xvdcYou will not be able to write the partition table.Command (m for help): q$docker run --device=/dev/sda:/dev/xvdc:rw --rm -it ubuntu fdisk  /dev/xvdcCommand (m for help): q$docker run --device=/dev/sda:/dev/xvdc:m --rm -it ubuntu fdisk  /dev/xvdcfdisk: unable to open /dev/xvdc: Operation not permitted\n```",
          "```\n$docker run --device=vendor.com/class=device-name --rm -it ubuntu\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2912
        }
      },
      {
        "header": "Attach to STDIN/STDOUT/STDERR (-a, --attach)",
        "content": "The --attach (or -a) flag tells docker run to bind to the container's STDIN, STDOUT or STDERR. This makes it possible to manipulate the output and input as needed. You can specify to which of the three standard streams (STDIN, STDOUT, STDERR) you'd like to connect instead, as in:\n\nThe following example pipes data into a container and prints the container's ID by attaching only to the container's STDIN.\n\nThe following example doesn't print anything to the console unless there's an error because output is only attached to the STDERR of the container. The container's logs still store what's written to STDERR and STDOUT.\n\nThe following example shows a way of using --attach to pipe a file into a container. The command prints the container's ID after the build completes and you can retrieve the build logs using docker logs. This is useful if you need to pipe a file or something else into a container and retrieve the container's ID once the container has finished running.\n\nA process running as PID 1 inside a container is treated specially by Linux: it ignores any signal with the default action. So, the process doesn't terminate on SIGINT or SIGTERM unless it's coded to do so.\n\nSee also the docker cp command.\n\n[Admonition] A process running as PID 1 inside a container is treated specially by Linux: it ignores any signal with the default action. So, the process doesn't terminate on SIGINT or SIGTERM unless it's coded to do so.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -a stdin -a stdout -i -t ubuntu /bin/bash\n```",
          "```\n$echo\"test\"|docker run -i -a stdin ubuntu cat -\n```",
          "```\n$docker run -a stderr ubuntuechotest\n```",
          "```\n$cat somefile|docker run -i -a stdin mybuilder dobuild\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1441
        }
      },
      {
        "header": "Keep STDIN open (-i, --interactive)",
        "content": "The --interactive (or -i) flag keeps the container's STDIN open, and lets you send input to the container through standard input.\n\nThe -i flag is most often used together with the --tty flag to bind the I/O streams of the container to a pseudo terminal, creating an interactive terminal session for the container. See Allocate a pseudo-TTY for more examples.\n\nUsing the -i flag on its own allows for composition, such as piping input to containers:",
        "code_examples": [],
        "usage_examples": [
          "```\n$echohello|docker run --rm -i busybox cathello\n```",
          "```\n$docker run -it debianroot@10a3e71492b0:/# factor 9090: 2 3 3 5root@10a3e71492b0:/# exitexit\n```",
          "```\n$docker run --rm -i busyboxecho\"foo bar baz\"\\| docker run --rm -i busybox awk '{ print $2 }' \\| docker run --rm -i busybox revrab\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 448
        }
      },
      {
        "header": "Specify an init process",
        "content": "You can use the --init flag to indicate that an init process should be used as the PID 1 in the container. Specifying an init process ensures the usual responsibilities of an init system, such as reaping zombie processes, are performed inside the created container.\n\nThe default init process used is the first docker-init executable found in the system path of the Docker daemon process. This docker-init binary, included in the default installation, is backed by tini.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 469
        }
      },
      {
        "header": "Allocate a pseudo-TTY (-t, --tty)",
        "content": "The --tty (or -t) flag attaches a pseudo-TTY to the container, connecting your terminal to the I/O streams of the container. Allocating a pseudo-TTY to the container means that you get access to input and output feature that TTY devices provide.\n\nFor example, the following command runs the passwd command in a debian container, to set a new password for the root user.\n\nIf you run this command with only the -i flag (which lets you send text to STDIN of the container), the passwd prompt displays the password in plain text. However, if you try the same thing but also adding the -t flag, the password is hidden:\n\nThis is because passwd can suppress the output of characters to the terminal using the echo-off TTY feature.\n\nYou can use the -t flag without -i flag. This still allocates a pseudo-TTY to the container, but with no way of writing to STDIN. The only time this might be useful is if the output of the container requires a TTY environment.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -i debian passwd rootNew password: karjalanpiirakka9Retype new password: karjalanpiirakka9passwd: password updated successfully\n```",
          "```\n$docker run -it debian passwd rootNew password:Retype new password:passwd: password updated successfully\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 951
        }
      },
      {
        "header": "Specify custom cgroups",
        "content": "Using the --cgroup-parent flag, you can pass a specific cgroup to run a container in. This allows you to create and manage cgroups on their own. You can define custom resources for those cgroups and put containers under a common parent group.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 242
        }
      },
      {
        "header": "Using dynamically created devices (--device-cgroup-rule)",
        "content": "Docker assigns devices available to a container at creation time. The assigned devices are added to the cgroup.allow file and created into the container when it runs. This poses a problem when you need to add a new device to running container.\n\nOne solution is to add a more permissive rule to a container allowing it access to a wider range of devices. For example, supposing the container needs access to a character device with major 42 and any number of minor numbers (added as new devices appear), add the following rule:\n\nThen, a user could ask udev to execute a script that would docker exec my-container mknod newDevX c 42 <minor> the required device when it is added.\n\nYou still need to explicitly add initially present devices to the docker run / docker create command.\n\n[Admonition] You still need to explicitly add initially present devices to the docker run / docker create command.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d --device-cgroup-rule='c 42:* rmw'--name my-container my-image\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 895
        }
      },
      {
        "header": "Access an NVIDIA GPU",
        "content": "The --gpus flag allows you to access NVIDIA GPU resources. First you need to install the nvidia-container-runtime.\n\nYou can also specify a GPU as a CDI device with the --device flag, see CDI devices.\n\nRead Specify a container's resources for more information.\n\nTo use --gpus, specify which GPUs (or all) to use. If you provide no value, Docker uses all available GPUs. The example below exposes all available GPUs.\n\nUse the device option to specify GPUs. The example below exposes a specific GPU.\n\nThe example below exposes the first and third GPUs.\n\n[Admonition] You can also specify a GPU as a CDI device with the --device flag, see CDI devices.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -it --rm --gpus all ubuntu nvidia-smi\n```",
          "```\n$docker run -it --rm --gpusdevice=GPU-3a23c669-1f69-c64e-cf85-44e9b07e7a2a ubuntu nvidia-smi\n```",
          "```\n$docker run -it --rm --gpus'\"device=0,2\"'ubuntu nvidia-smi\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 647
        }
      },
      {
        "header": "Restart policies (--restart)",
        "content": "Use the --restart flag to specify a container's restart policy. A restart policy controls whether the Docker daemon restarts a container after exit. Docker supports the following restart policies:\n\nThis runs the redis container with a restart policy of always. If the container exits, Docker restarts it.\n\nWhen a restart policy is active on a container, it shows as either Up or Restarting in docker ps. It can also be useful to use docker events to see the restart policy in effect.\n\nAn increasing delay (double the previous delay, starting at 100 milliseconds) is added before each restart to prevent flooding the server. This means the daemon waits for 100 ms, then 200 ms, 400, 800, 1600, and so on until either the on-failure limit, the maximum delay of 1 minute is hit, or when you docker stop or docker rm -f the container.\n\nIf a container is successfully restarted (the container is started and runs for at least 10 seconds), the delay is reset to its default value of 100 ms.\n\nYou can specify the maximum amount of times Docker attempts to restart the container when using the on-failure policy. By default, Docker never stops attempting to restart the container.\n\nThe following example runs the redis container with a restart policy of on-failure and a maximum restart count of 10.\n\nIf the redis container exits with a non-zero exit status more than 10 times in a row, Docker stops trying to restart the container. Providing a maximum restart limit is only valid for the on-failure policy.\n\nThe number of (attempted) restarts for a container can be obtained using the docker inspect command. For example, to get the number of restarts for container \"my-container\";\n\nOr, to get the last time the container was (re)started;\n\nCombining --restart (restart policy) with the --rm (clean up) flag results in an error. On container restart, attached clients are disconnected.\n\nFlag | Description\n--- | ---\nno | Don't automatically restart the container. (Default)\non-failure[:max-retries] | Restart the container if it exits due to an error, which manifests as a non-zero exit code. Optionally, limit the number of times the Docker daemon attempts to restart the container using the :max-retries option. The on-failure policy only prompts a restart if the container exits with a failure. It doesn't restart the container if the daemon restarts.\nalways | Always restart the container if it stops. If it's manually stopped, it's restarted only when Docker daemon restarts or the container itself is manually restarted.\nunless-stopped | Similar to always, except that when the container is stopped (manually or otherwise), it isn't restarted even after Docker daemon restarts.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --restart=always redis\n```",
          "```\n$docker run --restart=on-failure:10 redis\n```",
          "```\n$docker inspect -f\"{{ .RestartCount }}\"my-container2\n```",
          "```\n$docker inspect -f\"{{ .State.StartedAt }}\"my-container2015-03-04T23:47:07.691840179Z\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 11,
          "content_length": 2675
        }
      },
      {
        "header": "Clean up (--rm)",
        "content": "By default, a container's file system persists even after the container exits. This makes debugging a lot easier, since you can inspect the container's final state and you retain all your data.\n\nIf you are running short-term foreground processes, these container file systems can start to pile up. If you'd like Docker to automatically clean up the container and remove the file system when the container exits, use the --rm flag:\n\nIf you set the --rm flag, Docker also removes the anonymous volumes associated with the container when the container is removed. This is similar to running docker rm -v my-container. Only volumes that are specified without a name are removed. For example, when running the following command, volume /foo is removed, but not /bar:\n\nVolumes inherited via --volumes-from are removed with the same logic: if the original volume was specified with a name it isn't removed.\n\n[Admonition] If you set the --rm flag, Docker also removes the anonymous volumes associated with the container when the container is removed. This is similar to running docker rm -v my-container. Only volumes that are specified without a name are removed. For example, when running the following command, volume /foo is removed, but not /bar:$ docker run --rm -v /foo -v awesome:/bar busybox top Volumes inherited via --volumes-from are removed with the same logic: if the original volume was specified with a name it isn't removed.",
        "code_examples": [
          "```\n--rm: Automatically remove the container when it exits\n```"
        ],
        "usage_examples": [
          "```\n$docker run --rm -v /foo -v awesome:/bar busybox top\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1433
        }
      },
      {
        "header": "Add entries to container hosts file (--add-host)",
        "content": "You can add other hosts into a container's /etc/hosts file by using one or more --add-host flags. This example adds a static address for a host named my-hostname:\n\nYou can wrap an IPv6 address in square brackets:\n\nThe --add-host flag supports a special host-gateway value that resolves to the internal IP address of the host. This is useful when you want containers to connect to services running on the host machine.\n\nIt's conventional to use host.docker.internal as the hostname referring to host-gateway. Docker Desktop automatically resolves this hostname, see Explore networking features.\n\nThe following example shows how the special host-gateway value works. The example runs an HTTP server that serves a file from host to container over the host.docker.internal hostname, which resolves to the host's internal IP.\n\nThe --add-host flag also accepts a : separator, for example:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --add-host=my-hostname=8.8.8.8 --rm -it alpine/ # ping my-hostnamePING my-hostname (8.8.8.8): 56 data bytes64 bytes from 8.8.8.8: seq=0 ttl=37 time=93.052 ms64 bytes from 8.8.8.8: seq=1 ttl=37 time=92.467 ms64 bytes from 8.8.8.8: seq=2 ttl=37 time=92.252 ms^C--- my-hostname ping statistics ---4 packets transmitted, 4 packets received, 0% packet lossround-trip min/avg/max = 92.209/92.495/93.052 ms\n```",
          "```\n$docker run --add-host my-hostname=[2001:db8::33]--rm -it alpine\n```",
          "```\n$echo\"hello from host!\"> ./hello$python3 -m http.server8000Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...$docker run\\--add-host host.docker.internal=host-gateway \\curlimages/curl -s host.docker.internal:8000/hellohello from host!\n```",
          "```\n$docker run --add-host=my-hostname:8.8.8.8 --rm -it alpine\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 882
        }
      },
      {
        "header": "Logging drivers (--log-driver)",
        "content": "The container can have a different logging driver than the Docker daemon. Use the --log-driver=<DRIVER> with the docker run command to configure the container's logging driver.\n\nTo learn about the supported logging drivers and how to use them, refer to Configure logging drivers.\n\nTo disable logging for a container, set the --log-driver flag to none:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --log-driver=none -d nginx:alpine5101d3b7fe931c27c2ba0e65fd989654d297393ad65ae238f20b97a020e7295b$docker logs 5101d3bError response from daemon: configured logging driver does not support reading\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 351
        }
      },
      {
        "header": "Set ulimits in container (--ulimit)",
        "content": "Since setting ulimit settings in a container requires extra privileges not available in the default container, you can set these using the --ulimit flag. Specify --ulimit with a soft and hard limit in the format <type>=<soft limit>[:<hard limit>]. For example:\n\nIf you don't provide a hard limit value, Docker uses the soft limit value for both values. If you don't provide any values, they are inherited from the default ulimits set on the daemon.\n\nThe as option is deprecated. In other words, the following script is not supported:\n\nDocker sends the values to the appropriate OS syscall and doesn't perform any byte conversion. Take this into account when setting the values.\n\nBe careful setting nproc with the ulimit flag as Linux uses nproc to set the maximum number of processes available to a user, not to a container. For example, start four containers with daemon user:\n\nThe 4th container fails and reports a \"[8] System error: resource temporarily unavailable\" error. This fails because the caller set nproc=3 resulting in the first three containers using up the three processes quota set for the daemon user.\n\nOption | Description\n--- | ---\ncore | Maximum size of core files created (RLIMIT_CORE)\ncpu | CPU time limit in seconds (RLIMIT_CPU)\ndata | Maximum data segment size (RLIMIT_DATA)\nfsize | Maximum file size (RLIMIT_FSIZE)\nlocks | Maximum number of file locks (RLIMIT_LOCKS)\nmemlock | Maximum locked-in-memory address space (RLIMIT_MEMLOCK)\nmsgqueue | Maximum bytes in POSIX message queues (RLIMIT_MSGQUEUE)\nnice | Maximum nice priority adjustment (RLIMIT_NICE)\nnofile | Maximum number of open file descriptors (RLIMIT_NOFILE)\nnproc | Maximum number of processes available (RLIMIT_NPROC)\nrss | Maximum resident set size (RLIMIT_RSS)\nrtprio | Maximum real-time scheduling priority (RLIMIT_RTPRIO)\nrttime | Maximum real-time execution time (RLIMIT_RTTIME)\nsigpending | Maximum number of pending signals (RLIMIT_SIGPENDING)\nstack | Maximum stack size (RLIMIT_STACK)\n\n[Admonition] If you don't provide a hard limit value, Docker uses the soft limit value for both values. If you don't provide any values, they are inherited from the default ulimits set on the daemon.\n\n[Admonition] The as option is deprecated. In other words, the following script is not supported:$ docker run -it --ulimit as=1024 fedora /bin/bash",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --ulimitnofile=1024:1024 --rm debian sh -c\"ulimit -n\"1024\n```",
          "```\n$docker run -it --ulimitas=1024fedora /bin/bash\n```",
          "```\n$docker run -d -u daemon --ulimitnproc=3busybox top$docker run -d -u daemon --ulimitnproc=3busybox top$docker run -d -u daemon --ulimitnproc=3busybox top$docker run -d -u daemon --ulimitnproc=3busybox top\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 6,
          "content_length": 2328
        }
      },
      {
        "header": "Stop container with signal (--stop-signal)",
        "content": "The --stop-signal flag sends the system call signal to the container to exit. This signal can be a signal name in the format SIG<NAME>, for instance SIGKILL, or an unsigned number that matches a position in the kernel's syscall table, for instance 9.\n\nThe default value is defined by STOPSIGNAL in the image, or SIGTERM if the image has no STOPSIGNAL defined.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 359
        }
      },
      {
        "header": "Optional security options (--security-opt)",
        "content": "The --security-opt flag lets you override the default labeling scheme for a container. Specifying the level in the following command allows you to share the same content between containers.\n\nAutomatic translation of MLS labels isn't supported.\n\nTo disable the security labeling for a container entirely, you can use label=disable:\n\nIf you want a tighter security policy on the processes within a container, you can specify a custom type label. The following example runs a container that's only allowed to listen on Apache ports:\n\nYou would have to write policy defining a svirt_apache_t type.\n\nTo prevent your container processes from gaining additional privileges, you can use the following command:\n\nThis means that commands that raise privileges such as su or sudo no longer work. It also causes any seccomp filters to be applied later, after privileges have been dropped which may mean you can have a more restrictive set of filters. For more details, see the kernel documentation.\n\nOn Windows, you can use the --security-opt flag to specify the credentialspec option. The credentialspec must be in the format file://spec.txt or registry://keyname.\n\nOption | Description\n--- | ---\n--security-opt=\"label=user:USER\" | Set the label user for the container\n--security-opt=\"label=role:ROLE\" | Set the label role for the container\n--security-opt=\"label=type:TYPE\" | Set the label type for the container\n--security-opt=\"label=level:LEVEL\" | Set the label level for the container\n--security-opt=\"label=disable\" | Turn off label confinement for the container\n--security-opt=\"apparmor=PROFILE\" | Set the apparmor profile to be applied to the container\n--security-opt=\"no-new-privileges=true\" | Disable container processes from gaining new privileges\n--security-opt=\"seccomp=unconfined\" | Turn off seccomp confinement for the container\n--security-opt=\"seccomp=builtin\" | Use the default (built-in) seccomp profile for the container. This can be used to enable seccomp for a container running on a daemon with a custom default profile set, or with seccomp disabled (\"unconfined\").\n--security-opt=\"seccomp=profile.json\" | White-listed syscalls seccomp Json file to be used as a seccomp filter\n--security-opt=\"systempaths=unconfined\" | Turn off confinement for system paths (masked paths, read-only paths) for the container\n\n[Admonition] Automatic translation of MLS labels isn't supported.\n\n[Admonition] You would have to write policy defining a svirt_apache_t type.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --security-optlabel=level:s0:c100,c200 -it fedora bash\n```",
          "```\n$docker run --security-optlabel=disable -it ubuntu bash\n```",
          "```\n$docker run --security-optlabel=type:svirt_apache_t -it ubuntu bash\n```",
          "```\n$docker run --security-opt no-new-privileges -it ubuntu bash\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 8,
          "content_length": 2458
        }
      },
      {
        "header": "Stop container with timeout (--stop-timeout)",
        "content": "The --stop-timeout flag sets the number of seconds to wait for the container to stop after sending the pre-defined (see --stop-signal) system call signal. If the container does not exit after the timeout elapses, it's forcibly killed with a SIGKILL signal.\n\nIf you set --stop-timeout to -1, no timeout is applied, and the daemon waits indefinitely for the container to exit.\n\nThe Daemon determines the default, and is 10 seconds for Linux containers, and 30 seconds for Windows containers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 489
        }
      },
      {
        "header": "Specify isolation technology for container (--isolation)",
        "content": "This option is useful in situations where you are running Docker containers on Windows. The --isolation=<value> option sets a container's isolation technology. On Linux, the only supported is the default option which uses Linux namespaces. These two commands are equivalent on Linux:\n\nOn Windows, --isolation can take one of these values:\n\nThe default isolation on Windows server operating systems is process, and hyperv on Windows client operating systems, such as Windows 10. Process isolation has better performance, but requires that the image and host use the same kernel version.\n\nOn Windows server, assuming the default configuration, these commands are equivalent and result in process isolation:\n\nIf you have set the --exec-opt isolation=hyperv option on the Docker daemon, or are running against a Windows client-based daemon, these commands are equivalent and result in hyperv isolation:\n\nValue | Description\n--- | ---\ndefault | Use the value specified by the Docker daemon's --exec-opt or system default (see below).\nprocess | Shared-kernel namespace isolation.\nhyperv | Hyper-V hypervisor partition-based isolation.",
        "code_examples": [
          "```\nPSC:\\>dockerrun-dmicrosoft/nanoserverpowershellechoprocessPSC:\\>dockerrun-d--isolationdefaultmicrosoft/nanoserverpowershellechoprocessPSC:\\>dockerrun-d--isolationprocessmicrosoft/nanoserverpowershellechoprocess\n```",
          "```\nPSC:\\>dockerrun-dmicrosoft/nanoserverpowershellechohypervPSC:\\>dockerrun-d--isolationdefaultmicrosoft/nanoserverpowershellechohypervPSC:\\>dockerrun-d--isolationhypervmicrosoft/nanoserverpowershellechohyperv\n```"
        ],
        "usage_examples": [
          "```\n$docker run -d busybox top$docker run -d --isolation default busybox top\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 5,
          "content_length": 1128
        }
      },
      {
        "header": "Specify hard limits on memory available to containers (-m, --memory)",
        "content": "These parameters always set an upper limit on the memory available to the container. Linux sets this on the cgroup and applications in a container can query it at /sys/fs/cgroup/memory/memory.limit_in_bytes.\n\nOn Windows, this affects containers differently depending on what type of isolation you use.\n\nWith process isolation, Windows reports the full memory of the host system, not the limit to applications running inside the container\n\nWith hyperv isolation, Windows creates a utility VM that is big enough to hold the memory limit, plus the minimal OS needed to host the container. That size is reported as \"Total Physical Memory.\"\n\n• With process isolation, Windows reports the full memory of the host system, not the limit to applications running inside the containerPS C:\\> docker run -it -m 2GB --isolation=process microsoft/nanoserver powershell Get-ComputerInfo *memory* CsTotalPhysicalMemory : 17064509440 CsPhyicallyInstalledMemory : 16777216 OsTotalVisibleMemorySize : 16664560 OsFreePhysicalMemory : 14646720 OsTotalVirtualMemorySize : 19154928 OsFreeVirtualMemory : 17197440 OsInUseVirtualMemory : 1957488 OsMaxProcessMemorySize : 137438953344\n• With hyperv isolation, Windows creates a utility VM that is big enough to hold the memory limit, plus the minimal OS needed to host the container. That size is reported as \"Total Physical Memory.\"PS C:\\> docker run -it -m 2GB --isolation=hyperv microsoft/nanoserver powershell Get-ComputerInfo *memory* CsTotalPhysicalMemory : 2683355136 CsPhyicallyInstalledMemory : OsTotalVisibleMemorySize : 2620464 OsFreePhysicalMemory : 2306552 OsTotalVirtualMemorySize : 2620464 OsFreeVirtualMemory : 2356692 OsInUseVirtualMemory : 263772 OsMaxProcessMemorySize : 137438953344",
        "code_examples": [
          "```\nPSC:\\>dockerrun-it-m2GB--isolation=processmicrosoft/nanoserverpowershellGet-ComputerInfo*memory*CsTotalPhysicalMemory:17064509440CsPhyicallyInstalledMemory:16777216OsTotalVisibleMemorySize:16664560OsFreePhysicalMemory:14646720OsTotalVirtualMemorySize:19154928OsFreeVirtualMemory:17197440OsInUseVirtualMemory:1957488OsMaxProcessMemorySize:137438953344\n```",
          "```\nPSC:\\>dockerrun-it-m2GB--isolation=hypervmicrosoft/nanoserverpowershellGet-ComputerInfo*memory*CsTotalPhysicalMemory:2683355136CsPhyicallyInstalledMemory:OsTotalVisibleMemorySize:2620464OsFreePhysicalMemory:2306552OsTotalVirtualMemorySize:2620464OsFreeVirtualMemory:2356692OsInUseVirtualMemory:263772OsMaxProcessMemorySize:137438953344\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1726
        }
      },
      {
        "header": "Configure namespaced kernel parameters (sysctls) at runtime (--sysctl)",
        "content": "The --sysctl sets namespaced kernel parameters (sysctls) in the container. For example, to turn on IP forwarding in the containers network namespace, run this command:\n\nNot all sysctls are namespaced. Docker does not support changing sysctls inside of a container that also modify the host system. As the kernel evolves we expect to see more sysctls become namespaced.\n\n• kernel.msgmax, kernel.msgmnb, kernel.msgmni, kernel.sem, kernel.shmall, kernel.shmmax, kernel.shmmni, kernel.shm_rmid_forced.\n• Sysctls beginning with fs.mqueue.*\n• If you use the --ipc=host option these sysctls are not allowed.\n\n• Sysctls beginning with net.*\n• If you use the --network=host option using these sysctls are not allowed.\n\n[Admonition] Not all sysctls are namespaced. Docker does not support changing sysctls inside of a container that also modify the host system. As the kernel evolves we expect to see more sysctls become namespaced.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --sysctl net.ipv4.ip_forward=1someimage\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 922
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/container/run/",
    "doc_type": "docker",
    "total_sections": 44
  },
  {
    "title": "docker image pull",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Download an image from a registry\nUsage | docker image pull [OPTIONS] NAME[:TAG|@DIGEST]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker pull",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 205
        }
      },
      {
        "header": "Description",
        "content": "Most of your images will be created on top of a base image from the Docker Hub registry.\n\nDocker Hub contains many pre-built images that you can pull and try without needing to define and configure your own.\n\nTo download a particular image, or set of images (i.e., a repository), use docker pull.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 296
        }
      },
      {
        "header": "Proxy configuration",
        "content": "If you are behind an HTTP proxy server, for example in corporate settings, before open a connect to registry, you may need to configure the Docker daemon's proxy settings, refer to the dockerd command-line reference for details.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 228
        }
      },
      {
        "header": "Concurrent downloads",
        "content": "By default the Docker daemon will pull three layers of an image at a time. If you are on a low bandwidth connection this may cause timeout issues and you may want to lower this via the --max-concurrent-downloads daemon option. See the daemon documentation for more details.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 273
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-a, --all-tags | Download all tagged images in the repository\n--disable-content-trust | true | Skip image verification\n--platform | API 1.32+ Set platform if server is multi-platform capable\n-q, --quiet | Suppress verbose output",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 275
        }
      },
      {
        "header": "Pull an image from Docker Hub",
        "content": "To download a particular image, or set of images (i.e., a repository), use docker image pull (or the docker pull shorthand). If no tag is provided, Docker Engine uses the :latest tag as a default. This example pulls the debian:latest image:\n\nDocker images can consist of multiple layers. In the example above, the image consists of a single layer; e756f3fdd6a3.\n\nLayers can be reused by images. For example, the debian:bookworm image shares its layer with the debian:latest. Pulling the debian:bookworm image therefore only pulls its metadata, but not its layers, because the layer is already present locally:\n\nTo see which images are present locally, use the docker images command:\n\nDocker uses a content-addressable image store, and the image ID is a SHA256 digest covering the image's configuration and layers. In the example above, debian:bookworm and debian:latest have the same image ID because they are the same image tagged with different names. Because they are the same image, their layers are stored only once and do not consume extra disk space.\n\nFor more information about images, layers, and the content-addressable store, refer to understand images, containers, and storage drivers.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker image pull debianUsing default tag: latestlatest: Pulling from library/debiane756f3fdd6a3: Pull completeDigest: sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510Status: Downloaded newer image for debian:latestdocker.io/library/debian:latest\n```",
          "```\n$docker image pull debian:bookwormbookworm: Pulling from library/debianDigest: sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510Status: Downloaded newer image for debian:bookwormdocker.io/library/debian:bookworm\n```",
          "```\n$docker imagesREPOSITORY   TAG        IMAGE ID       CREATED        SIZEdebian       bookworm   4eacea30377a   8 days ago     124MBdebian       latest     4eacea30377a   8 days ago     124MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1197
        }
      },
      {
        "header": "Pull an image by digest (immutable identifier)",
        "content": "So far, you've pulled images by their name (and \"tag\"). Using names and tags is a convenient way to work with images. When using tags, you can docker pull an image again to make sure you have the most up-to-date version of that image. For example, docker pull ubuntu:24.04 pulls the latest version of the Ubuntu 24.04 image.\n\nIn some cases you don't want images to be updated to newer versions, but prefer to use a fixed version of an image. Docker enables you to pull an image by its digest. When pulling an image by digest, you specify exactly which version of an image to pull. Doing so, allows you to \"pin\" an image to that version, and guarantee that the image you're using is always the same.\n\nTo know the digest of an image, pull the image first. Let's pull the latest ubuntu:24.04 image from Docker Hub:\n\nDocker prints the digest of the image after the pull has finished. In the example above, the digest of the image is:\n\nDocker also prints the digest of an image when pushing to a registry. This may be useful if you want to pin to a version of the image you just pushed.\n\nA digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the following command:\n\nDigest can also be used in the FROM of a Dockerfile, for example:\n\nUsing this feature \"pins\" an image to a specific version in time. Docker does therefore not pull updated versions of an image, which may include security updates. If you want to pull an updated image, you need to change the digest accordingly.\n\n[Admonition] Using this feature \"pins\" an image to a specific version in time. Docker does therefore not pull updated versions of an image, which may include security updates. If you want to pull an updated image, you need to change the digest accordingly.",
        "code_examples": [
          "```\nsha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30\n```",
          "```\nFROMubuntu@sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30LABELorg.opencontainers.image.authors=\"some maintainer <maintainer@example.com>\"\n```"
        ],
        "usage_examples": [
          "```\n$docker pull ubuntu:24.0424.04: Pulling from library/ubuntu125a6e411906: Pull completeDigest: sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30Status: Downloaded newer image for ubuntu:24.04docker.io/library/ubuntu:24.04\n```",
          "```\n$docker pull ubuntu@sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30docker.io/library/ubuntu@sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30: Pulling from library/ubuntuDigest: sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30Status: Image is up to date for ubuntu@sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30docker.io/library/ubuntu@sha256:2e863c44b718727c860746568e1d54afd13b2fa71b160f5cd9058fc436217b30\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1786
        }
      },
      {
        "header": "Pull from a different registry",
        "content": "By default, docker pull pulls images from Docker Hub. It is also possible to manually specify the path of a registry to pull from. For example, if you have set up a local registry, you can specify its path to pull from it. A registry path is similar to a URL, but does not contain a protocol specifier (https://).\n\nThe following command pulls the testing/test-image image from a local registry listening on port 5000 (myregistry.local:5000):\n\nRegistry credentials are managed by docker login.\n\nDocker uses the https:// protocol to communicate with a registry, unless the registry is allowed to be accessed over an insecure connection. Refer to the insecure registries section for more information.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker image pull myregistry.local:5000/testing/test-image\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 697
        }
      },
      {
        "header": "Pull a repository with multiple images (-a, --all-tags)",
        "content": "By default, docker pull pulls a single image from the registry. A repository can contain multiple images. To pull all images from a repository, provide the -a (or --all-tags) option when using docker pull.\n\nThis command pulls all images from the ubuntu repository:\n\nAfter the pull has completed use the docker image ls command (or the docker images shorthand) to see the images that were pulled. The example below shows all the ubuntu images that are present locally:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker image pull --all-tags ubuntuPulling repository ubuntuad57ef8d78d7: Download complete105182bb5e8b: Download complete511136ea3c5a: Download complete73bd853d2ea5: Download complete....Status: Downloaded newer image for ubuntu\n```",
          "```\n$docker image ls --filterreference=ubuntuREPOSITORY   TAG       IMAGE ID       CREATED        SIZEubuntu       22.04     8a3cdc4d1ad3   3 weeks ago    77.9MBubuntu       jammy     8a3cdc4d1ad3   3 weeks ago    77.9MBubuntu       24.04     35a88802559d   6 weeks ago    78.1MBubuntu       latest    35a88802559d   6 weeks ago    78.1MBubuntu       noble     35a88802559d   6 weeks ago    78.1MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 467
        }
      },
      {
        "header": "Cancel a pull",
        "content": "Killing the docker pull process, for example by pressing CTRL-c while it is running in a terminal, will terminate the pull operation.\n\nThe Engine terminates a pull operation when the connection between the daemon and the client (initiating the pull) is cut or lost for any reason or the command is manually terminated.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker pull ubuntuUsing default tag: latestlatest: Pulling from library/ubuntua3ed95caeb02: Pulling fs layer236608c7b546: Pulling fs layer^C\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 318
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/image/pull/",
    "doc_type": "docker",
    "total_sections": 10
  },
  {
    "title": "docker image push",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Upload an image to a registry\nUsage | docker image push [OPTIONS] NAME[:TAG]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker push",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 193
        }
      },
      {
        "header": "Description",
        "content": "Use docker image push to share your images to the Docker Hub registry or to a self-hosted one.\n\nRefer to the docker image tag reference for more information about valid image and tag names.\n\nKilling the docker image push process, for example by pressing CTRL-c while it is running in a terminal, terminates the push operation.\n\nProgress bars are shown during docker push, which show the uncompressed size. The actual amount of data that's pushed will be compressed before sending, so the uploaded size will not be reflected by the progress bar.\n\nRegistry credentials are managed by docker login.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 595
        }
      },
      {
        "header": "Concurrent uploads",
        "content": "By default the Docker daemon will push five layers of an image at a time. If you are on a low bandwidth connection this may cause timeout issues and you may want to lower this via the --max-concurrent-uploads daemon option. See the daemon documentation for more details.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 270
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-a, --all-tags | Push all tags of an image to the repository\n--disable-content-trust | true | Skip image signing\n--platform | API 1.46+ Push a platform-specific manifest as a single-platform image to the registry.Image index won't be pushed, meaning that other manifests, including attestations won't be preserved.'os[/arch[/variant]]': Explicit platform (eg. linux/amd64)\n-q, --quiet | Suppress verbose output",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 457
        }
      },
      {
        "header": "Push a new image to a registry",
        "content": "First save the new image by finding the container ID (using docker container ls) and then committing it to a new image name. Note that only a-z0-9-_. are allowed when naming images:\n\nNow, push the image to the registry using the image ID. In this example the registry is on host named registry-host and listening on port 5000. To do this, tag the image with the host name or IP address, and the port of the registry:\n\nCheck that this worked by running:\n\nYou should see both rhel-httpd and registry-host:5000/myadmin/rhel-httpd listed.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker container commit c16378f943fe rhel-httpd:latest\n```",
          "```\n$docker image tag rhel-httpd:latest registry-host:5000/myadmin/rhel-httpd:latest$docker image push registry-host:5000/myadmin/rhel-httpd:latest\n```",
          "```\n$docker image ls\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 534
        }
      },
      {
        "header": "Push all tags of an image (-a, --all-tags)",
        "content": "Use the -a (or --all-tags) option to push all tags of a local image.\n\nThe following example creates multiple tags for an image, and pushes all those tags to Docker Hub.\n\nThe image is now tagged under multiple names:\n\nWhen pushing with the --all-tags option, all tags of the registry-host:5000/myname/myimage image are pushed:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker image tag myimage registry-host:5000/myname/myimage:latest$docker image tag myimage registry-host:5000/myname/myimage:v1.0.1$docker image tag myimage registry-host:5000/myname/myimage:v1.0$docker image tag myimage registry-host:5000/myname/myimage:v1\n```",
          "```\n$docker image lsREPOSITORY                          TAG        IMAGE ID       CREATED      SIZEmyimage                             latest     6d5fcfe5ff17   2 hours ago  1.22MBregistry-host:5000/myname/myimage   latest     6d5fcfe5ff17   2 hours ago  1.22MBregistry-host:5000/myname/myimage   v1         6d5fcfe5ff17   2 hours ago  1.22MBregistry-host:5000/myname/myimage   v1.0       6d5fcfe5ff17   2 hours ago  1.22MBregistry-host:5000/myname/myimage   v1.0.1     6d5fcfe5ff17   2 hours ago  1.22MB\n```",
          "```\n$docker image push --all-tags registry-host:5000/myname/myimageThe push refers to repository [registry-host:5000/myname/myimage]195be5f8be1d: Pushedlatest: digest: sha256:edafc0a0fb057813850d1ba44014914ca02d671ae247107ca70c94db686e7de6 size: 4527195be5f8be1d: Layer already existsv1: digest: sha256:edafc0a0fb057813850d1ba44014914ca02d671ae247107ca70c94db686e7de6 size: 4527195be5f8be1d: Layer already existsv1.0: digest: sha256:edafc0a0fb057813850d1ba44014914ca02d671ae247107ca70c94db686e7de6 size: 4527195be5f8be1d: Layer already existsv1.0.1: digest: sha256:edafc0a0fb057813850d1ba44014914ca02d671ae247107ca70c94db686e7de6 size: 4527\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 325
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/image/push/",
    "doc_type": "docker",
    "total_sections": 6
  },
  {
    "title": "docker container ls",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | List containers\nUsage | docker container ls [OPTIONS]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker container list docker container ps docker ps",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 210
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-a, --all | Show all containers (default shows just running)\n-f, --filter | Filter output based on conditions provided\n--format | Format output using a custom template:'table': Print output in table format with column headers (default)'table TEMPLATE': Print output in table format using the given Go template'json': Print in JSON format'TEMPLATE': Print output using the given Go template.Refer to https://docs.docker.com/go/formatting/ for more information about formatting output with templates\n-n, --last | -1 | Show n last created containers (includes all states)\n-l, --latest | Show the latest created container (includes all states)\n--no-trunc | Don't truncate output\n-q, --quiet | Only display container IDs\n-s, --size | Display total file sizes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 800
        }
      },
      {
        "header": "Do not truncate output (--no-trunc)",
        "content": "Running docker ps --no-trunc showing 2 linked containers.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker ps --no-truncCONTAINER ID                                                     IMAGE                        COMMAND                CREATED              STATUS              PORTS               NAMESca5534a51dd04bbcebe9b23ba05f389466cf0c190f1f8f182d7eea92a9671d00 ubuntu:24.04                 bash                   17 seconds ago       Up 16 seconds       3300-3310/tcp       webapp9ca9747b233100676a48cc7806131586213fa5dab86dd1972d6a8732e3a84a4d crosbymichael/redis:latest   /redis-server --dir    33 minutes ago       Up 33 minutes       6379/tcp            redis,webapp/db\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 57
        }
      },
      {
        "header": "Show both running and stopped containers (-a, --all)",
        "content": "The docker ps command only shows running containers by default. To see all containers, use the --all (or -a) flag:\n\ndocker ps groups exposed ports into a single range if possible. E.g., a container that exposes TCP ports 100, 101, 102 displays 100-102/tcp in the PORTS column.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker ps -a\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 276
        }
      },
      {
        "header": "Show disk usage by container (--size)",
        "content": "The docker ps --size (or -s) command displays two different on-disk-sizes for each container:\n\nFor more information, refer to the container size on disk section.\n\n• The \"size\" information shows the amount of data (on disk) that is used for the writable layer of each container\n• The \"virtual size\" is the total amount of disk-space used for the read-only image data used by the container and the writable layer.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker ps --sizeCONTAINER ID   IMAGE          COMMAND                  CREATED        STATUS       PORTS   NAMES        SIZEe90b8831a4b8   nginx          \"/bin/bash -c 'mkdir \"   11 weeks ago   Up 4 hours           my_nginx     35.58 kB (virtual 109.2 MB)00c6131c5e30   telegraf:1.5   \"/entrypoint.sh\"         11 weeks ago   Up 11 weeks          my_telegraf  0 B (virtual 209.5 MB)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 411
        }
      },
      {
        "header": "Filtering (--filter)",
        "content": "The --filter (or -f) flag format is a key=value pair. If there is more than one filter, then pass multiple flags (e.g. --filter \"foo=bar\" --filter \"bif=baz\").\n\nThe currently supported filters are:\n\nThe label filter matches containers based on the presence of a label alone or a label and a value.\n\nThe following filter matches containers with the color label regardless of its value.\n\nThe following filter matches containers with the color label with the blue value.\n\nThe name filter matches on all or part of a container's name.\n\nThe following filter matches all containers with a name containing the nostalgic_stallman string.\n\nYou can also filter for a substring in a name as this shows:\n\nThe exited filter matches containers by exist status code. For example, to filter for containers that have exited successfully:\n\nYou can use a filter to locate containers that exited with status of 137 meaning a SIGKILL(9) killed them.\n\nAny of these events result in a 137 status:\n\nThe status filter matches containers by status. The possible values for the container status are:\n\nFor example, to filter for running containers:\n\nTo filter for paused containers:\n\nThe ancestor filter matches containers based on its image or a descendant of it. The filter supports the following image representation:\n\nIf you don't specify a tag, the latest tag is used. For example, to filter for containers that use the latest ubuntu image:\n\nMatch containers based on the ubuntu-c1 image which, in this case, is a child of ubuntu:\n\nMatch containers based on the ubuntu version 24.04 image:\n\nThe following matches containers based on the layer d0e008c6cf02 or an image that have this layer in its layer stack.\n\nThe before filter shows only containers created before the container with a given ID or name. For example, having these containers created:\n\nFiltering with before would give:\n\nThe since filter shows only containers created since the container with a given ID or name. For example, with the same containers as in before filter:\n\nThe volume filter shows only containers that mount a specific volume or have a volume mounted in a specific path:\n\nThe network filter shows only containers that are connected to a network with a given name or ID.\n\nThe following filter matches all containers that are connected to a network with a name containing net1.\n\nThe network filter matches on both the network's name and ID. The following example shows all containers that are attached to the net1 network, using the network ID as a filter:\n\nThe publish and expose filters show only containers that have published or exposed port with a given port number, port range, and/or protocol. The default protocol is tcp when not specified.\n\nThe following filter matches all containers that have published port of 80:\n\nThe following filter matches all containers that have exposed TCP port in the range of 8000-8080:\n\nThe following filter matches all containers that have exposed UDP port 80:\n\n• the init process of the container is killed manually\n• docker kill kills the container\n• Docker daemon restarts which kills all running containers\n\n• image:tag@digest\n\nFilter | Description\n--- | ---\nid | Container's ID\nname | Container's name\nlabel | An arbitrary string representing either a key or a key-value pair. Expressed as <key> or <key>=<value>\nexited | An integer representing the container's exit code. Only useful with --all.\nstatus | One of created, restarting, running, removing, paused, exited, or dead\nancestor | Filters containers which share a given image as an ancestor. Expressed as <image-name>[:<tag>], <image id>, or <image@digest>\nbefore or since | Filters containers created before or after a given container ID or name\nvolume | Filters running containers which have mounted a given volume or bind mount.\nnetwork | Filters running containers connected to a given network.\npublish or expose | Filters containers which publish or expose a given port. Expressed as <port>[/<proto>] or <startport-endport>/[<proto>]\nhealth | Filters containers based on their healthcheck status. One of starting, healthy, unhealthy or none.\nisolation | Windows daemon only. One of default, process, or hyperv.\nis-task | Filters containers that are a \"task\" for a service. Boolean option (true or false)\n\nStatus | Description\n--- | ---\ncreated | A container that has never been started.\nrunning | A running container, started by either docker start or docker run.\npaused | A paused container. See docker pause.\nrestarting | A container which is starting due to the designated restart policy for that container.\nexited | A container which is no longer running. For example, the process inside the container completed or the container was stopped using the docker stop command.\nremoving | A container which is in the process of being removed. See docker rm.\ndead | A \"defunct\" container; for example, a container that was only partially removed because resources were kept busy by an external process. dead containers cannot be (re)started, only removed.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker ps --filter\"label=color\"CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES673394ef1d4c        busybox             \"top\"               47 seconds ago      Up 45 seconds                           nostalgic_shockleyd85756f57265        busybox             \"top\"               52 seconds ago      Up 51 seconds                           high_albattani\n```",
          "```\n$docker ps --filter\"label=color=blue\"CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMESd85756f57265        busybox             \"top\"               About a minute ago   Up About a minute                       high_albattani\n```",
          "```\n$docker ps --filter\"name=nostalgic_stallman\"CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES9b6247364a03        busybox             \"top\"               2 minutes ago       Up 2 minutes                            nostalgic_stallman\n```",
          "```\n$docker ps --filter\"name=nostalgic\"CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES715ebfcee040        busybox             \"top\"               3 seconds ago       Up 1 second                             i_am_nostalgic9b6247364a03        busybox             \"top\"               7 minutes ago       Up 7 minutes                            nostalgic_stallman673394ef1d4c        busybox             \"top\"               38 minutes ago      Up 38 minutes                           nostalgic_shockley\n```",
          "```\n$docker ps -a --filter'exited=0'CONTAINER ID        IMAGE             COMMAND                CREATED             STATUS                   PORTS                      NAMESea09c3c82f6e        registry:latest   /srv/run.sh            2 weeks ago         Exited (0) 2 weeks ago   127.0.0.1:5000->5000/tcp   desperate_leakey106ea823fe4e        fedora:latest     /bin/sh -c 'bash -l'   2 weeks ago         Exited (0) 2 weeks ago                              determined_albattani48ee228c9464        fedora:20         bash                   2 weeks ago         Exited (0) 2 weeks ago                              tender_torvalds\n```",
          "```\n$docker ps -a --filter'exited=137'CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS                       PORTS               NAMESb3e1c0ed5bfe        ubuntu:latest       \"sleep 1000\"           12 seconds ago      Exited (137) 5 seconds ago                       grave_kowalevskia2eb5558d669        redis:latest        \"/entrypoint.sh redi   2 hours ago         Exited (137) 2 hours ago                         sharp_lalande\n```",
          "```\n$docker ps --filterstatus=runningCONTAINER ID        IMAGE                  COMMAND             CREATED             STATUS              PORTS               NAMES715ebfcee040        busybox                \"top\"               16 minutes ago      Up 16 minutes                           i_am_nostalgicd5c976d3c462        busybox                \"top\"               23 minutes ago      Up 23 minutes                           top9b6247364a03        busybox                \"top\"               24 minutes ago      Up 24 minutes                           nostalgic_stallman\n```",
          "```\n$docker ps --filterstatus=pausedCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES673394ef1d4c        busybox             \"top\"               About an hour ago   Up About an hour (Paused)                       nostalgic_shockley\n```",
          "```\n$docker ps --filterancestor=ubuntuCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES919e1179bdb8        ubuntu-c1           \"top\"               About a minute ago   Up About a minute                       admiring_lovelace5d1e4a540723        ubuntu-c2           \"top\"               About a minute ago   Up About a minute                       admiring_sammet82a598284012        ubuntu              \"top\"               3 minutes ago        Up 3 minutes                            sleepy_bosebab2a34ba363        ubuntu              \"top\"               3 minutes ago        Up 3 minutes                            focused_yonath\n```",
          "```\n$docker ps --filterancestor=ubuntu-c1CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES919e1179bdb8        ubuntu-c1           \"top\"               About a minute ago   Up About a minute                       admiring_lovelace\n```",
          "```\n$docker ps --filterancestor=ubuntu:24.04CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES82a598284012        ubuntu:24.04        \"top\"               3 minutes ago        Up 3 minutes                            sleepy_bose\n```",
          "```\n$docker ps --filterancestor=d0e008c6cf02CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS               NAMES82a598284012        ubuntu:24.04        \"top\"               3 minutes ago        Up 3 minutes                            sleepy_bose\n```",
          "```\n$docker psCONTAINER ID        IMAGE       COMMAND       CREATED              STATUS              PORTS              NAMES9c3527ed70ce        busybox     \"top\"         14 seconds ago       Up 15 seconds                          desperate_dubinsky4aace5031105        busybox     \"top\"         48 seconds ago       Up 49 seconds                          focused_hamilton6e63f6ff38b0        busybox     \"top\"         About a minute ago   Up About a minute                      distracted_fermat\n```",
          "```\n$docker ps -fbefore=9c3527ed70ceCONTAINER ID        IMAGE       COMMAND       CREATED              STATUS              PORTS              NAMES4aace5031105        busybox     \"top\"         About a minute ago   Up About a minute                      focused_hamilton6e63f6ff38b0        busybox     \"top\"         About a minute ago   Up About a minute                      distracted_fermat\n```",
          "```\n$docker ps -fsince=6e63f6ff38b0CONTAINER ID        IMAGE       COMMAND       CREATED             STATUS              PORTS               NAMES9c3527ed70ce        busybox     \"top\"         10 minutes ago      Up 10 minutes                           desperate_dubinsky4aace5031105        busybox     \"top\"         10 minutes ago      Up 10 minutes                           focused_hamilton\n```",
          "```\n$docker ps --filtervolume=remote-volume --format\"table {{.ID}}\\t{{.Mounts}}\"CONTAINER ID        MOUNTS9c3527ed70ce        remote-volume$docker ps --filtervolume=/data --format\"table {{.ID}}\\t{{.Mounts}}\"CONTAINER ID        MOUNTS9c3527ed70ce        remote-volume\n```",
          "```\n$docker run -d --net=net1 --name=test1 ubuntu top$docker run -d --net=net2 --name=test2 ubuntu top$docker ps --filternetwork=net1CONTAINER ID        IMAGE       COMMAND       CREATED             STATUS              PORTS               NAMES9d4893ed80fe        ubuntu      \"top\"         10 minutes ago      Up 10 minutes                           test1\n```",
          "```\n$docker network inspect --format\"{{.ID}}\"net18c0b4110ae930dbe26b258de9bc34a03f98056ed6f27f991d32919bfe401d7c5$docker ps --filternetwork=8c0b4110ae930dbe26b258de9bc34a03f98056ed6f27f991d32919bfe401d7c5CONTAINER ID        IMAGE       COMMAND       CREATED             STATUS              PORTS               NAMES9d4893ed80fe        ubuntu      \"top\"         10 minutes ago      Up 10 minutes                           test1\n```",
          "```\n$docker run -d --publish=80busybox top$docker run -d --expose=8080busybox top$docker ps -aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                   NAMES9833437217a5        busybox             \"top\"               5 seconds ago       Up 4 seconds        8080/tcp                dreamy_mccarthyfc7e477723b7        busybox             \"top\"               50 seconds ago      Up 50 seconds       0.0.0.0:32768->80/tcp   admiring_roentgen$docker ps --filterpublish=80CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS              PORTS                   NAMESfc7e477723b7        busybox             \"top\"               About a minute ago   Up About a minute   0.0.0.0:32768->80/tcp   admiring_roentgen\n```",
          "```\n$docker ps --filterexpose=8000-8080/tcpCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES9833437217a5        busybox             \"top\"               21 seconds ago      Up 19 seconds       8080/tcp            dreamy_mccarthy\n```",
          "```\n$docker ps --filterpublish=80/udpCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 30,
          "content_length": 5014
        }
      },
      {
        "header": "Format the output (--format)",
        "content": "The formatting option (--format) pretty-prints container output using a Go template.\n\nValid placeholders for the Go template are listed below:\n\nWhen using the --format option, the ps command will either output the data exactly as the template declares or, when using the table directive, includes column headers as well.\n\nThe following example uses a template without headers and outputs the ID and Command entries separated by a colon (:) for all running containers:\n\nTo list all running containers with their labels in a table format you can use:\n\nTo list all running containers in JSON format, use the json directive:\n\nPlaceholder | Description\n--- | ---\n.ID | Container ID\n.Image | Image ID\n.Command | Quoted command\n.CreatedAt | Time when the container was created.\n.RunningFor | Elapsed time since the container was started.\n.Ports | Exposed ports.\n.State | Container status (for example; \"created\", \"running\", \"exited\").\n.Status | Container status with details about duration and health-status.\n.Size | Container disk size.\n.Names | Container names.\n.Labels | All labels assigned to the container.\n.Label | Value of a specific label for this container. For example '{{.Label \"com.docker.swarm.cpu\"}}'\n.Mounts | Names of the volumes mounted in this container.\n.Networks | Names of the networks attached to this container.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker ps --format\"{{.ID}}: {{.Command}}\"a87ecb4f327c: /bin/sh -c #(nop) MA01946d9d34d8: /bin/sh -c #(nop) MAc1d3b0166030: /bin/sh -c yum -y up41d50ecd2f57: /bin/sh -c #(nop) MA\n```",
          "```\n$docker ps --format\"table {{.ID}}\\t{{.Labels}}\"CONTAINER ID        LABELSa87ecb4f327c        com.docker.swarm.node=ubuntu,com.docker.swarm.storage=ssd01946d9d34d8c1d3b0166030        com.docker.swarm.node=debian,com.docker.swarm.cpu=641d50ecd2f57        com.docker.swarm.node=fedora,com.docker.swarm.cpu=3,com.docker.swarm.storage=ssd\n```",
          "```\n$docker ps --format json{\"Command\":\"\\\"/docker-entrypoint.â¦\\\"\",\"CreatedAt\":\"2021-03-10 00:15:05 +0100 CET\",\"ID\":\"a762a2b37a1d\",\"Image\":\"nginx\",\"Labels\":\"maintainer=NGINX Docker Maintainers \\u003cdocker-maint@nginx.com\\u003e\",\"LocalVolumes\":\"0\",\"Mounts\":\"\",\"Names\":\"boring_keldysh\",\"Networks\":\"bridge\",\"Ports\":\"80/tcp\",\"RunningFor\":\"4 seconds ago\",\"Size\":\"0B\",\"State\":\"running\",\"Status\":\"Up 3 seconds\"}\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 6,
          "content_length": 1327
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/container/ls/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "docker image ls",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | List images\nUsage | docker image ls [OPTIONS] [REPOSITORY[:TAG]]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker image list docker images",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 201
        }
      },
      {
        "header": "Description",
        "content": "The default docker images will show all top level images, their repository and tags, and their size.\n\nDocker images have intermediate layers that increase reusability, decrease disk usage, and speed up docker build by allowing each step to be cached. These intermediate layers are not shown by default.\n\nThe SIZE is the cumulative space taken up by the image and all its parent images. This is also the disk space used by the contents of the Tar file created when you docker save an image.\n\nAn image will be listed more than once if it has multiple repository names or tags. This single image (identifiable by its matching IMAGE ID) uses up the SIZE listed only once.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 667
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-a, --all | Show all images (default hides intermediate images)\n--digests | Show digests\n-f, --filter | Filter output based on conditions provided\n--format | Format output using a custom template:'table': Print output in table format with column headers (default)'table TEMPLATE': Print output in table format using the given Go template'json': Print in JSON format'TEMPLATE': Print output using the given Go template.Refer to https://docs.docker.com/go/formatting/ for more information about formatting output with templates\n--no-trunc | Don't truncate output\n-q, --quiet | Only show image IDs\n--tree | API 1.47+ experimental (CLI) List multi-platform images as a tree (EXPERIMENTAL)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 731
        }
      },
      {
        "header": "List images by name and tag",
        "content": "The docker images command takes an optional [REPOSITORY[:TAG]] argument that restricts the list to images that match the argument. If you specify REPOSITORYbut no TAG, the docker images command lists all images in the given repository.\n\nFor example, to list all images in the java repository, run the following command:\n\nThe [REPOSITORY[:TAG]] value must be an exact match. This means that, for example, docker images jav does not match the image java.\n\nIf both REPOSITORY and TAG are provided, only images matching that repository and tag are listed. To find all local images in the java repository with tag 8 you can use:\n\nIf nothing matches REPOSITORY[:TAG], the list is empty.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker images javaREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEjava                8                   308e519aac60        6 days ago          824.5 MBjava                7                   493d82594c15        3 months ago        656.3 MBjava                latest              2711b1d6f3aa        5 months ago        603.9 MB\n```",
          "```\n$docker images java:8REPOSITORY          TAG                 IMAGE ID            CREATED             SIZEjava                8                   308e519aac60        6 days ago          824.5 MB\n```",
          "```\n$docker images java:0REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 680
        }
      },
      {
        "header": "List image digests (--digests)",
        "content": "Images that use the v2 or later format have a content-addressable identifier called a digest. As long as the input used to generate the image is unchanged, the digest value is predictable. To list image digest values, use the --digests flag:\n\nWhen pushing or pulling to a 2.0 registry, the push or pull command output includes the image digest. You can pull using a digest value. You can also reference by digest in create, run, and rmi commands, as well as the FROM image reference in a Dockerfile.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker images --digestsREPOSITORY                         TAG                 DIGEST                                                                    IMAGE ID            CREATED             SIZElocalhost:5000/test/busybox        <none>              sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf   4986bf8c1536        9 weeks ago         2.43 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 499
        }
      },
      {
        "header": "Filtering (--filter)",
        "content": "The filtering flag (-f or --filter) format is of \"key=value\". If there is more than one filter, then pass multiple flags (e.g., --filter \"foo=bar\" --filter \"bif=baz\").\n\nThe currently supported filters are:\n\nThis will display untagged images that are the leaves of the images tree (not intermediary layers). These images occur when a new build of an image takes the repo:tag away from the image ID, leaving it as <none>:<none> or untagged. A warning will be issued if trying to remove an image when a container is presently using it. By having this flag it allows for batch cleanup.\n\nYou can use this in conjunction with docker rmi:\n\nDocker warns you if any containers exist that are using these untagged images.\n\nThe label filter matches images based on the presence of a label alone or a label and a value.\n\nThe following filter matches images with the com.example.version label regardless of its value.\n\nThe following filter matches images with the com.example.version label with the 1.0 value.\n\nIn this example, with the 0.1 value, it returns an empty set because no matches were found.\n\nThe before filter shows only images created before the image with a given ID or reference. For example, having these images:\n\nFiltering with before would give:\n\nFiltering with since would give:\n\nThe reference filter shows only images whose reference matches the specified pattern.\n\nFiltering with reference would give:\n\nFiltering with multiple reference would give, either match A or B:\n\n• dangling (boolean - true or false)\n• label (label=<key> or label=<key>=<value>)\n• before (<image-name>[:<tag>], <image id> or <image@digest>) - filter images created before given id or references\n• since (<image-name>[:<tag>], <image id> or <image@digest>) - filter images created since given id or references\n• reference (pattern of an image reference) - filter images whose reference matches the specified pattern",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker images --filter\"dangling=true\"REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE<none>              <none>              8abc22fbb042        4 weeks ago         0 B<none>              <none>              48e5f45168b9        4 weeks ago         2.489 MB<none>              <none>              bf747efa0e2f        4 weeks ago         0 B<none>              <none>              980fe10e5736        12 weeks ago        101.4 MB<none>              <none>              dea752e4e117        12 weeks ago        101.4 MB<none>              <none>              511136ea3c5a        8 months ago        0 B\n```",
          "```\n$docker rmi$(docker images -f\"dangling=true\"-q)8abc22fbb04248e5f45168b9bf747efa0e2f980fe10e5736dea752e4e117511136ea3c5a\n```",
          "```\n$docker images --filter\"label=com.example.version\"REPOSITORY          TAG                 IMAGE ID            CREATED              SIZEmatch-me-1          latest              eeae25ada2aa        About a minute ago   188.3 MBmatch-me-2          latest              dea752e4e117        About a minute ago   188.3 MB\n```",
          "```\n$docker images --filter\"label=com.example.version=1.0\"REPOSITORY          TAG                 IMAGE ID            CREATED              SIZEmatch-me            latest              511136ea3c5a        About a minute ago   188.3 MB\n```",
          "```\n$docker images --filter\"label=com.example.version=0.1\"REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE\n```",
          "```\n$docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED              SIZEimage1              latest              eeae25ada2aa        4 minutes ago        188.3 MBimage2              latest              dea752e4e117        9 minutes ago        188.3 MBimage3              latest              511136ea3c5a        25 minutes ago       188.3 MB\n```",
          "```\n$docker images --filter\"before=image1\"REPOSITORY          TAG                 IMAGE ID            CREATED              SIZEimage2              latest              dea752e4e117        9 minutes ago        188.3 MBimage3              latest              511136ea3c5a        25 minutes ago       188.3 MB\n```",
          "```\n$docker images --filter\"since=image3\"REPOSITORY          TAG                 IMAGE ID            CREATED              SIZEimage1              latest              eeae25ada2aa        4 minutes ago        188.3 MBimage2              latest              dea752e4e117        9 minutes ago        188.3 MB\n```",
          "```\n$docker imagesREPOSITORY          TAG                 IMAGE ID            CREATED             SIZEbusybox             latest              e02e811dd08f        5 weeks ago         1.09 MBbusybox             uclibc              e02e811dd08f        5 weeks ago         1.09 MBbusybox             musl                733eb3059dce        5 weeks ago         1.21 MBbusybox             glibc               21c16b6787c6        5 weeks ago         4.19 MB\n```",
          "```\n$docker images --filter=reference='busy*:*libc'REPOSITORY          TAG                 IMAGE ID            CREATED             SIZEbusybox             uclibc              e02e811dd08f        5 weeks ago         1.09 MBbusybox             glibc               21c16b6787c6        5 weeks ago         4.19 MB\n```",
          "```\n$docker images --filter=reference='busy*:uclibc'--filter=reference='busy*:glibc'REPOSITORY          TAG                 IMAGE ID            CREATED             SIZEbusybox             uclibc              e02e811dd08f        5 weeks ago         1.09 MBbusybox             glibc               21c16b6787c6        5 weeks ago         4.19 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 1896
        }
      },
      {
        "header": "Format the output (--format)",
        "content": "The formatting option (--format) will pretty print container output using a Go template.\n\nValid placeholders for the Go template are listed below:\n\nWhen using the --format option, the image command will either output the data exactly as the template declares or, when using the table directive, will include column headers as well.\n\nThe following example uses a template without headers and outputs the ID and Repository entries separated by a colon (:) for all images:\n\nTo list all images with their repository and tag in a table format you can use:\n\nTo list all images in JSON format, use the json directive:\n\nPlaceholder | Description\n--- | ---\n.ID | Image ID\n.Repository | Image repository\n.Tag | Image tag\n.Digest | Image digest\n.CreatedSince | Elapsed time since the image was created\n.CreatedAt | Time when the image was created\n.Size | Image disk size",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker images --format\"{{.ID}}: {{.Repository}}\"77af4d6b9913: <none>b6fa739cedf5: committ78a85c484f71: <none>30557a29d5ab: docker5ed6274db6ce: <none>746b819f315e: postgres746b819f315e: postgres746b819f315e: postgres746b819f315e: postgres\n```",
          "```\n$docker images --format\"table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}\"IMAGE ID            REPOSITORY                TAG77af4d6b9913        <none>                    <none>b6fa739cedf5        committ                   latest78a85c484f71        <none>                    <none>30557a29d5ab        docker                    latest5ed6274db6ce        <none>                    <none>746b819f315e        postgres                  9746b819f315e        postgres                  9.3746b819f315e        postgres                  9.3.5746b819f315e        postgres                  latest\n```",
          "```\n$docker images --format json{\"Containers\":\"N/A\",\"CreatedAt\":\"2021-03-04 03:24:42 +0100 CET\",\"CreatedSince\":\"5 days ago\",\"Digest\":\"\\u003cnone\\u003e\",\"ID\":\"4dd97cefde62\",\"Repository\":\"ubuntu\",\"SharedSize\":\"N/A\",\"Size\":\"72.9MB\",\"Tag\":\"latest\",\"UniqueSize\":\"N/A\",\"VirtualSize\":\"72.9MB\"}{\"Containers\":\"N/A\",\"CreatedAt\":\"2021-02-17 22:19:54 +0100 CET\",\"CreatedSince\":\"2 weeks ago\",\"Digest\":\"\\u003cnone\\u003e\",\"ID\":\"28f6e2705743\",\"Repository\":\"alpine\",\"SharedSize\":\"N/A\",\"Size\":\"5.61MB\",\"Tag\":\"latest\",\"UniqueSize\":\"N/A\",\"VirtualSize\":\"5.613MB\"}\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 6,
          "content_length": 859
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/image/ls/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "docker container exec",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "Introducing Docker Debug\n\nTo easily get a debug shell into any container, use docker debug. Docker Debug is a replacement for debugging with docker exec. With it, you can get a shell into any container or image, even slim ones, without modifications. Plus, you can bring along your favorite debugging tools in its customizable toolbox.\n\nExplore Docker Debug now.\n\n• Get started\n\nDescription | Execute a command in a running container\nUsage | docker container exec [OPTIONS] CONTAINER COMMAND [ARG...]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker exec",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 588
        }
      },
      {
        "header": "Description",
        "content": "The docker exec command runs a new command in a running container.\n\nThe command you specify with docker exec only runs while the container's primary process (PID 1) is running, and it isn't restarted if the container is restarted.\n\nThe command runs in the default working directory of the container.\n\nThe command must be an executable. A chained or a quoted command doesn't work.\n\n• This works: docker exec -it my_container sh -c \"echo a && echo b\"\n• This doesn't work: docker exec -it my_container \"echo a && echo b\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 517
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-d, --detach | Detached mode: run command in the background\n--detach-keys | Override the key sequence for detaching a container\n-e, --env | API 1.25+ Set environment variables\n--env-file | API 1.25+ Read in a file of environment variables\n-i, --interactive | Keep STDIN open even if not attached\n--privileged | Give extended privileges to the command\n-t, --tty | Allocate a pseudo-TTY\n-u, --user | Username or UID (format: <name|uid>[:<group|gid>])\n-w, --workdir | API 1.35+ Working directory inside the container",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 560
        }
      },
      {
        "header": "Run docker exec on a running container",
        "content": "First, start a container.\n\nThis creates and starts a container named mycontainer from an alpine image with an sh shell as its main process. The -d option (shorthand for --detach) sets the container to run in the background, in detached mode, with a pseudo-TTY attached (-t). The -i option is set to keep STDIN attached (-i), which prevents the sh process from exiting immediately.\n\nNext, execute a command on the container.\n\nThis creates a new file /tmp/execWorks inside the running container mycontainer, in the background.\n\nNext, execute an interactive sh shell on the container.\n\nThis starts a new shell session in the container mycontainer.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --name mycontainer -d -i -t alpine /bin/sh\n```",
          "```\n$dockerexec-d mycontainer touch /tmp/execWorks\n```",
          "```\n$dockerexec-it mycontainer sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 644
        }
      },
      {
        "header": "Set environment variables for the exec process (--env, -e)",
        "content": "Next, set environment variables in the current bash session.\n\nThe docker exec command inherits the environment variables that are set at the time the container is created. Use the --env (or the -e shorthand) to override global environment variables, or to set additional environment variables for the process started by docker exec.\n\nThe following example creates a new shell session in the container mycontainer, with environment variables $VAR_A set to 1, and $VAR_B set to 2. These environment variables are only valid for the sh process started by that docker exec command, and aren't available to other processes running inside the container.",
        "code_examples": [],
        "usage_examples": [
          "```\n$dockerexec-eVAR_A=1-eVAR_B=2mycontainer envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=f64a4851eb71VAR_A=1VAR_B=2HOME=/root\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 647
        }
      },
      {
        "header": "Escalate container privileges (--privileged)",
        "content": "See docker run --privileged.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 28
        }
      },
      {
        "header": "Set the working directory for the exec process (--workdir, -w)",
        "content": "By default docker exec command runs in the same working directory set when the container was created.\n\nYou can specify an alternative working directory for the command to execute using the --workdir option (or the -w shorthand):",
        "code_examples": [],
        "usage_examples": [
          "```\n$dockerexec-it mycontainerpwd/\n```",
          "```\n$dockerexec-it -w /root mycontainerpwd/root\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 228
        }
      },
      {
        "header": "Try to run docker exec on a paused container",
        "content": "If the container is paused, then the docker exec command fails with an error:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker pause mycontainermycontainer$docker psCONTAINER ID   IMAGE     COMMAND     CREATED          STATUS                   PORTS     NAMES482efdf39fac   alpine    \"/bin/sh\"   17 seconds ago   Up 16 seconds (Paused)             mycontainer$dockerexecmycontainer shError response from daemon: Container mycontainer is paused, unpause the container before exec$echo$?1\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 77
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/container/exec/",
    "doc_type": "docker",
    "total_sections": 8
  },
  {
    "title": "docker container logs",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Fetch the logs of a container\nUsage | docker container logs [OPTIONS] CONTAINER\nAliasesAn alias is a short or memorable alternative for a longer command. | docker logs",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 196
        }
      },
      {
        "header": "Description",
        "content": "The docker logs command batch-retrieves logs present at the time of execution.\n\nFor more information about selecting and configuring logging drivers, refer to Configure logging drivers.\n\nThe docker logs --follow command will continue streaming the new output from the container's STDOUT and STDERR.\n\nPassing a negative number or a non-integer to --tail is invalid and the value is set to all in that case.\n\nThe docker logs --timestamps command will add an RFC3339Nano timestamp , for example 2014-09-16T06:17:46.000000000Z, to each log entry. To ensure that the timestamps are aligned the nano-second part of the timestamp will be padded with zero when necessary.\n\nThe docker logs --details command will add on extra attributes, such as environment variables and labels, provided to --log-opt when creating the container.\n\nThe --since option shows only the container logs generated after a given date. You can specify the date as an RFC 3339 date, a UNIX timestamp, or a Go duration string (e.g. 1m30s, 3h). Besides RFC3339 date format you may also use RFC3339Nano, 2006-01-02T15:04:05, 2006-01-02T15:04:05.999999999, 2006-01-02T07:00, and 2006-01-02. The local timezone on the client will be used if you do not provide either a Z or a +-00:00 timezone offset at the end of the timestamp. When providing Unix timestamps enter seconds[.nanoseconds], where seconds is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (aka Unix epoch or Unix time), and the optional .nanoseconds field is a fraction of a second no more than nine digits long. You can combine the --since option with either or both of the --follow or --tail options.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1687
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n--details | Show extra details provided to logs\n-f, --follow | Follow log output\n--since | Show logs since timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)\n-n, --tail | all | Number of lines to show from the end of the logs\n-t, --timestamps | Show timestamps\n--until | API 1.35+ Show logs before a timestamp (e.g. 2013-01-02T13:23:37Z) or relative (e.g. 42m for 42 minutes)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 447
        }
      },
      {
        "header": "Retrieve logs until a specific point in time (--until)",
        "content": "In order to retrieve logs before a specific point in time, run:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --nametest-d busybox sh -c\"while true; do$(echodate); sleep 1; done\"$dateTue 14 Nov 2017 16:40:00 CET$docker logs -f --until=2stestTue 14 Nov 2017 16:40:00 CETTue 14 Nov 2017 16:40:01 CETTue 14 Nov 2017 16:40:02 CET\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 63
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/container/logs/",
    "doc_type": "docker",
    "total_sections": 4
  },
  {
    "title": "docker container stop",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Stop one or more running containers\nUsage | docker container stop [OPTIONS] CONTAINER [CONTAINER...]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker stop",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 217
        }
      },
      {
        "header": "Description",
        "content": "The main process inside the container will receive SIGTERM, and after a grace period, SIGKILL. The first signal can be changed with the STOPSIGNAL instruction in the container's Dockerfile, or the --stop-signal option to docker run and docker create.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 250
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-s, --signal | Signal to send to the container\n-t, --timeout | Seconds to wait before killing the container",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 154
        }
      },
      {
        "header": "Stop container with signal (-s, --signal)",
        "content": "The --signal flag sends the system call signal to the container to exit. This signal can be a signal name in the format SIG<NAME>, for instance SIGKILL, or an unsigned number that matches a position in the kernel's syscall table, for instance 9. Refer to signal(7) for available signals.\n\nThe default signal to use is defined by the image's StopSignal, which can be set through the STOPSIGNAL Dockerfile instruction when building the image, or configured using the --stop-signal option when creating the container. If no signal is configured for the container, SIGTERM is used as default.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 588
        }
      },
      {
        "header": "Stop container with timeout (-t, --timeout)",
        "content": "The --timeout flag sets the number of seconds to wait for the container to stop after sending the pre-defined (see [--signal]{#signal)) system call signal. If the container does not exit after the timeout elapses, it's forcibly killed with a SIGKILL signal.\n\nIf you set --timeout to -1, no timeout is applied, and the daemon waits indefinitely for the container to exit.\n\nThe default timeout can be specified using the --stop-timeout option when creating the container. If no default is configured for the container, the Daemon determines the default, and is 10 seconds for Linux containers, and 30 seconds for Windows containers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 630
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/container/stop/",
    "doc_type": "docker",
    "total_sections": 5
  },
  {
    "title": "docker container rm",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Remove one or more containers\nUsage | docker container rm [OPTIONS] CONTAINER [CONTAINER...]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker container remove docker rm",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 231
        }
      },
      {
        "header": "Description",
        "content": "Remove one or more containers",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 29
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-f, --force | Force the removal of a running container (uses SIGKILL)\n-l, --link | Remove the specified link\n-v, --volumes | Remove anonymous volumes associated with the container",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 226
        }
      },
      {
        "header": "Remove a container",
        "content": "This removes the container referenced under the link /redis.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker rm /redis/redis\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 60
        }
      },
      {
        "header": "Remove a link specified with --link on the default bridge network (--link)",
        "content": "This removes the underlying link between /webapp and the /redis containers on the default bridge network, removing all network communication between the two containers. This does not apply when --link is used with user-specified networks.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker rm --link /webapp/redis/webapp/redis\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 238
        }
      },
      {
        "header": "Force-remove a running container (--force)",
        "content": "This command force-removes a running container.\n\nThe main process inside the container referenced under the link redis will receive SIGKILL, then the container will be removed.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker rm --force redisredis\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 176
        }
      },
      {
        "header": "Remove all stopped containers",
        "content": "Use the docker container prune command to remove all stopped containers, or refer to the docker system prune command to remove unused containers in addition to other Docker resources, such as (unused) images and networks.\n\nAlternatively, you can use the docker ps with the -q / --quiet option to generate a list of container IDs to remove, and use that list as argument for the docker rm command.\n\nCombining commands can be more flexible, but is less portable as it depends on features provided by the shell, and the exact syntax may differ depending on what shell is used. To use this approach on Windows, consider using PowerShell or Bash.\n\nThe example below uses docker ps -q to print the IDs of all containers that have exited (--filter status=exited), and removes those containers with the docker rm command:\n\nOr, using the xargs Linux utility:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker rm$(docker ps --filterstatus=exited -q)\n```",
          "```\n$docker ps --filterstatus=exited -q|xargs docker rm\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 849
        }
      },
      {
        "header": "Remove a container and its volumes (-v, --volumes)",
        "content": "This command removes the container and any volumes associated with it. Note that if a volume was specified with a name, it will not be removed.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker rm --volumes redisredis\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 143
        }
      },
      {
        "header": "Remove a container and selectively remove volumes",
        "content": "In this example, the volume for /foo remains intact, but the volume for /bar is removed. The same behavior holds for volumes inherited with --volumes-from.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker create -v awesome:/foo -v /bar --name hello redishello$docker rm -v hello\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 155
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/container/rm/",
    "doc_type": "docker",
    "total_sections": 9
  },
  {
    "title": "docker image rm",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Remove one or more images\nUsage | docker image rm [OPTIONS] IMAGE [IMAGE...]\nAliasesAn alias is a short or memorable alternative for a longer command. | docker image remove docker rmi",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 212
        }
      },
      {
        "header": "Description",
        "content": "Removes (and un-tags) one or more images from the host node. If an image has multiple tags, using this command with the tag as a parameter only removes the tag. If the tag is the only one for the image, both the image and the tag are removed.\n\nThis does not remove images from a registry. You cannot remove an image of a running container unless you use the -f option. To see all images on a host use the docker image ls command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 429
        }
      },
      {
        "header": "Options",
        "content": "Option | Default | Description\n--- | --- | ---\n-f, --force | Force removal of the image\n--no-prune | Do not delete untagged parents\n--platform | API 1.50+ Remove only the given platform variant. Formatted as os[/arch[/variant]] (e.g., linux/amd64)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 247
        }
      },
      {
        "header": "Examples",
        "content": "You can remove an image using its short or long ID, its tag, or its digest. If an image has one or more tags referencing it, you must remove all of them before the image is removed. Digest references are removed automatically when an image is removed by tag.\n\nIf you use the -f flag and specify the image's short or long ID, then this command untags and removes all images that match the specified ID.\n\nAn image pulled by digest has no tag associated with it:\n\nTo remove an image using its digest:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker imagesREPOSITORY                TAG                 IMAGE ID            CREATED             SIZEtest1                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)test                      latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)test2                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)$docker rmi fd484f19954fError: Conflict, cannot delete image fd484f19954f because it is tagged in multiple repositories, use -f to force2013/12/11 05:47:16 Error: failed to remove one or more images$docker rmi test1:latestUntagged: test1:latest$docker rmi test2:latestUntagged: test2:latest$docker imagesREPOSITORY                TAG                 IMAGE ID            CREATED             SIZEtest                      latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)$docker rmi test:latestUntagged: test:latestDeleted: fd484f19954f4920da7ff372b5067f5b7ddb2fd3830cecd17b96ea9e286ba5b8\n```",
          "```\n$docker imagesREPOSITORY                TAG                 IMAGE ID            CREATED             SIZEtest1                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)test                      latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)test2                     latest              fd484f19954f        23 seconds ago      7 B (virtual 4.964 MB)$docker rmi -f fd484f19954fUntagged: test1:latestUntagged: test:latestUntagged: test2:latestDeleted: fd484f19954f4920da7ff372b5067f5b7ddb2fd3830cecd17b96ea9e286ba5b8\n```",
          "```\n$docker images --digestsREPOSITORY                     TAG       DIGEST                                                                    IMAGE ID        CREATED         SIZElocalhost:5000/test/busybox    <none>    sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbf   4986bf8c1536    9 weeks ago     2.43 MB\n```",
          "```\n$docker rmi localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbfUntagged: localhost:5000/test/busybox@sha256:cbbf2f9a99b47fc460d422812b6a5adff7dfee951d8fa2e4a98caa0382cfbdbfDeleted: 4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125Deleted: ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2Deleted: df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 497
        }
      },
      {
        "header": "Remove specific platforms (--platform)",
        "content": "The --platform option allows you to specify which platform variants of the image to remove. By default, docker image remove removes all platform variants that are present. Use the --platform option to specify which platform variant of the image to remove.\n\nRemoving a specific platform removes the image from all images that reference the same content, and requires the --force option to be used. Omitting the --force option produces a warning, and the remove is canceled:\n\nThe platform option takes the os[/arch[/variant]] format; for example, linux/amd64 or linux/arm64/v8. Architecture and variant are optional, and default to the daemon's native architecture if omitted.\n\nYou can pass multiple platforms either by passing the --platform flag multiple times, or by passing a comma-separated list of platforms to remove. The following uses of this option are equivalent;\n\nThe following example removes the linux/amd64 and linux/ppc64le variants of an alpine image that contains multiple platform variants in the image cache:\n\nAfter the command completes, the given variants of the alpine image are removed from the image cache:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker image rm --platform=linux/amd64 alpineError response from daemon: Content will be removed from all images referencing this variant. Use â-force to force delete.\n```",
          "```\n$docker image rm --plaform linux/amd64 --platform linux/ppc64le myimage$docker image rm --plaform linux/amd64,linux/ppc64le myimage\n```",
          "```\n$docker image ls --treeIMAGE                   ID             DISK USAGE   CONTENT SIZE   EXTRAalpine:latest           a8560b36e8b8       37.8MB         11.2MB    Uââ linux/amd64          1c4eef651f65       12.1MB         3.64MB    Uââ linux/arm/v6         903bfe2ae994           0B             0Bââ linux/arm/v7         9c2d245b3c01           0B             0Bââ linux/arm64/v8       757d680068d7       12.8MB         3.99MBââ linux/386            2436f2b3b7d2           0B             0Bââ linux/ppc64le        9ed53fd3b831       12.8MB         3.58MBââ linux/riscv64        1de5eb4a9a67           0B             0Bââ linux/s390x          fe0dcdd1f783           0B             0B$docker image --platform=linux/amd64,linux/ppc64le --force alpineDeleted: sha256:1c4eef651f65e2f7daee7ee785882ac164b02b78fb74503052a26dc061c90474Deleted: sha256:9ed53fd3b83120f78b33685d930ce9bf5aa481f6e2d165c42cbbddbeaa196f6f\n```",
          "```\n$docker image ls --treeIMAGE                   ID             DISK USAGE   CONTENT SIZE   EXTRAalpine:latest           a8560b36e8b8       12.8MB         3.99MBââ linux/amd64          1c4eef651f65           0B             0Bââ linux/arm/v6         903bfe2ae994           0B             0Bââ linux/arm/v7         9c2d245b3c01           0B             0Bââ linux/arm64/v8       757d680068d7       12.8MB         3.99MBââ linux/386            2436f2b3b7d2           0B             0Bââ linux/ppc64le        9ed53fd3b831           0B             0Bââ linux/riscv64        1de5eb4a9a67           0B             0Bââ linux/s390x          fe0dcdd1f783           0B             0B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1129
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/image/rm/",
    "doc_type": "docker",
    "total_sections": 5
  },
  {
    "title": "Dockerfile reference",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. This page describes the commands you can use in a Dockerfile.\n\n• Get started",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 284
        }
      },
      {
        "header": "Overview",
        "content": "The Dockerfile supports the following instructions:\n\nInstruction | Description\n--- | ---\nADD | Add local or remote files and directories.\nARG | Use build-time variables.\nCMD | Specify default commands.\nCOPY | Copy files and directories.\nENTRYPOINT | Specify default executable.\nENV | Set environment variables.\nEXPOSE | Describe which ports your application is listening on.\nFROM | Create a new build stage from a base image.\nHEALTHCHECK | Check a container's health on startup.\nLABEL | Add metadata to an image.\nMAINTAINER | Specify the author of an image.\nONBUILD | Specify instructions for when the image is used in a build.\nRUN | Execute build commands.\nSHELL | Set the default shell of an image.\nSTOPSIGNAL | Specify the system call signal for exiting a container.\nUSER | Set user and group ID.\nVOLUME | Create volume mounts.\nWORKDIR | Change working directory.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 1,
          "content_length": 866
        }
      },
      {
        "header": "Format",
        "content": "Here is the format of the Dockerfile:\n\nThe instruction is not case-sensitive. However, convention is for them to be UPPERCASE to distinguish them from arguments more easily.\n\nDocker runs instructions in a Dockerfile in order. A Dockerfile must begin with a FROM instruction. This may be after parser directives, comments, and globally scoped ARGs. The FROM instruction specifies the base image from which you are building. FROM may only be preceded by one or more ARG instructions, which declare arguments that are used in FROM lines in the Dockerfile.\n\nBuildKit treats lines that begin with # as a comment, unless the line is a valid parser directive. A # marker anywhere else in a line is treated as an argument. This allows statements like:\n\nComment lines are removed before the Dockerfile instructions are executed. The comment in the following example is removed before the shell executes the echo command.\n\nThe following examples is equivalent.\n\nComments don't support line continuation characters.\n\nFor backward compatibility, leading whitespace before comments (#) and instructions (such as RUN) are ignored, but discouraged. Leading whitespace is not preserved in these cases, and the following examples are therefore equivalent:\n\nWhitespace in instruction arguments, however, isn't ignored. The following example prints hello world with leading whitespace as specified:\n\n[Admonition] Note on whitespaceFor backward compatibility, leading whitespace before comments (#) and instructions (such as RUN) are ignored, but discouraged. Leading whitespace is not preserved in these cases, and the following examples are therefore equivalent: # this is a comment-line RUN echo helloRUN echo world# this is a comment-lineRUN echo helloRUN echo worldWhitespace in instruction arguments, however, isn't ignored. The following example prints hello world with leading whitespace as specified:RUN echo \"\\ hello\\ world\"",
        "code_examples": [
          "```\n# CommentINSTRUCTION arguments\n```",
          "```\n# CommentRUNecho'we are running some # of cool things'\n```",
          "```\nRUNechohello\\# commentworld\n```",
          "```\nRUNechohello\\world\n```",
          "```\n# this is a comment-lineRUNechohelloRUNechoworld\n```",
          "```\n# this is a comment-lineRUNechohelloRUNechoworld\n```",
          "```\nRUNecho\"\\hello\\world\"\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1914
        }
      },
      {
        "header": "Parser directives",
        "content": "Parser directives are optional, and affect the way in which subsequent lines in a Dockerfile are handled. Parser directives don't add layers to the build, and don't show up as build steps. Parser directives are written as a special type of comment in the form # directive=value. A single directive may only be used once.\n\nThe following parser directives are supported:\n\nOnce a comment, empty line or builder instruction has been processed, BuildKit no longer looks for parser directives. Instead it treats anything formatted as a parser directive as a comment and doesn't attempt to validate if it might be a parser directive. Therefore, all parser directives must be at the top of a Dockerfile.\n\nParser directive keys, such as syntax or check, aren't case-sensitive, but they're lowercase by convention. Values for a directive are case-sensitive and must be written in the appropriate case for the directive. For example, #check=skip=jsonargsrecommended is invalid because the check name must use Pascal case, not lowercase. It's also conventional to include a blank line following any parser directives. Line continuation characters aren't supported in parser directives.\n\nDue to these rules, the following examples are all invalid:\n\nInvalid due to line continuation:\n\nInvalid due to appearing twice:\n\nTreated as a comment because it appears after a builder instruction:\n\nTreated as a comment because it appears after a comment that isn't a parser directive:\n\nThe following unknowndirective is treated as a comment because it isn't recognized. The known syntax directive is treated as a comment because it appears after a comment that isn't a parser directive.\n\nNon line-breaking whitespace is permitted in a parser directive. Hence, the following lines are all treated identically:\n\n• check (since Dockerfile v1.8.0)",
        "code_examples": [
          "```\n# direc \\tive=value\n```",
          "```\n# directive=value1# directive=value2FROMImageName\n```",
          "```\nFROMImageName# directive=value\n```",
          "```\n# About my dockerfile# directive=valueFROMImageName\n```",
          "```\n# unknowndirective=value# syntax=value\n```",
          "```\n#directive=value# directive =value#\tdirective= value# directive = value#\t  dIrEcTiVe=value\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 1819
        }
      },
      {
        "header": "syntax",
        "content": "Use the syntax parser directive to declare the Dockerfile syntax version to use for the build. If unspecified, BuildKit uses a bundled version of the Dockerfile frontend. Declaring a syntax version lets you automatically use the latest Dockerfile version without having to upgrade BuildKit or Docker Engine, or even use a custom Dockerfile implementation.\n\nMost users will want to set this parser directive to docker/dockerfile:1, which causes BuildKit to pull the latest stable version of the Dockerfile syntax before the build.\n\nFor more information about how the parser directive works, see Custom Dockerfile syntax.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 619
        }
      },
      {
        "header": "escape",
        "content": "The escape directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is \\.\n\nThe escape character is used both to escape characters in a line, and to escape a newline. This allows a Dockerfile instruction to span multiple lines. Note that regardless of whether the escape parser directive is included in a Dockerfile, escaping is not performed in a RUN command, except at the end of a line.\n\nSetting the escape character to ` is especially useful on Windows, where \\ is the directory path separator. ` is consistent with Windows PowerShell.\n\nConsider the following example which would fail in a non-obvious way on Windows. The second \\ at the end of the second line would be interpreted as an escape for the newline, instead of a target of the escape from the first \\. Similarly, the \\ at the end of the third line would, assuming it was actually handled as an instruction, cause it be treated as a line continuation. The result of this Dockerfile is that second and third lines are considered a single instruction:\n\nOne solution to the above would be to use / as the target of both the COPY instruction, and dir. However, this syntax is, at best, confusing as it is not natural for paths on Windows, and at worst, error prone as not all commands on Windows support / as the path separator.\n\nBy adding the escape parser directive, the following Dockerfile succeeds as expected with the use of natural platform semantics for file paths on Windows:",
        "code_examples": [
          "```\n# escape=\\\n```",
          "```\n# escape=`\n```",
          "```\nFROMmicrosoft/nanoserverCOPYtestfile.txt c:\\\\RUN dir c:\\\n```",
          "```\nPS E:\\myproject> docker build -t cmd .Sending build context to Docker daemon 3.072 kBStep 1/2 : FROM microsoft/nanoserver---> 22738ff49c6dStep 2/2 : COPY testfile.txt c:\\RUN dir c:GetFileAttributesEx c:RUN: The system cannot find the file specified.PS E:\\myproject>\n```",
          "```\n# escape=`FROMmicrosoft/nanoserverCOPYtestfile.txt c:\\RUN dir c:\\\n```",
          "```\nPS E:\\myproject> docker build -t succeeds --no-cache=true .Sending build context to Docker daemon 3.072 kBStep 1/3 : FROM microsoft/nanoserver---> 22738ff49c6dStep 2/3 : COPY testfile.txt c:\\---> 96655de338deRemoving intermediate container 4db9acbb1682Step 3/3 : RUN dir c:\\---> Running in a2c157f842f5Volume in drive C has no label.Volume Serial Number is 7E6D-E0F7Directory of c:\\10/05/2016  05:04 PM             1,894 License.txt10/05/2016  02:22 PM    <DIR>          Program Files10/05/2016  02:14 PM    <DIR>          Program Files (x86)10/28/2016  11:18 AM                62 testfile.txt10/28/2016  11:20 AM    <DIR>          Users10/28/2016  11:20 AM    <DIR>          Windows2 File(s)          1,956 bytes4 Dir(s)  21,259,096,064 bytes free---> 01c7f3bef04fRemoving intermediate container a2c157f842f5Successfully built 01c7f3bef04fPS E:\\myproject>\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1506
        }
      },
      {
        "header": "check",
        "content": "The check directive is used to configure how build checks are evaluated. By default, all checks are run, and failures are treated as warnings.\n\nYou can disable specific checks using #check=skip=<check-name>. To specify multiple checks to skip, separate them with a comma:\n\nTo disable all checks, use #check=skip=all.\n\nBy default, builds with failing build checks exit with a zero status code despite warnings. To make the build fail on warnings, set #check=error=true.\n\nWhen using the check directive, with error=true option, it is recommended to pin the Dockerfile syntax to a specific version. Otherwise, your build may start to fail when new checks are added in the future versions.\n\nTo combine both the skip and error options, use a semi-colon to separate them:\n\nTo see all available checks, see the build checks reference. Note that the checks available depend on the Dockerfile syntax version. To make sure you're getting the most up-to-date checks, use the syntax directive to specify the Dockerfile syntax version to the latest stable version.\n\n[Admonition] When using the check directive, with error=true option, it is recommended to pin the Dockerfile syntax to a specific version. Otherwise, your build may start to fail when new checks are added in the future versions.",
        "code_examples": [
          "```\n# check=skip=<checks|all># check=error=<boolean>\n```",
          "```\n# check=skip=JSONArgsRecommended,StageNameCasing\n```",
          "```\n# check=error=true\n```",
          "```\n# check=skip=JSONArgsRecommended;error=true\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1281
        }
      },
      {
        "header": "Environment replacement",
        "content": "Environment variables (declared with the ENV statement) can also be used in certain instructions as variables to be interpreted by the Dockerfile. Escapes are also handled for including variable-like syntax into a statement literally.\n\nEnvironment variables are notated in the Dockerfile either with $variable_name or ${variable_name}. They are treated equivalently and the brace syntax is typically used to address issues with variable names with no whitespace, like ${foo}_bar.\n\nThe ${variable_name} syntax also supports a few of the standard bash modifiers as specified below:\n\nThe following variable replacements are supported in a pre-release version of Dockerfile syntax, when using the # syntax=docker/dockerfile-upstream:master syntax directive in your Dockerfile:\n\n${variable#pattern} removes the shortest match of pattern from variable, seeking from the start of the string.\n\n${variable##pattern} removes the longest match of pattern from variable, seeking from the start of the string.\n\n${variable%pattern} removes the shortest match of pattern from variable, seeking backwards from the end of the string.\n\n${variable%%pattern} removes the longest match of pattern from variable, seeking backwards from the end of the string.\n\n${variable/pattern/replacement} replace the first occurrence of pattern in variable with replacement\n\n${variable//pattern/replacement} replaces all occurrences of pattern in variable with replacement\n\nIn all cases, word can be any string, including additional environment variables.\n\npattern is a glob pattern where ? matches any single character and * any number of characters (including zero). To match literal ? and *, use a backslash escape: \\? and \\*.\n\nYou can escape whole variable names by adding a \\ before the variable: \\$foo or \\${foo}, for example, will translate to $foo and ${foo} literals respectively.\n\nExample (parsed representation is displayed after the #):\n\nEnvironment variables are supported by the following list of instructions in the Dockerfile:\n\nYou can also use environment variables with RUN, CMD, and ENTRYPOINT instructions, but in those cases the variable substitution is handled by the command shell, not the builder. Note that instructions using the exec form don't invoke a command shell automatically. See Variable substitution.\n\nEnvironment variable substitution use the same value for each variable throughout the entire instruction. Changing the value of a variable only takes effect in subsequent instructions. Consider the following example:\n\n• ${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result.\n• ${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.\n\n• ${variable#pattern} removes the shortest match of pattern from variable, seeking from the start of the string.str=foobarbaz echo ${str#f*b} # arbaz\n• ${variable##pattern} removes the longest match of pattern from variable, seeking from the start of the string.str=foobarbaz echo ${str##f*b} # az\n• ${variable%pattern} removes the shortest match of pattern from variable, seeking backwards from the end of the string.string=foobarbaz echo ${string%b*} # foobar\n• ${variable%%pattern} removes the longest match of pattern from variable, seeking backwards from the end of the string.string=foobarbaz echo ${string%%b*} # foo\n• ${variable/pattern/replacement} replace the first occurrence of pattern in variable with replacementstring=foobarbaz echo ${string/ba/fo} # fooforbaz\n• ${variable//pattern/replacement} replaces all occurrences of pattern in variable with replacementstring=foobarbaz echo ${string//ba/fo} # fooforfoz\n\n• ONBUILD (when combined with one of the supported instructions above)\n\n• The value of def becomes hello\n• The value of ghi becomes bye",
        "code_examples": [],
        "usage_examples": [
          "```\nstr=foobarbazecho${str#f*b}# arbaz\n```",
          "```\nstr=foobarbazecho${str##f*b}# az\n```",
          "```\nstring=foobarbazecho${string%b*}# foobar\n```",
          "```\nstring=foobarbazecho${string%%b*}# foo\n```",
          "```\nstring=foobarbazecho${string/ba/fo}# fooforbaz\n```",
          "```\nstring=foobarbazecho${string//ba/fo}# fooforfoz\n```",
          "```\nFROMbusyboxENVFOO=/barWORKDIR${FOO}   # WORKDIR /barADD.$FOO# ADD . /barCOPY\\$FOO /quux# COPY $FOO /quux\n```",
          "```\nENVabc=helloENVabc=byedef=$abcENVghi=$abc\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 3852
        }
      },
      {
        "header": ".dockerignore file",
        "content": "You can use .dockerignore file to exclude files and directories from the build context. For more information, see .dockerignore file.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 133
        }
      },
      {
        "header": "Shell and exec form",
        "content": "The RUN, CMD, and ENTRYPOINT instructions all have two possible forms:\n\nThe exec form makes it possible to avoid shell string munging, and to invoke commands using a specific command shell, or any other executable. It uses a JSON array syntax, where each element in the array is a command, flag, or argument.\n\nThe shell form is more relaxed, and emphasizes ease of use, flexibility, and readability. The shell form automatically uses a command shell, whereas the exec form does not.\n\n• INSTRUCTION [\"executable\",\"param1\",\"param2\"] (exec form)\n• INSTRUCTION command param1 param2 (shell form)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 591
        }
      },
      {
        "header": "Exec form",
        "content": "The exec form is parsed as a JSON array, which means that you must use double-quotes (\") around words, not single-quotes (').\n\nThe exec form is best used to specify an ENTRYPOINT instruction, combined with CMD for setting default arguments that can be overridden at runtime. For more information, see ENTRYPOINT.\n\nUsing the exec form doesn't automatically invoke a command shell. This means that normal shell processing, such as variable substitution, doesn't happen. For example, RUN [ \"echo\", \"$HOME\" ] won't handle variable substitution for $HOME.\n\nIf you want shell processing then either use the shell form or execute a shell directly with the exec form, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ]. When using the exec form and executing a shell directly, as in the case for the shell form, it's the shell that's doing the environment variable substitution, not the builder.\n\nIn exec form, you must escape backslashes. This is particularly relevant on Windows where the backslash is the path separator. The following line would otherwise be treated as shell form due to not being valid JSON, and fail in an unexpected way:\n\nThe correct syntax for this example is:",
        "code_examples": [
          "```\nENTRYPOINT[\"/bin/bash\",\"-c\",\"echo hello\"]\n```",
          "```\nRUN[\"c:\\windows\\system32\\tasklist.exe\"]\n```",
          "```\nRUN[\"c:\\\\windows\\\\system32\\\\tasklist.exe\"]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1171
        }
      },
      {
        "header": "Shell form",
        "content": "Unlike the exec form, instructions using the shell form always use a command shell. The shell form doesn't use the JSON array format, instead it's a regular string. The shell form string lets you escape newlines using the escape character (backslash by default) to continue a single instruction onto the next line. This makes it easier to use with longer commands, because it lets you split them up into multiple lines. For example, consider these two lines:\n\nThey're equivalent to the following line:\n\nYou can also use heredocs with the shell form to break up supported commands.\n\nFor more information about heredocs, see Here-documents.",
        "code_examples": [],
        "usage_examples": [
          "```\nRUNsource$HOME/.bashrc&&\\echo$HOME\n```",
          "```\nRUNsource$HOME/.bashrc&&echo$HOME\n```",
          "```\nRUN<<EOFsource$HOME/.bashrcecho$HOMEEOF\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 638
        }
      },
      {
        "header": "Use a different shell",
        "content": "You can change the default shell using the SHELL command. For example:\n\nFor more information, see SHELL.",
        "code_examples": [
          "```\nSHELL[\"/bin/bash\",\"-c\"]RUNechohello\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 104
        }
      },
      {
        "header": "FROM",
        "content": "The FROM instruction initializes a new build stage and sets the base image for subsequent instructions. As such, a valid Dockerfile must start with a FROM instruction. The image can be any valid image.\n\nThe optional --platform flag can be used to specify the platform of the image in case FROM references a multi-platform image. For example, linux/amd64, linux/arm64, or windows/amd64. By default, the target platform of the build request is used. Global build arguments can be used in the value of this flag, for example automatic platform ARGs allow you to force a stage to native build platform (--platform=$BUILDPLATFORM), and use it to cross-compile to the target platform inside the stage.\n\n• ARG is the only instruction that may precede FROM in the Dockerfile. See Understand how ARG and FROM interact.\n• FROM can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another. Simply make a note of the last image ID output by the commit before each new FROM instruction. Each FROM instruction clears any state created by previous instructions.\n• Optionally a name can be given to a new build stage by adding AS name to the FROM instruction. The name can be used in subsequent FROM <name>, COPY --from=<name>, and RUN --mount=type=bind,from=<name> instructions to refer to the image built in this stage.\n• The tag or digest values are optional. If you omit either of them, the builder assumes a latest tag by default. The builder returns an error if it can't find the tag value.",
        "code_examples": [
          "```\nFROM[--platform=<platform>] <image> [AS <name>]\n```",
          "```\nFROM[--platform=<platform>] <image>[:<tag>] [AS <name>]\n```",
          "```\nFROM[--platform=<platform>] <image>[@<digest>] [AS <name>]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1549
        }
      },
      {
        "header": "Understand how ARG and FROM interact",
        "content": "FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM.\n\nAn ARG declared before a FROM is outside of a build stage, so it can't be used in any instruction after a FROM. To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage:",
        "code_examples": [],
        "usage_examples": [
          "```\nARGCODE_VERSION=latestFROMbase:${CODE_VERSION}CMD/code/run-appFROMextras:${CODE_VERSION}CMD/code/run-extras\n```",
          "```\nARGVERSION=latestFROMbusybox:$VERSIONARGVERSIONRUNecho$VERSION> image_version\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 354
        }
      },
      {
        "header": "RUN",
        "content": "The RUN instruction will execute any commands to create a new layer on top of the current image. The added layer is used in the next step in the Dockerfile. RUN has two forms:\n\nFor more information about the differences between these two forms, see shell or exec forms.\n\nThe shell form is most commonly used, and lets you break up longer instructions into multiple lines, either using newline escapes, or with heredocs:\n\nThe available [OPTIONS] for the RUN instruction are:\n\nOption | Minimum Dockerfile version\n--- | ---\n--device | 1.14-labs\n--mount | 1.2\n--network | 1.3\n--security | 1.20",
        "code_examples": [
          "```\n# Shell form:RUN[OPTIONS]<command> ...# Exec form:RUN[OPTIONS][\"<command>\", ...]\n```",
          "```\nRUN<<EOFapt-get updateapt-get install -y curlEOF\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 4,
          "content_length": 589
        }
      },
      {
        "header": "Cache invalidation for RUN instructions",
        "content": "The cache for RUN instructions isn't invalidated automatically during the next build. The cache for an instruction like RUN apt-get dist-upgrade -y will be reused during the next build. The cache for RUN instructions can be invalidated by using the --no-cache flag, for example docker build --no-cache.\n\nSee the Dockerfile Best Practices guide for more information.\n\nThe cache for RUN instructions can be invalidated by ADD and COPY instructions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 446
        }
      },
      {
        "header": "RUN --device",
        "content": "Not yet available in stable syntax, use docker/dockerfile:1-labs version. It also needs BuildKit 0.20.0 or later.\n\nRUN --device allows build to request CDI devices to be available to the build step.\n\nThe use of --device is protected by the device entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement device flag or in buildkitd config, and for a build request with --allow device flag.\n\nThe device name is provided by the CDI specification registered in BuildKit.\n\nIn the following example, multiple devices are registered in the CDI specification for the vendor1.com/device vendor.\n\nThe device name format is flexible and accepts various patterns to support multiple device configurations:\n\nAnnotations are supported by the CDI specification since 0.6.0.\n\nTo automatically allow all devices registered in the CDI specification, you can set the org.mobyproject.buildkit.device.autoallow annotation. You can also set this annotation for a specific device.\n\nIn this example we use the --device flag to run llama.cpp inference using an NVIDIA GPU device through CDI:\n\n• vendor1.com/device: request the first device found for this vendor\n• vendor1.com/device=foo: request a specific device\n• vendor1.com/device=*: request all devices for this vendor\n• class1: request devices by org.mobyproject.buildkit.device.class annotation\n\n[Admonition] Not yet available in stable syntax, use docker/dockerfile:1-labs version. It also needs BuildKit 0.20.0 or later.\n\n[Admonition] The use of --device is protected by the device entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement device flag or in buildkitd config, and for a build request with --allow device flag.\n\n[Admonition] Annotations are supported by the CDI specification since 0.6.0.\n\n[Admonition] To automatically allow all devices registered in the CDI specification, you can set the org.mobyproject.buildkit.device.autoallow annotation. You can also set this annotation for a specific device.",
        "code_examples": [
          "```\nRUN--device=name,[required]\n```",
          "```\ncdiVersion:\"0.6.0\"kind:\"vendor1.com/device\"devices:-name:foocontainerEdits:env:-FOO=injected-name:barannotations:org.mobyproject.buildkit.device.class:class1containerEdits:env:-BAR=injected-name:bazannotations:org.mobyproject.buildkit.device.class:class1containerEdits:env:-BAZ=injected-name:quxannotations:org.mobyproject.buildkit.device.class:class2containerEdits:env:-QUX=injectedannotations:org.mobyproject.buildkit.device.autoallow:true\n```",
          "```\n# syntax=docker/dockerfile:1-labsFROMscratch AS modelADDhttps://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf /model.ggufFROMscratch AS promptCOPY<<EOF prompt.txtQ: Generate  a list of10unique biggest countries by population in JSON with their estimated poulation in1900and 2024. Answer only newline formatted JSON with keys\"country\",\"population_1900\",\"population_2024\"with10items.A:[{EOFFROMghcr.io/ggml-org/llama.cpp:full-cuda-b5124RUN--device=nvidia.com/gpu=all\\--mount=from=model,target=/models\\--mount=from=prompt,target=/tmp\\./llama-cli -m /models/model.gguf -no-cnv -ngl99-f /tmp/prompt.txt\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2051
        }
      },
      {
        "header": "RUN --mount",
        "content": "RUN --mount allows you to create filesystem mounts that the build can access. This can be used to:\n\nThe supported mount types are:\n\n• Create bind mount to the host filesystem or other build stages\n• Access build secrets or ssh-agent sockets\n• Use a persistent package management cache to speed up your build\n\nType | Description\n--- | ---\nbind (default) | Bind-mount context directories (read-only).\ncache | Mount a temporary directory to cache directories for compilers and package managers.\ntmpfs | Mount a tmpfs in the build container.\nsecret | Allow the build container to access secure files such as private keys without baking them into the image or build cache.\nssh | Allow the build container to access SSH keys via SSH agents, with support for passphrases.",
        "code_examples": [
          "```\nRUN--mount=[type=<TYPE>][,option=<value>[,option=<value>]...]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 764
        }
      },
      {
        "header": "RUN --mount=type=bind",
        "content": "This mount type allows binding files or directories to the build container. A bind mount is read-only by default.\n\nOption | Description\n--- | ---\ntarget, dst, destination1 | Mount path.\nsource | Source path in the from. Defaults to the root of the from.\nfrom | Build stage, context, or image name for the root of the source. Defaults to the build context.\nrw,readwrite | Allow writes on the mount. Written data will be discarded.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 1,
          "content_length": 429
        }
      },
      {
        "header": "RUN --mount=type=cache",
        "content": "This mount type allows the build container to cache directories for compilers and package managers.\n\nContents of the cache directories persists between builder invocations without invalidating the instruction cache. Cache mounts should only be used for better performance. Your build should work with any contents of the cache directory as another build may overwrite the files or GC may clean it if more storage space is needed.\n\nApt needs exclusive access to its data, so the caches use the option sharing=locked, which will make sure multiple parallel builds using the same cache mount will wait for each other and not access the same cache files at the same time. You could also use sharing=private if you prefer to have each build create another cache directory in this case.\n\nOption | Description\n--- | ---\nid | Optional ID to identify separate/different caches. Defaults to value of target.\ntarget, dst, destination1 | Mount path.\nro,readonly | Read-only if set.\nsharing | One of shared, private, or locked. Defaults to shared. A shared cache mount can be used concurrently by multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount.\nfrom | Build stage, context, or image name to use as a base of the cache mount. Defaults to empty directory.\nsource | Subpath in the from to mount. Defaults to the root of the from.\nmode | File mode for new cache directory in octal. Default 0755.\nuid | User ID for new cache directory. Default 0.\ngid | Group ID for new cache directory. Default 0.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMgolangRUN--mount=type=cache,target=/root/.cache/go-build\\go build ...\n```",
          "```\n# syntax=docker/dockerfile:1FROMubuntuRUNrm -f /etc/apt/apt.conf.d/docker-clean;echo'Binary::apt::APT::Keep-Downloaded-Packages \"true\";'> /etc/apt/apt.conf.d/keep-cacheRUN--mount=type=cache,target=/var/cache/apt,sharing=locked\\--mount=type=cache,target=/var/lib/apt,sharing=locked\\apt update&&apt-get --no-install-recommends install -y gcc\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 1580
        }
      },
      {
        "header": "RUN --mount=type=tmpfs",
        "content": "This mount type allows mounting tmpfs in the build container.\n\nOption | Description\n--- | ---\ntarget, dst, destination1 | Mount path.\nsize | Specify an upper limit on the size of the filesystem.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 1,
          "content_length": 194
        }
      },
      {
        "header": "RUN --mount=type=secret",
        "content": "This mount type allows the build container to access secret values, such as tokens or private keys, without baking them into the image.\n\nBy default, the secret is mounted as a file. You can also mount the secret as an environment variable by setting the env option.\n\nThe following example takes the secret API_KEY and mounts it as an environment variable with the same name.\n\nAssuming that the API_KEY environment variable is set in the build environment, you can build this with the following command:\n\nOption | Description\n--- | ---\nid | ID of the secret. Defaults to basename of the target path.\ntarget, dst, destination | Mount the secret to the specified path. Defaults to /run/secrets/ + id if unset and if env is also unset.\nenv | Mount the secret to an environment variable instead of a file, or both. (since Dockerfile v1.10.0)\nrequired | If set to true, the instruction errors out when the secret is unavailable. Defaults to false.\nmode | File mode for secret file in octal. Default 0400.\nuid | User ID for secret file. Default 0.\ngid | Group ID for secret file. Default 0.",
        "code_examples": [],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1FROMpython:3RUNpip install awscliRUN--mount=type=secret,id=aws,target=/root/.aws/credentials\\aws s3 cp s3://... ...\n```",
          "```\n$docker buildx build --secretid=aws,src=$HOME/.aws/credentials .\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineRUN--mount=type=secret,id=API_KEY,env=API_KEY\\some-command --token-from-env$API_KEY\n```",
          "```\n$docker buildx build --secretid=API_KEY .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 4,
          "content_length": 1083
        }
      },
      {
        "header": "RUN --mount=type=ssh",
        "content": "This mount type allows the build container to access SSH keys via SSH agents, with support for passphrases.\n\nYou can also specify a path to *.pem file on the host directly instead of $SSH_AUTH_SOCK. However, pem files with passphrases are not supported.\n\nOption | Description\n--- | ---\nid | ID of SSH agent socket or key. Defaults to \"default\".\ntarget, dst, destination | SSH agent socket path. Defaults to /run/buildkit/ssh_agent.${N}.\nrequired | If set to true, the instruction errors out when the key is unavailable. Defaults to false.\nmode | File mode for socket in octal. Default 0600.\nuid | User ID for socket. Default 0.\ngid | Group ID for socket. Default 0.",
        "code_examples": [],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1FROMalpineRUNapk add --no-cache openssh-clientRUNmkdir -p -m0700~/.ssh&&ssh-keyscan gitlab.com >> ~/.ssh/known_hostsRUN--mount=type=ssh\\ssh -q -T git@gitlab.com 2>&1|tee /hello# \"Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY\" should be printed here# with the type of build progress is defined as `plain`.\n```",
          "```\n$eval$(ssh-agent)$ssh-add ~/.ssh/id_rsa(Input your passphrase here)$docker buildx build --sshdefault=$SSH_AUTH_SOCK.\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 665
        }
      },
      {
        "header": "RUN --network",
        "content": "RUN --network allows control over which networking environment the command is run in.\n\nThe supported network types are:\n\nType | Description\n--- | ---\ndefault (default) | Run in the default network.\nnone | Run with no network access.\nhost | Run in the host's network environment.",
        "code_examples": [
          "```\nRUN--network=<TYPE>\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 278
        }
      },
      {
        "header": "RUN --network=default",
        "content": "Equivalent to not supplying a flag at all, the command is run in the default network for the build.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 99
        }
      },
      {
        "header": "RUN --network=none",
        "content": "The command is run with no network access (lo is still available, but is isolated to this process)\n\npip will only be able to install the packages provided in the tarfile, which can be controlled by an earlier build stage.",
        "code_examples": [],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1FROMpython:3.6ADDmypackage.tgz wheels/RUN--network=none pip install --find-links wheels mypackage\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 221
        }
      },
      {
        "header": "RUN --network=host",
        "content": "The command is run in the host's network environment (similar to docker build --network=host, but on a per-instruction basis)\n\nThe use of --network=host is protected by the network.host entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement network.host flag or in buildkitd config, and for a build request with --allow network.host flag.\n\n[Admonition] The use of --network=host is protected by the network.host entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement network.host flag or in buildkitd config, and for a build request with --allow network.host flag.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 670
        }
      },
      {
        "header": "RUN --security",
        "content": "The default security mode is sandbox. With --security=insecure, the builder runs the command without sandbox in insecure mode, which allows to run flows requiring elevated privileges (e.g. containerd). This is equivalent to running docker run --privileged.\n\nIn order to access this feature, entitlement security.insecure should be enabled when starting the buildkitd daemon with --allow-insecure-entitlement security.insecure flag or in buildkitd config, and for a build request with --allow security.insecure flag.\n\nDefault sandbox mode can be activated via --security=sandbox, but that is no-op.\n\n[Admonition] In order to access this feature, entitlement security.insecure should be enabled when starting the buildkitd daemon with --allow-insecure-entitlement security.insecure flag or in buildkitd config, and for a build request with --allow security.insecure flag.",
        "code_examples": [
          "```\nRUN--security=<sandbox|insecure>\n```",
          "```\n# syntax=docker/dockerfile:1FROMubuntuRUN--security=insecure cat /proc/self/status|grep CapEff\n```",
          "```\n#84 0.093 CapEff:\t0000003fffffffff\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 869
        }
      },
      {
        "header": "CMD",
        "content": "The CMD instruction sets the command to be executed when running a container from an image.\n\nYou can specify CMD instructions using shell or exec forms:\n\nThere can only be one CMD instruction in a Dockerfile. If you list more than one CMD, only the last one takes effect.\n\nThe purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well.\n\nIf you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD. See ENTRYPOINT. If the user specifies arguments to docker run then they will override the default specified in CMD, but still use the default ENTRYPOINT.\n\nIf CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified in the exec form.\n\nDon't confuse RUN with CMD. RUN actually runs a command and commits the result; CMD doesn't execute anything at build time, but specifies the intended command for the image.\n\n• CMD [\"executable\",\"param1\",\"param2\"] (exec form)\n• CMD [\"param1\",\"param2\"] (exec form, as default parameters to ENTRYPOINT)\n• CMD command param1 param2 (shell form)\n\n[Admonition] Don't confuse RUN with CMD. RUN actually runs a command and commits the result; CMD doesn't execute anything at build time, but specifies the intended command for the image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1466
        }
      },
      {
        "header": "LABEL",
        "content": "The LABEL instruction adds metadata to an image. A LABEL is a key-value pair. To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing. A few usage examples:\n\nAn image can have more than one label. You can specify multiple labels on a single line. Prior to Docker 1.10, this decreased the size of the final image, but this is no longer the case. You may still choose to specify multiple labels in a single instruction, in one of the following two ways:\n\nBe sure to use double quotes and not single quotes. Particularly when you are using string interpolation (e.g. LABEL example=\"foo-$ENV_VAR\"), single quotes will take the string as is without unpacking the variable's value.\n\nLabels included in base images (images in the FROM line) are inherited by your image. If a label already exists but with a different value, the most-recently-applied value overrides any previously-set value.\n\nTo view an image's labels, use the docker image inspect command. You can use the --format option to show just the labels;\n\n[Admonition] Be sure to use double quotes and not single quotes. Particularly when you are using string interpolation (e.g. LABEL example=\"foo-$ENV_VAR\"), single quotes will take the string as is without unpacking the variable's value.",
        "code_examples": [
          "```\nLABEL<key>=<value>[<key>=<value>...]\n```",
          "```\nLABEL\"com.example.vendor\"=\"ACME Incorporated\"LABELcom.example.label-with-value=\"foo\"LABELversion=\"1.0\"LABELdescription=\"This text illustrates \\that label-values can span multiple lines.\"\n```",
          "```\nLABELmulti.label1=\"value1\"multi.label2=\"value2\"other=\"value3\"\n```",
          "```\nLABELmulti.label1=\"value1\"\\multi.label2=\"value2\"\\other=\"value3\"\n```",
          "```\n{\"com.example.vendor\":\"ACME Incorporated\",\"com.example.label-with-value\":\"foo\",\"version\":\"1.0\",\"description\":\"This text illustrates that label-values can span multiple lines.\",\"multi.label1\":\"value1\",\"multi.label2\":\"value2\",\"other\":\"value3\"}\n```"
        ],
        "usage_examples": [
          "```\n$docker image inspect --format='{{json .Config.Labels}}'myimage\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1292
        }
      },
      {
        "header": "MAINTAINER (deprecated)",
        "content": "The MAINTAINER instruction sets the Author field of the generated images. The LABEL instruction is a much more flexible version of this and you should use it instead, as it enables setting any metadata you require, and can be viewed easily, for example with docker inspect. To set a label corresponding to the MAINTAINER field you could use:\n\nThis will then be visible from docker inspect with the other labels.",
        "code_examples": [
          "```\nMAINTAINER<name>\n```",
          "```\nLABELorg.opencontainers.image.authors=\"SvenDowideit@home.org.au\"\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 411
        }
      },
      {
        "header": "EXPOSE",
        "content": "The EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if you don't specify a protocol.\n\nThe EXPOSE instruction doesn't actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To publish the port when running the container, use the -p flag on docker run to publish and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports.\n\nBy default, EXPOSE assumes TCP. You can also specify UDP:\n\nTo expose on both TCP and UDP, include two lines:\n\nIn this case, if you use -P with docker run, the port will be exposed once for TCP and once for UDP. Remember that -P uses an ephemeral high-ordered host port on the host, so TCP and UDP doesn't use the same port.\n\nRegardless of the EXPOSE settings, you can override them at runtime by using the -p flag. For example\n\nTo set up port redirection on the host system, see using the -P flag. The docker network command supports creating networks for communication among containers without the need to expose or publish specific ports, because the containers connected to the network can communicate with each other over any port. For detailed information, see the overview of this feature.",
        "code_examples": [
          "```\nEXPOSE<port> [<port>/<protocol>...]\n```",
          "```\nEXPOSE80/udp\n```",
          "```\nEXPOSE80/tcpEXPOSE80/udp\n```"
        ],
        "usage_examples": [
          "```\n$docker run -p 80:80/tcp -p 80:80/udp ...\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1436
        }
      },
      {
        "header": "ENV",
        "content": "The ENV instruction sets the environment variable <key> to the value <value>. This value will be in the environment for all subsequent instructions in the build stage and can be replaced inline in many as well. The value will be interpreted for other environment variables, so quote characters will be removed if they are not escaped. Like command line parsing, quotes and backslashes can be used to include spaces within values.\n\nThe ENV instruction allows for multiple <key>=<value> ... variables to be set at one time, and the example below will yield the same net results in the final image:\n\nThe environment variables set using ENV will persist when a container is run from the resulting image. You can view the values using docker inspect, and change them using docker run --env <key>=<value>.\n\nA stage inherits any environment variables that were set using ENV by its parent stage or any ancestor. Refer to the multi-stage builds section in the manual for more information.\n\nEnvironment variable persistence can cause unexpected side effects. For example, setting ENV DEBIAN_FRONTEND=noninteractive changes the behavior of apt-get, and may confuse users of your image.\n\nIf an environment variable is only needed during build, and not in the final image, consider setting a value for a single command instead:\n\nOr using ARG, which is not persisted in the final image:\n\nThe ENV instruction also allows an alternative syntax ENV <key> <value>, omitting the =. For example:\n\nThis syntax does not allow for multiple environment-variables to be set in a single ENV instruction, and can be confusing. For example, the following sets a single environment variable (ONE) with value \"TWO= THREE=world\":\n\nThe alternative syntax is supported for backward compatibility, but discouraged for the reasons outlined above, and may be removed in a future release.\n\n[Admonition] Alternative syntaxThe ENV instruction also allows an alternative syntax ENV <key> <value>, omitting the =. For example:ENV MY_VAR my-valueThis syntax does not allow for multiple environment-variables to be set in a single ENV instruction, and can be confusing. For example, the following sets a single environment variable (ONE) with value \"TWO= THREE=world\":ENV ONE TWO= THREE=worldThe alternative syntax is supported for backward compatibility, but discouraged for the reasons outlined above, and may be removed in a future release.",
        "code_examples": [
          "```\nENV<key>=<value>[<key>=<value>...]\n```",
          "```\nENVMY_NAME=\"John Doe\"ENVMY_DOG=Rex\\The\\DogENVMY_CAT=fluffy\n```",
          "```\nENVMY_NAME=\"John Doe\"MY_DOG=Rex\\The\\Dog\\MY_CAT=fluffy\n```",
          "```\nRUNDEBIAN_FRONTEND=noninteractive apt-get update&&apt-get install -y ...\n```",
          "```\nARGDEBIAN_FRONTEND=noninteractiveRUNapt-get update&&apt-get install -y ...\n```",
          "```\nENVMY_VAR my-value\n```",
          "```\nENVONETWO=THREE=world\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2401
        }
      },
      {
        "header": "ADD",
        "content": "ADD has two forms. The latter form is required for paths containing whitespace.\n\nThe available [OPTIONS] are:\n\nThe ADD instruction copies new files or directories from <src> and adds them to the filesystem of the image at the path <dest>. Files and directories can be copied from the build context, a remote URL, or a Git repository.\n\nThe ADD and COPY instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between ADD and COPY.\n\nOption | Minimum Dockerfile version\n--- | ---\n--keep-git-dir | 1.1\n--checksum | 1.6\n--chown\n--chmod | 1.2\n--link | 1.4\n--exclude | 1.19",
        "code_examples": [
          "```\nADD[OPTIONS]<src> ... <dest>ADD[OPTIONS][\"<src>\", ...\"<dest>\"]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 4,
          "content_length": 621
        }
      },
      {
        "header": "Source",
        "content": "You can specify multiple source files or directories with ADD. The last argument must always be the destination. For example, to add two files, file1.txt and file2.txt, from the build context to /usr/src/things/ in the build container:\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash /).\n\nTo add files from a remote location, you can specify a URL or the address of a Git repository as the source. For example:\n\nBuildKit detects the type of <src> and processes it accordingly.\n\nAny relative or local path that doesn't begin with a http://, https://, or git@ protocol prefix is considered a local file path. The local file path is relative to the build context. For example, if the build context is the current directory, ADD file.txt / adds the file at ./file.txt to the root of the filesystem in the build container.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as ADD ../something /something, automatically removes any parent directory navigation (../). Trailing slashes in the source path are also disregarded, making ADD something/ /something equivalent to ADD something /something.\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build (docker build - < Dockerfile), there is no build context. In this case, you can only use the ADD instruction to copy remote files. You can also pass a tar archive through stdin: (docker build - < archive.tar), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png:\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts.\n\nWhen adding files or directories that contain special characters (such as [ and ]), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt, use the following;\n\nWhen using a local tar archive as the source for ADD, and the archive is in a recognized compression format (gzip, bzip2 or xz, or uncompressed), the archive is decompressed and extracted into the specified destination. Local tar archives are extracted by default, see the [ADD --unpack flag].\n\nWhen a directory is extracted, it has the same behavior as tar -x. The result is the union of:\n\nWhether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with .tar.gz this isn't recognized as a compressed file and doesn't generate any kind of decompression error message, rather the file will simply be copied to the destination.\n\nIn the case where source is a remote file URL, the destination will have permissions of 600. If the HTTP response contains a Last-Modified header, the timestamp from that header will be used to set the mtime on the destination file. However, like any other file processed during an ADD, mtime isn't included in the determination of whether or not the file has changed and the cache should be updated.\n\nIf remote file is a tar archive, the archive is not extracted by default. To download and extract the archive, use the [ADD --unpack flag].\n\nIf the destination ends with a trailing slash, then the filename is inferred from the URL path. For example, ADD http://example.com/foobar / would create the file /foobar. The URL must have a nontrivial path so that an appropriate filename can be discovered (http://example.com doesn't work).\n\nIf the destination doesn't end with a trailing slash, the destination path becomes the filename of the file downloaded from the URL. For example, ADD http://example.com/foo /bar creates the file /bar.\n\nIf your URL files are protected using authentication, you need to use RUN wget, RUN curl or use another tool from within the container as the ADD instruction doesn't support authentication.\n\nTo use a Git repository as the source for ADD, you can reference the repository's HTTP or SSH address as the source. The repository is cloned to the specified destination in the image.\n\nYou can use URL fragments to specify a specific branch, tag, commit, or subdirectory. For example, to add the docs directory of the v0.14.1 tag of the buildkit repository:\n\nFor more information about Git URL fragments, see URL fragments.\n\nWhen adding from a Git repository, the permissions bits for files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nWhen using a Git repository as the source, the repository must be accessible from the build context. To add a repository via SSH, whether public or private, you must pass an SSH key for authentication. For example, given the following Dockerfile:\n\nTo build this Dockerfile, pass the --ssh flag to the docker build to mount the SSH agent socket to the build. For example:\n\nFor more information about building with secrets, see Build secrets.\n\n• If <src> is a local file or directory, the contents of the directory are copied to the specified destination. See Adding files from the build context.\n• If <src> is a local tar archive, it is decompressed and extracted to the specified destination. See Adding local tar archives.\n• If <src> is a URL, the contents of the URL are downloaded and placed at the specified destination. See Adding files from a URL.\n• If <src> is a Git repository, the repository is cloned to the specified destination. See Adding files from a Git repository.\n\n• Whatever existed at the destination path, and\n• The contents of the source tree, with conflicts resolved in favor of the content being added, on a file-by-file basis.\n\n[Admonition] Whether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with .tar.gz this isn't recognized as a compressed file and doesn't generate any kind of decompression error message, rather the file will simply be copied to the destination.",
        "code_examples": [
          "```\nADDfile1.txt file2.txt /usr/src/things/\n```",
          "```\nADD*.png /dest/\n```",
          "```\nADDindex.?s /dest/\n```",
          "```\nADDarr[[]0].txt /dest/\n```"
        ],
        "usage_examples": [
          "```\nADDhttps://example.com/archive.zip /usr/src/things/ADDgit@github.com:user/repo.git /usr/src/things/\n```",
          "```\nADDhttps://github.com/user/repo.git /mydir/\n```",
          "```\nADDgit@github.com:moby/buildkit.git#v0.14.1:docs /buildkit-docs\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineADDgit@git.example.com:foo/bar.git /bar\n```",
          "```\n$docker build --ssh default .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 28,
          "content_length": 7023
        }
      },
      {
        "header": "Destination",
        "content": "If the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\nTrailing slashes are significant. For example, ADD test.txt /abs creates a file at /abs, whereas ADD test.txt /abs/ creates /abs/test.txt.\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.",
        "code_examples": [
          "```\n# create /abs/test.txtADDtest.txt /abs/\n```",
          "```\nWORKDIR/usr/src/app# create /usr/src/app/rel/test.txtADDtest.txt rel/\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 722
        }
      },
      {
        "header": "ADD --keep-git-dir",
        "content": "When <src> is the HTTP or SSH address of a remote Git repository, BuildKit adds the contents of the Git repository to the image excluding the .git directory by default.\n\nThe --keep-git-dir=true flag lets you preserve the .git directory.",
        "code_examples": [],
        "usage_examples": [
          "```\nADD[--keep-git-dir=<boolean>]<src> ... <dir>\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineADD--keep-git-dir=truehttps://github.com/moby/buildkit.git#v0.10.1 /buildkit\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 236
        }
      },
      {
        "header": "ADD --checksum",
        "content": "The --checksum flag lets you verify the checksum of a remote resource. The checksum is formatted as sha256:<hash>. SHA-256 is the only supported hash algorithm.\n\nThe --checksum flag only supports HTTP(S) sources.",
        "code_examples": [
          "```\nADD[--checksum=<hash>]<src> ... <dir>\n```",
          "```\nADD--checksum=sha256:24454f830cdb571e2c4ad15481119c43b3cafd48dd869a9b2945d1036d1dc68d https://mirrors.edge.kernel.org/pub/linux/kernel/Historic/linux-0.01.tar.gz /\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 212
        }
      },
      {
        "header": "ADD --chown --chmod",
        "content": "See COPY --chown --chmod.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 25
        }
      },
      {
        "header": "ADD --unpack",
        "content": "The --unpack flag controls whether or not to automatically unpack tar archives (including compressed formats like gzip or bzip2) when adding them to the image. Local tar archives are unpacked by default, whereas remote tar archives (where src is a URL) are downloaded without unpacking.",
        "code_examples": [
          "```\nADD[--unpack=<bool>]<src> ... <dir>\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpine# Download and unpack archive.tar.gz into /download:ADD--unpack=truehttps://example.com/archive.tar.gz /download# Add local tar without unpacking:ADD--unpack=falsemy-archive.tar.gz .\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 286
        }
      },
      {
        "header": "COPY",
        "content": "COPY has two forms. The latter form is required for paths containing whitespace.\n\nThe available [OPTIONS] are:\n\nThe COPY instruction copies new files or directories from <src> and adds them to the filesystem of the image at the path <dest>. Files and directories can be copied from the build context, build stage, named context, or an image.\n\nThe ADD and COPY instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between ADD and COPY.\n\nOption | Minimum Dockerfile version\n--- | ---\n--from\n--chown\n--chmod | 1.2\n--link | 1.4\n--parents | 1.20\n--exclude | 1.19",
        "code_examples": [
          "```\nCOPY[OPTIONS]<src> ... <dest>COPY[OPTIONS][\"<src>\", ...\"<dest>\"]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 4,
          "content_length": 615
        }
      },
      {
        "header": "Source",
        "content": "You can specify multiple source files or directories with COPY. The last argument must always be the destination. For example, to copy two files, file1.txt and file2.txt, from the build context to /usr/src/things/ in the build container:\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash /).\n\nCOPY accepts a flag --from=<name> that lets you specify the source location to be a build stage, context, or image. The following example copies files from a stage named build:\n\nFor more information about copying from named sources, see the --from flag.\n\nWhen copying source files from the build context, paths are interpreted as relative to the root of the context.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as COPY ../something /something, automatically removes any parent directory navigation (../). Trailing slashes in the source path are also disregarded, making COPY something/ /something equivalent to COPY something /something.\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build (docker build - < Dockerfile), there is no build context. In this case, you can only use the COPY instruction to copy files from other stages, named contexts, or images, using the --from flag. You can also pass a tar archive through stdin: (docker build - < archive.tar), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nWhen using a Git repository as the build context, the permissions bits for copied files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png:\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts.\n\nWhen adding files or directories that contain special characters (such as [ and ]), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt, use the following;",
        "code_examples": [
          "```\nCOPYfile1.txt file2.txt /usr/src/things/\n```",
          "```\nFROMgolang AS buildWORKDIR/appRUN--mount=type=bind,target=. go build -o /myapp ./cmdCOPY--from=build /myapp /usr/bin/\n```",
          "```\nCOPY*.png /dest/\n```",
          "```\nCOPYindex.?s /dest/\n```",
          "```\nCOPYarr[[]0].txt /dest/\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3001
        }
      },
      {
        "header": "Destination",
        "content": "If the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\nTrailing slashes are significant. For example, COPY test.txt /abs creates a file at /abs, whereas COPY test.txt /abs/ creates /abs/test.txt.\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.",
        "code_examples": [
          "```\n# create /abs/test.txtCOPYtest.txt /abs/\n```",
          "```\nWORKDIR/usr/src/app# create /usr/src/app/rel/test.txtCOPYtest.txt rel/\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 724
        }
      },
      {
        "header": "COPY --from",
        "content": "By default, the COPY instruction copies files from the build context. The COPY --from flag lets you copy files from an image, a build stage, or a named context instead.\n\nTo copy from a build stage in a multi-stage build, specify the name of the stage you want to copy from. You specify stage names using the AS keyword with the FROM instruction.\n\nYou can also copy files directly from named contexts (specified with --build-context <name>=<source>) or images. The following example copies an nginx.conf file from the official Nginx image.\n\nThe source path of COPY --from is always resolved from filesystem root of the image or stage that you specify.",
        "code_examples": [
          "```\nCOPY[--from=<image|stage|context>]<src> ... <dest>\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpine AS buildCOPY. .RUNapk add clangRUNclang -o /hello hello.cFROMscratchCOPY--from=build /hello /\n```",
          "```\nCOPY--from=nginx:latest /etc/nginx/nginx.conf /nginx.conf\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 650
        }
      },
      {
        "header": "COPY --chown --chmod",
        "content": "Only octal notation is currently supported. Non-octal support is tracked in moby/buildkit#1951.\n\nThe --chown and --chmod features are only supported on Dockerfiles used to build Linux containers, and doesn't work on Windows containers. Since user and group ownership concepts do not translate between Linux and Windows, the use of /etc/passwd and /etc/group for translating user and group names to IDs restricts this feature to only be viable for Linux OS-based containers.\n\nAll files and directories copied from the build context are created with a UID and GID of 0 unless the optional --chown flag specifies a given username, groupname, or UID/GID combination to request specific ownership of the copied content. The format of the --chown flag allows for either username and groupname strings or direct integer UID and GID in any combination. Providing a username without groupname or a UID without GID will use the same numeric UID as the GID. If a username or groupname is provided, the container's root filesystem /etc/passwd and /etc/group files will be used to perform the translation from name to integer UID or GID respectively. The following examples show valid definitions for the --chown flag:\n\nIf the container root filesystem doesn't contain either /etc/passwd or /etc/group files and either user or group names are used in the --chown flag, the build will fail on the COPY operation. Using numeric IDs requires no lookup and does not depend on container root filesystem content.\n\nWith the Dockerfile syntax version 1.10.0 and later, the --chmod flag supports variable interpolation, which lets you define the permission bits using build arguments:\n\n[Admonition] Only octal notation is currently supported. Non-octal support is tracked in moby/buildkit#1951.",
        "code_examples": [
          "```\nCOPY[--chown=<user>:<group>][--chmod=<perms> ...]<src> ... <dest>\n```",
          "```\nCOPY--chown=55:mygroup files* /somedir/COPY--chown=bin files* /somedir/COPY--chown=1files* /somedir/COPY--chown=10:11 files* /somedir/COPY--chown=myuser:mygroup --chmod=644files* /somedir/\n```"
        ],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1.10FROMalpineWORKDIR/srcARGMODE=440COPY--chmod=$MODE. .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1772
        }
      },
      {
        "header": "COPY --link",
        "content": "Enabling this flag in COPY or ADD commands allows you to copy files with enhanced semantics where your files remain independent on their own layer and don't get invalidated when commands on previous layers are changed.\n\nWhen --link is used your source files are copied into an empty destination directory. That directory is turned into a layer that is linked on top of your previous state.\n\nIs equivalent of doing two builds:\n\nand merging all the layers of both images together.\n\nUse --link to reuse already built layers in subsequent builds with --cache-from even if the previous layers have changed. This is especially important for multi-stage builds where a COPY --from statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With --link the layer the previous build generated is reused and merged on top of the new layers. This also means you can easily rebase your images when the base images receive updates, without having to execute the whole build again. In backends that support it, BuildKit can do this rebase action without the need to push or pull any layers between the client and the registry. BuildKit will detect this case and only create new image manifest that contains the new layers and old layers in correct order.\n\nThe same behavior where BuildKit can avoid pulling down the base image can also happen when using --link and no other commands that would require access to the files in the base image. In that case BuildKit will only build the layers for the COPY commands and push them to the registry directly on top of the layers of the base image.\n\nWhen using --link the COPY/ADD commands are not allowed to read any files from the previous state. This means that if in previous state the destination directory was a path that contained a symlink, COPY/ADD can not follow it. In the final image the destination path created with --link will always be a path containing only directories.\n\nIf you don't rely on the behavior of following symlinks in the destination path, using --link is always recommended. The performance of --link is equivalent or better than the default behavior and, it creates much better conditions for cache reuse.",
        "code_examples": [
          "```\nCOPY[--link[=<boolean>]]<src> ... <dest>\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineCOPY--link /foo /bar\n```",
          "```\nFROMalpine\n```",
          "```\nFROMscratchCOPY/foo /bar\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2260
        }
      },
      {
        "header": "COPY --parents",
        "content": "The --parents flag preserves parent directories for src entries. This flag defaults to false.\n\nThis behavior is similar to the Linux cp utility's --parents or rsync --relative flag.\n\nAs with Rsync, it is possible to limit which parent directories are preserved by inserting a dot and a slash (./) into the source path. If such point exists, only parent directories after it will be preserved. This may be especially useful copies between stages with --from where the source paths need to be absolute.\n\nNote that, without the --parents flag specified, any filename collision will fail the Linux cp operation with an explicit error message (cp: will not overwrite just-created './x/a.txt' with './y/a.txt'), where the Buildkit will silently overwrite the target file at the destination.\n\nWhile it is possible to preserve the directory structure for COPY instructions consisting of only one src entry, usually it is more beneficial to keep the layer count in the resulting image as low as possible. Therefore, with the --parents flag, the Buildkit is capable of packing multiple COPY instructions together, keeping the directory structure intact.",
        "code_examples": [
          "```\nCOPY[--parents[=<boolean>]]<src> ... <dest>\n```",
          "```\n# syntax=docker/dockerfile:1FROMscratchCOPY./x/a.txt ./y/a.txt /no_parents/COPY--parents ./x/a.txt ./y/a.txt /parents/# /no_parents/a.txt# /parents/x/a.txt# /parents/y/a.txt\n```",
          "```\n# syntax=docker/dockerfile:1FROMscratchCOPY--parents ./x/./y/*.txt /parents/# Build context:# ./x/y/a.txt# ./x/y/b.txt## Output:# /parents/y/a.txt# /parents/y/b.txt\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1143
        }
      },
      {
        "header": "COPY --exclude",
        "content": "The --exclude flag lets you specify a path expression for files to be excluded.\n\nThe path expression follows the same format as <src>, supporting wildcards and matching using Go's filepath.Match rules. For example, to add all files starting with \"hom\", excluding files with a .txt extension:\n\nYou can specify the --exclude option multiple times for a COPY instruction. Multiple --excludes are files matching its patterns not to be copied, even if the files paths match the pattern specified in <src>. To add all files starting with \"hom\", excluding files with either .txt or .md extensions:",
        "code_examples": [
          "```\nCOPY[--exclude=<path> ...]<src> ... <dest>\n```",
          "```\n# syntax=docker/dockerfile:1FROMscratchCOPY--exclude=*.txt hom* /mydir/\n```",
          "```\n# syntax=docker/dockerfile:1FROMscratchCOPY--exclude=*.txt --exclude=*.md hom* /mydir/\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 590
        }
      },
      {
        "header": "ENTRYPOINT",
        "content": "An ENTRYPOINT allows you to configure a container that will run as an executable.\n\nENTRYPOINT has two possible forms:\n\nThe exec form, which is the preferred form:\n\nFor more information about the different forms, see Shell and exec form.\n\nThe following command starts a container from the nginx with its default content, listening on port 80:\n\nCommand line arguments to docker run <image> will be appended after all elements in an exec form ENTRYPOINT, and will override all elements specified using CMD.\n\nThis allows arguments to be passed to the entry point, i.e., docker run <image> -d will pass the -d argument to the entry point. You can override the ENTRYPOINT instruction using the docker run --entrypoint flag.\n\nThe shell form of ENTRYPOINT prevents any CMD command line arguments from being used. It also starts your ENTRYPOINT as a subcommand of /bin/sh -c, which does not pass signals. This means that the executable will not be the container's PID 1, and will not receive Unix signals. In this case, your executable doesn't receive a SIGTERM from docker stop <container>.\n\nOnly the last ENTRYPOINT instruction in the Dockerfile will have an effect.\n\n• The exec form, which is the preferred form:ENTRYPOINT [\"executable\", \"param1\", \"param2\"]\n• The shell form:ENTRYPOINT command param1 param2",
        "code_examples": [
          "```\nENTRYPOINT[\"executable\",\"param1\",\"param2\"]\n```",
          "```\nENTRYPOINTcommandparam1 param2\n```"
        ],
        "usage_examples": [
          "```\n$docker run -i -t --rm -p 80:80 nginx\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1301
        }
      },
      {
        "header": "Exec form ENTRYPOINT example",
        "content": "You can use the exec form of ENTRYPOINT to set fairly stable default commands and arguments and then use either form of CMD to set additional defaults that are more likely to be changed.\n\nWhen you run the container, you can see that top is the only process:\n\nTo examine the result further, you can use docker exec:\n\nAnd you can gracefully request top to shut down using docker stop test.\n\nThe following Dockerfile shows using the ENTRYPOINT to run Apache in the foreground (i.e., as PID 1):\n\nIf you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using exec and gosu commands:\n\nLastly, if you need to do some extra cleanup (or communicate with other containers) on shutdown, or are co-ordinating more than one executable, you may need to ensure that the ENTRYPOINT script receives the Unix signals, passes them on, and then does some more work:\n\nIf you run this image with docker run -it --rm -p 80:80 --name test apache, you can then examine the container's processes with docker exec, or docker top, and then ask the script to stop Apache:\n\nYou can override the ENTRYPOINT setting using --entrypoint, but this can only set the binary to exec (no sh -c will be used).\n\n[Admonition] You can override the ENTRYPOINT setting using --entrypoint, but this can only set the binary to exec (no sh -c will be used).",
        "code_examples": [
          "```\nFROMubuntuENTRYPOINT[\"top\",\"-b\"]CMD[\"-c\"]\n```",
          "```\nFROMdebian:stableRUNapt-get update&&apt-get install -y --force-yes apache2EXPOSE80 443VOLUME[\"/var/www\",\"/var/log/apache2\",\"/etc/apache2\"]ENTRYPOINT[\"/usr/sbin/apache2ctl\",\"-D\",\"FOREGROUND\"]\n```"
        ],
        "usage_examples": [
          "```\n$docker run -it --rm --nametesttop -Htop - 08:25:00 up  7:27,  0 users,  load average: 0.00, 0.01, 0.05Threads:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem:   2056668 total,  1616832 used,   439836 free,    99352 buffersKiB Swap:  1441840 total,        0 used,  1441840 free.  1324440 cached MemPID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND1 root      20   0   19744   2336   2080 R  0.0  0.1   0:00.04 top\n```",
          "```\n$dockerexec-ittestps auxUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDroot         1  2.6  0.1  19752  2352 ?        Ss+  08:24   0:00 top -b -Hroot         7  0.0  0.1  15572  2164 ?        R+   08:25   0:00 ps aux\n```",
          "```\n#!/usr/bin/env bashset-eif[\"$1\"='postgres'];thenchown -R postgres\"$PGDATA\"if[-z\"$(ls -A\"$PGDATA\")\"];thengosu postgres initdbfiexecgosu postgres\"$@\"fiexec\"$@\"\n```",
          "```\n#!/bin/sh# Note: I've written this using sh so it works in the busybox container too# USE the trap if you need to also do manual cleanup after the service is stopped,#     or need to start multiple services in the one containertrap\"echo TRAPed signal\"HUP INT QUIT TERM# start service in background here/usr/sbin/apachectl startecho\"[hit enter key to exit] or run 'docker stop <container>'\"read# stop service and clean up hereecho\"stopping apache\"/usr/sbin/apachectl stopecho\"exited$0\"\n```",
          "```\n$dockerexec-ittestps auxUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDroot         1  0.1  0.0   4448   692 ?        Ss+  00:42   0:00 /bin/sh /run.sh 123 cmd cmd2root        19  0.0  0.2  71304  4440 ?        Ss   00:42   0:00 /usr/sbin/apache2 -k startwww-data    20  0.2  0.2 360468  6004 ?        Sl   00:42   0:00 /usr/sbin/apache2 -k startwww-data    21  0.2  0.2 360468  6000 ?        Sl   00:42   0:00 /usr/sbin/apache2 -k startroot        81  0.0  0.1  15572  2140 ?        R+   00:44   0:00 ps aux$docker toptestPID                 USER                COMMAND10035               root                {run.sh} /bin/sh /run.sh 123 cmd cmd210054               root                /usr/sbin/apache2 -k start10055               33                  /usr/sbin/apache2 -k start10056               33                  /usr/sbin/apache2 -k start$/usr/bin/time docker stoptesttestreal\t0m 0.27suser\t0m 0.03ssys\t0m 0.03s\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1386
        }
      },
      {
        "header": "Shell form ENTRYPOINT example",
        "content": "You can specify a plain string for the ENTRYPOINT and it will execute in /bin/sh -c. This form will use shell processing to substitute shell environment variables, and will ignore any CMD or docker run command line arguments. To ensure that docker stop will signal any long running ENTRYPOINT executable correctly, you need to remember to start it with exec:\n\nWhen you run this image, you'll see the single PID 1 process:\n\nWhich exits cleanly on docker stop:\n\nIf you forget to add exec to the beginning of your ENTRYPOINT:\n\nYou can then run it (giving it a name for the next step):\n\nYou can see from the output of top that the specified ENTRYPOINT is not PID 1.\n\nIf you then run docker stop test, the container will not exit cleanly - the stop command will be forced to send a SIGKILL after the timeout:",
        "code_examples": [
          "```\nFROMubuntuENTRYPOINTexectop -b\n```",
          "```\nFROMubuntuENTRYPOINTtop -bCMD-- --ignored-param1\n```"
        ],
        "usage_examples": [
          "```\n$docker run -it --rm --nametesttopMem: 1704520K used, 352148K free, 0K shrd, 0K buff, 140368121167873K cachedCPU:   5% usr   0% sys   0% nic  94% idle   0% io   0% irq   0% sirqLoad average: 0.08 0.03 0.05 2/98 6PID  PPID USER     STAT   VSZ %VSZ %CPU COMMAND1     0 root     R     3164   0%   0% top -b\n```",
          "```\n$/usr/bin/time docker stoptesttestreal\t0m 0.20suser\t0m 0.02ssys\t0m 0.04s\n```",
          "```\n$docker run -it --nametesttop --ignored-param2top - 13:58:24 up 17 min,  0 users,  load average: 0.00, 0.00, 0.00Tasks:   2 total,   1 running,   1 sleeping,   0 stopped,   0 zombie%Cpu(s): 16.7 us, 33.3 sy,  0.0 ni, 50.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 stMiB Mem :   1990.8 total,   1354.6 free,    231.4 used,    404.7 buff/cacheMiB Swap:   1024.0 total,   1024.0 free,      0.0 used.   1639.8 avail MemPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND1 root      20   0    2612    604    536 S   0.0   0.0   0:00.02 sh6 root      20   0    5956   3188   2768 R   0.0   0.2   0:00.00 top\n```",
          "```\n$dockerexec-ittestps wauxUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDroot         1  0.4  0.0   2612   604 pts/0    Ss+  13:58   0:00 /bin/sh -c top -b --ignored-param2root         6  0.0  0.1   5956  3188 pts/0    S+   13:58   0:00 top -broot         7  0.0  0.1   5884  2816 pts/1    Rs+  13:58   0:00 ps waux$/usr/bin/time docker stoptesttestreal\t0m 10.19suser\t0m 0.04ssys\t0m 0.03s\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 803
        }
      },
      {
        "header": "Understand how CMD and ENTRYPOINT interact",
        "content": "Both CMD and ENTRYPOINT instructions define what command gets executed when running a container. There are few rules that describe their co-operation.\n\nDockerfile should specify at least one of CMD or ENTRYPOINT commands.\n\nENTRYPOINT should be defined when using the container as an executable.\n\nCMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.\n\nCMD will be overridden when running the container with alternative arguments.\n\nThe table below shows what command is executed for different ENTRYPOINT / CMD combinations:\n\nIf CMD is defined from the base image, setting ENTRYPOINT will reset CMD to an empty value. In this scenario, CMD must be defined in the current image to have a value.\n\n• Dockerfile should specify at least one of CMD or ENTRYPOINT commands.\n• ENTRYPOINT should be defined when using the container as an executable.\n• CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container.\n• CMD will be overridden when running the container with alternative arguments.\n\nNo ENTRYPOINT | ENTRYPOINT exec_entry p1_entry | ENTRYPOINT [\"exec_entry\", \"p1_entry\"]\n--- | --- | ---\nNo CMD | error, not allowed | /bin/sh -c exec_entry p1_entry | exec_entry p1_entry\nCMD [\"exec_cmd\", \"p1_cmd\"] | exec_cmd p1_cmd | /bin/sh -c exec_entry p1_entry | exec_entry p1_entry exec_cmd p1_cmd\nCMD exec_cmd p1_cmd | /bin/sh -c exec_cmd p1_cmd | /bin/sh -c exec_entry p1_entry | exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd\n\n[Admonition] If CMD is defined from the base image, setting ENTRYPOINT will reset CMD to an empty value. In this scenario, CMD must be defined in the current image to have a value.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 7,
          "content_length": 1751
        }
      },
      {
        "header": "VOLUME",
        "content": "The VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, VOLUME [\"/var/log/\"], or a plain string with multiple arguments, such as VOLUME /var/log or VOLUME /var/log /var/db. For more information/examples and mounting instructions via the Docker client, refer to Share Directories via Volumes documentation.\n\nThe docker run command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet:\n\nThis Dockerfile results in an image that causes docker run to create a new mount point at /myvol and copy the greeting file into the newly created volume.",
        "code_examples": [
          "```\nVOLUME[\"/data\"]\n```",
          "```\nFROMubuntuRUNmkdir /myvolRUNecho\"hello world\"> /myvol/greetingVOLUME/myvol\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 786
        }
      },
      {
        "header": "Notes about specifying volumes",
        "content": "Keep the following things in mind about volumes in the Dockerfile.\n\nVolumes on Windows-based containers: When using Windows-based containers, the destination of a volume inside the container must be one of:\n\nChanging the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded when using the legacy builder. When using Buildkit, the changes will instead be kept.\n\nJSON formatting: The list is parsed as a JSON array. You must enclose words with double quotes (\") rather than single quotes (').\n\nThe host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can't be guaranteed to be available on all hosts. For this reason, you can't mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.\n\n• Volumes on Windows-based containers: When using Windows-based containers, the destination of a volume inside the container must be one of:a non-existing or empty directorya drive other than C:\n• Changing the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded when using the legacy builder. When using Buildkit, the changes will instead be kept.\n• JSON formatting: The list is parsed as a JSON array. You must enclose words with double quotes (\") rather than single quotes (').\n• The host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can't be guaranteed to be available on all hosts. For this reason, you can't mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.\n\n• a non-existing or empty directory\n• a drive other than C:",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2154
        }
      },
      {
        "header": "USER",
        "content": "The USER instruction sets the user name (or UID) and optionally the user group (or GID) to use as the default user and group for the remainder of the current stage. The specified user is used for RUN instructions and at runtime, runs the relevant ENTRYPOINT and CMD commands.\n\nNote that when specifying a group for the user, the user will have only the specified group membership. Any other configured group memberships will be ignored.\n\nWhen the user doesn't have a primary group then the image (or the next instructions) will be run with the root group.\n\nOn Windows, the user must be created first if it's not a built-in account. This can be done with the net user command called as part of a Dockerfile.\n\n[Admonition] When the user doesn't have a primary group then the image (or the next instructions) will be run with the root group.On Windows, the user must be created first if it's not a built-in account. This can be done with the net user command called as part of a Dockerfile.",
        "code_examples": [
          "```\nUSER<user>[:<group>]\n```",
          "```\nUSER<UID>[:<GID>]\n```",
          "```\nFROMmicrosoft/windowsservercore# Create Windows user in the containerRUNnet user /add patrick# Set it for subsequent commandsUSERpatrick\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 987
        }
      },
      {
        "header": "WORKDIR",
        "content": "The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn't exist, it will be created even if it's not used in any subsequent Dockerfile instruction.\n\nThe WORKDIR instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\nThe output of the final pwd command in this Dockerfile would be /a/b/c.\n\nThe WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:\n\nThe output of the final pwd command in this Dockerfile would be /path/$DIRNAME\n\nIf not specified, the default working directory is /. In practice, if you aren't building a Dockerfile from scratch (FROM scratch), the WORKDIR may likely be set by the base image you're using.\n\nTherefore, to avoid unintended operations in unknown directories, it's best practice to set your WORKDIR explicitly.",
        "code_examples": [
          "```\nWORKDIR/path/to/workdir\n```",
          "```\nWORKDIR/aWORKDIRbWORKDIRcRUNpwd\n```"
        ],
        "usage_examples": [
          "```\nENVDIRPATH=/pathWORKDIR$DIRPATH/$DIRNAMERUNpwd\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1073
        }
      },
      {
        "header": "ARG",
        "content": "The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\n\nIt isn't recommended to use build arguments for passing secrets such as user credentials, API tokens, etc. Build arguments are visible in the docker history command and in max mode provenance attestations, which are attached to the image by default if you use the Buildx GitHub Actions and your GitHub repository is public.\n\nRefer to the RUN --mount=type=secret section to learn about secure ways to use secrets when building images.\n\nA Dockerfile may include one or more ARG instructions. For example, the following is a valid Dockerfile:\n\n[Admonition] It isn't recommended to use build arguments for passing secrets such as user credentials, API tokens, etc. Build arguments are visible in the docker history command and in max mode provenance attestations, which are attached to the image by default if you use the Buildx GitHub Actions and your GitHub repository is public.Refer to the RUN --mount=type=secret section to learn about secure ways to use secrets when building images.",
        "code_examples": [
          "```\nARG<name>[=<default value>][<name>[=<default value>]...]\n```",
          "```\nFROMbusyboxARGuser1ARGbuildno# ...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1150
        }
      },
      {
        "header": "Default values",
        "content": "An ARG instruction can optionally include a default value:\n\nIf an ARG instruction has a default value and if there is no value passed at build-time, the builder uses the default.",
        "code_examples": [
          "```\nFROMbusyboxARGuser1=someuserARGbuildno=1# ...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 178
        }
      },
      {
        "header": "Scope",
        "content": "An ARG variable comes into effect from the line on which it is declared in the Dockerfile. For example, consider this Dockerfile:\n\nA user builds this file by calling:\n\nAn ARG variable declared within a build stage is automatically inherited by other stages based on that stage. Unrelated build stages do not have access to the variable. To use an argument in multiple distinct stages, each stage must include the ARG instruction, or they must both be based on a shared base stage in the same Dockerfile where the variable is declared.\n\nFor more information, refer to variable scoping.\n\n• The USER instruction on line 2 evaluates to the some_user fallback, because the username variable is not yet declared.\n• The username variable is declared on line 3, and available for reference in Dockerfile instruction from that point onwards.\n• The USER instruction on line 4 evaluates to what_user, since at that point the username argument has a value of what_user which was passed on the command line. Prior to its definition by an ARG instruction, any use of a variable results in an empty string.",
        "code_examples": [],
        "usage_examples": [
          "```\nFROMbusyboxUSER${username:-some_user}ARGusernameUSER$username# ...\n```",
          "```\n$docker build --build-argusername=what_user .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "Using ARG variables",
        "content": "You can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Consider this Dockerfile with an ENV and ARG instruction.\n\nThen, assume this image is built with this command:\n\nIn this case, the RUN instruction uses v1.0.0 instead of the ARG setting passed by the user:v2.0.1 This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition.\n\nUsing the example above but a different ENV specification you can create more useful interactions between ARG and ENV instructions:\n\nUnlike an ARG instruction, ENV values are always persisted in the built image. Consider a docker build without the --build-arg flag:\n\nUsing this Dockerfile example, CONT_IMG_VER is still persisted in the image but its value would be v1.0.0 as it is the default set in line 3 by the ENV instruction.\n\nThe variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the ENV instruction. Variable expansion is only supported for a limited set of Dockerfile instructions.",
        "code_examples": [],
        "usage_examples": [
          "```\nFROMubuntuARGCONT_IMG_VERENVCONT_IMG_VER=v1.0.0RUNecho$CONT_IMG_VER\n```",
          "```\n$docker build --build-argCONT_IMG_VER=v2.0.1 .\n```",
          "```\nFROMubuntuARGCONT_IMG_VERENVCONT_IMG_VER=${CONT_IMG_VER:-v1.0.0}RUNecho$CONT_IMG_VER\n```",
          "```\n$docker build .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1292
        }
      },
      {
        "header": "Predefined ARGs",
        "content": "Docker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile.\n\nTo use these, pass them on the command line using the --build-arg flag, for example:\n\nBy default, these pre-defined variables are excluded from the output of docker history. Excluding them reduces the risk of accidentally leaking sensitive authentication information in an HTTP_PROXY variable.\n\nFor example, consider building the following Dockerfile using --build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.com\n\nIn this case, the value of the HTTP_PROXY variable is not available in the docker history and is not cached. If you were to change location, and your proxy server changed to http://user:pass@proxy.sfo.example.com, a subsequent build does not result in a cache miss.\n\nIf you need to override this behaviour then you may do so by adding an ARG statement in the Dockerfile as follows:\n\nWhen building this Dockerfile, the HTTP_PROXY is preserved in the docker history, and changing its value invalidates the build cache.\n\n• HTTPS_PROXY\n• https_proxy",
        "code_examples": [
          "```\nFROMubuntuRUNecho\"Hello World\"\n```",
          "```\nFROMubuntuARGHTTP_PROXYRUNecho\"Hello World\"\n```"
        ],
        "usage_examples": [
          "```\n$docker build --build-argHTTPS_PROXY=https://my-proxy.example.com .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1087
        }
      },
      {
        "header": "Automatic platform ARGs in the global scope",
        "content": "This feature is only available when using the BuildKit backend.\n\nBuildKit supports a predefined set of ARG variables with information on the platform of the node performing the build (build platform) and on the platform of the resulting image (target platform). The target platform can be specified with the --platform flag on docker build.\n\nThe following ARG variables are set automatically:\n\nThese arguments are defined in the global scope so are not automatically available inside build stages or for your RUN commands. To expose one of these arguments inside the build stage redefine it without value.\n\n• TARGETPLATFORM - platform of the build result. Eg linux/amd64, linux/arm/v7, windows/amd64.\n• TARGETOS - OS component of TARGETPLATFORM\n• TARGETARCH - architecture component of TARGETPLATFORM\n• TARGETVARIANT - variant component of TARGETPLATFORM\n• BUILDPLATFORM - platform of the node performing the build.\n• BUILDOS - OS component of BUILDPLATFORM\n• BUILDARCH - architecture component of BUILDPLATFORM\n• BUILDVARIANT - variant component of BUILDPLATFORM",
        "code_examples": [],
        "usage_examples": [
          "```\nFROMalpineARGTARGETPLATFORMRUNecho\"I'm building for$TARGETPLATFORM\"\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1063
        }
      },
      {
        "header": "BuildKit built-in build args",
        "content": "When using a Git context, .git dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build:\n\nArg | Type | Description\n--- | --- | ---\nBUILDKIT_BUILD_NAME | String | Override the build name shown in buildx history command and Docker Desktop Builds view.\nBUILDKIT_CACHE_MOUNT_NS | String | Set optional cache ID namespace.\nBUILDKIT_CONTEXT_KEEP_GIT_DIR | Bool | Trigger Git context to keep the .git directory.\nBUILDKIT_HISTORY_PROVENANCE_V1 | Bool | Enable SLSA Provenance v1 for build history record.\nBUILDKIT_INLINE_CACHE2 | Bool | Inline cache metadata to image config or not.\nBUILDKIT_MULTI_PLATFORM | Bool | Opt into deterministic output regardless of multi-platform output or not.\nBUILDKIT_SANDBOX_HOSTNAME | String | Set the hostname (default buildkitsandbox)\nBUILDKIT_SYNTAX | String | Set frontend image\nSOURCE_DATE_EPOCH | Int | Set the Unix timestamp for created image and layers. More info from reproducible builds. Supported since Dockerfile 1.5, BuildKit 0.11",
        "code_examples": [],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1FROMalpineWORKDIR/srcRUN--mount=target=.\\makeREVISION=$(git rev-parse HEAD)build\n```",
          "```\n$docker build --build-argBUILDKIT_CONTEXT_KEEP_GIT_DIR=1https://github.com/user/repo.git#main\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 1,
          "content_length": 1034
        }
      },
      {
        "header": "Impact on build caching",
        "content": "ARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile.\n\nFor example, consider these two Dockerfile:\n\nIf you specify --build-arg CONT_IMG_VER=<value> on the command line, in both cases, the specification on line 2 doesn't cause a cache miss; line 3 does cause a cache miss. ARG CONT_IMG_VER causes the RUN line to be identified as the same as running CONT_IMG_VER=<value> echo hello, so if the <value> changes, you get a cache miss.\n\nConsider another example under the same command line:\n\nIn this example, the cache miss occurs on line 3. The miss happens because the variable's value in the ENV references the ARG variable and that variable is changed through the command line. In this example, the ENV command causes the image to include the value.\n\nIf an ENV instruction overrides an ARG instruction of the same name, like this Dockerfile:\n\nLine 3 doesn't cause a cache miss because the value of CONT_IMG_VER is a constant (hello). As a result, the environment variables and values used on the RUN (line 4) doesn't change between builds.",
        "code_examples": [
          "```\nFROMubuntuARGCONT_IMG_VERRUNechohello\n```"
        ],
        "usage_examples": [
          "```\nFROMubuntuARGCONT_IMG_VERRUNecho$CONT_IMG_VER\n```",
          "```\nFROMubuntuARGCONT_IMG_VERENVCONT_IMG_VER=$CONT_IMG_VERRUNecho$CONT_IMG_VER\n```",
          "```\nFROMubuntuARGCONT_IMG_VERENVCONT_IMG_VER=helloRUNecho$CONT_IMG_VER\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1551
        }
      },
      {
        "header": "ONBUILD",
        "content": "The ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile.\n\nThis is useful if you are building an image which will be used as a base to build other images, for example an application build environment or a daemon which may be customized with user-specific configuration.\n\nFor example, if your image is a reusable Python application builder, it will require application source code to be added in a particular directory, and it might require a build script to be called after that. You can't just call ADD and RUN now, because you don't yet have access to the application source code, and it will be different for each application build. You could simply provide application developers with a boilerplate Dockerfile to copy-paste into their application, but that's inefficient, error-prone and difficult to update because it mixes with application-specific code.\n\nThe solution is to use ONBUILD to register advance instructions to run later, during the next build stage.\n\nHere's how it works:\n\nFor example you might add something like this:\n\n• When it encounters an ONBUILD instruction, the builder adds a trigger to the metadata of the image being built. The instruction doesn't otherwise affect the current build.\n• At the end of the build, a list of all triggers is stored in the image manifest, under the key OnBuild. They can be inspected with the docker inspect command.\n• Later the image may be used as a base for a new build, using the FROM instruction. As part of processing the FROM instruction, the downstream builder looks for ONBUILD triggers, and executes them in the same order they were registered. If any of the triggers fail, the FROM instruction is aborted which in turn causes the build to fail. If all triggers succeed, the FROM instruction completes and the build continues as usual.\n• Triggers are cleared from the final image after being executed. In other words they aren't inherited by \"grand-children\" builds.",
        "code_examples": [
          "```\nONBUILD<INSTRUCTION>\n```"
        ],
        "usage_examples": [
          "```\nONBUILDADD. /app/srcONBUILDRUN/usr/local/bin/python-build --dir /app/src\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2191
        }
      },
      {
        "header": "Copy or mount from stage, image, or context",
        "content": "As of Dockerfile syntax 1.11, you can use ONBUILD with instructions that copy or mount files from other stages, images, or build contexts. For example:\n\nIf the source of from is a build stage, the stage must be defined in the Dockerfile where ONBUILD gets triggered. If it's a named context, that context must be passed to the downstream build.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1.11FROMalpine AS baseimageONBUILDCOPY--from=build /usr/bin/app /appONBUILDRUN--mount=from=config,target=/opt/appconfig ...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 344
        }
      },
      {
        "header": "ONBUILD limitations",
        "content": "• Chaining ONBUILD instructions using ONBUILD ONBUILD isn't allowed.\n• The ONBUILD instruction may not trigger FROM or MAINTAINER instructions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 143
        }
      },
      {
        "header": "STOPSIGNAL",
        "content": "The STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit. This signal can be a signal name in the format SIG<NAME>, for instance SIGKILL, or an unsigned number that matches a position in the kernel's syscall table, for instance 9. The default is SIGTERM if not defined.\n\nThe image's default stopsignal can be overridden per container, using the --stop-signal flag on docker run and docker create.",
        "code_examples": [
          "```\nSTOPSIGNALsignal\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 437
        }
      },
      {
        "header": "HEALTHCHECK",
        "content": "The HEALTHCHECK instruction has two forms:\n\nThe HEALTHCHECK instruction tells Docker how to test a container to check that it's still working. This can detect cases such as a web server stuck in an infinite loop and unable to handle new connections, even though the server process is still running.\n\nWhen a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially starting. Whenever a health check passes, it becomes healthy (whatever state it was previously in). After a certain number of consecutive failures, it becomes unhealthy.\n\nThe options that can appear before CMD are:\n\nThe health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes.\n\nIf a single run of the check takes longer than timeout seconds then the check is considered to have failed. The process performing the check is abruptly stopped with a SIGKILL.\n\nIt takes retries consecutive failures of the health check for the container to be considered unhealthy.\n\nstart period provides initialization time for containers that need time to bootstrap. Probe failure during that period will not be counted towards the maximum number of retries. However, if a health check succeeds during the start period, the container is considered started and all consecutive failures will be counted towards the maximum number of retries.\n\nstart interval is the time between health checks during the start period. This option requires Docker Engine version 25.0 or later.\n\nThere can only be one HEALTHCHECK instruction in a Dockerfile. If you list more than one then only the last HEALTHCHECK will take effect.\n\nThe command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK CMD /bin/check-running) or an exec array (as with other Dockerfile commands; see e.g. ENTRYPOINT for details).\n\nThe command's exit status indicates the health status of the container. The possible values are:\n\nFor example, to check every five minutes or so that a web-server is able to serve the site's main page within three seconds:\n\nTo help debug failing probes, any output text (UTF-8 encoded) that the command writes on stdout or stderr will be stored in the health status and can be queried with docker inspect. Such output should be kept short (only the first 4096 bytes are stored currently).\n\nWhen the health status of a container changes, a health_status event is generated with the new status.\n\n• HEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container)\n• HEALTHCHECK NONE (disable any healthcheck inherited from the base image)\n\n• --interval=DURATION (default: 30s)\n• --timeout=DURATION (default: 30s)\n• --start-period=DURATION (default: 0s)\n• --start-interval=DURATION (default: 5s)\n• --retries=N (default: 3)\n\n• 0: success - the container is healthy and ready for use\n• 1: unhealthy - the container isn't working correctly\n• 2: reserved - don't use this exit code",
        "code_examples": [
          "```\nHEALTHCHECK --interval=5m --timeout=3s \\CMDcurl -f http://localhost/||exit1\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 3013
        }
      },
      {
        "header": "SHELL",
        "content": "The SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is [\"/bin/sh\", \"-c\"], and on Windows is [\"cmd\", \"/S\", \"/C\"]. The SHELL instruction must be written in JSON form in a Dockerfile.\n\nThe SHELL instruction is particularly useful on Windows where there are two commonly used and quite different native shells: cmd and powershell, as well as alternate shells available including sh.\n\nThe SHELL instruction can appear multiple times. Each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent instructions. For example:\n\nThe following instructions can be affected by the SHELL instruction when the shell form of them is used in a Dockerfile: RUN, CMD and ENTRYPOINT.\n\nThe following example is a common pattern found on Windows which can be streamlined by using the SHELL instruction:\n\nThe command invoked by the builder will be:\n\nThis is inefficient for two reasons. First, there is an unnecessary cmd.exe command processor (aka shell) being invoked. Second, each RUN instruction in the shell form requires an extra powershell -command prefixing the command.\n\nTo make this more efficient, one of two mechanisms can be employed. One is to use the JSON form of the RUN command such as:\n\nWhile the JSON form is unambiguous and does not use the unnecessary cmd.exe, it does require more verbosity through double-quoting and escaping. The alternate mechanism is to use the SHELL instruction and the shell form, making a more natural syntax for Windows users, especially when combined with the escape parser directive:\n\nThe SHELL instruction could also be used to modify the way in which a shell operates. For example, using SHELL cmd /S /C /V:ON|OFF on Windows, delayed environment variable expansion semantics could be modified.\n\nThe SHELL instruction can also be used on Linux should an alternate shell be required such as zsh, csh, tcsh and others.",
        "code_examples": [
          "```\nSHELL[\"executable\",\"parameters\"]\n```",
          "```\nFROMmicrosoft/windowsservercore# Executed as cmd /S /C echo defaultRUNechodefault# Executed as cmd /S /C powershell -command Write-Host defaultRUNpowershell -command Write-Host default# Executed as powershell -command Write-Host helloSHELL[\"powershell\",\"-command\"]RUNWrite-Host hello# Executed as cmd /S /C echo helloSHELL[\"cmd\",\"/S\",\"/C\"]RUNechohello\n```",
          "```\nRUNpowershell -command Execute-MyCmdlet -param1\"c:\\foo.txt\"\n```",
          "```\ncmd/S/Cpowershell-commandExecute-MyCmdlet-param1\"c:\\foo.txt\"\n```",
          "```\nRUN[\"powershell\",\"-command\",\"Execute-MyCmdlet\",\"-param1 \\\"c:\\\\foo.txt\\\"\"]\n```",
          "```\n# escape=`FROMmicrosoft/nanoserverSHELL[\"powershell\",\"-command\"]RUNNew-Item -ItemType Directory C:\\ExampleADDExecute-MyCmdlet.ps1 c:\\example\\RUN c:\\example\\Execute-MyCmdlet -sample'hello world'\n```",
          "```\nPS E:\\myproject> docker build -t shell .Sending build context to Docker daemon 4.096 kBStep 1/5 : FROM microsoft/nanoserver---> 22738ff49c6dStep 2/5 : SHELL powershell -command---> Running in 6fcdb6855ae2---> 6331462d4300Removing intermediate container 6fcdb6855ae2Step 3/5 : RUN New-Item -ItemType Directory C:\\Example---> Running in d0eef8386e97Directory: C:\\Mode         LastWriteTime              Length Name----         -------------              ------ ----d-----       10/28/2016  11:26 AM              Example---> 3f2fbf1395d9Removing intermediate container d0eef8386e97Step 4/5 : ADD Execute-MyCmdlet.ps1 c:\\example\\---> a955b2621c31Removing intermediate container b825593d39fcStep 5/5 : RUN c:\\example\\Execute-MyCmdlet 'hello world'---> Running in be6d8e63fe75hello world---> 8e559e9bf424Removing intermediate container be6d8e63fe75Successfully built 8e559e9bf424PS E:\\myproject>\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 1951
        }
      },
      {
        "header": "Here-Documents",
        "content": "Here-documents allow redirection of subsequent Dockerfile lines to the input of RUN or COPY commands. If such command contains a here-document the Dockerfile considers the next lines until the line only containing a here-doc delimiter as part of the same command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 263
        }
      },
      {
        "header": "Example: Running a multi-line script",
        "content": "If the command only contains a here-document, its contents is evaluated with the default shell.\n\nAlternatively, shebang header can be used to define an interpreter.\n\nMore complex examples may use multiple here-documents.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMdebianRUN<<EOT bashset-exapt-get updateapt-get install -y vimEOT\n```",
          "```\n# syntax=docker/dockerfile:1FROMdebianRUN<<EOTmkdir -p foo/barEOT\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineRUN<<FILE1 cat > file1 && <<FILE2cat > file2I amfirstFILE1I amsecondFILE2\n```"
        ],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1FROMpython:3.6RUN<<EOT#!/usr/bin/env pythonprint(\"hello world\")EOT\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 220
        }
      },
      {
        "header": "Example: Creating inline files",
        "content": "With COPY instructions, you can replace the source parameter with a here-doc indicator to write the contents of the here-document directly to a file. The following example creates a greeting.txt file containing hello world using a COPY instruction.\n\nRegular here-doc variable expansion and tab stripping rules apply. The following example shows a small Dockerfile that creates a hello.sh script file using a COPY instruction with a here-document.\n\nIn this case, file script prints \"hello bar\", because the variable is expanded when the COPY instruction gets executed.\n\nIf instead you were to quote any part of the here-document word EOT, the variable would not be expanded at build-time.\n\nNote that ARG FOO=bar is excessive here, and can be removed. The variable gets interpreted at runtime, when the script is invoked:",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMalpineCOPY<<EOF greeting.txthello worldEOF\n```"
        ],
        "usage_examples": [
          "```\n# syntax=docker/dockerfile:1FROMalpineARGFOO=barCOPY<<-EOT /script.shecho\"hello${FOO}\"EOTENTRYPOINTash /script.sh\n```",
          "```\n$docker build -t heredoc .$docker run heredochello bar\n```",
          "```\n# syntax=docker/dockerfile:1FROMalpineARGFOO=barCOPY<<-\"EOT\"/script.shecho\"hello${FOO}\"EOTENTRYPOINTash /script.sh\n```",
          "```\n$docker build -t heredoc .$docker run -eFOO=world heredochello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 819
        }
      },
      {
        "header": "Dockerfile examples",
        "content": "For examples of Dockerfiles, refer to:\n\nValue required ↩︎ ↩︎ ↩︎\n\nFor Docker-integrated BuildKit and docker buildx build ↩︎\n\n• The building best practices page\n• The \"get started\" tutorials\n• The language-specific getting started guides\n\n• Value required ↩︎ ↩︎ ↩︎\n• For Docker-integrated BuildKit and docker buildx build ↩︎\n\n[Note] Value required ↩︎ ↩︎ ↩︎For Docker-integrated BuildKit and docker buildx build ↩︎",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 411
        }
      }
    ],
    "url": "https://docs.docker.com/reference/dockerfile/",
    "doc_type": "docker",
    "total_sections": 75
  },
  {
    "title": "Docker Compose",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "Docker Compose is a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience.\n\nCompose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single YAML configuration file. Then, with a single command, you create and start all the services from your configuration file.\n\nCompose works in all environments - production, staging, development, testing, as well as CI workflows. It also has commands for managing the whole lifecycle of your application:\n\n• Get started\n\n• Start, stop, and rebuild services\n• View the status of running services\n• Stream the log output of running services\n• Run a one-off command on a service",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 781
        }
      },
      {
        "header": "Why use Compose?",
        "content": "Understand Docker Compose's key benefits",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 40
        }
      },
      {
        "header": "How Compose works",
        "content": "Understand how Compose works",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 28
        }
      },
      {
        "header": "Install Compose",
        "content": "Follow the instructions on how to install Docker Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 57
        }
      },
      {
        "header": "Quickstart",
        "content": "Learn the key concepts of Docker Compose whilst building a simple Python web application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 89
        }
      },
      {
        "header": "View the release notes",
        "content": "Find out about the latest enhancements and bug fixes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 53
        }
      },
      {
        "header": "Explore the Compose file reference",
        "content": "Find information on defining services, networks, and volumes for a Docker application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 86
        }
      },
      {
        "header": "Use Compose Bridge",
        "content": "Transform your Compose configuration file into configuration files for different platforms, such as Kubernetes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 111
        }
      },
      {
        "header": "Browse common FAQs",
        "content": "Explore general FAQs and find out how to give feedback.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 55
        }
      },
      {
        "header": "Migrate to Compose v2",
        "content": "Learn how to migrate from Compose v1 to v2",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 42
        }
      }
    ],
    "url": "https://docs.docker.com/compose/",
    "doc_type": "docker",
    "total_sections": 10
  },
  {
    "title": "Compose file reference",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "New to Docker Compose?\n\nFind more information about the key features and use cases of Docker Compose or try the quickstart guide.\n\nThe Compose Specification is the latest and recommended version of the Compose file format. It helps you define a Compose file which is used to configure your Docker applicationâs services, networks, volumes, and more.\n\nLegacy versions 2.x and 3.x of the Compose file format were merged into the Compose Specification. It is implemented in versions 1.27.0 and above (also known as Compose v2) of the Docker Compose CLI.\n\nThe Compose Specification on Docker Docs is the Docker Compose implementation. If you wish to implement your own version of the Compose Specification, see the Compose Specification repository.\n\nUse the following links to navigate key sections of the Compose Specification.\n\nWant a better editing experience for Compose files in VS Code? Check out the Docker VS Code Extension (Beta) for linting, code navigation, and vulnerability scanning.\n\n• Get started\n\n[Admonition] Want a better editing experience for Compose files in VS Code? Check out the Docker VS Code Extension (Beta) for linting, code navigation, and vulnerability scanning.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1190
        }
      },
      {
        "header": "Version and name top-level element",
        "content": "Understand version and name attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 51
        }
      },
      {
        "header": "Services top-level element",
        "content": "Explore all services attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 44
        }
      },
      {
        "header": "Networks top-level element",
        "content": "Find all networks attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 41
        }
      },
      {
        "header": "Volumes top-level element",
        "content": "Explore all volumes attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 43
        }
      },
      {
        "header": "Configs top-level element",
        "content": "Find out about configs in Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 34
        }
      },
      {
        "header": "Secrets top-level element",
        "content": "Learn about secrets in Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 31
        }
      }
    ],
    "url": "https://docs.docker.com/compose/compose-file/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "Docker Compose Quickstart",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "This tutorial aims to introduce fundamental concepts of Docker Compose by guiding you through the development of a basic Python web application.\n\nUsing the Flask framework, the application features a hit counter in Redis, providing a practical example of how Docker Compose can be applied in web development scenarios.\n\nThe concepts demonstrated here should be understandable even if you're not familiar with Python.\n\nThis is a non-normative example that demonstrates core Compose functionality.\n\n• Get started",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 510
        }
      },
      {
        "header": "Prerequisites",
        "content": "• Installed the latest version of Docker Compose\n• A basic understanding of Docker concepts and how Docker works",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 112
        }
      },
      {
        "header": "Step 1: Set up",
        "content": "Create a directory for the project:\n\nCreate a file called app.py in your project directory and paste the following code in:\n\nIn this example, redis is the hostname of the redis container on the application's network and the default port, 6379 is used.\n\nNote the way the get_hit_count function is written. This basic retry loop attempts the request multiple times if the Redis service is not available. This is useful at startup while the application comes online, but also makes the application more resilient if the Redis service needs to be restarted anytime during the app's lifetime. In a cluster, this also helps handling momentary connection drops between nodes.\n\nCreate another file called requirements.txt in your project directory and paste the following code in:\n\nCreate a Dockerfile and paste the following code in:\n\nThis tells Docker to:\n\nCheck that the Dockerfile has no file extension like .txt. Some editors may append this file extension automatically which results in an error when you run the application.\n\nFor more information on how to write Dockerfiles, see the Dockerfile reference.\n\n• Create a directory for the project:$ mkdir composetest $ cd composetest\n• Create a file called app.py in your project directory and paste the following code in:import time import redis from flask import Flask app = Flask(__name__) cache = redis.Redis(host='redis', port=6379) def get_hit_count(): retries = 5 while True: try: return cache.incr('hits') except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5) @app.route('/') def hello(): count = get_hit_count() return f'Hello World! I have been seen {count} times.\\n'In this example, redis is the hostname of the redis container on the application's network and the default port, 6379 is used.NoteNote the way the get_hit_count function is written. This basic retry loop attempts the request multiple times if the Redis service is not available. This is useful at startup while the application comes online, but also makes the application more resilient if the Redis service needs to be restarted anytime during the app's lifetime. In a cluster, this also helps handling momentary connection drops between nodes.\n• Create another file called requirements.txt in your project directory and paste the following code in:flask redis\n• Create a Dockerfile and paste the following code in:# syntax=docker/dockerfile:1FROM python:3.10-alpineWORKDIR /codeENV FLASK_APP=app.pyENV FLASK_RUN_HOST=0.0.0.0RUN apk add --no-cache gcc musl-dev linux-headersCOPY requirements.txt requirements.txtRUN pip install -r requirements.txtEXPOSE 5000COPY . .CMD [\"flask\", \"run\", \"--debug\"]Understand the Dockerfile This tells Docker to:Build an image starting with the Python 3.10 image.Set the working directory to /code.Set environment variables used by the flask command.Install gcc and other dependenciesCopy requirements.txt and install the Python dependencies.Add metadata to the image to describe that the container is listening on port 5000Copy the current directory . in the project to the workdir . in the image.Set the default command for the container to flask run --debug.ImportantCheck that the Dockerfile has no file extension like .txt. Some editors may append this file extension automatically which results in an error when you run the application.For more information on how to write Dockerfiles, see the Dockerfile reference.\n\n• Build an image starting with the Python 3.10 image.\n• Set the working directory to /code.\n• Set environment variables used by the flask command.\n• Install gcc and other dependencies\n• Copy requirements.txt and install the Python dependencies.\n• Add metadata to the image to describe that the container is listening on port 5000\n• Copy the current directory . in the project to the workdir . in the image.\n• Set the default command for the container to flask run --debug.\n\n[Admonition] Note the way the get_hit_count function is written. This basic retry loop attempts the request multiple times if the Redis service is not available. This is useful at startup while the application comes online, but also makes the application more resilient if the Redis service needs to be restarted anytime during the app's lifetime. In a cluster, this also helps handling momentary connection drops between nodes.\n\n[Admonition] Check that the Dockerfile has no file extension like .txt. Some editors may append this file extension automatically which results in an error when you run the application.",
        "code_examples": [
          "```\nimporttimeimportredisfromflaskimportFlaskapp=Flask(__name__)cache=redis.Redis(host='redis',port=6379)defget_hit_count():retries=5whileTrue:try:returncache.incr('hits')exceptredis.exceptions.ConnectionErrorasexc:ifretries==0:raiseexcretries-=1time.sleep(0.5)@app.route('/')defhello():count=get_hit_count()returnf'Hello World! I have been seen{count}times.\\n'\n```",
          "```\nflaskredis\n```"
        ],
        "usage_examples": [
          "```\n$mkdir composetest$cdcomposetest\n```",
          "```\n# syntax=docker/dockerfile:1FROMpython:3.10-alpineWORKDIR/codeENVFLASK_APP=app.pyENVFLASK_RUN_HOST=0.0.0.0RUNapk add --no-cache gcc musl-dev linux-headersCOPYrequirements.txt requirements.txtRUNpip install -r requirements.txtEXPOSE5000COPY. .CMD[\"flask\",\"run\",\"--debug\"]\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 4518
        }
      },
      {
        "header": "Step 2: Define services in a Compose file",
        "content": "Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file.\n\nCreate a file called compose.yaml in your project directory and paste the following:\n\nThis Compose file defines two services: web and redis.\n\nThe web service uses an image that's built from the Dockerfile in the current directory. It then binds the container and the host machine to the exposed port, 8000. This example service uses the default port for the Flask web server, 5000.\n\nThe redis service uses a public Redis image pulled from the Docker Hub registry.\n\nFor more information on the compose.yaml file, see How Compose works.",
        "code_examples": [
          "```\nservices:web:build:.ports:-\"8000:5000\"redis:image:\"redis:alpine\"\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 710
        }
      },
      {
        "header": "Step 3: Build and run your app with Compose",
        "content": "With a single command, you create and start all the services from your configuration file.\n\nFrom your project directory, start up your application by running docker compose up.\n\nCompose pulls a Redis image, builds an image for your code, and starts the services you defined. In this case, the code is statically copied into the image at build time.\n\nEnter http://localhost:8000/ in a browser to see the application running.\n\nIf this doesn't resolve, you can also try http://127.0.0.1:8000.\n\nYou should see a message in your browser saying:\n\nThe number should increment.\n\nSwitch to another terminal window, and type docker image ls to list local images.\n\nListing images at this point should return redis and web.\n\nYou can inspect images with docker inspect <tag or id>.\n\nStop the application, either by running docker compose down from within your project directory in the second terminal, or by hitting CTRL+C in the original terminal where you started the app.\n\n• From your project directory, start up your application by running docker compose up.$ docker compose up Creating network \"composetest_default\" with the default driver Creating composetest_web_1 ... Creating composetest_redis_1 ... Creating composetest_web_1 Creating composetest_redis_1 ... done Attaching to composetest_web_1, composetest_redis_1 web_1 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) redis_1 | 1:C 17 Aug 22:11:10.480 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis_1 | 1:C 17 Aug 22:11:10.480 # Redis version=4.0.1, bits=64, commit=00000000, modified=0, pid=1, just started redis_1 | 1:C 17 Aug 22:11:10.480 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf web_1 | * Restarting with stat redis_1 | 1:M 17 Aug 22:11:10.483 * Running mode=standalone, port=6379. redis_1 | 1:M 17 Aug 22:11:10.483 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. web_1 | * Debugger is active! redis_1 | 1:M 17 Aug 22:11:10.483 # Server initialized redis_1 | 1:M 17 Aug 22:11:10.483 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. web_1 | * Debugger PIN: 330-787-903 redis_1 | 1:M 17 Aug 22:11:10.483 * Ready to accept connections Compose pulls a Redis image, builds an image for your code, and starts the services you defined. In this case, the code is statically copied into the image at build time.\n• Enter http://localhost:8000/ in a browser to see the application running.If this doesn't resolve, you can also try http://127.0.0.1:8000.You should see a message in your browser saying:Hello World! I have been seen 1 times.\n• Refresh the page.The number should increment.Hello World! I have been seen 2 times.\n• Switch to another terminal window, and type docker image ls to list local images.Listing images at this point should return redis and web.$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE composetest_web latest e2c21aa48cc1 4 minutes ago 93.8MB python 3.4-alpine 84e6077c7ab6 7 days ago 82.5MB redis alpine 9d8fa9aa0e5b 3 weeks ago 27.5MB You can inspect images with docker inspect <tag or id>.\n• Stop the application, either by running docker compose down from within your project directory in the second terminal, or by hitting CTRL+C in the original terminal where you started the app.",
        "code_examples": [
          "```\nHello World! I have been seen 1 times.\n```",
          "```\nHello World! I have been seen 2 times.\n```"
        ],
        "usage_examples": [
          "```\n$docker compose upCreating network \"composetest_default\" with the default driverCreating composetest_web_1 ...Creating composetest_redis_1 ...Creating composetest_web_1Creating composetest_redis_1 ... doneAttaching to composetest_web_1, composetest_redis_1web_1    |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)redis_1  | 1:C 17 Aug 22:11:10.480 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Ooredis_1  | 1:C 17 Aug 22:11:10.480 # Redis version=4.0.1, bits=64, commit=00000000, modified=0, pid=1, just startedredis_1  | 1:C 17 Aug 22:11:10.480 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.confweb_1    |  * Restarting with statredis_1  | 1:M 17 Aug 22:11:10.483 * Running mode=standalone, port=6379.redis_1  | 1:M 17 Aug 22:11:10.483 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.web_1    |  * Debugger is active!redis_1  | 1:M 17 Aug 22:11:10.483 # Server initializedredis_1  | 1:M 17 Aug 22:11:10.483 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.web_1    |  * Debugger PIN: 330-787-903redis_1  | 1:M 17 Aug 22:11:10.483 * Ready to accept connections\n```",
          "```\n$docker image lsREPOSITORY        TAG           IMAGE ID      CREATED        SIZEcomposetest_web   latest        e2c21aa48cc1  4 minutes ago  93.8MBpython            3.4-alpine    84e6077c7ab6  7 days ago     82.5MBredis             alpine        9d8fa9aa0e5b  3 weeks ago    27.5MB\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 3679
        }
      },
      {
        "header": "Step 4: Edit the Compose file to use Compose Watch",
        "content": "Edit the compose.yaml file in your project directory to use watch so you can preview your running Compose services which are automatically updated as you edit and save your code:\n\nWhenever a file is changed, Compose syncs the file to the corresponding location under /code inside the container. Once copied, the bundler updates the running application without a restart.\n\nFor more information on how Compose Watch works, see Use Compose Watch. Alternatively, see Manage data in containers for other options.\n\nFor this example to work, the --debug option is added to the Dockerfile. The --debug option in Flask enables automatic code reload, making it possible to work on the backend API without the need to restart or rebuild the container. After changing the .py file, subsequent API calls will use the new code, but the browser UI will not automatically refresh in this small example. Most frontend development servers include native live reload support that works with Compose.\n\n[Admonition] For this example to work, the --debug option is added to the Dockerfile. The --debug option in Flask enables automatic code reload, making it possible to work on the backend API without the need to restart or rebuild the container. After changing the .py file, subsequent API calls will use the new code, but the browser UI will not automatically refresh in this small example. Most frontend development servers include native live reload support that works with Compose.",
        "code_examples": [
          "```\nservices:web:build:.ports:-\"8000:5000\"develop:watch:-action:syncpath:.target:/coderedis:image:\"redis:alpine\"\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1466
        }
      },
      {
        "header": "Step 5: Re-build and run the app with Compose",
        "content": "From your project directory, type docker compose watch or docker compose up --watch to build and launch the app and start the file watch mode.\n\nCheck the Hello World message in a web browser again, and refresh to see the count increment.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker compose watch[+] Running 2/2â Container docs-redis-1 Created                                                                                                                                                                                                        0.0sâ Container docs-web-1    Recreated                                                                                                                                                                                                      0.1sAttaching to redis-1, web-1â¦¿ watch enabled...\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 237
        }
      },
      {
        "header": "Step 6: Update the application",
        "content": "To see Compose Watch in action:\n\nChange the greeting in app.py and save it. For example, change the Hello World! message to Hello from Docker!:\n\nRefresh the app in your browser. The greeting should be updated, and the counter should still be incrementing.\n\nOnce you're done, run docker compose down.\n\n• Change the greeting in app.py and save it. For example, change the Hello World! message to Hello from Docker!:return f'Hello from Docker! I have been seen {count} times.\\n'\n• Refresh the app in your browser. The greeting should be updated, and the counter should still be incrementing.\n• Once you're done, run docker compose down.",
        "code_examples": [
          "```\nreturnf'Hello from Docker! I have been seen{count}times.\\n'\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 633
        }
      },
      {
        "header": "Step 7: Split up your services",
        "content": "Using multiple Compose files lets you customize a Compose application for different environments or workflows. This is useful for large applications that may use dozens of containers, with ownership distributed across multiple teams.\n\nIn your project folder, create a new Compose file called infra.yaml.\n\nCut the Redis service from your compose.yaml file and paste it into your new infra.yaml file. Make sure you add the services top-level attribute at the top of your file. Your infra.yaml file should now look like this:\n\nIn your compose.yaml file, add the include top-level attribute along with the path to the infra.yaml file.\n\nRun docker compose up to build the app with the updated Compose files, and run it. You should see the Hello world message in your browser.\n\nThis is a simplified example, but it demonstrates the basic principle of include and how it can make it easier to modularize complex applications into sub-Compose files. For more information on include and working with multiple Compose files, see Working with multiple Compose files.\n\n• In your project folder, create a new Compose file called infra.yaml.\n• Cut the Redis service from your compose.yaml file and paste it into your new infra.yaml file. Make sure you add the services top-level attribute at the top of your file. Your infra.yaml file should now look like this:services:redis:image:\"redis:alpine\"\n• In your compose.yaml file, add the include top-level attribute along with the path to the infra.yaml file.include:- infra.yamlservices:web:build:.ports:- \"8000:5000\"develop:watch:- action:syncpath:.target:/code\n• Run docker compose up to build the app with the updated Compose files, and run it. You should see the Hello world message in your browser.",
        "code_examples": [
          "```\nservices:redis:image:\"redis:alpine\"\n```",
          "```\ninclude:-infra.yamlservices:web:build:.ports:-\"8000:5000\"develop:watch:-action:syncpath:.target:/code\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1736
        }
      },
      {
        "header": "Step 8: Experiment with some other commands",
        "content": "If you want to run your services in the background, you can pass the -d flag (for \"detached\" mode) to docker compose up and use docker compose ps to see what is currently running:\n\nRun docker compose --help to see other available commands.\n\nIf you started Compose with docker compose up -d, stop your services once you've finished with them:\n\nYou can bring everything down, removing the containers entirely, with the docker compose down command.\n\n• If you want to run your services in the background, you can pass the -d flag (for \"detached\" mode) to docker compose up and use docker compose ps to see what is currently running:$ docker compose up -d Starting composetest_redis_1... Starting composetest_web_1... $ docker compose ps Name Command State Ports ------------------------------------------------------------------------------------- composetest_redis_1 docker-entrypoint.sh redis ... Up 6379/tcp composetest_web_1 flask run Up 0.0.0.0:8000->5000/tcp\n• Run docker compose --help to see other available commands.\n• If you started Compose with docker compose up -d, stop your services once you've finished with them:$ docker compose stop\n• You can bring everything down, removing the containers entirely, with the docker compose down command.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker compose up -dStarting composetest_redis_1...Starting composetest_web_1...$docker compose psName                      Command               State           Ports-------------------------------------------------------------------------------------composetest_redis_1   docker-entrypoint.sh redis ...   Up      6379/tcpcomposetest_web_1     flask run                        Up      0.0.0.0:8000->5000/tcp\n```",
          "```\n$docker compose stop\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1250
        }
      },
      {
        "header": "Where to go next",
        "content": "• Try the Sample apps with Compose\n• Explore the full list of Compose commands\n• Explore the Compose file reference\n• Check out the Learning Docker Compose video on LinkedIn Learning",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 182
        }
      }
    ],
    "url": "https://docs.docker.com/compose/gettingstarted/",
    "doc_type": "docker",
    "total_sections": 11
  },
  {
    "title": "Volumes",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "Volumes are persistent data stores for containers, created and managed by Docker. You can create a volume explicitly using the docker volume create command, or Docker can create a volume during container or service creation.\n\nWhen you create a volume, it's stored within a directory on the Docker host. When you mount the volume into a container, this directory is what's mounted into the container. This is similar to the way that bind mounts work, except that volumes are managed by Docker and are isolated from the core functionality of the host machine.\n\n• Get started",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 572
        }
      },
      {
        "header": "When to use volumes",
        "content": "Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes are a good choice for the following use cases:\n\nVolumes are not a good choice if you need to access the files from the host, as the volume is completely managed by Docker. Use bind mounts if you need to access files or directories from both containers and the host.\n\nVolumes are often a better choice than writing data directly to a container, because a volume doesn't increase the size of the containers using it. Using a volume is also faster; writing into a container's writable layer requires a storage driver to manage the filesystem. The storage driver provides a union filesystem, using the Linux kernel. This extra abstraction reduces performance as compared to using volumes, which write directly to the host filesystem.\n\nIf your container generates non-persistent state data, consider using a tmpfs mount to avoid storing the data anywhere permanently, and to increase the container's performance by avoiding writing into the container's writable layer.\n\nVolumes use rprivate (recursive private) bind propagation, and bind propagation isn't configurable for volumes.\n\n• Volumes are easier to back up or migrate than bind mounts.\n• You can manage volumes using Docker CLI commands or the Docker API.\n• Volumes work on both Linux and Windows containers.\n• Volumes can be more safely shared among multiple containers.\n• New volumes can have their content pre-populated by a container or build.\n• When your application requires high-performance I/O.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1692
        }
      },
      {
        "header": "A volume's lifecycle",
        "content": "A volume's contents exist outside the lifecycle of a given container. When a container is destroyed, the writable layer is destroyed with it. Using a volume ensures that the data is persisted even if the container using it is removed.\n\nA given volume can be mounted into multiple containers simultaneously. When no running container is using a volume, the volume is still available to Docker and isn't removed automatically. You can remove unused volumes using docker volume prune.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 481
        }
      },
      {
        "header": "Mounting a volume over existing data",
        "content": "If you mount a non-empty volume into a directory in the container in which files or directories exist, the pre-existing files are obscured by the mount. This is similar to if you were to save files into /mnt on a Linux host, and then mounted a USB drive into /mnt. The contents of /mnt would be obscured by the contents of the USB drive until the USB drive was unmounted.\n\nWith containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.\n\nIf you mount an empty volume into a directory in the container in which files or directories exist, these files or directories are propagated (copied) into the volume by default. Similarly, if you start a container and specify a volume which does not already exist, an empty volume is created for you. This is a good way to pre-populate data that another container needs.\n\nTo prevent Docker from copying a container's pre-existing files into an empty volume, use the volume-nocopy option, see Options for --mount.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1056
        }
      },
      {
        "header": "Named and anonymous volumes",
        "content": "A volume may be named or anonymous. Anonymous volumes are given a random name that's guaranteed to be unique within a given Docker host. Just like named volumes, anonymous volumes persist even if you remove the container that uses them, except if you use the --rm flag when creating the container, in which case the anonymous volume associated with the container is destroyed. See Remove anonymous volumes.\n\nIf you create multiple containers consecutively that each use anonymous volumes, each container creates its own volume. Anonymous volumes aren't reused or shared between containers automatically. To share an anonymous volume between two or more containers, you must mount the anonymous volume using the random volume ID.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 728
        }
      },
      {
        "header": "Syntax",
        "content": "To mount a volume with the docker run command, you can use either the --mount or --volume flag.\n\nIn general, --mount is preferred. The main difference is that the --mount flag is more explicit and supports all the available options.\n\nYou must use --mount if you want to:\n\n• Specify volume driver options\n• Mount a volume subdirectory\n• Mount a volume into a Swarm service",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --mounttype=volume,src=<volume-name>,dst=<mount-path>$docker run --volume <volume-name>:<mount-path>\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 371
        }
      },
      {
        "header": "Options for --mount",
        "content": "The --mount flag consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. The order of the keys isn't significant.\n\nValid options for --mount type=volume include:\n\nOption | Description\n--- | ---\nsource, src | The source of the mount. For named volumes, this is the name of the volume. For anonymous volumes, this field is omitted.\ndestination, dst, target | The path where the file or directory is mounted in the container.\nvolume-subpath | A path to a subdirectory within the volume to mount into the container. The subdirectory must exist in the volume before the volume is mounted to a container. See Mount a volume subdirectory.\nreadonly, ro | If present, causes the volume to be mounted into the container as read-only.\nvolume-nocopy | If present, data at the destination isn't copied into the volume if the volume is empty. By default, content at the target destination gets copied into a mounted volume if empty.\nvolume-opt | Can be specified more than once, takes a key-value pair consisting of the option name and its value.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --mounttype=volume[,src=<volume-name>],dst=<mount-path>[,<key>=<value>...]\n```",
          "```\n$docker run --mounttype=volume,src=myvolume,dst=/data,ro,volume-subpath=/foo\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 1080
        }
      },
      {
        "header": "Options for --volume",
        "content": "The --volume or -v flag consists of three fields, separated by colon characters (:). The fields must be in the correct order.\n\nIn the case of named volumes, the first field is the name of the volume, and is unique on a given host machine. For anonymous volumes, the first field is omitted. The second field is the path where the file or directory is mounted in the container.\n\nThe third field is optional, and is a comma-separated list of options. Valid options for --volume with a data volume include:\n\nOption | Description\n--- | ---\nreadonly, ro | If present, causes the volume to be mounted into the container as read-only.\nvolume-nocopy | If present, data at the destination isn't copied into the volume if the volume is empty. By default, content at the target destination gets copied into a mounted volume if empty.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -v[<volume-name>:]<mount-path>[:opts]\n```",
          "```\n$docker run -v myvolume:/data:ro\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 821
        }
      },
      {
        "header": "Create and manage volumes",
        "content": "Unlike a bind mount, you can create and manage volumes outside the scope of any container.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker volume create my-vol\n```",
          "```\n$docker volume lslocal               my-vol\n```",
          "```\n$docker volume inspect my-vol[{\"Driver\": \"local\",\"Labels\": {},\"Mountpoint\": \"/var/lib/docker/volumes/my-vol/_data\",\"Name\": \"my-vol\",\"Options\": {},\"Scope\": \"local\"}]\n```",
          "```\n$docker volume rm my-vol\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 90
        }
      },
      {
        "header": "Start a container with a volume",
        "content": "If you start a container with a volume that doesn't yet exist, Docker creates the volume for you. The following example mounts the volume myvol2 into /app/ in the container.\n\nThe following -v and --mount examples produce the same result. You can't run them both unless you remove the devtest container and the myvol2 volume after running the first one.\n\nUse docker inspect devtest to verify that Docker created the volume and it mounted correctly. Look for the Mounts section:\n\nThis shows that the mount is a volume, it shows the correct source and destination, and that the mount is read-write.\n\nStop the container and remove the volume. Note volume removal is a separate step.",
        "code_examples": [
          "```\n\"Mounts\":[{\"Type\":\"volume\",\"Name\":\"myvol2\",\"Source\":\"/var/lib/docker/volumes/myvol2/_data\",\"Destination\":\"/app\",\"Driver\":\"local\",\"Mode\":\"\",\"RW\":true,\"Propagation\":\"\"}],\n```"
        ],
        "usage_examples": [
          "```\n$docker run -d\\--name devtest \\--mount source=myvol2,target=/app \\nginx:latest\n```",
          "```\n$docker run -d\\--name devtest \\-v myvol2:/app \\nginx:latest\n```",
          "```\n$docker container stop devtest$docker container rm devtest$docker volume rm myvol2\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 678
        }
      },
      {
        "header": "Use a volume with Docker Compose",
        "content": "The following example shows a single Docker Compose service with a volume:\n\nRunning docker compose up for the first time creates a volume. Docker reuses the same volume when you run the command subsequently.\n\nYou can create a volume directly outside of Compose using docker volume create and then reference it inside compose.yaml as follows:\n\nFor more information about using volumes with Compose, refer to the Volumes section in the Compose specification.",
        "code_examples": [
          "```\nservices:frontend:image:node:ltsvolumes:-myapp:/home/node/appvolumes:myapp:\n```",
          "```\nservices:frontend:image:node:ltsvolumes:-myapp:/home/node/appvolumes:myapp:external:true\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 456
        }
      },
      {
        "header": "Start a service with volumes",
        "content": "When you start a service and define a volume, each service container uses its own local volume. None of the containers can share this data if you use the local volume driver. However, some volume drivers do support shared storage.\n\nThe following example starts an nginx service with four replicas, each of which uses a local volume called myvol2.\n\nUse docker service ps devtest-service to verify that the service is running:\n\nYou can remove the service to stop the running tasks:\n\nRemoving the service doesn't remove any volumes created by the service. Volume removal is a separate step.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker service create -d\\--replicas=4 \\--name devtest-service \\--mount source=myvol2,target=/app \\nginx:latest\n```",
          "```\n$docker service ps devtest-serviceID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS4d7oz1j85wwn        devtest-service.1   nginx:latest        moby                Running             Running 14 seconds ago\n```",
          "```\n$docker service rm devtest-service\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 587
        }
      },
      {
        "header": "Populate a volume using a container",
        "content": "If you start a container which creates a new volume, and the container has files or directories in the directory to be mounted such as /app/, Docker copies the directory's contents into the volume. The container then mounts and uses the volume, and other containers which use the volume also have access to the pre-populated content.\n\nTo show this, the following example starts an nginx container and populates the new volume nginx-vol with the contents of the container's /usr/share/nginx/html directory. This is where Nginx stores its default HTML content.\n\nThe --mount and -v examples have the same end result.\n\nAfter running either of these examples, run the following commands to clean up the containers and volumes. Note volume removal is a separate step.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d\\--name=nginxtest \\--mount source=nginx-vol,destination=/usr/share/nginx/html \\nginx:latest\n```",
          "```\n$docker run -d\\--name=nginxtest \\-v nginx-vol:/usr/share/nginx/html \\nginx:latest\n```",
          "```\n$docker container stop nginxtest$docker container rm nginxtest$docker volume rm nginx-vol\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 761
        }
      },
      {
        "header": "Use a read-only volume",
        "content": "For some development applications, the container needs to write into the bind mount so that changes are propagated back to the Docker host. At other times, the container only needs read access to the data. Multiple containers can mount the same volume. You can simultaneously mount a single volume as read-write for some containers and as read-only for others.\n\nThe following example changes the previous one. It mounts the directory as a read-only volume, by adding ro to the (empty by default) list of options, after the mount point within the container. Where multiple options are present, you can separate them using commas.\n\nThe --mount and -v examples have the same result.\n\nUse docker inspect nginxtest to verify that Docker created the read-only mount correctly. Look for the Mounts section:\n\nStop and remove the container, and remove the volume. Volume removal is a separate step.",
        "code_examples": [
          "```\n\"Mounts\":[{\"Type\":\"volume\",\"Name\":\"nginx-vol\",\"Source\":\"/var/lib/docker/volumes/nginx-vol/_data\",\"Destination\":\"/usr/share/nginx/html\",\"Driver\":\"local\",\"Mode\":\"\",\"RW\":false,\"Propagation\":\"\"}],\n```"
        ],
        "usage_examples": [
          "```\n$docker run -d\\--name=nginxtest \\--mount source=nginx-vol,destination=/usr/share/nginx/html,readonly \\nginx:latest\n```",
          "```\n$docker run -d\\--name=nginxtest \\-v nginx-vol:/usr/share/nginx/html:ro \\nginx:latest\n```",
          "```\n$docker container stop nginxtest$docker container rm nginxtest$docker volume rm nginx-vol\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 889
        }
      },
      {
        "header": "Mount a volume subdirectory",
        "content": "When you mount a volume to a container, you can specify a subdirectory of the volume to use, with the volume-subpath parameter for the --mount flag. The subdirectory that you specify must exist in the volume before you attempt to mount it into a container; if it doesn't exist, the mount fails.\n\nSpecifying volume-subpath is useful if you only want to share a specific portion of a volume with a container. Say for example that you have multiple containers running and you want to store logs from each container in a shared volume. You can create a subdirectory for each container in the shared volume, and mount the subdirectory to the container.\n\nThe following example creates a logs volume and initiates the subdirectories app1 and app2 in the volume. It then starts two containers and mounts one of the subdirectories of the logs volume to each container. This example assumes that the processes in the containers write their logs to /var/log/app1 and /var/log/app2.\n\nWith this setup, the containers write their logs to separate subdirectories of the logs volume. The containers can't access the other container's logs.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker volume create logs$docker run --rm\\--mount src=logs,dst=/logs \\alpine mkdir -p /logs/app1 /logs/app2$docker run -d\\--name=app1 \\--mount src=logs,dst=/var/log/app1,volume-subpath=app1 \\app1:latest$docker run -d\\--name=app2 \\--mount src=logs,dst=/var/log/app2,volume-subpath=app2 \\app2:latest\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1123
        }
      },
      {
        "header": "Share data between machines",
        "content": "When building fault-tolerant applications, you may need to configure multiple replicas of the same service to have access to the same files.\n\nThere are several ways to achieve this when developing your applications. One is to add logic to your application to store files on a cloud object storage system like Amazon S3. Another is to create volumes with a driver that supports writing files to an external storage system like NFS or Amazon S3.\n\nVolume drivers let you abstract the underlying storage system from the application logic. For example, if your services use a volume with an NFS driver, you can update the services to use a different driver. For example, to store data in the cloud, without changing the application logic.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 733
        }
      },
      {
        "header": "Use a volume driver",
        "content": "When you create a volume using docker volume create, or when you start a container which uses a not-yet-created volume, you can specify a volume driver. The following examples use the rclone/docker-volume-rclone volume driver, first when creating a standalone volume, and then when starting a container which creates a new volume.\n\nIf your volume driver accepts a comma-separated list as an option, you must escape the value from the outer CSV parser. To escape a volume-opt, surround it with double quotes (\") and surround the entire mount parameter with single quotes (').\n\nFor example, the local driver accepts mount options as a comma-separated list in the o parameter. This example shows the correct way to escape the list.\n\n[Admonition] If your volume driver accepts a comma-separated list as an option, you must escape the value from the outer CSV parser. To escape a volume-opt, surround it with double quotes (\") and surround the entire mount parameter with single quotes (').For example, the local driver accepts mount options as a comma-separated list in the o parameter. This example shows the correct way to escape the list.$ docker service create \\ --mount 'type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=local,volume-opt=type=nfs,volume-opt=device=<nfs-server>:<nfs-path>,\"volume-opt=o=addr=<nfs-address>,vers=4,soft,timeo=180,bg,tcp,rw\"' --name myservice \\ <IMAGE>",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker service create\\--mount 'type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=local,volume-opt=type=nfs,volume-opt=device=<nfs-server>:<nfs-path>,\"volume-opt=o=addr=<nfs-address>,vers=4,soft,timeo=180,bg,tcp,rw\"'--name myservice \\<IMAGE>\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1398
        }
      },
      {
        "header": "Initial setup",
        "content": "The following example assumes that you have two nodes, the first of which is a Docker host and can connect to the second node using SSH.\n\nOn the Docker host, install the rclone/docker-volume-rclone plugin:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker plugin install --grant-all-permissions rclone/docker-volume-rclone --aliases rclone\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 205
        }
      },
      {
        "header": "Create a volume using a volume driver",
        "content": "This example mounts the /remote directory on host 1.2.3.4 into a volume named rclonevolume. Each volume driver may have zero or more configurable options, you specify each of them using an -o flag.\n\nThis volume can now be mounted into containers.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker volume create\\-d rclone \\--name rclonevolume \\-o type=sftp \\-o path=remote \\-o sftp-host=1.2.3.4 \\-o sftp-user=user \\-o \"sftp-password=$(cat file_containing_password_for_remote_host)\"\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 246
        }
      },
      {
        "header": "Start a container which creates a volume using a volume driver",
        "content": "If the volume driver requires you to pass any options, you must use the --mount flag to mount the volume, and not -v.\n\n[Admonition] If the volume driver requires you to pass any options, you must use the --mount flag to mount the volume, and not -v.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d\\--name rclone-container \\--mount type=volume,volume-driver=rclone,src=rclonevolume,target=/app,volume-opt=type=sftp,volume-opt=path=remote, volume-opt=sftp-host=1.2.3.4,volume-opt=sftp-user=user,volume-opt=-o \"sftp-password=$(cat file_containing_password_for_remote_host)\" \\nginx:latest\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 249
        }
      },
      {
        "header": "Create a service which creates an NFS volume",
        "content": "The following example shows how you can create an NFS volume when creating a service. It uses 10.0.0.10 as the NFS server and /var/docker-nfs as the exported directory on the NFS server. Note that the volume driver specified is local.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker service create -d\\--name nfs-service \\--mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,volume-opt=o=addr=10.0.0.10' \\nginx:latest\n```",
          "```\n$docker service create -d\\--name nfs-service \\--mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,\"volume-opt=o=addr=10.0.0.10,rw,nfsvers=4,async\"' \\nginx:latest\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Create CIFS/Samba volumes",
        "content": "You can mount a Samba share directly in Docker without configuring a mount point on your host.\n\nThe addr option is required if you specify a hostname instead of an IP. This lets Docker perform the hostname lookup.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker volume create\\--driver local \\--opt type=cifs \\--opt device=//uxxxxx.your-server.de/backup \\--opt o=addr=uxxxxx.your-server.de,username=uxxxxxxx,password=*****,file_mode=0777,dir_mode=0777 \\--name cifs-volume\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 213
        }
      },
      {
        "header": "Block storage devices",
        "content": "You can mount a block storage device, such as an external drive or a drive partition, to a container. The following example shows how to create and use a file as a block storage device, and how to mount the block device as a container volume.\n\nThe following procedure is only an example. The solution illustrated here isn't recommended as a general practice. Don't attempt this approach unless you're confident about what you're doing.\n\nUnder the hood, the --mount flag using the local storage driver invokes the Linux mount syscall and forwards the options you pass to it unaltered. Docker doesn't implement any additional functionality on top of the native mount features supported by the Linux kernel.\n\nIf you're familiar with the Linux mount command, you can think of the --mount options as forwarded to the mount command in the following manner:\n\nTo explain this further, consider the following mount command example. This command mounts the /dev/loop5 device to the path /external-drive on the system.\n\nThe following docker run command achieves a similar result, from the point of view of the container being run. Running a container with this --mount option sets up the mount in the same way as if you had executed the mount command from the previous example.\n\nYou can't run the mount command inside the container directly, because the container is unable to access the /dev/loop5 device. That's why the docker run command uses the --mount option.\n\nThe following steps create an ext4 filesystem and mounts it into a container. The filesystem support of your system depends on the version of the Linux kernel you are using.\n\nCreate a file and allocate some space to it:\n\nBuild a filesystem onto the disk.raw file:\n\nCreate a loop device:\n\nlosetup creates an ephemeral loop device that's removed after system reboot, or manually removed with losetup -d.\n\nRun a container that mounts the loop device as a volume:\n\nWhen the container starts, the path /external-drive mounts the disk.raw file from the host filesystem as a block device.\n\nWhen you're done, and the device is unmounted from the container, detach the loop device to remove the device from the host system:\n\n• Create a file and allocate some space to it:$ fallocate -l 1G disk.raw\n• Build a filesystem onto the disk.raw file:$ mkfs.ext4 disk.raw\n• Create a loop device:$ losetup -f --show disk.raw /dev/loop5 Notelosetup creates an ephemeral loop device that's removed after system reboot, or manually removed with losetup -d.\n• Run a container that mounts the loop device as a volume:$ docker run -it --rm \\ --mount='type=volume,dst=/external-drive,volume-driver=local,volume-opt=device=/dev/loop5,volume-opt=type=ext4' \\ ubuntu bash When the container starts, the path /external-drive mounts the disk.raw file from the host filesystem as a block device.\n• When you're done, and the device is unmounted from the container, detach the loop device to remove the device from the host system:$ losetup -d /dev/loop5\n\n[Admonition] The following procedure is only an example. The solution illustrated here isn't recommended as a general practice. Don't attempt this approach unless you're confident about what you're doing.\n\n[Admonition] losetup creates an ephemeral loop device that's removed after system reboot, or manually removed with losetup -d.",
        "code_examples": [],
        "usage_examples": [
          "```\n$mount -t <mount.volume-opt.type> <mount.volume-opt.device> <mount.dst> -o <mount.volume-opts.o>\n```",
          "```\n$mount -t ext4 /dev/loop5 /external-drive\n```",
          "```\n$docker run\\--mount='type=volume,dst=/external-drive,volume-driver=local,volume-opt=device=/dev/loop5,volume-opt=type=ext4'\n```",
          "```\n$fallocate -l 1G disk.raw\n```",
          "```\n$mkfs.ext4 disk.raw\n```",
          "```\n$losetup -f --show disk.raw/dev/loop5\n```",
          "```\n$docker run -it --rm\\--mount='type=volume,dst=/external-drive,volume-driver=local,volume-opt=device=/dev/loop5,volume-opt=type=ext4' \\ubuntu bash\n```",
          "```\n$losetup -d /dev/loop5\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 3310
        }
      },
      {
        "header": "Back up, restore, or migrate data volumes",
        "content": "Volumes are useful for backups, restores, and migrations. Use the --volumes-from flag to create a new container that mounts that volume.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 136
        }
      },
      {
        "header": "Back up a volume",
        "content": "For example, create a new container named dbstore:\n\nIn the next command:\n\nWhen the command completes and the container stops, it creates a backup of the dbdata volume.\n\n• Launch a new container and mount the volume from the dbstore container\n• Mount a local host directory as /backup\n• Pass a command that tars the contents of the dbdata volume to a backup.tar file inside the /backup directory.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -v /dbdata --name dbstore ubuntu /bin/bash\n```",
          "```\n$docker run --rm --volumes-from dbstore -v$(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 395
        }
      },
      {
        "header": "Restore volume from a backup",
        "content": "With the backup just created, you can restore it to the same container, or to another container that you created elsewhere.\n\nFor example, create a new container named dbstore2:\n\nThen, un-tar the backup file in the new containerâs data volume:\n\nYou can use these techniques to automate backup, migration, and restore testing using your preferred tools.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -v /dbdata --name dbstore2 ubuntu /bin/bash\n```",
          "```\n$docker run --rm --volumes-from dbstore2 -v$(pwd):/backup ubuntu bash -c\"cd /dbdata && tar xvf /backup/backup.tar --strip 1\"\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 353
        }
      },
      {
        "header": "Remove volumes",
        "content": "A Docker data volume persists after you delete a container. There are two types of volumes to consider:\n\n• Named volumes have a specific source from outside the container, for example, awesome:/bar.\n• Anonymous volumes have no specific source. Therefore, when the container is deleted, you can instruct the Docker Engine daemon to remove them.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 343
        }
      },
      {
        "header": "Remove anonymous volumes",
        "content": "To automatically remove anonymous volumes, use the --rm option. For example, this command creates an anonymous /foo volume. When you remove the container, the Docker Engine removes the /foo volume but not the awesome volume.\n\nIf another container binds the volumes with --volumes-from, the volume definitions are copied and the anonymous volume also stays after the first container is removed.\n\n[Admonition] If another container binds the volumes with --volumes-from, the volume definitions are copied and the anonymous volume also stays after the first container is removed.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --rm -v /foo -v awesome:/bar busybox top\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 575
        }
      },
      {
        "header": "Remove all volumes",
        "content": "To remove all unused volumes and free up space:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker volume prune\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 47
        }
      },
      {
        "header": "Next steps",
        "content": "• Learn about bind mounts.\n• Learn about tmpfs mounts.\n• Learn about storage drivers.\n• Learn about third-party volume driver plugins.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 134
        }
      }
    ],
    "url": "https://docs.docker.com/storage/volumes/",
    "doc_type": "docker",
    "total_sections": 30
  },
  {
    "title": "Bind mounts",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "When you use a bind mount, a file or directory on the host machine is mounted from the host into a container. By contrast, when you use a volume, a new directory is created within Docker's storage directory on the host machine, and Docker manages that directory's contents.\n\n• Get started",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 288
        }
      },
      {
        "header": "When to use bind mounts",
        "content": "Bind mounts are appropriate for the following types of use case:\n\nSharing source code or build artifacts between a development environment on the Docker host and a container.\n\nWhen you want to create or generate files in a container and persist the files onto the host's filesystem.\n\nSharing configuration files from the host machine to containers. This is how Docker provides DNS resolution to containers by default, by mounting /etc/resolv.conf from the host machine into each container.\n\nBind mounts are also available for builds: you can bind mount source code from the host into the build container to test, lint, or compile a project.\n\n• Sharing source code or build artifacts between a development environment on the Docker host and a container.\n• When you want to create or generate files in a container and persist the files onto the host's filesystem.\n• Sharing configuration files from the host machine to containers. This is how Docker provides DNS resolution to containers by default, by mounting /etc/resolv.conf from the host machine into each container.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1069
        }
      },
      {
        "header": "Bind-mounting over existing data",
        "content": "If you bind mount file or directory into a directory in the container in which files or directories exist, the pre-existing files are obscured by the mount. This is similar to if you were to save files into /mnt on a Linux host, and then mounted a USB drive into /mnt. The contents of /mnt would be obscured by the contents of the USB drive until the USB drive was unmounted.\n\nWith containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 545
        }
      },
      {
        "header": "Considerations and constraints",
        "content": "Bind mounts have write access to files on the host by default.\n\nOne side effect of using bind mounts is that you can change the host filesystem via processes running in a container, including creating, modifying, or deleting important system files or directories. This capability can have security implications. For example, it may affect non-Docker processes on the host system.\n\nYou can use the readonly or ro option to prevent the container from writing to the mount.\n\nBind mounts are created to the Docker daemon host, not the client.\n\nIf you're using a remote Docker daemon, you can't create a bind mount to access files on the client machine in a container.\n\nFor Docker Desktop, the daemon runs inside a Linux VM, not directly on the native host. Docker Desktop has built-in mechanisms that transparently handle bind mounts, allowing you to share native host filesystem paths with containers running in the virtual machine.\n\nContainers with bind mounts are strongly tied to the host.\n\nBind mounts rely on the host machine's filesystem having a specific directory structure available. This reliance means that containers with bind mounts may fail if run on a different host without the same directory structure.\n\n• Bind mounts have write access to files on the host by default.One side effect of using bind mounts is that you can change the host filesystem via processes running in a container, including creating, modifying, or deleting important system files or directories. This capability can have security implications. For example, it may affect non-Docker processes on the host system.You can use the readonly or ro option to prevent the container from writing to the mount.\n• Bind mounts are created to the Docker daemon host, not the client.If you're using a remote Docker daemon, you can't create a bind mount to access files on the client machine in a container.For Docker Desktop, the daemon runs inside a Linux VM, not directly on the native host. Docker Desktop has built-in mechanisms that transparently handle bind mounts, allowing you to share native host filesystem paths with containers running in the virtual machine.\n• Containers with bind mounts are strongly tied to the host.Bind mounts rely on the host machine's filesystem having a specific directory structure available. This reliance means that containers with bind mounts may fail if run on a different host without the same directory structure.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2428
        }
      },
      {
        "header": "Syntax",
        "content": "To create a bind mount, you can use either the --mount or --volume flag.\n\nIn general, --mount is preferred. The main difference is that the --mount flag is more explicit and supports all the available options.\n\nIf you use --volume to bind-mount a file or directory that does not yet exist on the Docker host, Docker automatically creates the directory on the host for you. It's always created as a directory.\n\n--mount does not automatically create a directory if the specified mount path does not exist on the host. Instead, it produces an error:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --mounttype=bind,src=<host-path>,dst=<container-path>$docker run --volume <host-path>:<container-path>\n```",
          "```\n$docker run --mounttype=bind,src=/dev/noexist,dst=/mnt/foo alpinedocker: Error response from daemon: invalid mount config for type \"bind\": bind source path does not exist: /dev/noexist.\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 546
        }
      },
      {
        "header": "Options for --mount",
        "content": "The --mount flag consists of multiple key-value pairs, separated by commas and each consisting of a <key>=<value> tuple. The order of the keys isn't significant.\n\nValid options for --mount type=bind include:\n\nOption | Description\n--- | ---\nsource, src | The location of the file or directory on the host. This can be an absolute or relative path.\ndestination, dst, target | The path where the file or directory is mounted in the container. Must be an absolute path.\nreadonly, ro | If present, causes the bind mount to be mounted into the container as read-only.\nbind-propagation | If present, changes the bind propagation.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --mounttype=bind,src=<host-path>,dst=<container-path>[,<key>=<value>...]\n```",
          "```\n$docker run --mounttype=bind,src=.,dst=/project,ro,bind-propagation=rshared\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 622
        }
      },
      {
        "header": "Options for --volume",
        "content": "The --volume or -v flag consists of three fields, separated by colon characters (:). The fields must be in the correct order.\n\nThe first field is the path on the host to bind mount into the container. The second field is the path where the file or directory is mounted in the container.\n\nThe third field is optional, and is a comma-separated list of options. Valid options for --volume with a bind mount include:\n\nOption | Description\n--- | ---\nreadonly, ro | If present, causes the bind mount to be mounted into the container as read-only.\nz, Z | Configures SELinux labeling. See Configure the SELinux label\nrprivate (default) | Sets bind propagation to rprivate for this mount. See Configure bind propagation.\nprivate | Sets bind propagation to private for this mount. See Configure bind propagation.\nrshared | Sets bind propagation to rshared for this mount. See Configure bind propagation.\nshared | Sets bind propagation to shared for this mount. See Configure bind propagation.\nrslave | Sets bind propagation to rslave for this mount. See Configure bind propagation.\nslave | Sets bind propagation to slave for this mount. See Configure bind propagation.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -v <host-path>:<container-path>[:opts]\n```",
          "```\n$docker run -v .:/project:ro,rshared\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 1158
        }
      },
      {
        "header": "Start a container with a bind mount",
        "content": "Consider a case where you have a directory source and that when you build the source code, the artifacts are saved into another directory, source/target/. You want the artifacts to be available to the container at /app/, and you want the container to get access to a new build each time you build the source on your development host. Use the following command to bind-mount the target/ directory into your container at /app/. Run the command from within the source directory. The $(pwd) sub-command expands to the current working directory on Linux or macOS hosts. If you're on Windows, see also Path conversions on Windows.\n\nThe following --mount and -v examples produce the same result. You can't run them both unless you remove the devtest container after running the first one.\n\nUse docker inspect devtest to verify that the bind mount was created correctly. Look for the Mounts section:\n\nThis shows that the mount is a bind mount, it shows the correct source and destination, it shows that the mount is read-write, and that the propagation is set to rprivate.\n\nStop and remove the container:",
        "code_examples": [
          "```\n\"Mounts\":[{\"Type\":\"bind\",\"Source\":\"/tmp/source/target\",\"Destination\":\"/app\",\"Mode\":\"\",\"RW\":true,\"Propagation\":\"rprivate\"}],\n```"
        ],
        "usage_examples": [
          "```\n$docker run -d\\-it \\--name devtest \\--mount type=bind,source=\"$(pwd)\"/target,target=/app \\nginx:latest\n```",
          "```\n$docker run -d\\-it \\--name devtest \\-v \"$(pwd)\"/target:/app \\nginx:latest\n```",
          "```\n$docker container rm -fv devtest\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1096
        }
      },
      {
        "header": "Mount into a non-empty directory on the container",
        "content": "If you bind-mount a directory into a non-empty directory on the container, the directory's existing contents are obscured by the bind mount. This can be beneficial, such as when you want to test a new version of your application without building a new image. However, it can also be surprising and this behavior differs from that of volumes.\n\nThis example is contrived to be extreme, but replaces the contents of the container's /usr/ directory with the /tmp/ directory on the host machine. In most cases, this would result in a non-functioning container.\n\nThe --mount and -v examples have the same end result.\n\nThe container is created but does not start. Remove it:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d\\-it \\--name broken-container \\--mount type=bind,source=/tmp,target=/usr \\nginx:latestdocker: Error response from daemon: oci runtime error: container_linux.go:262:starting container process caused \"exec: \\\"nginx\\\": executable file not found in $PATH\".\n```",
          "```\n$docker run -d\\-it \\--name broken-container \\-v /tmp:/usr \\nginx:latestdocker: Error response from daemon: oci runtime error: container_linux.go:262:starting container process caused \"exec: \\\"nginx\\\": executable file not found in $PATH\".\n```",
          "```\n$docker container rm broken-container\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 667
        }
      },
      {
        "header": "Use a read-only bind mount",
        "content": "For some development applications, the container needs to write into the bind mount, so changes are propagated back to the Docker host. At other times, the container only needs read access.\n\nThis example modifies the previous one, but mounts the directory as a read-only bind mount, by adding ro to the (empty by default) list of options, after the mount point within the container. Where multiple options are present, separate them by commas.\n\nThe --mount and -v examples have the same result.\n\nUse docker inspect devtest to verify that the bind mount was created correctly. Look for the Mounts section:\n\nStop and remove the container:",
        "code_examples": [
          "```\n\"Mounts\":[{\"Type\":\"bind\",\"Source\":\"/tmp/source/target\",\"Destination\":\"/app\",\"Mode\":\"ro\",\"RW\":false,\"Propagation\":\"rprivate\"}],\n```"
        ],
        "usage_examples": [
          "```\n$docker run -d\\-it \\--name devtest \\--mount type=bind,source=\"$(pwd)\"/target,target=/app,readonly \\nginx:latest\n```",
          "```\n$docker run -d\\-it \\--name devtest \\-v \"$(pwd)\"/target:/app:ro \\nginx:latest\n```",
          "```\n$docker container rm -fv devtest\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 636
        }
      },
      {
        "header": "Recursive mounts",
        "content": "When you bind mount a path that itself contains mounts, those submounts are also included in the bind mount by default. This behavior is configurable, using the bind-recursive option for --mount. This option is only supported with the --mount flag, not with -v or --volume.\n\nIf the bind mount is read-only, the Docker Engine makes a best-effort attempt at making the submounts read-only as well. This is referred to as recursive read-only mounts. Recursive read-only mounts require Linux kernel version 5.12 or later. If you're running an older kernel version, submounts are automatically mounted as read-write by default. Attempting to set submounts to be read-only on a kernel version earlier than 5.12, using the bind-recursive=readonly option, results in an error.\n\nSupported values for the bind-recursive option are:\n\nValue | Description\n--- | ---\nenabled (default) | Read-only mounts are made recursively read-only if kernel is v5.12 or later. Otherwise, submounts are read-write.\ndisabled | Submounts are ignored (not included in the bind mount).\nwritable | Submounts are read-write.\nreadonly | Submounts are read-only. Requires kernel v5.12 or later.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 1158
        }
      },
      {
        "header": "Configure bind propagation",
        "content": "Bind propagation defaults to rprivate for both bind mounts and volumes. It is only configurable for bind mounts, and only on Linux host machines. Bind propagation is an advanced topic and many users never need to configure it.\n\nBind propagation refers to whether or not mounts created within a given bind-mount can be propagated to replicas of that mount. Consider a mount point /mnt, which is also mounted on /tmp. The propagation settings control whether a mount on /tmp/a would also be available on /mnt/a. Each propagation setting has a recursive counterpoint. In the case of recursion, consider that /tmp/a is also mounted as /foo. The propagation settings control whether /mnt/a and/or /tmp/a would exist.\n\nMount propagation doesn't work with Docker Desktop.\n\nBefore you can set bind propagation on a mount point, the host filesystem needs to already support bind propagation.\n\nFor more information about bind propagation, see the Linux kernel documentation for shared subtree.\n\nThe following example mounts the target/ directory into the container twice, and the second mount sets both the ro option and the rslave bind propagation option.\n\nThe --mount and -v examples have the same result.\n\nNow if you create /app/foo/, /app2/foo/ also exists.\n\nPropagation setting | Description\n--- | ---\nshared | Sub-mounts of the original mount are exposed to replica mounts, and sub-mounts of replica mounts are also propagated to the original mount.\nslave | similar to a shared mount, but only in one direction. If the original mount exposes a sub-mount, the replica mount can see it. However, if the replica mount exposes a sub-mount, the original mount cannot see it.\nprivate | The mount is private. Sub-mounts within it are not exposed to replica mounts, and sub-mounts of replica mounts are not exposed to the original mount.\nrshared | The same as shared, but the propagation also extends to and from mount points nested within any of the original or replica mount points.\nrslave | The same as slave, but the propagation also extends to and from mount points nested within any of the original or replica mount points.\nrprivate | The default. The same as private, meaning that no mount points anywhere within the original or replica mount points propagate in either direction.\n\n[Admonition] Mount propagation doesn't work with Docker Desktop.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d\\-it \\--name devtest \\--mount type=bind,source=\"$(pwd)\"/target,target=/app \\--mount type=bind,source=\"$(pwd)\"/target,target=/app2,readonly,bind-propagation=rslave \\nginx:latest\n```",
          "```\n$docker run -d\\-it \\--name devtest \\-v \"$(pwd)\"/target:/app \\-v \"$(pwd)\"/target:/app2:ro,rslave \\nginx:latest\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": true,
          "paragraph_count": 8,
          "content_length": 2341
        }
      },
      {
        "header": "Configure the SELinux label",
        "content": "If you use SELinux, you can add the z or Z options to modify the SELinux label of the host file or directory being mounted into the container. This affects the file or directory on the host machine itself and can have consequences outside of the scope of Docker.\n\nUse extreme caution with these options. Bind-mounting a system directory such as /home or /usr with the Z option renders your host machine inoperable and you may need to relabel the host machine files by hand.\n\nWhen using bind mounts with services, SELinux labels (:Z and :z), as well as :ro are ignored. See moby/moby #32579 for details.\n\nThis example sets the z option to specify that multiple containers can share the bind mount's contents:\n\nIt is not possible to modify the SELinux label using the --mount flag.\n\n• The z option indicates that the bind mount content is shared among multiple containers.\n• The Z option indicates that the bind mount content is private and unshared.\n\n[Admonition] When using bind mounts with services, SELinux labels (:Z and :z), as well as :ro are ignored. See moby/moby #32579 for details.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d\\-it \\--name devtest \\-v \"$(pwd)\"/target:/app:z \\nginx:latest\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1090
        }
      },
      {
        "header": "Use a bind mount with Docker Compose",
        "content": "A single Docker Compose service with a bind mount looks like this:\n\nFor more information about using volumes of the bind type with Compose, see Compose reference on the volumes top-level element. and Compose reference on the volume attribute.",
        "code_examples": [
          "```\nservices:frontend:image:node:ltsvolumes:-type:bindsource:./statictarget:/opt/app/staticvolumes:myapp:\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 242
        }
      },
      {
        "header": "Next steps",
        "content": "• Learn about volumes.\n• Learn about tmpfs mounts.\n• Learn about storage drivers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 81
        }
      }
    ],
    "url": "https://docs.docker.com/storage/bind-mounts/",
    "doc_type": "docker",
    "total_sections": 15
  },
  {
    "title": "Networking overview",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "Container networking refers to the ability for containers to connect to and communicate with each other, and with non-Docker network services.\n\nContainers have networking enabled by default, and they can make outgoing connections. A container has no information about what kind of network it's attached to, or whether its network peers are also Docker containers. A container only sees a network interface with an IP address, a gateway, a routing table, DNS services, and other networking details.\n\nThis page describes networking from the point of view of the container, and the concepts around container networking.\n\nWhen Docker Engine on Linux starts for the first time, it has a single built-in network called the \"default bridge\" network. When you run a container without the --network option, it is connected to the default bridge.\n\nContainers attached to the default bridge have access to network services outside the Docker host. They use \"masquerading\" which means, if the Docker host has Internet access, no additional configuration is needed for the container to have Internet access.\n\nFor example, to run a container on the default bridge network, and have it ping an Internet host:\n\n• Get started",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --rm -ti busybox ping -c1 docker.comPING docker.com (23.185.0.4): 56 data bytes64 bytes from 23.185.0.4: seq=0 ttl=62 time=6.564 ms--- docker.com ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 6.564/6.564/6.564 ms\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1208
        }
      },
      {
        "header": "User-defined networks",
        "content": "With the default configuration, containers attached to the default bridge network have unrestricted network access to each other using container IP addresses. They cannot refer to each other by name.\n\nIt can be useful to separate groups of containers that should have full access to each other, but restricted access to containers in other groups.\n\nYou can create custom, user-defined networks, and connect groups of containers to the same network. Once connected to a user-defined network, containers can communicate with each other using container IP addresses or container names.\n\nThe following example creates a network using the bridge network driver and runs a container in that network:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker network create -d bridge my-net$docker run --network=my-net -it busybox\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 693
        }
      },
      {
        "header": "Drivers",
        "content": "Docker Engine has a number of network drivers, as well as the default \"bridge\". On Linux, the following built-in network drivers are available:\n\nMore information can be found in the network driver specific pages, including their configuration options and details about their functionality.\n\nNative Windows containers have a different set of drivers, see Windows container network drivers.\n\nDriver | Description\n--- | ---\nbridge | The default network driver.\nhost | Remove network isolation between the container and the Docker host.\nnone | Completely isolate a container from the host and other containers.\noverlay | Swarm Overlay networks connect multiple Docker daemons together.\nipvlan | Connect containers to external VLANs.\nmacvlan | Containers appear as devices on the host's network.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 790
        }
      },
      {
        "header": "Connecting to multiple networks",
        "content": "Connecting a container to a network can be compared to connecting an Ethernet cable to a physical host. Just as a host can be connected to multiple Ethernet networks, a container can be connected to multiple Docker networks.\n\nFor example, a frontend container may be connected to a bridge network with external access, and a --internal network to communicate with containers running backend services that do not need external network access.\n\nA container may also be connected to different types of network. For example, an ipvlan network to provide internet access, and a bridge network for access to local services.\n\nContainers can also share networking stacks, see Container networks.\n\nWhen sending packets, if the destination is an address in a directly connected network, packets are sent to that network. Otherwise, packets are sent to a default gateway for routing to their destination. In the example above, the ipvlan network's gateway must be the default gateway.\n\nThe default gateway is selected by Docker, and may change whenever a container's network connections change. To make Docker choose a specific default gateway when creating the container or connecting a new network, set a gateway priority. See option gw-priority for the docker run and docker network connect commands.\n\nThe default gw-priority is 0 and the gateway in the network with the highest priority is the default gateway. So, when a network should always be the default gateway, it is enough to set its gw-priority to 1.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --networkname=gwnet,gw-priority=1--network anet1 --name myctr myimage$docker network connect anet2 myctr\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1502
        }
      },
      {
        "header": "Published ports",
        "content": "When you create or run a container using docker create or docker run, all ports of containers on bridge networks are accessible from the Docker host and other containers connected to the same network. Ports are not accessible from outside the host or, with the default configuration, from containers in other networks.\n\nUse the --publish or -p flag to make a port available outside the host, and to containers in other bridge networks.\n\nFor more information about port mapping, including how to disable it and use direct routing to containers, see port publishing.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 564
        }
      },
      {
        "header": "IP address and hostname",
        "content": "When creating a network, IPv4 address allocation is enabled by default, it can be disabled using --ipv4=false. IPv6 address allocation can be enabled using --ipv6.\n\nBy default, the container gets an IP address for every Docker network it attaches to. A container receives an IP address out of the IP subnet of the network. The Docker daemon performs dynamic subnetting and IP address allocation for containers. Each network also has a default subnet mask and gateway.\n\nYou can connect a running container to multiple networks, either by passing the --network flag multiple times when creating the container, or using the docker network connect command for already running containers. In both cases, you can use the --ip or --ip6 flags to specify the container's IP address on that particular network.\n\nIn the same way, a container's hostname defaults to be the container's ID in Docker. You can override the hostname using --hostname. When connecting to an existing network using docker network connect, you can use the --alias flag to specify an additional network alias for the container on that network.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker network create --ipv6 --ipv4=falsev6net\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1106
        }
      },
      {
        "header": "Subnet allocation",
        "content": "Docker networks can use either explicitly configured subnets or automatically allocated ones from default pools.\n\nYou can specify exact subnets when creating a network:\n\nWhen no --subnet option is provided, Docker automatically selects a subnet from predefined \"default address pools\". These pools can be configured in /etc/docker/daemon.json. Docker's built-in default is equivalent to:\n\nWhen an IPv6 subnet is required and there are no IPv6 addresses in default-address-pools, Docker allocates subnets from a Unique Local Address (ULA) prefix. To use specific IPv6 subnets instead, add them to your default-address-pools. See Dynamic IPv6 subnet allocation for more information.\n\nDocker attempts to avoid address prefixes already in use on the host. However, you may need to customize default-address-pools to prevent routing conflicts in some network environments.\n\nThe default pools use large subnets, which limits the number of networks you can create. You can divide base subnets into smaller pools to support more networks.\n\nFor example, this configuration allows Docker to create 256 networks from 172.17.0.0/16. Docker will allocate subnets 172.17.0.0/24, 172.17.1.0/24, and so on, up to 172.17.255.0/24:\n\nYou can also request a subnet with a specific prefix length from the default pools by using unspecified addresses in the --subnet option:\n\nSupport for unspecified addresses in --subnet was introduced in Docker 29.0.0. If Docker is downgraded to an older version, networks created in this way will become unusable. They can be removed and re-created, or will function again if the daemon is restored to 29.0.0 or later.\n\n• base: The subnet that can be allocated from.\n• size: The prefix length used for each allocated subnet.\n\n[Admonition] Support for unspecified addresses in --subnet was introduced in Docker 29.0.0. If Docker is downgraded to an older version, networks created in this way will become unusable. They can be removed and re-created, or will function again if the daemon is restored to 29.0.0 or later.",
        "code_examples": [
          "```\n{\"default-address-pools\":[{\"base\":\"172.17.0.0/16\",\"size\":16},{\"base\":\"172.18.0.0/16\",\"size\":16},{\"base\":\"172.19.0.0/16\",\"size\":16},{\"base\":\"172.20.0.0/14\",\"size\":16},{\"base\":\"172.24.0.0/14\",\"size\":16},{\"base\":\"172.28.0.0/14\",\"size\":16},{\"base\":\"192.168.0.0/16\",\"size\":20}]}\n```",
          "```\n{\"default-address-pools\":[{\"base\":\"172.17.0.0/16\",\"size\":24}]}\n```"
        ],
        "usage_examples": [
          "```\n$docker network create --ipv6 --subnet 192.0.2.0/24 --subnet 2001:db8::/64 mynet\n```",
          "```\n$docker network create --ipv6 --subnet ::/56 --subnet 0.0.0.0/24 mynet6686a6746b17228f5052528113ddad0e6d68e2e3905d648e336b33409f2d3b64$docker network inspect mynet -f'{{json .IPAM.Config}}'|jq .[{\"Subnet\": \"172.19.0.0/24\",\"Gateway\": \"172.19.0.1\"},{\"Subnet\": \"fdd3:6f80:972c::/56\",\"Gateway\": \"fdd3:6f80:972c::1\"}]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2033
        }
      },
      {
        "header": "DNS services",
        "content": "Containers use the same DNS servers as the host by default, but you can override this with --dns.\n\nBy default, containers inherit the DNS settings as defined in the /etc/resolv.conf configuration file. Containers that attach to the default bridge network receive a copy of this file. Containers that attach to a custom network use Docker's embedded DNS server. The embedded DNS server forwards external DNS lookups to the DNS servers configured on the host.\n\nYou can configure DNS resolution on a per-container basis, using flags for the docker run or docker create command used to start the container. The following table describes the available docker run flags related to DNS configuration.\n\nFlag | Description\n--- | ---\n--dns | The IP address of a DNS server. To specify multiple DNS servers, use multiple --dns flags. DNS requests will be forwarded from the container's network namespace so, for example, --dns=127.0.0.1 refers to the container's own loopback address.\n--dns-search | A DNS search domain to search non-fully qualified hostnames. To specify multiple DNS search prefixes, use multiple --dns-search flags.\n--dns-opt | A key-value pair representing a DNS option and its value. See your operating system's documentation for resolv.conf for valid options.\n--hostname | The hostname a container uses for itself. Defaults to the container's ID if not specified.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 1374
        }
      },
      {
        "header": "Custom hosts",
        "content": "Your container will have lines in /etc/hosts which define the hostname of the container itself, as well as localhost and a few other common things. Custom hosts, defined in /etc/hosts on the host machine, aren't inherited by containers. To pass additional hosts into a container, refer to add entries to container hosts file in the docker run reference documentation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 367
        }
      },
      {
        "header": "Container networks",
        "content": "In addition to user-defined networks, you can attach a container to another container's networking stack directly, using the --network container:<name|id> flag format.\n\nThe following flags aren't supported for containers using the container: networking mode:\n\nThe following example runs a Redis container, with Redis binding to 127.0.0.1, then running the redis-cli command and connecting to the Redis server over 127.0.0.1.\n\n• --dns-search\n• --dns-option\n• --mac-address\n• --publish-all",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run -d --name redis redis --bind 127.0.0.1$docker run --rm -it --network container:redis redis redis-cli -h 127.0.0.1\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 487
        }
      }
    ],
    "url": "https://docs.docker.com/network/",
    "doc_type": "docker",
    "total_sections": 10
  },
  {
    "title": "Bridge network driver",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "A Docker bridge network has an IPv4 subnet and, optionally, an IPv6 subnet. Each container connected to the bridge network has a network interface with addresses in the network's subnets. By default, it:\n\nIn terms of Docker, a bridge network uses a software bridge which lets containers connected to the same bridge network communicate, while providing isolation from containers that aren't connected to that bridge network. By default, the Docker bridge driver automatically installs rules in the host machine so that containers connected to different bridge networks can only communicate with each other using published ports.\n\nBridge networks apply to containers running on the same Docker daemon host. For communication among containers running on different Docker daemon hosts, you can either manage routing at the OS level, or you can use an overlay network.\n\nWhen you start Docker, a default bridge network (also called bridge) is created automatically, and newly-started containers connect to it unless otherwise specified. You can also create user-defined custom bridge networks. User-defined bridge networks are superior to the default bridge network.\n\n• Get started\n\n• Allows unrestricted network access to containers in the network from the host, and from other containers connected to the same bridge network.\n• Blocks access from containers in other networks and from outside the Docker host.\n• Uses masquerading to give containers external network access. Devices on the host's external networks only see the IP address of the Docker host.\n• Supports port publishing, where network traffic is forwarded between container ports and ports on host IP addresses. The published ports can be accessed from outside the Docker host, on its IP addresses.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1760
        }
      },
      {
        "header": "Differences between user-defined bridges and the default bridge",
        "content": "User-defined bridges provide automatic DNS resolution between containers.\n\nContainers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias.\n\nImagine an application with a web front-end and a database back-end. If you call your containers web and db, the web container can connect to the db container at db, no matter which Docker host the application stack is running on.\n\nIf you run the same application stack on the default bridge network, you need to manually create links between the containers (using the legacy --link flag). These links need to be created in both directions, so you can see this gets complex with more than two containers which need to communicate. Alternatively, you can manipulate the /etc/hosts files within the containers, but this creates problems that are difficult to debug.\n\nUser-defined bridges provide better isolation.\n\nAll containers without a --network specified, are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate.\n\nUsing a user-defined network provides a scoped network in which only containers attached to that network are able to communicate.\n\nContainers can be attached and detached from user-defined networks on the fly.\n\nDuring a container's lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.\n\nEach user-defined network creates a configurable bridge.\n\nIf your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and iptables rules. In addition, configuring the default bridge network happens outside of Docker itself, and requires a restart of Docker.\n\nUser-defined bridge networks are created and configured using docker network create. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it.\n\nLinked containers on the default bridge network share environment variables.\n\nOriginally, the only way to share environment variables between two containers was to link them using the --link flag. This type of variable sharing isn't possible with user-defined networks. However, there are superior ways to share environment variables. A few ideas:\n\nMultiple containers can mount a file or directory containing the shared information, using a Docker volume.\n\nMultiple containers can be started together using docker-compose and the compose file can define the shared variables.\n\nYou can use swarm services instead of standalone containers, and take advantage of shared secrets and configs.\n\nContainers connected to the same user-defined bridge network effectively expose all ports to each other. For a port to be accessible to containers or non-Docker hosts on different networks, that port must be published using the -p or --publish flag.\n\n• User-defined bridges provide automatic DNS resolution between containers.Containers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias.Imagine an application with a web front-end and a database back-end. If you call your containers web and db, the web container can connect to the db container at db, no matter which Docker host the application stack is running on.If you run the same application stack on the default bridge network, you need to manually create links between the containers (using the legacy --link flag). These links need to be created in both directions, so you can see this gets complex with more than two containers which need to communicate. Alternatively, you can manipulate the /etc/hosts files within the containers, but this creates problems that are difficult to debug.\n• User-defined bridges provide better isolation.All containers without a --network specified, are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate.Using a user-defined network provides a scoped network in which only containers attached to that network are able to communicate.\n• Containers can be attached and detached from user-defined networks on the fly.During a container's lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.\n• Each user-defined network creates a configurable bridge.If your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and iptables rules. In addition, configuring the default bridge network happens outside of Docker itself, and requires a restart of Docker.User-defined bridge networks are created and configured using docker network create. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it.\n• Linked containers on the default bridge network share environment variables.Originally, the only way to share environment variables between two containers was to link them using the --link flag. This type of variable sharing isn't possible with user-defined networks. However, there are superior ways to share environment variables. A few ideas:Multiple containers can mount a file or directory containing the shared information, using a Docker volume.Multiple containers can be started together using docker-compose and the compose file can define the shared variables.You can use swarm services instead of standalone containers, and take advantage of shared secrets and configs.\n\n• Multiple containers can mount a file or directory containing the shared information, using a Docker volume.\n• Multiple containers can be started together using docker-compose and the compose file can define the shared variables.\n• You can use swarm services instead of standalone containers, and take advantage of shared secrets and configs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 6384
        }
      },
      {
        "header": "Options",
        "content": "The following table describes the driver-specific options that you can pass to --opt when creating a custom network using the bridge driver.\n\nSome of these options are also available as flags to the dockerd CLI, and you can use them to configure the default docker0 bridge when starting the Docker daemon. The following table shows which options have equivalent flags in the dockerd CLI.\n\nThe Docker daemon supports a --bridge flag, which you can use to define your own docker0 bridge. Use this option if you want to run multiple daemon instances on the same host. For details, see Run multiple daemons.\n\nOption | Default | Description\n--- | --- | ---\ncom.docker.network.bridge.name | Interface name to use when creating the Linux bridge.\ncom.docker.network.bridge.enable_ip_masquerade | true | Enable IP masquerading.\ncom.docker.network.host_ipv4com.docker.network.host_ipv6 | Address to use for source NAT. See Packet filtering and firewalls.\ncom.docker.network.bridge.gateway_mode_ipv4com.docker.network.bridge.gateway_mode_ipv6 | nat | Control external connectivity. See Packet filtering and firewalls.\ncom.docker.network.bridge.enable_icc | true | Enable or Disable inter-container connectivity.\ncom.docker.network.bridge.host_binding_ipv4 | all IPv4 and IPv6 addresses | Default IP when binding container ports.\ncom.docker.network.driver.mtu | 0 (no limit) | Set the containers network Maximum Transmission Unit (MTU).\ncom.docker.network.container_iface_prefix | eth | Set a custom prefix for container interfaces.\ncom.docker.network.bridge.inhibit_ipv4 | false | Prevent Docker from assigning an IP address to the bridge.\n\nOption | Flag\n--- | ---\ncom.docker.network.bridge.name | -\ncom.docker.network.bridge.enable_ip_masquerade | --ip-masq\ncom.docker.network.bridge.enable_icc | --icc\ncom.docker.network.bridge.host_binding_ipv4 | --ip\ncom.docker.network.driver.mtu | --mtu\ncom.docker.network.container_iface_prefix | -",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 1927
        }
      },
      {
        "header": "Default host binding address",
        "content": "When no host address is given in port publishing options like -p 80 or -p 8080:80, the default is to make the container's port 80 available on all host addresses, IPv4 and IPv6.\n\nThe bridge network driver option com.docker.network.bridge.host_binding_ipv4 can be used to modify the default address for published ports.\n\nDespite the option's name, it is possible to specify an IPv6 address.\n\nWhen the default binding address is an address assigned to a specific interface, the container's port will only be accessible via that address.\n\nSetting the default binding address to :: means published ports will only be available on the host's IPv6 addresses. However, setting it to 0.0.0.0 means it will be available on the host's IPv4 and IPv6 addresses.\n\nTo restrict a published port to IPv4 only, the address must be included in the container's publishing options. For example, -p 0.0.0.0:8080:80.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 894
        }
      },
      {
        "header": "Manage a user-defined bridge",
        "content": "Use the docker network create command to create a user-defined bridge network.\n\nYou can specify the subnet, the IP address range, the gateway, and other options. See the docker network create reference or the output of docker network create --help for details.\n\nUse the docker network rm command to remove a user-defined bridge network. If containers are currently connected to the network, disconnect them first.\n\nWhat's really happening?\n\nWhen you create or remove a user-defined bridge or connect or disconnect a container from a user-defined bridge, Docker uses tools specific to the operating system to manage the underlying network infrastructure (such as adding or removing bridge devices or configuring iptables rules on Linux). These details should be considered implementation details. Let Docker manage your user-defined networks for you.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker network create my-net\n```",
          "```\n$docker network rm my-net\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 849
        }
      },
      {
        "header": "Connect a container to a user-defined bridge",
        "content": "When you create a new container, you can specify one or more --network flags. This example connects an Nginx container to the my-net network. It also publishes port 80 in the container to port 8080 on the Docker host, so external clients can access that port. Any other container connected to the my-net network has access to all ports on the my-nginx container, and vice versa.\n\nTo connect a running container to an existing user-defined bridge, use the docker network connect command. The following command connects an already-running my-nginx container to an already-existing my-net network:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker create --name my-nginx\\--network my-net \\--publish 8080:80 \\nginx:latest\n```",
          "```\n$docker network connect my-net my-nginx\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 594
        }
      },
      {
        "header": "Disconnect a container from a user-defined bridge",
        "content": "To disconnect a running container from a user-defined bridge, use the docker network disconnect command. The following command disconnects the my-nginx container from the my-net network.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker network disconnect my-net my-nginx\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 186
        }
      },
      {
        "header": "Use IPv6 in a user-defined bridge network",
        "content": "When you create your network, you can specify the --ipv6 flag to enable IPv6.\n\nIf you do not provide a --subnet option, a Unique Local Address (ULA) prefix will be chosen automatically.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker network create --ipv6 --subnet 2001:db8:1234::/64 my-net\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 185
        }
      },
      {
        "header": "IPv6-only bridge networks",
        "content": "To skip IPv4 address configuration on the bridge and in its containers, create the network with option --ipv4=false, and enable IPv6 using --ipv6.\n\nIPv4 address configuration cannot be disabled in the default bridge network.",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker network create --ipv6 --ipv4=falsev6net\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 224
        }
      },
      {
        "header": "Use the default bridge network",
        "content": "The default bridge network is considered a legacy detail of Docker and is not recommended for production use. Configuring it is a manual operation, and it has technical shortcomings.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 182
        }
      },
      {
        "header": "Connect a container to the default bridge network",
        "content": "If you do not specify a network using the --network flag, and you do specify a network driver, your container is connected to the default bridge network by default. Containers connected to the default bridge network can communicate, but only by IP address, unless they're linked using the legacy --link flag.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 308
        }
      },
      {
        "header": "Configure the default bridge network",
        "content": "To configure the default bridge network, you specify options in daemon.json. Here is an example daemon.json with several options specified. Only specify the settings you need to customize.\n\n• The bridge's address is \"192.168.1.1/24\" (from bip).\n• The bridge network's subnet is \"192.168.1.0/24\" (from bip).\n• Container addresses will be allocated from \"192.168.1.0/25\" (from fixed-cidr).",
        "code_examples": [
          "```\n{\"bip\":\"192.168.1.1/24\",\"fixed-cidr\":\"192.168.1.0/25\",\"mtu\":1500,\"default-gateway\":\"192.168.1.254\",\"dns\":[\"10.20.1.2\",\"10.20.1.3\"]}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 387
        }
      },
      {
        "header": "Use IPv6 with the default bridge network",
        "content": "IPv6 can be enabled for the default bridge using the following options in daemon.json, or their command line equivalents.\n\nThese three options only affect the default bridge, they are not used by user-defined networks. The addresses in below are examples from the IPv6 documentation range.\n\nIf no bip6 is specified, fixed-cidr-v6 defines the subnet for the bridge network. If no bip6 or fixed-cidr-v6 is specified, a ULA prefix will be chosen.\n\nRestart Docker for changes to take effect.\n\n• Option ipv6 is required.\n• Option bip6 is optional, it specifies the address of the default bridge, which will be used as the default gateway by containers. It also specifies the subnet for the bridge network.\n• Option fixed-cidr-v6 is optional, it specifies the address range Docker may automatically allocate to containers.The prefix should normally be /64 or shorter.For experimentation on a local network, it is better to use a Unique Local Address (ULA) prefix (matching fd00::/8) than a Link Local prefix (matching fe80::/10).\n• Option default-gateway-v6 is optional. If unspecified, the default is the first address in the fixed-cidr-v6 subnet.\n\n• The prefix should normally be /64 or shorter.\n• For experimentation on a local network, it is better to use a Unique Local Address (ULA) prefix (matching fd00::/8) than a Link Local prefix (matching fe80::/10).",
        "code_examples": [
          "```\n{\"ipv6\":true,\"bip6\":\"2001:db8::1111/64\",\"fixed-cidr-v6\":\"2001:db8::/64\",\"default-gateway-v6\":\"2001:db8:abcd::89\"}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1356
        }
      },
      {
        "header": "Connection limit for bridge networks",
        "content": "Due to limitations set by the Linux kernel, bridge networks become unstable and inter-container communications may break when 1000 containers or more connect to a single network.\n\nFor more information about this limitation, see moby/moby#44973.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 244
        }
      },
      {
        "header": "Skip Bridge IP address configuration",
        "content": "The bridge is normally assigned the network's --gateway address, which is used as the default route from the bridge network to other networks.\n\nThe com.docker.network.bridge.inhibit_ipv4 option lets you create a network without the IPv4 gateway address being assigned to the bridge. This is useful if you want to configure the gateway IP address for the bridge manually. For instance if you add a physical interface to your bridge, and need it to have the gateway address.\n\nWith this configuration, north-south traffic (to and from the bridge network) won't work unless you've manually configured the gateway address on the bridge, or a device attached to it.\n\nThis option can only be used with user-defined bridge networks.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 724
        }
      },
      {
        "header": "Next steps",
        "content": "• Go through the standalone networking tutorial\n• Learn about networking from the container's point of view\n• Learn about overlay networks\n• Learn about Macvlan networks",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 169
        }
      }
    ],
    "url": "https://docs.docker.com/network/drivers/bridge/",
    "doc_type": "docker",
    "total_sections": 16
  },
  {
    "title": "Host network driver",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "If you use the host network mode for a container, that container's network stack isn't isolated from the Docker host (the container shares the host's networking namespace), and the container doesn't get its own IP-address allocated. For instance, if you run a container which binds to port 80 and you use host networking, the container's application is available on port 80 on the host's IP address.\n\nGiven that the container does not have its own IP-address when using host mode networking, port-mapping doesn't take effect, and the -p, --publish, -P, and --publish-all option are ignored, producing a warning instead:\n\nHost mode networking can be useful for the following use cases:\n\nThis is because it doesn't require network address translation (NAT), and no \"userland-proxy\" is created for each port.\n\nThe host networking driver is supported on Docker Engine (Linux only) and Docker Desktop version 4.34 and later.\n\nYou can also use a host network for a swarm service, by passing --network host to the docker service create command. In this case, control traffic (traffic related to managing the swarm and the service) is still sent across an overlay network, but the individual swarm service containers send data using the Docker daemon's host network and ports. This creates some extra limitations. For instance, if a service container binds to port 80, only one service container can run on a given swarm node.\n\n• Get started\n\n• To optimize performance\n• In situations where a container needs to handle a large range of ports\n\n[Admonition] Given that the container does not have its own IP-address when using host mode networking, port-mapping doesn't take effect, and the -p, --publish, -P, and --publish-all option are ignored, producing a warning instead:WARNING: Published ports are discarded when using host network mode",
        "code_examples": [
          "```\nWARNING: Published ports are discarded when using host network mode\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1833
        }
      },
      {
        "header": "Docker Desktop",
        "content": "Host networking is supported on Docker Desktop version 4.34 and later. To enable this feature:\n\nThis feature works in both directions. This means you can access a server that is running in a container from your host and you can access servers running on your host from any container that is started with host networking enabled. TCP as well as UDP are supported as communication protocols.\n\n• Sign in to your Docker account in Docker Desktop.\n• Navigate to Settings.\n• Under the Resources tab, select Network.\n• Check the Enable host networking option.\n• Select Apply and restart.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 580
        }
      },
      {
        "header": "Examples",
        "content": "The following command starts netcat in a container that listens on port 8000:\n\nPort 8000 will then be available on the host and you can connect to it with the following command from another terminal:\n\nWhat you type in here will then appear on the terminal where the container is running.\n\nTo access a service running on the host from the container, you can start a container with host networking enabled with this command:\n\nIf you then want to access a service on your host from the container (in this example a web server running on port 80), you can do it like this:",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker run --rm -it --net=host nicolaka/netshoot nc -lkv 0.0.0.08000\n```",
          "```\n$nc localhost8000\n```",
          "```\n$docker run --rm -it --net=host nicolaka/netshoot\n```",
          "```\n$nc localhost80\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 568
        }
      },
      {
        "header": "Limitations",
        "content": "• Processes inside the container cannot bind to the IP addresses of the host because the container has no direct access to the interfaces of the host.\n• The host network feature of Docker Desktop works on layer 4. This means that unlike with Docker on Linux, network protocols that operate below TCP or UDP are not supported.\n• This feature doesn't work with Enhanced Container Isolation enabled, since isolating your containers from the host and allowing them access to the host network contradict each other.\n• Only Linux containers are supported. Host networking does not work with Windows containers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 604
        }
      },
      {
        "header": "Next steps",
        "content": "• Go through the host networking tutorial\n• Learn about networking from the container's point of view\n• Learn about bridge networks\n• Learn about overlay networks\n• Learn about Macvlan networks",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 193
        }
      }
    ],
    "url": "https://docs.docker.com/network/drivers/host/",
    "doc_type": "docker",
    "total_sections": 5
  },
  {
    "title": "Security for developers",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "Docker helps you protect your local environments, infrastructure, and networks with its developer-level security features.\n\nUse tools like two-factor authentication (2FA), personal access tokens, and Docker Scout to manage access and detect vulnerabilities early in your workflow. You can also integrate secrets securely into your development stack using Docker Compose, or enhance your software supply security with Docker Hardened Images.\n\nExplore the following sections to learn more.\n\n• Get started",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 502
        }
      },
      {
        "header": "Set up two-factor authentication",
        "content": "Add an extra layer of authentication to your Docker account.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 60
        }
      },
      {
        "header": "Manage access tokens",
        "content": "Create personal access tokens as an alternative to your password.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 65
        }
      },
      {
        "header": "Static vulnerability scanning",
        "content": "Automatically run a point-in-time scan on your Docker images for vulnerabilities.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 81
        }
      },
      {
        "header": "Docker Engine security",
        "content": "Understand how to keep Docker Engine secure.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 44
        }
      },
      {
        "header": "Secrets in Docker Compose",
        "content": "Learn how to use secrets in Docker Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 43
        }
      },
      {
        "header": "Security FAQs",
        "content": "Explore common security FAQs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 29
        }
      },
      {
        "header": "Security best practices",
        "content": "Understand the steps you can take to improve the security of your container.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 76
        }
      },
      {
        "header": "Suppress CVEs with VEX",
        "content": "Learn how to suppress non-applicable or fixed vulnerabilities found in your images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 83
        }
      },
      {
        "header": "Docker Hardened Images",
        "content": "Learn how to use Docker Hardened Images to enhance your software supply security.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 81
        }
      }
    ],
    "url": "https://docs.docker.com/security/",
    "doc_type": "docker",
    "total_sections": 10
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#introduction",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#getting-started",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#hello-world",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#webapps-with-docker",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#docker-images",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#dockerfile",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#docker-compose",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#multi-container-environments",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#docker-networking",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#docker-volumes",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#docker-swarm",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#deployment",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "A Docker Tutorial for Beginners",
    "summary": "Learn to build and deploy your distributed applications easily to the cloud with Docker Written and developed by Prakhar Srivastav   Star Introduction What is Docker? Wikipedia defines Docker as an open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux. Wow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easil",
    "sections": [
      {
        "header": "",
        "content": "Learn to build and deploy your distributed applications easily to the cloud with Docker\n\nWritten and developed by Prakhar Srivastav",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 131
        }
      },
      {
        "header": "What is Docker?",
        "content": "Wikipedia defines Docker as\n\nan open-source project that automates the deployment of software applications inside containers by providing an additional layer of abstraction and automation of OS-level virtualization on Linux.\n\nWow! That's a mouthful. In simpler words, Docker is a tool that allows developers, sys-admins etc. to easily deploy their applications in a sandbox (called containers) to run on the host operating system i.e. Linux. The key benefit of Docker is that it allows users to package an application with all of its dependencies into a standardized unit for software development. Unlike virtual machines, containers do not have high overhead and hence enable more efficient usage of the underlying system and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 737
        }
      },
      {
        "header": "What are containers?",
        "content": "The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.\n\nVMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.\n\nContainers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 759
        }
      },
      {
        "header": "Why use containers?",
        "content": "Containers offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. This decoupling allows container-based applications to be deployed easily and consistently, regardless of whether the target environment is a private data center, the public cloud, or even a developer’s personal laptop. This gives developers the ability to create predictable environments that are isolated from the rest of the applications and can be run anywhere.\n\nFrom an operations standpoint, apart from portability containers also give more granular control over resources giving your infrastructure improved efficiency which can result in better utilization of your compute resources.\n\nGoogle Trends for Docker\n\nDue to these benefits, containers (& Docker) have seen widespread adoption. Companies like Google, Facebook, Netflix and Salesforce leverage containers to make large engineering teams more productive and to improve utilization of compute resources. In fact, Google credited containers for eliminating the need for an entire data center.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1091
        }
      },
      {
        "header": "What will this tutorial teach me?",
        "content": "This tutorial aims to be the one-stop shop for getting your hands dirty with Docker. Apart from demystifying the Docker landscape, it'll give you hands-on experience with building and deploying your own webapps on the Cloud. We'll be using Amazon Web Services to deploy a static website, and two dynamic webapps on EC2 using Elastic Beanstalk and Elastic Container Service. Even if you have no prior experience with deployments, this tutorial should be all you need to get started.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 481
        }
      },
      {
        "header": "Getting Started",
        "content": "This document contains a series of several sections, each of which explains a particular aspect of Docker. In each section, we will be typing commands (or writing code). All the code used in the tutorial is available in the Github repo.\n\nNote: This tutorial uses version 18.05.0-ce of Docker. If you find any part of the tutorial incompatible with a future version, please raise an issue. Thanks!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Prerequisites",
        "content": "There are no specific skills needed for this tutorial beyond a basic comfort with the command line and using a text editor. This tutorial uses git clone to clone the repository locally. If you don't have Git installed on your system, either install it or remember to manually download the zip files from Github. Prior experience in developing web applications will be helpful but is not required. As we proceed further along the tutorial, we'll make use of a few cloud services. If you're interested in following along, please create an account on each of these websites:\n\n• Amazon Web Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 594
        }
      },
      {
        "header": "Setting up your computer",
        "content": "Getting all the tooling setup on your computer can be a daunting task, but thankfully as Docker has become stable, getting Docker up and running on your favorite OS has become very easy.\n\nUntil a few releases ago, running Docker on OSX and Windows was quite a hassle. Lately however, Docker has invested significantly into improving the on-boarding experience for its users on these OSes, thus running Docker now is a cakewalk. The getting started guide on Docker has detailed instructions for setting up Docker on Mac, Linux and Windows.\n\nOnce you are done installing Docker, test your Docker installation by running the following:",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run hello-world\n\nHello from Docker.\nThis message shows that your installation appears to be working correctly.\n...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 632
        }
      },
      {
        "header": "Playing with Busybox",
        "content": "Now that we have everything setup, it's time to get our hands dirty. In this section, we are going to run a Busybox container on our system and get a taste of the docker run command.\n\nTo get started, let's run the following in our terminal:\n\nNote: Depending on how you've installed docker on your system, you might see a permission denied error after running the above command. If you're on a Mac, make sure the Docker engine is running. If you're on Linux, then prefix your docker commands with sudo. Alternatively, you can create a docker group to get rid of this issue.\n\nThe pull command fetches the busybox image from the Docker registry and saves it to our system. You can use the docker images command to see a list of all images on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker pull busybox\n```",
          "```\n$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbusybox                 latest              c51f86c28340        4 weeks ago         1.109 MB\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 751
        }
      },
      {
        "header": "Docker Run",
        "content": "Great! Let's now run a Docker container based on this image. To do that we are going to use the almighty docker run command.\n\nWait, nothing happened! Is that a bug? Well, no. Behind the scenes, a lot of stuff happened. When you call run, the Docker client finds the image (busybox in this case), loads up the container and then runs a command in that container. When we run docker run busybox, we didn't provide a command, so the container booted up, ran an empty command and then exited. Well, yeah - kind of a bummer. Let's try something more exciting.\n\nNice - finally we see some output. In this case, the Docker client dutifully ran the echo command in our busybox container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Ok, now it's time to see the docker ps command. The docker ps command shows you all containers that are currently running.\n\nSince no containers are running, we see a blank line. Let's try a more useful variant: docker ps -a\n\nSo what we see above is a list of all containers that we ran. Do notice that the STATUS column shows that these containers exited a few minutes ago.\n\nYou're probably wondering if there is a way to run more than just one command in a container. Let's try that now:\n\nRunning the run command with the -it flags attaches us to an interactive tty in the container. Now we can run as many commands in the container as we want. Take some time to run your favorite commands.\n\nDanger Zone: If you're feeling particularly adventurous you can try rm -rf bin in the container. Make sure you run this command in the container and not in your laptop/desktop. Doing this will make any other commands like ls, uptime not work. Once everything stops working, you can exit the container (type exit and press Enter) and then start it up again with the docker run -it busybox sh command. Since Docker creates a new container every time, everything should start working again.\n\nThat concludes a whirlwind tour of the mighty docker run command, which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about run, use docker run --help to see a list of all flags it supports. As we proceed further, we'll see a few more variants of docker run.\n\nBefore we move ahead though, let's quickly talk about deleting containers. We saw above that we can still see remnants of the container even after we've exited by running docker ps -a. Throughout this tutorial, you'll run docker run multiple times and leaving stray containers will eat up disk space. Hence, as a rule of thumb, I clean up containers once I'm done with them. To do that, you can run the docker rm command. Just copy the container IDs from above and paste them alongside the command.\n\nOn deletion, you should see the IDs echoed back to you. If you have a bunch of containers to delete in one go, copy-pasting IDs can be tedious. In that case, you can simply run -\n\nThis command deletes all containers that have a status of exited. In case you're wondering, the -q flag, only returns the numeric IDs and -f filters output based on conditions provided. One last thing that'll be useful is the --rm flag that can be passed to docker run which automatically deletes the container once it's exited from. For one off docker runs, --rm flag is very useful.\n\nIn later versions of Docker, the docker container prune command can be used to achieve the same effect.\n\nLastly, you can also delete images that you no longer need by running docker rmi.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run busybox\n$\n```",
          "```\n$ docker run busyboxecho\"hello from busybox\"hello from busybox\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\n$ docker ps-aCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n305297d7a235        busybox\"uptime\"11 minutes ago      Exited (0) 11 minutes ago                       distracted_goldstine\nff0a5c3750b9        busybox\"sh\"12 minutes ago      Exited (0) 12 minutes ago                       elated_ramanujan\n14e5bd11d164        hello-world\"/hello\"2 minutes ago       Exited (0) 2 minutes ago                        thirsty_euclid\n```",
          "```\n$ docker run -it busybox sh\n/# lsbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/# uptime05:45:21 up  5:58,  0 users,  load average: 0.00, 0.01, 0.04\n```",
          "```\n$ docker rm 305297d7a235 ff0a5c3750b9\n305297d7a235\nff0a5c3750b9\n```",
          "```\n$ docker rm $(docker ps-a-q-fstatus=exited)\n```",
          "```\n$ docker container prune\nWARNING! This will remove all stopped containers.\nAre you sure you want tocontinue? [y/N] y\nDeleted Containers:\n4a7f7eebae0f63178aff7eb0aa39f0627a203ab2df258c1a00b456cf20063\nf98f9c2aa1eaf727e4ec9c0283bcaa4762fbdba7f26191f26c97f64090360\n\nTotal reclaimed space: 212 B\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3663
        }
      },
      {
        "header": "Terminology",
        "content": "In the last section, we used a lot of Docker-specific jargon which might be confusing to some. So before we go further, let me clarify some terminology that is used frequently in the Docker ecosystem.\n\n• Images - The blueprints of our application which form the basis of containers. In the demo above, we used the docker pull command to download the busybox image.\n• Containers - Created from Docker images and run the actual application. We create a container using docker run which we did using the busybox image that we downloaded. A list of running containers can be seen using the docker ps command.\n• Docker Daemon - The background service running on the host that manages building, running and distributing Docker containers. The daemon is the process that runs in the operating system which clients talk to.\n• Docker Client - The command line tool that allows the user to interact with the daemon. More generally, there can be other forms of clients too - such as Kitematic which provide a GUI to the users.\n• Docker Hub - A registry of Docker images. You can think of the registry as a directory of all available Docker images. If required, one can host their own Docker registries and can use them for pulling images.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1227
        }
      },
      {
        "header": "Webapps with Docker",
        "content": "Great! So we have now looked at docker run, played with a Docker container and also got a hang of some terminology. Armed with all this knowledge, we are now ready to get to the real-stuff, i.e. deploying web applications with Docker!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 234
        }
      },
      {
        "header": "Static Sites",
        "content": "Let's start by taking baby-steps. The first thing we're going to look at is how we can run a dead-simple static website. We're going to pull a Docker image from Docker Hub, run the container and see how easy it is to run a webserver.\n\nLet's begin. The image that we are going to use is a single-page website that I've already created for the purpose of this demo and hosted on the registry - prakhar1989/static-site. We can download and run the image directly in one go using docker run. As noted above, the --rm flag automatically removes the container when it exits and the -it flag specifies an interactive terminal which makes it easier to kill the container with Ctrl+C (on windows).\n\nSince the image doesn't exist locally, the client will first fetch the image from the registry and then run the image. If all goes well, you should see a Nginx is running... message in your terminal. Okay now that the server is running, how to see the website? What port is it running on? And more importantly, how do we access the container directly from our host machine? Hit Ctrl+C to stop the container.\n\nWell, in this case, the client is not exposing any ports so we need to re-run the docker run command to publish ports. While we're at it, we should also find a way so that our terminal is not attached to the running container. This way, you can happily close your terminal and keep the container running. This is called detached mode.\n\nIn the above command, -d will detach our terminal, -P will publish all exposed ports to random ports and finally --name corresponds to a name we want to give. Now we can see the ports by running the docker port [CONTAINER] command\n\nYou can open http://localhost:32769 in your browser.\n\nNote: If you're using docker-toolbox, then you might need to use docker-machine ip default to get the IP.\n\nYou can also specify a custom port to which the client will forward connections to the container.\n\nTo stop a detached container, run docker stop by giving the container ID. In this case, we can use the name static-site we used to start the container.\n\nI'm sure you agree that was super simple. To deploy this on a real server you would just need to install Docker, and run the above Docker command. Now that you've seen how to run a webserver inside a Docker image, you must be wondering - how do I create my own Docker image? This is the question we'll be exploring in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker run --rm -it prakhar1989/static-site\n```",
          "```\n$ docker run-d-P --name static-site prakhar1989/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n```",
          "```\n$ docker port static-site\n80/tcp -> 0.0.0.0:32769\n443/tcp -> 0.0.0.0:32768\n```",
          "```\n$ docker run -p 8888:80 prakhar1989/static-site\nNginx is running...\n```",
          "```\n$ docker stop static-site\nstatic-site\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2415
        }
      },
      {
        "header": "Docker Images",
        "content": "We've looked at images before, but in this section we'll dive deeper into what Docker images are and build our own image! Lastly, we'll also use that image to run our application locally and finally deploy on AWS to share it with our friends! Excited? Great! Let's get started.\n\nDocker images are the basis of containers. In the previous example, we pulled the Busybox image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally, use the docker images command.\n\nThe above gives a list of images that I've pulled from the registry, along with ones that I've created myself (we'll shortly see how). The TAG refers to a particular snapshot of the image and the IMAGE ID is the corresponding unique identifier for that image.\n\nFor simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. If you don't provide a specific version number, the client defaults to latest. For example, you can pull a specific version of ubuntu image\n\nTo get a new Docker image you can either get it from a registry (such as the Docker Hub) or create your own. There are tens of thousands of images available on Docker Hub. You can also search for images directly from the command line using docker search.\n\nAn important distinction to be aware of when it comes to images is the difference between base and child images.\n\nBase images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n\nChild images are images that build on base images and add additional functionality.\n\nThen there are official and user images, which can be both base and child images.\n\nOfficial images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n\nUser images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.\n\n• Base images are images that have no parent image, usually images with an OS like ubuntu, busybox or debian.\n• Child images are images that build on base images and add additional functionality.\n\n• Official images are images that are officially maintained and supported by the folks at Docker. These are typically one word long. In the list of images above, the python, ubuntu, busybox and hello-world images are official images.\n• User images are images created and shared by users like you and me. They build on base images and add additional functionality. Typically, these are formatted as user/image-name.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker images\nREPOSITORY                      TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nprakhar1989/catnip              latest              c7ffb5626a50        2 hours ago         697.9 MB\nprakhar1989/static-site         latest              b270625a1631        21 hours ago        133.9 MB\npython                          3-onbuild           cf4002b2c383        5 days ago          688.8 MB\nmartin/docker-cleanup-volumes   latest              b42990daaca2        7 weeks ago         22.14 MB\nubuntu                          latest              e9ae3c220b23        7 weeks ago         187.9 MB\nbusybox                         latest              c51f86c28340        9 weeks ago         1.109 MB\nhello-world                     latest              0a6ba66e537a        11 weeks ago        960 B\n```",
          "```\n$ docker pull ubuntu:18.04\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2754
        }
      },
      {
        "header": "Our First Image",
        "content": "Now that we have a better understanding of images, it's time to create our own. Our goal in this section will be to create an image that sandboxes a simple Flask application. For the purposes of this workshop, I've already created a fun little Flask app that displays a random cat .gif every time it is loaded - because you know, who doesn't like cats? If you haven't already, please go ahead and clone the repository locally like so -\n\nThis should be cloned on the machine where you are running the docker commands and not inside a docker container.\n\nThe next step now is to create an image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, the base image we're going to use will be Python 3.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/docker-curriculum.git\n$cddocker-curriculum/flask-app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 769
        }
      },
      {
        "header": "Dockerfile",
        "content": "A Dockerfile is a simple text file that contains a list of commands that the Docker client calls while creating an image. It's a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own dockerfiles.\n\nThe application directory does contain a Dockerfile but since we're doing this for the first time, we'll create one from scratch. To start, create a new blank file in our favorite text-editor and save it in the same folder as the flask app by the name of Dockerfile.\n\nWe start with specifying our base image. Use the FROM keyword to do that -\n\nThe next step usually is to write the commands of copying the files and installing the dependencies. First, we set a working directory and then copy all the files for our app.\n\nNow, that we have the files, we can install the dependencies.\n\nThe next thing we need to specify is the port number that needs to be exposed. Since our flask app is running on port 5000, that's what we'll indicate.\n\nThe last step is to write the command for running the application, which is simply - python ./app.py. We use the CMD command to do that -\n\nThe primary purpose of CMD is to tell the container which command it should run when it is started. With that, our Dockerfile is now ready. This is how it looks -\n\nNow that we have our Dockerfile, we can build our image. The docker build command does the heavy-lifting of creating a Docker image from a Dockerfile.\n\nThe section below shows you the output of running the same. Before you run the command yourself (don't forget the period), make sure to replace my username with yours. This username should be the same one you created when you registered on Docker hub. If you haven't done that yet, please go ahead and create an account. The docker build command is quite simple - it takes an optional tag name with -t and a location of the directory containing the Dockerfile.\n\nIf you don't have the python:3.8 image, the client will first pull the image and then create your image. Hence, your output from running the command will look different from mine. If everything went well, your image should be ready! Run docker images and see if your image shows.\n\nThe last step in this section is to run the image and see if it actually works (replacing my username with yours).\n\nThe command we just ran used port 5000 for the server inside the container and exposed this externally on port 8888. Head over to the URL with port 8888, where your app should be live.\n\nCongratulations! You have successfully created your first docker image.",
        "code_examples": [
          "```\n# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .\n```",
          "```\n# install dependenciesRUNpip install --no-cache-dir -r requirements.txt\n```",
          "```\nEXPOSE5000\n```"
        ],
        "usage_examples": [
          "```\nFROMpython:3.8\n```",
          "```\nCMD[\"python\",\"./app.py\"]\n```",
          "```\nFROMpython:3.8# set a directory for the appWORKDIR/usr/src/app# copy all the files to the containerCOPY. .# install dependenciesRUNpip install --no-cache-dir -r requirements.txt# define the port number the container should exposeEXPOSE5000# run the commandCMD[\"python\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/catnip .\nSending build context to Docker daemon 8.704 kB\nStep 1 : FROM python:3.8# Executing 3 build triggers...Step 1 : COPY requirements.txt /usr/src/app/\n ---> Using cache\nStep 1 : RUN pip install --no-cache-dir -r requirements.txt\n ---> Using cache\nStep 1 : COPY . /usr/src/app\n ---> 1d61f639ef9e\nRemoving intermediate container 4de6ddf5528c\nStep 2 : EXPOSE 5000\n ---> Runningin12cfcf6d67ee\n ---> f423c2f179d1\nRemoving intermediate container 12cfcf6d67ee\nStep 3 : CMD python ./app.py\n ---> Runninginf01401a5ace9\n ---> 13e87ed1fbc2\nRemoving intermediate container f01401a5ace9\nSuccessfully built 13e87ed1fbc2\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2692
        }
      },
      {
        "header": "Docker on AWS",
        "content": "What good is an application that can't be shared with friends, right? So in this section we are going to see how we can deploy our awesome application to the cloud so that we can share it with our friends! We're going to use AWS Elastic Beanstalk to get our application up and running in a few clicks. We'll also see how easy it is to make our application scalable and manageable with Beanstalk!\n\nThe first thing that we need to do before we deploy our app to AWS is to publish our image on a registry which can be accessed by AWS. There are many different Docker registries you can use (you can even host your own). For now, let's use Docker Hub to publish the image.\n\nIf this is the first time you are pushing an image, the client will ask you to login. Provide the same credentials that you used for logging into Docker Hub.\n\nTo publish, just type the below command remembering to replace the name of the image tag above with yours. It is important to have the format of yourusername/image_name so that the client knows where to publish.\n\nOnce that is done, you can view your image on Docker Hub. For example, here's the web page for my image.\n\nNote: One thing that I'd like to clarify before we go ahead is that it is not imperative to host your image on a public registry (or any registry) in order to deploy to AWS. In case you're writing code for the next million-dollar unicorn startup you can totally skip this step. The reason why we're pushing our images publicly is that it makes deployment super simple by skipping a few intermediate configuration steps.\n\nNow that your image is online, anyone who has docker installed can play with your app by typing just a single command.\n\nIf you've pulled your hair out in setting up local dev environments / sharing application configuration in the past, you very well know how awesome this sounds. That's why Docker is so cool!\n\nAWS Elastic Beanstalk (EB) is a PaaS (Platform as a Service) offered by AWS. If you've used Heroku, Google App Engine etc. you'll feel right at home. As a developer, you just tell EB how to run your app and it takes care of the rest - including scaling, monitoring and even updates. In April 2014, EB added support for running single-container Docker deployments which is what we'll use to deploy our app. Although EB has a very intuitive CLI, it does require some setup, and to keep things simple we'll use the web UI to launch our application.\n\nTo follow along, you need a functioning AWS account. If you haven't already, please go ahead and do that now - you will need to enter your credit card information. But don't worry, it's free and anything we do in this tutorial will also be free! Let's get started.\n\nWhile we wait, let's quickly see what the Dockerrun.aws.json file contains. This file is basically an AWS specific file that tells EB details about our application and docker configuration.\n\nThe file should be pretty self-explanatory, but you can always reference the official documentation for more information. We provide the name of the image that EB should use along with a port that the container should open.\n\nHopefully by now, our instance should be ready. Head over to the EB page and you should see a green tick indicating that your app is alive and kicking.\n\nGo ahead and open the URL in your browser and you should see the application in all its glory. Feel free to email / IM / snapchat this link to your friends and family so that they can enjoy a few cat gifs, too.\n\nOnce you done basking in the glory of your app, remember to terminate the environment so that you don't end up getting charged for extra resources.\n\nCongratulations! You have deployed your first Docker application! That might seem like a lot of steps, but with the command-line tool for EB you can almost mimic the functionality of Heroku in a few keystrokes! Hopefully, you agree that Docker takes away a lot of the pains of building and deploying applications in the cloud. I would encourage you to read the AWS documentation on single-container Docker environments to get an idea of what features exist.\n\nIn the next (and final) part of the tutorial, we'll up the ante a bit and deploy an application that mimics the real-world more closely; an app with a persistent back-end storage tier. Let's get straight to it!\n\n• Login to your AWS console.\n• Click on Elastic Beanstalk. It will be in the compute section on the top left. Alternatively, you can access the Elastic Beanstalk console.\n\n• Click on \"Create New Application\" in the top right\n• Give your app a memorable (but unique) name and provide an (optional) description\n• In the New Environment screen, create a new environment and choose the Web Server Environment.\n• Fill in the environment information by choosing a domain. This URL is what you'll share with your friends so make sure it's easy to remember.\n• Under base configuration section. Choose Docker from the predefined platform.\n\n• Now we need to upload our application code. But since our application is packaged in a Docker container, we just need to tell EB about our container. Open the Dockerrun.aws.json file located in the flask-app folder and edit the Name of the image to your image's name. Don't worry, I'll explain the contents of the file shortly. When you are done, click on the radio button for \"Upload your Code\", choose this file, and click on \"Upload\".\n• Now click on \"Create environment\". The final screen that you see will have a few spinners indicating that your environment is being set up. It typically takes around 5 minutes for the first-time setup.",
        "code_examples": [
          "```\n{\"AWSEBDockerrunVersion\":\"1\",\"Image\": {\"Name\":\"prakhar1989/catnip\",\"Update\":\"true\"},\"Ports\": [\n    {\"ContainerPort\":5000,\"HostPort\":8000}\n  ],\"Logging\":\"/var/log/nginx\"}\n```"
        ],
        "usage_examples": [
          "```\n$ docker login\nLogininwith your Docker ID to push and pull images from Docker Hub. If youdonot have a Docker ID, head over to https://hub.docker.com to create one.\nUsername: yourusername\nPassword:\nWARNING! Your password will be stored unencryptedin/Users/yourusername/.docker/config.json\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/credential-store\n\nLogin Succeeded\n```",
          "```\n$ docker push yourusername/catnip\n```",
          "```\n$ docker run -p 8888:5000 yourusername/catnip\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 5569
        }
      },
      {
        "header": "Multi-container Environments",
        "content": "In the last section, we saw how easy and fun it is to run applications with Docker. We started with a simple static website and then tried a Flask app. Both of which we could run locally and in the cloud with just a few commands. One thing both these apps had in common was that they were running in a single container.\n\nThose of you who have experience running services in production know that usually apps nowadays are not that simple. There's almost always a database (or any other kind of persistent storage) involved. Systems such as Redis and Memcached have become de rigueur of most web application architectures. Hence, in this section we are going to spend some time learning how to Dockerize applications which rely on different services to run.\n\nIn particular, we are going to see how we can run and manage multi-container docker environments. Why multi-container you might ask? Well, one of the key points of Docker is the way it provides isolation. The idea of bundling a process with its dependencies in a sandbox (called containers) is what makes this so powerful.\n\nJust like it's a good strategy to decouple your application tiers, it is wise to keep containers for each of the services separate. Each tier is likely to have different resource needs and those needs might grow at different rates. By separating the tiers into different containers, we can compose each tier using the most appropriate instance type based on different resource needs. This also plays in very well with the whole microservices movement which is one of the main reasons why Docker (or any other container technology) is at the forefront of modern microservices architectures.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1670
        }
      },
      {
        "header": "SF Food Trucks",
        "content": "The app that we're going to Dockerize is called SF Food Trucks. My goal in building this app was to have something that is useful (in that it resembles a real-world application), relies on at least one service, but is not too complex for the purpose of this tutorial. This is what I came up with.\n\nThe app's backend is written in Python (Flask) and for search it uses Elasticsearch. Like everything else in this tutorial, the entire source is available on Github. We'll use this as our candidate application for learning out how to build, run and deploy a multi-container environment.\n\nFirst up, let's clone the repository locally.\n\nThe flask-app folder contains the Python application, while the utils folder has some utilities to load the data into Elasticsearch. The directory also contains some YAML files and a Dockerfile, all of which we'll see in greater detail as we progress through this tutorial. If you are curious, feel free to take a look at the files.\n\nNow that you're excited (hopefully), let's think of how we can Dockerize the app. We can see that the application consists of a Flask backend server and an Elasticsearch service. A natural way to split this app would be to have two containers - one running the Flask process and another running the Elasticsearch (ES) process. That way if our app becomes popular, we can scale it by adding more containers depending on where the bottleneck lies.\n\nGreat, so we need two containers. That shouldn't be hard right? We've already built our own Flask container in the previous section. And for Elasticsearch, let's see if we can find something on the hub.\n\nQuite unsurprisingly, there exists an officially supported image for Elasticsearch. To get ES running, we can simply use docker run and have a single-node ES container running locally within no time.\n\nNote: Elastic, the company behind Elasticsearch, maintains its own registry for Elastic products. It's recommended to use the images from that registry if you plan to use Elasticsearch.\n\nLet's first pull the image\n\nand then run it in development mode by specifying ports and setting an environment variable that configures the Elasticsearch cluster to run as a single-node.\n\nNote: If your container runs into memory issues, you might need to tweak some JVM flags to limit its memory consumption.\n\nAs seen above, we use --name es to give our container a name which makes it easy to use in subsequent commands. Once the container is started, we can see the logs by running docker container logs with the container name (or ID) to inspect the logs. You should see logs similar to below if Elasticsearch started successfully.\n\nNote: Elasticsearch takes a few seconds to start so you might need to wait before you see initialized in the logs.\n\nNow, lets try to see if can send a request to the Elasticsearch container. We use the 9200 port to send a cURL request to the container.\n\nSweet! It's looking good! While we are at it, let's get our Flask container running too. But before we get to that, we need a Dockerfile. In the last section, we used python:3.8 image as our base image. This time, however, apart from installing Python dependencies via pip, we want our application to also generate our minified Javascript file for production. For this, we'll require Nodejs. Since we need a custom build step, we'll start from the ubuntu base image to build our Dockerfile from scratch.\n\nNote: if you find that an existing image doesn't cater to your needs, feel free to start from another base image and tweak it yourself. For most of the images on Docker Hub, you should be able to find the corresponding Dockerfile on Github. Reading through existing Dockerfiles is one of the best ways to learn how to roll your own.\n\nOur Dockerfile for the flask app looks like below -\n\nQuite a few new things here so let's quickly go over this file. We start off with the Ubuntu LTS base image and use the package manager apt-get to install the dependencies namely - Python and Node. The yqq flag is used to suppress output and assumes \"Yes\" to all prompts.\n\nWe then use the ADD command to copy our application into a new volume in the container - /opt/flask-app. This is where our code will reside. We also set this as our working directory, so that the following commands will be run in the context of this location. Now that our system-wide dependencies are installed, we get around to installing app-specific ones. First off we tackle Node by installing the packages from npm and running the build command as defined in our package.json file. We finish the file off by installing the Python packages, exposing the port and defining the CMD to run as we did in the last section.\n\nFinally, we can go ahead, build the image and run the container (replace yourusername with your username below).\n\nIn the first run, this will take some time as the Docker client will download the ubuntu image, run all the commands and prepare your image. Re-running docker build after any subsequent changes you make to the application code will almost be instantaneous. Now let's try running our app.\n\nOops! Our flask app was unable to run since it was unable to connect to Elasticsearch. How do we tell one container about the other container and get them to talk to each other? The answer lies in the next section.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ tree -L 2\n.\n├── Dockerfile\n├── README.md\n├── aws-compose.yml\n├── docker-compose.yml\n├── flask-app\n│   ├── app.py\n│   ├── package-lock.json\n│   ├── package.json\n│   ├── requirements.txt\n│   ├── static\n│   ├── templates\n│   └── webpack.config.js\n├── setup-aws-ecs.sh\n├── setup-docker.sh\n├── shot.png\n└── utils\n    ├── generate_geojson.py\n    └── trucks.geojson\n```",
          "```\n$ docker search elasticsearch\nNAME                              DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\nelasticsearch                     Elasticsearch is a powerful opensourcese...   697       [OK]\nitzg/elasticsearch                Provides an easily configurable Elasticsea...   17                   [OK]\ntutum/elasticsearch               Elasticsearch image - listensinport 9200.     15                   [OK]\nbarnybug/elasticsearch            Latest Elasticsearch 1.7.2 and previous re...   15                   [OK]\ndigitalwonderland/elasticsearch   Latest Elasticsearch with Marvel & Kibana       12                   [OK]\nmonsantoco/elasticsearch          ElasticSearch Docker image                      9                    [OK]\n```",
          "```\n$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n```",
          "```\n$ docker run-d--name es -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\n```",
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"2 minutes ago       Up 2 minutes        0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ docker container logs es\n[2018-07-29T05:49:09,304][INFO ][o.e.n.Node               ] [] initializing ...\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] using [1] data paths, mounts [[/ (overlay)]], net usable_space [54.1gb], net total_space [62.7gb], types [overlay]\n[2018-07-29T05:49:09,385][INFO ][o.e.e.NodeEnvironment    ] [L1VMyzt] heap size [990.7mb], compressed ordinary object pointers [true]\n[2018-07-29T05:49:11,979][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-security]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-sql]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-upgrade]\n[2018-07-29T05:49:11,980][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded module [x-pack-watcher]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-geoip]\n[2018-07-29T05:49:11,981][INFO ][o.e.p.PluginsService     ] [L1VMyzt] loaded plugin [ingest-user-agent]\n[2018-07-29T05:49:17,659][INFO ][o.e.d.DiscoveryModule    ] [L1VMyzt] using discoverytype[single-node]\n[2018-07-29T05:49:18,962][INFO ][o.e.n.Node               ] [L1VMyzt] initialized\n[2018-07-29T05:49:18,963][INFO ][o.e.n.Node               ] [L1VMyzt] starting ...\n[2018-07-29T05:49:19,218][INFO ][o.e.t.TransportService   ] [L1VMyzt] publish_address {172.17.0.2:9300}, bound_addresses {0.0.0.0:9300}\n[2018-07-29T05:49:19,302][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [L1VMyzt] publish_address {172.17.0.2:9200}, bound_addresses {0.0.0.0:9200}\n[2018-07-29T05:49:19,303][INFO ][o.e.n.Node               ] [L1VMyzt] started\n[2018-07-29T05:49:19,439][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [L1VMyzt] Failed to clear cacheforrealms [[]]\n[2018-07-29T05:49:19,542][INFO ][o.e.g.GatewayService     ] [L1VMyzt] recovered [0] indices into cluster_state\n```",
          "```\n$ curl 0.0.0.0:9200\n{\"name\":\"ijJDAOm\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"a_nSV3XmTCqpzYYzb-LhNw\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\n```",
          "```\n# start from baseFROMubuntu:18.04MAINTAINERPrakhar Srivastav <prakhar@prakhar.me># install system-wide deps for python and nodeRUNapt-get -yqq updateRUNapt-get -yqq install python3-pip python3-dev curl gnupgRUNcurl-sL https://deb.nodesource.com/setup_10.x | bashRUNapt-get install -yq nodejs# copy our application codeADDflask-app /opt/flask-appWORKDIR/opt/flask-app# fetch app specific depsRUNnpm installRUNnpm run buildRUNpip3 install -r requirements.txt# expose portEXPOSE5000# start appCMD[\"python3\",\"./app.py\"]\n```",
          "```\n$ docker build -t yourusername/foodtrucks-web .\n```",
          "```\n$ docker run -P --rm yourusername/foodtrucks-web\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nUnable to connect to ES. Retyingin5 secs...\nOut of retries. Bailing out...\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 5302
        }
      },
      {
        "header": "Docker Network",
        "content": "Before we talk about the features Docker provides especially to deal with such scenarios, let's see if we can figure out a way to get around the problem. Hopefully, this should give you an appreciation for the specific feature that we are going to study.\n\nOkay, so let's run docker container ls (which is same as docker ps) and see what we have.\n\nSo we have one ES container running on 0.0.0.0:9200 port which we can directly access. If we can tell our Flask app to connect to this URL, it should be able to connect and talk to ES, right? Let's dig into our Python code and see how the connection details are defined.\n\nTo make this work, we need to tell the Flask container that the ES container is running on 0.0.0.0 host (the port by default is 9200) and that should make it work, right? Unfortunately, that is not correct since the IP 0.0.0.0 is the IP to access ES container from the host machine i.e. from my Mac. Another container will not be able to access this on the same IP address. Okay if not that IP, then which IP address should the ES container be accessible by? I'm glad you asked this question.\n\nNow is a good time to start our exploration of networking in Docker. When docker is installed, it creates three networks automatically.\n\nThe bridge network is the network in which containers are run by default. So that means that when I ran the ES container, it was running in this bridge network. To validate this, let's inspect the network.\n\nYou can see that our container 277451c15ec1 is listed under the Containers section in the output. What we also see is the IP address this container has been allotted - 172.17.0.2. Is this the IP address that we're looking for? Let's find out by running our flask container and trying to access this IP.\n\nThis should be fairly straightforward to you by now. We start the container in the interactive mode with the bash process. The --rm is a convenient flag for running one off commands since the container gets cleaned up when its work is done. We try a curl but we need to install it first. Once we do that, we see that we can indeed talk to ES on 172.17.0.2:9200. Awesome!\n\nAlthough we have figured out a way to make the containers talk to each other, there are still two problems with this approach -\n\nHow do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n\nSince the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?\n\nThe good news that Docker has a great answer to our questions. It allows us to define our own networks while keeping them isolated using the docker network command.\n\nLet's first go ahead and create our own network.\n\nThe network create command creates a new bridge network, which is what we need at the moment. In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. There are other kinds of networks that you can create, and you are encouraged to read about them in the official docs.\n\nNow that we have a network, we can launch our containers inside this network using the --net flag. Let's do that - but first, in order to launch a new container with the same name, we will stop and remove our ES container that is running in the bridge (default) network.\n\nAs you can see, our es container is now running inside the foodtrucks-net bridge network. Now let's inspect what happens when we launch in our foodtrucks-net network.\n\nWohoo! That works! On user-defined networks like foodtrucks-net, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Great! Let's launch our Flask container for real now -\n\nHead over to http://0.0.0.0:5000 and see your glorious app live! Although that might have seemed like a lot of work, we actually just typed 4 commands to go from zero to running. I've collated the commands in a bash script.\n\nNow imagine you are distributing your app to a friend, or running on a server that has docker installed. You can get a whole app running with just one command!\n\nAnd that's it! If you ask me, I find this to be an extremely awesome, and a powerful way of sharing and running your applications!\n\n• How do we tell the Flask container that es hostname stands for 172.17.0.2 or some other IP since the IP can change?\n• Since the bridge network is shared by every container by default, this method is not secure. How do we isolate our network?",
        "code_examples": [
          "```\nes = Elasticsearch(host='es')\n```",
          "```\n#!/bin/bash# build the flask containerdocker build -t yourusername/foodtrucks-web .# create the networkdocker network create foodtrucks-net# start the ES containerdocker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2# start the flask app containerdocker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                                            NAMES\n277451c15ec1        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago      Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker network inspect bridge\n[\n    {\"Name\":\"bridge\",\"Id\":\"c2c695315b3aaf8fc30530bb3c6b8f6692cedd5cc7579663f0550dfdd21c9a26\",\"Created\":\"2018-07-28T20:32:39.405687265Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.17.0.0/16\",\"Gateway\":\"172.17.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"277451c15ec183dd939e80298ea4bcf55050328a39b04124b387d668e3ed3943\": {\"Name\":\"es\",\"EndpointID\":\"5c417a2fc6b13d8ec97b76bbd54aaf3ee2d48f328c3f7279ee335174fbb4d6bb\",\"MacAddress\":\"02:42:ac:11:00:02\",\"IPv4Address\":\"172.17.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {\"com.docker.network.bridge.default_bridge\":\"true\",\"com.docker.network.bridge.enable_icc\":\"true\",\"com.docker.network.bridge.enable_ip_masquerade\":\"true\",\"com.docker.network.bridge.host_binding_ipv4\":\"0.0.0.0\",\"com.docker.network.bridge.name\":\"docker0\",\"com.docker.network.driver.mtu\":\"1500\"},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm yourusername/foodtrucks-web bash\nroot@35180ccc206a:/opt/flask-app# curl 172.17.0.2:9200{\"name\":\"Jane Foster\",\"cluster_name\":\"elasticsearch\",\"version\": {\"number\":\"2.1.1\",\"build_hash\":\"40e2c53a6b6c2972b3d13846e450e66f4375bd71\",\"build_timestamp\":\"2015-12-15T13:05:55Z\",\"build_snapshot\":false,\"lucene_version\":\"5.3.1\"},\"tagline\":\"You Know, for Search\"}\nroot@35180ccc206a:/opt/flask-app# exit\n```",
          "```\n$ docker network create foodtrucks-net\n0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\n\n$ docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nc2c695315b3a        bridge              bridgelocal0815b2a3bb7a        foodtrucks-net      bridgelocala875bec5d6fd        host                hostlocalead0e804a67b        none                nulllocal\n```",
          "```\n$ docker container stop es\nes\n\n$ docker container rm es\nes\n\n$ docker run-d--name es --net foodtrucks-net -p 9200:9200 -p 9300:9300-e\"discovery.type=single-node\"docker.elastic.co/elasticsearch/elasticsearch:6.3.2\n13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\n\n$ docker network inspect foodtrucks-net\n[\n    {\"Name\":\"foodtrucks-net\",\"Id\":\"0815b2a3bb7a6608e850d05553cc0bda98187c4528d94621438f31d97a6fea3c\",\"Created\":\"2018-07-30T00:01:29.1500984Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": {},\"Config\": [\n                {\"Subnet\":\"172.18.0.0/16\",\"Gateway\":\"172.18.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":false,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"13d6415f73c8d88bddb1f236f584b63dbaf2c3051f09863a3f1ba219edba3673\": {\"Name\":\"es\",\"EndpointID\":\"29ba2d33f9713e57eb6b38db41d656e4ee2c53e4a2f7cf636bdca0ec59cd3aa7\",\"MacAddress\":\"02:42:ac:12:00:02\",\"IPv4Address\":\"172.18.0.2/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {}\n    }\n]\n```",
          "```\n$ docker run -it --rm --net foodtrucks-net yourusername/foodtrucks-web bash\nroot@9d2722cf282c:/opt/flask-app# curl es:9200{\"name\":\"wWALl9M\",\"cluster_name\":\"docker-cluster\",\"cluster_uuid\":\"BA36XuOiRPaghPNBLBHleQ\",\"version\": {\"number\":\"6.3.2\",\"build_flavor\":\"default\",\"build_type\":\"tar\",\"build_hash\":\"053779d\",\"build_date\":\"2018-07-20T05:20:23.451332Z\",\"build_snapshot\":false,\"lucene_version\":\"7.3.1\",\"minimum_wire_compatibility_version\":\"5.6.0\",\"minimum_index_compatibility_version\":\"5.0.0\"},\"tagline\":\"You Know, for Search\"}\nroot@53af252b771a:/opt/flask-app# lsapp.py  node_modules  package.json  requirements.txt  static  templates  webpack.config.js\nroot@53af252b771a:/opt/flask-app# python3 app.pyIndex not found...\nLoading datainelasticsearch ...\nTotal trucks loaded:  733\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nroot@53af252b771a:/opt/flask-app# exit\n```",
          "```\n$ docker run-d--net foodtrucks-net -p 5000:5000 --name foodtrucks-web yourusername/foodtrucks-web\n852fc74de2954bb72471b858dce64d764181dca0cf7693fed201d76da33df794\n\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                                            NAMES\n852fc74de295        yourusername/foodtrucks-web\"python3 ./app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp                           foodtrucks-web\n13d6415f73c8        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"17 minutes ago       Up 17 minutes       0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp   es\n\n$ curl -I 0.0.0.0:5000\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 3697\nServer: Werkzeug/0.11.2 Python/2.7.6\nDate: Sun, 10 Jan 2016 23:58:53 GMT\n```",
          "```\n$ gitclonehttps://github.com/prakhar1989/FoodTrucks\n$cdFoodTrucks\n$ ./setup-docker.sh\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 4812
        }
      },
      {
        "header": "Docker Compose",
        "content": "Till now we've spent all our time exploring the Docker client. In the Docker ecosystem, however, there are a bunch of other open-source tools which play very nicely with Docker. A few of them are -\n\nIn this section, we are going to look at one of these tools, Docker Compose, and see how it can make dealing with multi-container apps easier.\n\nThe background story of Docker Compose is quite interesting. Roughly around January 2014, a company called OrchardUp launched a tool called Fig. The idea behind Fig was to make isolated development environments work with Docker. The project was very well received on Hacker News - I oddly remember reading about it but didn't quite get the hang of it.\n\nThe first comment on the forum actually does a good job of explaining what Fig is all about.\n\nSo really at this point, that's what Docker is about: running processes. Now Docker offers a quite rich API to run the processes: shared volumes (directories) between containers (i.e. running images), forward port from the host to the container, display logs, and so on. But that's it: Docker as of now, remains at the process level.\n\nWhile it provides options to orchestrate multiple containers to create a single \"app\", it doesn't address the management of such group of containers as a single entity. And that's where tools such as Fig come in: talking about a group of containers as a single entity. Think \"run an app\" (i.e. \"run an orchestrated cluster of containers\") instead of \"run a container\".\n\nIt turns out that a lot of people using docker agree with this sentiment. Slowly and steadily as Fig became popular, Docker Inc. took notice, acquired the company and re-branded Fig as Docker Compose.\n\nSo what is Compose used for? Compose is a tool that is used for defining and running multi-container Docker apps in an easy way. It provides a configuration file called docker-compose.yml that can be used to bring up an application and the suite of services it depends on with just one command. Compose works in all environments: production, staging, development, testing, as well as CI workflows, although Compose is ideal for development and testing environments.\n\nLet's see if we can create a docker-compose.yml file for our SF-Foodtrucks app and evaluate whether Docker Compose lives up to its promise.\n\nThe first step, however, is to install Docker Compose. If you're running Windows or Mac, Docker Compose is already installed as it comes in the Docker Toolbox. Linux users can easily get their hands on Docker Compose by following the instructions on the docs. Since Compose is written in Python, you can also simply do pip install docker-compose. Test your installation with -\n\nNow that we have it installed, we can jump on the next step i.e. the Docker Compose file docker-compose.yml. The syntax for YAML is quite simple and the repo already contains the docker-compose file that we'll be using.\n\nLet me breakdown what the file above means. At the parent level, we define the names of our services - es and web. The image parameter is always required, and for each service that we want Docker to run, we can add additional parameters. For es, we just refer to the elasticsearch image available on Elastic registry. For our Flask app, we refer to the image that we built at the beginning of this section.\n\nOther parameters such as command and ports provide more information about the container. The volumes parameter specifies a mount point in our web container where the code will reside. This is purely optional and is useful if you need access to logs, etc. We'll later see how this can be useful during development. Refer to the online reference to learn more about the parameters this file supports. We also add volumes for the es container so that the data we load persists between restarts. We also specify depends_on, which tells docker to start the es container before web. You can read more about it on docker compose docs.\n\nNote: You must be inside the directory with the docker-compose.yml file in order to execute most Compose commands.\n\nGreat! Now the file is ready, let's see docker-compose in action. But before we start, we need to make sure the ports and names are free. So if you have the Flask and ES containers running, lets turn them off.\n\nNow we can run docker-compose. Navigate to the food trucks directory and run docker-compose up.\n\nHead over to the IP to see your app live. That was amazing wasn't it? Just a few lines of configuration and we have two Docker containers running successfully in unison. Let's stop the services and re-run in detached mode.\n\nUnsurprisingly, we can see both the containers running successfully. Where do the names come from? Those were created automatically by Compose. But does Compose also create the network automatically? Good question! Let's find out.\n\nFirst off, let us stop the services from running. We can always bring them back up in just one command. Data volumes will persist, so it’s possible to start the cluster again with the same data using docker-compose up. To destroy the cluster and the data volumes, just type docker-compose down -v.\n\nWhile we're are at it, we'll also remove the foodtrucks network that we created last time.\n\nGreat! Now that we have a clean slate, let's re-run our services and see if Compose does its magic.\n\nSo far, so good. Time to see if any networks were created.\n\nYou can see that compose went ahead and created a new network called foodtrucks_default and attached both the new services in that network so that each of these are discoverable to the other. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by them at a hostname identical to the container name.\n\n• Docker Machine - Create Docker hosts on your computer, on cloud providers, and inside your own data center\n• Docker Compose - A tool for defining and running multi-container Docker applications.\n• Docker Swarm - A native clustering solution for Docker\n• Kubernetes - Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ docker-compose --version\ndocker-compose version 1.21.2, build a133471\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:image:yourusername/foodtrucks-webcommand:python3 app.pydepends_on:-esports:-5000:5000volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker stop es foodtrucks-web\nes\nfoodtrucks-web\n\n$ docker rm es foodtrucks-web\nes\nfoodtrucks-web\n```",
          "```\n$ docker-compose up\nCreating network\"foodtrucks_default\"with the default driver\nCreating foodtrucks_es_1\nCreating foodtrucks_web_1\nAttaching to foodtrucks_es_1, foodtrucks_web_1\nes_1  | [2016-01-11 03:43:50,300][INFO ][node                     ] [Comet] version[2.1.1], pid[1], build[40e2c53/2015-12-15T13:05:55Z]\nes_1  | [2016-01-11 03:43:50,307][INFO ][node                     ] [Comet] initializing ...\nes_1  | [2016-01-11 03:43:50,366][INFO ][plugins                  ] [Comet] loaded [], sites []\nes_1  | [2016-01-11 03:43:50,421][INFO ][env                      ] [Comet] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/sda1)]], net usable_space [16gb], net total_space [18.1gb], spins? [possibly], types [ext4]\nes_1  | [2016-01-11 03:43:52,626][INFO ][node                     ] [Comet] initialized\nes_1  | [2016-01-11 03:43:52,632][INFO ][node                     ] [Comet] starting ...\nes_1  | [2016-01-11 03:43:52,703][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:52,704][INFO ][transport                ] [Comet] publish_address {172.17.0.2:9300}, bound_addresses {[::]:9300}\nes_1  | [2016-01-11 03:43:52,721][INFO ][discovery                ] [Comet] elasticsearch/cEk4s7pdQ-evRc9MqS2wqw\nes_1  | [2016-01-11 03:43:55,785][INFO ][cluster.service          ] [Comet] new_master {Comet}{cEk4s7pdQ-evRc9MqS2wqw}{172.17.0.2}{172.17.0.2:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)\nes_1  | [2016-01-11 03:43:55,818][WARN ][common.network           ] [Comet] publish address: {0.0.0.0} is a wildcard address, falling back to first non-loopback: {172.17.0.2}\nes_1  | [2016-01-11 03:43:55,819][INFO ][http                     ] [Comet] publish_address {172.17.0.2:9200}, bound_addresses {[::]:9200}\nes_1  | [2016-01-11 03:43:55,819][INFO ][node                     ] [Comet] started\nes_1  | [2016-01-11 03:43:55,826][INFO ][gateway                  ] [Comet] recovered [0] indices into cluster_state\nes_1  | [2016-01-11 03:44:01,825][INFO ][cluster.metadata         ] [Comet] [sfdata] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [truck]\nes_1  | [2016-01-11 03:44:02,373][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,510][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,593][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:02,708][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nes_1  | [2016-01-11 03:44:03,047][INFO ][cluster.metadata         ] [Comet] [sfdata] update_mapping [truck]\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n```",
          "```\nweb_1 |  * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nKilling foodtrucks_web_1 ...doneKilling foodtrucks_es_1 ...done$ docker-compose up-dCreating es               ...doneCreating foodtrucks_web_1 ...done$ docker-compose ps\n      Name                    Command               State                Ports\n--------------------------------------------------------------------------------------------\nes                 /usr/local/bin/docker-entr ...   Up      0.0.0.0:9200->9200/tcp, 9300/tcp\nfoodtrucks_web_1   python3 app.py                   Up      0.0.0.0:5000->5000/tcp\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n```",
          "```\n$ docker network rm foodtrucks-net\n$ docker network ls\nNETWORK ID          NAME                 DRIVER              SCOPE\nc2c695315b3a        bridge               bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker-compose up-dRecreating foodtrucks_es_1\nRecreating foodtrucks_web_1\n\n$ docker container ls\nCONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES\nf50bb33a3242        yourusername/foodtrucks-web\"python3 app.py\"14 seconds ago      Up 13 seconds       0.0.0.0:5000->5000/tcp   foodtrucks_web_1\ne299ceeb4caa        elasticsearch\"/docker-entrypoint.s\"14 seconds ago      Up 14 seconds       9200/tcp, 9300/tcp       foodtrucks_es_1\n```",
          "```\n$ docker network ls\nNETWORK ID          NAME                 DRIVER\nc2c695315b3a        bridge               bridgelocalf3b80f381ed3        foodtrucks_default   bridgelocala875bec5d6fd        host                 hostlocalead0e804a67b        none                 nulllocal\n```",
          "```\n$ docker ps\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED              STATUS              PORTS                              NAMES\n8c6bb7e818ec        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"About a minute ago   Up About a minute   0.0.0.0:9200->9200/tcp, 9300/tcp   es\n7640cec7feb7        yourusername/foodtrucks-web\"python3 app.py\"About a minute ago   Up About a minute   0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n\n$ docker network inspect foodtrucks_default\n[\n    {\"Name\":\"foodtrucks_default\",\"Id\":\"f3b80f381ed3e03b3d5e605e42c4a576e32d38ba24399e963d7dad848b3b4fe7\",\"Created\":\"2018-07-30T03:36:06.0384826Z\",\"Scope\":\"local\",\"Driver\":\"bridge\",\"EnableIPv6\":false,\"IPAM\": {\"Driver\":\"default\",\"Options\": null,\"Config\": [\n                {\"Subnet\":\"172.19.0.0/16\",\"Gateway\":\"172.19.0.1\"}\n            ]\n        },\"Internal\":false,\"Attachable\":true,\"Ingress\":false,\"ConfigFrom\": {\"Network\":\"\"},\"ConfigOnly\":false,\"Containers\": {\"7640cec7feb7f5615eaac376271a93fb8bab2ce54c7257256bf16716e05c65a5\": {\"Name\":\"foodtrucks_web_1\",\"EndpointID\":\"b1aa3e735402abafea3edfbba605eb4617f81d94f1b5f8fcc566a874660a0266\",\"MacAddress\":\"02:42:ac:13:00:02\",\"IPv4Address\":\"172.19.0.2/16\",\"IPv6Address\":\"\"},\"8c6bb7e818ec1f88c37f375c18f00beb030b31f4b10aee5a0952aad753314b57\": {\"Name\":\"es\",\"EndpointID\":\"649b3567d38e5e6f03fa6c004a4302508c14a5f2ac086ee6dcf13ddef936de7b\",\"MacAddress\":\"02:42:ac:13:00:03\",\"IPv4Address\":\"172.19.0.3/16\",\"IPv6Address\":\"\"}\n        },\"Options\": {},\"Labels\": {\"com.docker.compose.network\":\"default\",\"com.docker.compose.project\":\"foodtrucks\",\"com.docker.compose.version\":\"1.21.2\"}\n    }\n]\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 23,
          "content_length": 6138
        }
      },
      {
        "header": "Development Workflow",
        "content": "Before we jump to the next section, there's one last thing I wanted to cover about docker-compose. As stated earlier, docker-compose is really great for development and testing. So let's see how we can configure compose to make our lives easier during development.\n\nThroughout this tutorial, we've worked with readymade docker images. While we've built images from scratch, we haven't touched any application code yet and mostly restricted ourselves to editing Dockerfiles and YAML configurations. One thing that you must be wondering is how does the workflow look during development? Is one supposed to keep creating Docker images for every change, then publish it and then run it to see if the changes work as expected? I'm sure that sounds super tedious. There has to be a better way. In this section, that's what we're going to explore.\n\nLet's see how we can make a change in the Foodtrucks app we just ran. Make sure you have the app running,\n\nNow let's see if we can change this app to display a Hello world! message when a request is made to /hello route. Currently, the app responds with a 404.\n\nWhy does this happen? Since ours is a Flask app, we can see app.py (link) for answers. In Flask, routes are defined with @app.route syntax. In the file, you'll see that we only have three routes defined - /,/debugand/search. The/route renders the main app, thedebugroute is used to return some debug information and finallysearch is used by the app to query elasticsearch.\n\nGiven that context, how would we add a new route for hello? You guessed it! Let's open flask-app/app.py in our favorite editor and make the following change\n\nNow let's try making a request again\n\nOh no! That didn't work! What did we do wrong? While we did make the change in app.py, the file resides in our machine (or the host machine), but since Docker is running our containers based off the yourusername/foodtrucks-web image, it doesn't know about this change. To validate this, lets try the following -\n\nWhat we're trying to do here is to validate that our changes are not in the app.py that's running in the container. We do this by running the command docker-compose run, which is similar to its cousin docker run but takes additional arguments for the service (which is web in our case). As soon as we run bash, the shell opens in /opt/flask-app as specified in our Dockerfile. From the grep command we can see that our changes are not in the file.\n\nLets see how we can fix it. First off, we need to tell docker compose to not use the image and instead use the files locally. We'll also set debug mode to true so that Flask knows to reload the server when app.py changes. Replace the web portion of the docker-compose.yml file like so:\n\nWith that change (diff), let's stop and start the containers.\n\nAs a final step, lets make the change in app.py by adding a new route. Now we try to curl\n\nWohoo! We get a valid response! Try playing around by making more changes in the app.\n\nThat concludes our tour of Docker Compose. With Docker Compose, you can also pause your services, run a one-off command on a container and even scale the number of containers. I also recommend you checkout a few other use-cases of Docker compose. Hopefully, I was able to show you how easy it is to manage multi-container environments with Compose. In the final section, we are going to deploy our app to AWS!",
        "code_examples": [
          "```\n@app.route('/')defindex():returnrender_template(\"index.html\")# add a new hello route@app.route('/hello')defhello():return\"hello world!\"\n```"
        ],
        "usage_examples": [
          "```\n$ docker container ls\nCONTAINER ID        IMAGE                                                 COMMAND                  CREATED             STATUS              PORTS                              NAMES\n5450ebedd03c        yourusername/foodtrucks-web\"python3 app.py\"9 seconds ago       Up 6 seconds        0.0.0.0:5000->5000/tcp             foodtrucks_web_1\n05d408b25dfe        docker.elastic.co/elasticsearch/elasticsearch:6.3.2\"/usr/local/bin/dock…\"10 hours ago        Up 10 hours         0.0.0.0:9200->9200/tcp, 9300/tcp   es\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$ curl 0.0.0.0:5000/debug\n{\"msg\":\"yellow open sfdata Ibkx7WYjSt-g8NZXOEtTMg 5 1 618 0 1.3mb 1.3mb\\n\",\"status\":\"success\"}\n```",
          "```\n$ curl -I 0.0.0.0:5000/hello\nHTTP/1.0 404 NOT FOUND\nContent-Type: text/html\nContent-Length: 233\nServer: Werkzeug/0.11.2 Python/2.7.15rc1\nDate: Mon, 30 Jul 2018 15:34:38 GMT\n```",
          "```\n$docker-compose run web bash\nStarting es ... done\nroot@581e351c82b0:/opt/flask-app# lsapp.py        package-lock.json  requirements.txt  templates\nnode_modules  package.json       static            webpack.config.js\nroot@581e351c82b0:/opt/flask-app# grep hello app.pyroot@581e351c82b0:/opt/flask-app# exit\n```",
          "```\nversion:\"3\"services:es:image:docker.elastic.co/elasticsearch/elasticsearch:6.3.2container_name:esenvironment:-discovery.type=single-nodeports:-9200:9200volumes:- esdata1:/usr/share/elasticsearch/dataweb:build:.# replaced image with buildcommand:python3 app.pyenvironment:-DEBUG=True# set an env var for flaskdepends_on:-esports:-\"5000:5000\"volumes:-./flask-app:/opt/flask-appvolumes:esdata1:driver:local\n```",
          "```\n$ docker-compose down -v\nStopping foodtrucks_web_1 ...doneStopping es               ...doneRemoving foodtrucks_web_1 ...doneRemoving es               ...doneRemoving network foodtrucks_default\nRemoving volume foodtrucks_esdata1\n\n$ docker-compose up-dCreating network\"foodtrucks_default\"with the default driver\nCreating volume\"foodtrucks_esdata1\"withlocaldriver\nCreating es ...doneCreating foodtrucks_web_1 ...done\n```",
          "```\n$ curl 0.0.0.0:5000/hello\nhello world\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3373
        }
      },
      {
        "header": "AWS Elastic Container Service",
        "content": "In the last section we used docker-compose to run our app locally with a single command: docker-compose up. Now that we have a functioning app we want to share this with the world, get some users, make tons of money and buy a big house in Miami. Executing the last three are beyond the scope of the tutorial, so we'll spend our time instead on figuring out how we can deploy our multi-container apps on the cloud with AWS.\n\nIf you've read this far you are pretty much convinced that Docker is a pretty cool technology. And you are not alone. Seeing the meteoric rise of Docker, almost all Cloud vendors started working on adding support for deploying Docker apps on their platform. As of today, you can deploy containers on Google Cloud Platform, AWS, Azure and many others. We already got a primer on deploying single container apps with Elastic Beanstalk and in this section we are going to look at Elastic Container Service (or ECS) by AWS.\n\nAWS ECS is a scalable and super flexible container management service that supports Docker containers. It allows you to operate a Docker cluster on top of EC2 instances via an easy-to-use API. Where Beanstalk came with reasonable defaults, ECS allows you to completely tune your environment as per your needs. This makes ECS, in my opinion, quite complex to get started with.\n\nLuckily for us, ECS has a friendly CLI tool that understands Docker Compose files and automatically provisions the cluster on ECS! Since we already have a functioning docker-compose.yml it should not take a lot of effort in getting up and running on AWS. So let's get started!\n\nThe first step is to install the CLI. Instructions to install the CLI on both Mac and Linux are explained very clearly in the official docs. Go ahead, install the CLI and when you are done, verify the install by running\n\nNext, we'll be working on configuring the CLI so that we can talk to ECS. We'll be following the steps as detailed in the official guide on AWS ECS docs. In case of any confusion, please feel free to refer to that guide.\n\nThe first step will involve creating a profile that we'll use for the rest of the tutorial. To continue, you'll need your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. To obtain these, follow the steps as detailed under the section titled Access Key and Secret Access Key on this page.\n\nNext, we need to get a keypair which we'll be using to log into the instances. Head over to your EC2 Console and create a new keypair. Download the keypair and store it in a safe location. Another thing to note before you move away from this screen is the region name. In my case, I have named my key - ecs and set my region as us-east-1. This is what I'll assume for the rest of this walkthrough.\n\nThe next step is to configure the CLI.\n\nWe provide the configure command with the region name we want our cluster to reside in and a cluster name. Make sure you provide the same region name that you used when creating the keypair. If you've not configured the AWS CLI on your computer before, you can use the official guide, which explains everything in great detail on how to get everything going.\n\nThe next step enables the CLI to create a CloudFormation template.\n\nHere we provide the name of the keypair we downloaded initially (ecs in my case), the number of instances that we want to use (--size) and the type of instances that we want the containers to run on. The --capability-iam flag tells the CLI that we acknowledge that this command may create IAM resources.\n\nThe last and final step is where we'll use our docker-compose.yml file. We'll need to make a few minor changes, so instead of modifying the original, let's make a copy of it. The contents of this file (after making the changes) look like (below) -\n\nThe only changes we made from the original docker-compose.yml are of providing the mem_limit (in bytes) and cpu_shares values for each container and adding some logging configuration. This allows us to view logs generated by our containers in AWS CloudWatch. Head over to CloudWatch to create a log group called foodtrucks. Note that since ElasticSearch typically ends up taking more memory, we've given around 3.4 GB of memory limit. Another thing we need to do before we move onto the next step is to publish our image on Docker Hub.\n\nGreat! Now let's run the final command that will deploy our app on ECS!\n\nIt's not a coincidence that the invocation above looks similar to the one we used with Docker Compose. If everything went well, you should see a desiredStatus=RUNNING lastStatus=RUNNING as the last line.\n\nAwesome! Our app is live, but how can we access it?\n\nGo ahead and open http://54.86.14.14 in your browser and you should see the Food Trucks in all its black-yellow glory! Since we're on the topic, let's see how our AWS ECS console looks.\n\nWe can see above that our ECS cluster called 'foodtrucks' was created and is now running 1 task with 2 container instances. Spend some time browsing this console to get a hang of all the options that are here.",
        "code_examples": [
          "```\nversion:'2'services:\n  es:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    cpu_shares: 100\n    mem_limit: 3621440000\n    environment:\n      - discovery.type=single-node\n      - bootstrap.memory_lock=true-\"ES_JAVA_OPTS=-Xms512m -Xmx512m\"logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: es\n  web:\n    image: yourusername/foodtrucks-web\n    cpu_shares: 100\n    mem_limit: 262144000\n    ports:\n      -\"80:5000\"links:\n      - es\n    logging:\n      driver: awslogs\n      options:\n        awslogs-group: foodtrucks\n        awslogs-region: us-east-1\n        awslogs-stream-prefix: web\n```",
          "```\necs-cli ps\nName                                      State    Ports                     TaskDefinition\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/web  RUNNING  54.86.14.14:80->5000/tcp  ecscompose-foodtrucks:2\n845e2368-170d-44a7-bf9f-84c7fcd9ae29/es   RUNNING                            ecscompose-foodtrucks:2\n```"
        ],
        "usage_examples": [
          "```\n$ ecs-cli --version\necs-cli version 1.18.1 (7e9df84)\n```",
          "```\n$ ecs-cli configure profile --profile-name ecs-foodtrucks --access-key$AWS_ACCESS_KEY_ID--secret-key$AWS_SECRET_ACCESS_KEY\n```",
          "```\n$ ecs-cli configure --region us-east-1 --cluster foodtrucks\nINFO[0000] Saved ECS CLI configurationforcluster (foodtrucks)\n```",
          "```\n$ ecs-cli up --keypair ecs --capability-iam --size 1 --instance-type t2.medium\nINFO[0000] Using recommended Amazon Linux 2 AMI with ECS Agent 1.39.0 and Docker version 18.09.9-ce\nINFO[0000] Created cluster                               cluster=foodtrucks\nINFO[0001] Waitingforyour cluster resources to be created\nINFO[0001] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0062] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0122] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0182] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nINFO[0242] Cloudformation stack status                   stackStatus=CREATE_IN_PROGRESS\nVPC created: vpc-0bbed8536930053a6\nSecurity Group created: sg-0cf767fb4d01a3f99\nSubnet created: subnet-05de1db2cb1a50ab8\nSubnet created: subnet-01e1e8bc95d49d0fd\nCluster creation succeeded.\n```",
          "```\n$ docker push yourusername/foodtrucks-web\n```",
          "```\n$cdaws-ecs\n$ ecs-cli compose up\nINFO[0000] Using ECS task definition                     TaskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es\nINFO[0000] Starting container...                         container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0000] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0036] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0048] Describe ECS container status                 container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=PENDING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/web desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\nINFO[0060] Started container...                          container=845e2368-170d-44a7-bf9f-84c7fcd9ae29/es desiredStatus=RUNNING lastStatus=RUNNING taskDefinition=ecscompose-foodtrucks:2\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5014
        }
      },
      {
        "header": "Cleanup",
        "content": "Once you've played around with the deployed app, remember to turn down the cluster -\n\nSo there you have it. With just a few commands we were able to deploy our awesome app on the AWS cloud!",
        "code_examples": [],
        "usage_examples": [
          "```\n$ ecs-clidown --force\nINFO[0001] Waitingforyourclusterresources to be deleted...\nINFO[0001] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0062] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0124] Cloudformationstackstatus                   stackStatus=DELETE_IN_PROGRESS\nINFO[0155] Deletedclustercluster=foodtrucks\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 189
        }
      },
      {
        "header": "Conclusion",
        "content": "And that's a wrap! After a long, exhaustive but fun tutorial you are now ready to take the container world by storm! If you followed along till the very end then you should definitely be proud of yourself. You learned how to setup Docker, run your own containers, play with static and dynamic websites and most importantly got hands on experience with deploying your applications to the cloud!\n\nI hope that finishing this tutorial makes you more confident in your abilities to deal with servers. When you have an idea of building your next app, you can be sure that you'll be able to get it in front of people with minimal effort.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 630
        }
      },
      {
        "header": "Next Steps",
        "content": "Your journey into the container world has just started! My goal with this tutorial was to whet your appetite and show you the power of Docker. In the sea of new technology, it can be hard to navigate the waters alone and tutorials such as this one can provide a helping hand. This is the Docker tutorial I wish I had when I was starting out. Hopefully, it served its purpose of getting you excited about containers so that you no longer have to watch the action from the sides.\n\nBelow are a few additional resources that will be beneficial. For your next project, I strongly encourage you to use Docker. Keep in mind - practice makes perfect!\n\nAdditional Resources\n\nOff you go, young padawan!\n\n• Awesome Docker\n• Docker Weekly and archives\n• Codeship Blog",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 755
        }
      },
      {
        "header": "Give Feedback",
        "content": "Now that the tutorial is over, it's my turn to ask questions. How did you like the tutorial? Did you find the tutorial to be a complete mess or did you have fun and learn something?\n\nSend in your thoughts directly to me or just create an issue. I'm on Twitter, too, so if that's your deal, feel free to holler there!\n\nI would totally love to hear about your experience with this tutorial. Give suggestions on how to make this better or let me know about my mistakes. I want this tutorial to be one of the best introductory tutorials on the web and I can't do it without your help.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 580
        }
      }
    ],
    "url": "https://docker-curriculum.com#conclusion",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "Compose file reference",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "New to Docker Compose?\n\nFind more information about the key features and use cases of Docker Compose or try the quickstart guide.\n\nThe Compose Specification is the latest and recommended version of the Compose file format. It helps you define a Compose file which is used to configure your Docker applicationâs services, networks, volumes, and more.\n\nLegacy versions 2.x and 3.x of the Compose file format were merged into the Compose Specification. It is implemented in versions 1.27.0 and above (also known as Compose v2) of the Docker Compose CLI.\n\nThe Compose Specification on Docker Docs is the Docker Compose implementation. If you wish to implement your own version of the Compose Specification, see the Compose Specification repository.\n\nUse the following links to navigate key sections of the Compose Specification.\n\nWant a better editing experience for Compose files in VS Code? Check out the Docker VS Code Extension (Beta) for linting, code navigation, and vulnerability scanning.\n\n• Get started\n\n[Admonition] Want a better editing experience for Compose files in VS Code? Check out the Docker VS Code Extension (Beta) for linting, code navigation, and vulnerability scanning.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1190
        }
      },
      {
        "header": "Version and name top-level element",
        "content": "Understand version and name attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 51
        }
      },
      {
        "header": "Services top-level element",
        "content": "Explore all services attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 44
        }
      },
      {
        "header": "Networks top-level element",
        "content": "Find all networks attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 41
        }
      },
      {
        "header": "Volumes top-level element",
        "content": "Explore all volumes attributes for Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 43
        }
      },
      {
        "header": "Configs top-level element",
        "content": "Find out about configs in Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 34
        }
      },
      {
        "header": "Secrets top-level element",
        "content": "Learn about secrets in Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 31
        }
      }
    ],
    "url": "https://docs.docker.com/reference/compose-file/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "docker image",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Manage images\nUsage | docker image",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 63
        }
      },
      {
        "header": "Subcommands",
        "content": "Command | Description\n--- | ---\ndocker image historyShow the history of an image | Show the history of an image\ndocker image importImport the contents from a tarball to create a filesystem image | Import the contents from a tarball to create a filesystem image\ndocker image inspectDisplay detailed information on one or more images | Display detailed information on one or more images\ndocker image loadLoad an image from a tar archive or STDIN | Load an image from a tar archive or STDIN\ndocker image lsList images | List images\ndocker image pruneRemove unused images | Remove unused images\ndocker image pullDownload an image from a registry | Download an image from a registry\ndocker image pushUpload an image to a registry | Upload an image to a registry\ndocker image rmRemove one or more images | Remove one or more images\ndocker image saveSave one or more images to a tar archive (streamed to STDOUT by default) | Save one or more images to a tar archive (streamed to STDOUT by default)\ndocker image tagCreate a tag TARGET_IMAGE that refers to SOURCE_IMAGE | Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 1116
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/image/",
    "doc_type": "docker",
    "total_sections": 2
  },
  {
    "title": "docker container",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Reference Get startedGuidesManuals",
    "sections": [
      {
        "header": "",
        "content": "• Get started\n\nDescription | Manage containers\nUsage | docker container",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 71
        }
      },
      {
        "header": "Subcommands",
        "content": "Command | Description\n--- | ---\ndocker container attachAttach local standard input, output, and error streams to a running container | Attach local standard input, output, and error streams to a running container\ndocker container commitCreate a new image from a container's changes | Create a new image from a container's changes\ndocker container cpCopy files/folders between a container and the local filesystem | Copy files/folders between a container and the local filesystem\ndocker container createCreate a new container | Create a new container\ndocker container diffInspect changes to files or directories on a container's filesystem | Inspect changes to files or directories on a container's filesystem\ndocker container execExecute a command in a running container | Execute a command in a running container\ndocker container exportExport a container's filesystem as a tar archive | Export a container's filesystem as a tar archive\ndocker container inspectDisplay detailed information on one or more containers | Display detailed information on one or more containers\ndocker container killKill one or more running containers | Kill one or more running containers\ndocker container logsFetch the logs of a container | Fetch the logs of a container\ndocker container lsList containers | List containers\ndocker container pausePause all processes within one or more containers | Pause all processes within one or more containers\ndocker container portList port mappings or a specific mapping for the container | List port mappings or a specific mapping for the container\ndocker container pruneRemove all stopped containers | Remove all stopped containers\ndocker container renameRename a container | Rename a container\ndocker container restartRestart one or more containers | Restart one or more containers\ndocker container rmRemove one or more containers | Remove one or more containers\ndocker container runCreate and run a new container from an image | Create and run a new container from an image\ndocker container startStart one or more stopped containers | Start one or more stopped containers\ndocker container statsDisplay a live stream of container(s) resource usage statistics | Display a live stream of container(s) resource usage statistics\ndocker container stopStop one or more running containers | Stop one or more running containers\ndocker container topDisplay the running processes of a container | Display the running processes of a container\ndocker container unpauseUnpause all processes within one or more containers | Unpause all processes within one or more containers\ndocker container updateUpdate configuration of one or more containers | Update configuration of one or more containers\ndocker container waitBlock until one or more containers stop, then print their exit codes | Block until one or more containers stop, then print their exit codes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 2863
        }
      }
    ],
    "url": "https://docs.docker.com/reference/cli/docker/container/",
    "doc_type": "docker",
    "total_sections": 2
  },
  {
    "title": "Building best practices",
    "summary": "Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that are needed to run the application. Using multiple stages can also let you build more efficiently by executing build steps in parallel.",
    "sections": [
      {
        "header": "Use multi-stage builds",
        "content": "Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that are needed to run the application.\n\nUsing multiple stages can also let you build more efficiently by executing build steps in parallel.\n\nSee Multi-stage builds for more information.\n\nCreate reusable stages\n\nIf you have multiple images with a lot in common, consider creating a reusable stage that includes the shared components, and basing your unique stages on that. Docker only needs to build the common stage once. This means that your derivative images use memory on the Docker host more efficiently and load more quickly.\n\nIt's also easier to maintain a common base stage (\"Don't repeat yourself\"), than it is to have multiple different stages doing similar things.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 948
        }
      },
      {
        "header": "Create reusable stages",
        "content": "If you have multiple images with a lot in common, consider creating a reusable stage that includes the shared components, and basing your unique stages on that. Docker only needs to build the common stage once. This means that your derivative images use memory on the Docker host more efficiently and load more quickly.\n\nIt's also easier to maintain a common base stage (\"Don't repeat yourself\"), than it is to have multiple different stages doing similar things.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 463
        }
      },
      {
        "header": "Choose the right base image",
        "content": "The first step towards achieving a secure image is to choose the right base image. When choosing an image, ensure it's built from a trusted source and keep it small.\n\nDocker Official Images are a curated collection that have clear documentation, promote best practices, and are regularly updated. They provide a trusted starting point for many applications. Verified Publisher images are high-quality images published and maintained by the organizations partnering with Docker, with Docker verifying the authenticity of the content in their repositories. Docker-Sponsored Open Source are published and maintained by open source projects sponsored by Docker through an open source program .\n\nWhen you pick your base image, look out for the badges indicating that the image is part of these programs.\n\nWhen building your own image from a Dockerfile, ensure you choose a minimal base image that matches your requirements. A smaller base image not only offers portability and fast downloads, but also shrinks the size of your image and minimizes the number of vulnerabilities introduced through the dependencies.\n\nYou should also consider using two types of base image: one for building and unit testing, and another (typically slimmer) image for production. In the later stages of development, your image may not require build tools such as compilers, build systems, and debugging tools. A small image with minimal dependencies can considerably lower the attack surface.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1467
        }
      },
      {
        "header": "Rebuild your images often",
        "content": "Docker images are immutable. Building an image is taking a snapshot of that image at that moment. That includes any base images, libraries, or other software you use in your build. To keep your images up-to-date and secure, make sure to rebuild your image often, with updated dependencies.\n\nTo ensure that you're getting the latest versions of dependencies in your build, you can use the --no-cache option to avoid cache hits.\n\n$ docker build --no-cache -t my-image:my-tag .\n\nThe following Dockerfile uses the 24.04 tag of the ubuntu image. Over time, that tag may resolve to a different underlying version of the ubuntu image, as the publisher rebuilds the image with new security patches and updated libraries. Using the --no-cache , you can avoid cache hits and ensure a fresh download of base images and dependencies.\n\n# syntax=docker/dockerfile:1 FROM ubuntu:24.04 RUN apt-get -y update && apt-get install -y --no-install-recommends python3\n\nAlso consider pinning base image versions .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 990
        }
      },
      {
        "header": "Exclude with .dockerignore",
        "content": "To exclude files not relevant to the build, without restructuring your source repository, use a .dockerignore file. This file supports exclusion patterns similar to .gitignore files.\n\nFor example, to exclude all files with the .md extension:\n\n*.md\n\nFor information on creating one, see Dockerignore file .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 305
        }
      },
      {
        "header": "Create ephemeral containers",
        "content": "The image defined by your Dockerfile should generate containers that are as ephemeral as possible. Ephemeral means that the container can be stopped and destroyed, then rebuilt and replaced with an absolute minimum set up and configuration.\n\nRefer to Processes under The Twelve-factor App methodology to get a feel for the motivations of running containers in such a stateless fashion.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 385
        }
      },
      {
        "header": "Don't install unnecessary packages",
        "content": "Avoid installing extra or unnecessary packages just because they might be nice to have. For example, you donât need to include a text editor in a database image.\n\nWhen you avoid installing extra or unnecessary packages, your images have reduced complexity, reduced dependencies, reduced file sizes, and reduced build times.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 325
        }
      },
      {
        "header": "Decouple applications",
        "content": "Each container should have only one concern. Decoupling applications into multiple containers makes it easier to scale horizontally and reuse containers. For instance, a web application stack might consist of three separate containers, each with its own unique image, to manage the web application, database, and an in-memory cache in a decoupled manner.\n\nLimiting each container to one process is a good rule of thumb, but it's not a hard and fast rule. For example, not only can containers be spawned with an init process , some programs might spawn additional processes of their own accord. For instance, Celery can spawn multiple worker processes, and Apache can create one process per request.\n\nUse your best judgment to keep containers as clean and modular as possible. If containers depend on each other, you can use Docker container networks to ensure that these containers can communicate.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 898
        }
      },
      {
        "header": "Sort multi-line arguments",
        "content": "Whenever possible, sort multi-line arguments alphanumerically to make maintenance easier. This helps to avoid duplication of packages and make the list much easier to update. This also makes PRs a lot easier to read and review. Adding a space before a backslash ( \\ ) helps as well.\n\nHereâs an example from the buildpack-deps image :\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ bzr \\ cvs \\ git \\ mercurial \\ subversion \\ && rm -rf /var/lib/apt/lists/*",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 477
        }
      },
      {
        "header": "Leverage build cache",
        "content": "When building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. For each instruction, Docker checks whether it can reuse the instruction from the build cache.\n\nUnderstanding how the build cache works, and how cache invalidation occurs, is critical for ensuring faster builds. For more information about the Docker build cache and how to optimize your builds, see Docker build cache .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 440
        }
      },
      {
        "header": "Pin base image versions",
        "content": "Image tags are mutable, meaning a publisher can update a tag to point to a new image. This is useful because it lets publishers update tags to point to newer versions of an image. And as an image consumer, it means you automatically get the new version when you re-build your image.\n\nFor example, if you specify FROM alpine:3.21 in your Dockerfile, 3.21 resolves to the latest patch version for 3.21 .\n\n# syntax=docker/dockerfile:1 FROM alpine:3.21\n\nAt one point in time, the 3.21 tag might point to version 3.21.1 of the image. If you rebuild the image 3 months later, the same tag might point to a different version, such as 3.21.4. This publishing workflow is best practice, and most publishers use this tagging strategy, but it isn't enforced.\n\nThe downside with this is that you're not guaranteed to get the same for every build. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.\n\nTo fully secure your supply chain integrity, you can pin the image version to a specific digest. By pinning your images to a digest, you're guaranteed to always use the same image version, even if a publisher replaces the tag with a new image. For example, the following Dockerfile pins the Alpine image to the same tag as earlier, 3.21 , but this time with a digest reference as well.\n\n# syntax=docker/dockerfile:1 FROM alpine:3.21@sha256:a8560b36e8b8210634f77d9f7f9efd7ffa463e380b75e2e74aff4511df3ef88c\n\nWith this Dockerfile, even if the publisher updates the 3.21 tag, your builds would still use the pinned image version: a8560b36e8b8210634f77d9f7f9efd7ffa463e380b75e2e74aff4511df3ef88c .\n\nWhile this helps you avoid unexpected changes, it's also more tedious to have to look up and include the image digest for base image versions manually each time you want to update it. And you're opting out of automated security fixes, which is likely something you want to get.\n\nDocker Scout's default Up-to-Date Base Images policy checks whether the base image version you're using is in fact the latest version. This policy also checks if pinned digests in your Dockerfile correspond to the correct version. If a publisher updates an image that you've pinned, the policy evaluation returns a non-compliant status, indicating that you should update your image.\n\nDocker Scout also supports an automated remediation workflow for keeping your base images up-to-date. When a new image digest is available, Docker Scout can automatically raise a pull request on your repository to update your Dockerfiles to use the latest version. This is better than using a tag that changes the version automatically, because you're in control and you have an audit trail of when and how the change occurred.\n\nFor more information about automatically updating your base images with Docker Scout, see Remediation .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2859
        }
      },
      {
        "header": "Build and test your images in CI",
        "content": "When you check in a change to source control or create a pull request, use GitHub Actions or another CI/CD pipeline to automatically build and tag a Docker image and test it.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 174
        }
      },
      {
        "header": "Dockerfile instructions",
        "content": "Follow these recommendations on how to properly use the Dockerfile instructions to create an efficient and maintainable Dockerfile.\n\nTip To improve linting, code navigation, and vulnerability scanning of your Dockerfiles in Visual Studio Code see Docker VS Code Extension .\n\nFROM\n\nWhenever possible, use current official images as the basis for your images. Docker recommends the Alpine image as it is tightly controlled and small in size (currently under 6 MB), while still being a full Linux distribution.\n\nFor more information about the FROM instruction, see Dockerfile reference for the FROM instruction .\n\nLABEL\n\nYou can add labels to your image to help organize images by project, record licensing information, to aid in automation, or for other reasons. For each label, add a line beginning with LABEL with one or more key-value pairs. The following examples show the different acceptable formats. Explanatory comments are included inline.\n\nStrings with spaces must be quoted or the spaces must be escaped. Inner quote characters ( \" ), must also be escaped. For example:\n\n# Set one or more individual labels LABEL com.example.version = \"0.0.1-beta\" LABEL vendor1 = \"ACME Incorporated\" LABEL vendor2 = ZENITH \\ Incorporated LABEL com.example.release-date = \"2015-02-12\" LABEL com.example.version.is-production = \"\"\n\nAn image can have more than one label. Prior to Docker 1.10, it was recommended to combine all labels into a single LABEL instruction, to prevent extra layers from being created. This is no longer necessary, but combining labels is still supported. For example:\n\n# Set multiple labels on one line LABEL com.example.version = \"0.0.1-beta\" com.example.release-date = \"2015-02-12\"\n\nThe above example can also be written as:\n\n# Set multiple labels at once, using line-continuation characters to break long lines LABEL vendor = ACME \\ Incorporated \\ com.example.is-beta = \\ com.example.is-production = \"\" \\ com.example.version = \"0.0.1-beta\" \\ com.example.release-date = \"2015-02-12\"\n\nSee Understanding object labels for guidelines about acceptable label keys and values. For information about querying labels, refer to the items related to filtering in Managing labels on objects . See also LABEL in the Dockerfile reference.\n\nRUN\n\nSplit long or complex RUN statements on multiple lines separated with backslashes to make your Dockerfile more readable, understandable, and maintainable.\n\nFor example, you can chain commands with the && operator, and use escape characters to break long commands into multiple lines.\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo\n\nBy default, backslash escapes a newline character, but you can change it with the escape directive .\n\nYou can also use here documents to run multiple commands without chaining them with a pipeline operator:\n\nRUN <<EOF apt-get update apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo EOF\n\nFor more information about RUN , see Dockerfile reference for the RUN instruction .\n\napt-get\n\nOne common use case for RUN instructions in Debian-based images is to install software using apt-get . Because apt-get installs packages, the RUN apt-get command has several counter-intuitive behaviors to look out for.\n\nAlways combine RUN apt-get update with apt-get install in the same RUN statement. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo\n\nUsing apt-get update alone in a RUN statement causes caching issues and subsequent apt-get install instructions to fail. For example, this issue will occur in the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM ubuntu:22.04 RUN apt-get update RUN apt-get install -y --no-install-recommends curl\n\nAfter building the image, all layers are in the Docker cache. Suppose you later modify apt-get install by adding an extra package as shown in the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM ubuntu:22.04 RUN apt-get update RUN apt-get install -y --no-install-recommends curl nginx\n\nDocker sees the initial and modified instructions as identical and reuses the cache from previous steps. As a result the apt-get update isn't executed because the build uses the cached version. Because the apt-get update isn't run, your build can potentially get an outdated version of the curl and nginx packages.\n\nUsing RUN apt-get update && apt-get install -y --no-install-recommends ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting. You can also achieve cache busting by specifying a package version. This is known as version pinning. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo = 1.3.*\n\nVersion pinning forces the build to retrieve a particular version regardless of whatâs in the cache. This technique can also reduce failures due to unanticipated changes in required packages.\n\nBelow is a well-formed RUN instruction that demonstrates all the apt-get recommendations.\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ aufs-tools \\ automake \\ build-essential \\ curl \\ dpkg-sig \\ libcap-dev \\ libsqlite3-dev \\ mercurial \\ reprepro \\ ruby1.9.1 \\ ruby1.9.1-dev \\ s3cmd = 1.1.* \\ && rm -rf /var/lib/apt/lists/*\n\nThe s3cmd argument specifies a version 1.1.* . If the image previously used an older version, specifying the new one causes a cache bust of apt-get update and ensures the installation of the new version. Listing packages on each line can also prevent mistakes in package duplication.\n\nIn addition, when you clean up the apt cache by removing /var/lib/apt/lists it reduces the image size, since the apt cache isn't stored in a layer. Since the RUN statement starts with apt-get update , the package cache is always refreshed prior to apt-get install .\n\nOfficial Debian and Ubuntu images automatically run apt-get clean , so explicit invocation is not required.\n\nUsing pipes\n\nSome RUN commands depend on the ability to pipe the output of one command into another, using the pipe character ( | ), as in the following example:\n\nRUN wget -O - https://some.site | wc -l > /number\n\nDocker executes these commands using the /bin/sh -c interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above, this build step succeeds and produces a new image so long as the wc -l command succeeds, even if the wget command fails.\n\nIf you want the command to fail due to an error at any stage in the pipe, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\n\nRUN set -o pipefail && wget -O - https://some.site | wc -l > /number\n\nNote Not all shells support the -o pipefail option. In cases such as the dash shell on Debian-based images, consider using the exec form of RUN to explicitly choose a shell that does support the pipefail option. For example: RUN [ \"/bin/bash\" , \"-c\" , \"set -o pipefail && wget -O - https://some.site | wc -l > /number\" ]\n\nCMD\n\nThe CMD instruction should be used to run the software contained in your image, along with any arguments. CMD should almost always be used in the form of CMD [\"executable\", \"param1\", \"param2\"] . Thus, if the image is for a service, such as Apache and Rails, you would run something like CMD [\"apache2\",\"-DFOREGROUND\"] . Indeed, this form of the instruction is recommended for any service-based image.\n\nIn most other cases, CMD should be given an interactive shell, such as bash, Python and perl. For example, CMD [\"perl\", \"-de0\"] , CMD [\"python\"] , or CMD [\"php\", \"-a\"] . Using this form means that when you execute something like docker run -it python , youâll get dropped into a usable shell, ready to go. CMD should rarely be used in the manner of CMD [\"param\", \"param\"] in conjunction with ENTRYPOINT , unless you and your expected users are already quite familiar with how ENTRYPOINT works.\n\nFor more information about CMD , see Dockerfile reference for the CMD instruction .\n\nEXPOSE\n\nThe EXPOSE instruction indicates the ports on which a container listens for connections. Consequently, you should use the common, traditional port for your application. For example, an image containing the Apache web server would use EXPOSE 80 , while an image containing MongoDB would use EXPOSE 27017 and so on.\n\nFor external access, your users can execute docker run with a flag indicating how to map the specified port to the port of their choice. For container linking, Docker provides environment variables for the path from the recipient container back to the source (for example, MYSQL_PORT_3306_TCP ).\n\nFor more information about EXPOSE , see Dockerfile reference for the EXPOSE instruction .\n\nENV\n\nTo make new software easier to run, you can use ENV to update the PATH environment variable for the software your container installs. For example, ENV PATH=/usr/local/nginx/bin:$PATH ensures that CMD [\"nginx\"] just works.\n\nThe ENV instruction is also useful for providing the required environment variables specific to services you want to containerize, such as Postgresâs PGDATA .\n\nLastly, ENV can also be used to set commonly used version numbers so that version bumps are easier to maintain, as seen in the following example:\n\nENV PG_MAJOR = 9 .3 ENV PG_VERSION = 9 .3.4 RUN curl -SL https://example.com/postgres- $PG_VERSION .tar.xz | tar -xJC /usr/src/postgres && â¦ ENV PATH = /usr/local/postgres- $PG_MAJOR /bin: $PATH\n\nSimilar to having constant variables in a program, as opposed to hard-coding values, this approach lets you change a single ENV instruction to automatically bump the version of the software in your container.\n\nEach ENV line creates a new intermediate layer, just like RUN commands. This means that even if you unset the environment variable in a future layer, it still persists in this layer and its value can be dumped. You can test this by creating a Dockerfile like the following, and then building it.\n\n# syntax=docker/dockerfile:1 FROM alpine ENV ADMIN_USER = \"mark\" RUN echo $ADMIN_USER > ./mark RUN unset ADMIN_USER\n\n$ docker run --rm test sh -c 'echo $ADMIN_USER' mark\n\nTo prevent this and unset the environment variable, use a RUN command with shell commands, to set, use, and unset the variable all in a single layer. You can separate your commands with ; or && . If you use the second method, and one of the commands fails, the docker build also fails. This is usually a good idea. Using \\ as a line continuation character for Linux Dockerfiles improves readability. You could also put all of the commands into a shell script and have the RUN command just run that shell script.\n\n# syntax=docker/dockerfile:1 FROM alpine RUN export ADMIN_USER = \"mark\" \\ && echo $ADMIN_USER > ./mark \\ && unset ADMIN_USER CMD sh\n\n$ docker run --rm test sh -c 'echo $ADMIN_USER'\n\nFor more information about ENV , see Dockerfile reference for the ENV instruction .\n\nADD or COPY\n\nADD and COPY are functionally similar. COPY supports basic copying of files into the container, from the build context or from a stage in a multi-stage build . ADD supports features for fetching files from remote HTTPS and Git URLs, and extracting tar files automatically when adding files from the build context.\n\nYou'll mostly want to use COPY for copying files from one stage to another in a multi-stage build. If you need to add files from the build context to the container temporarily to execute a RUN instruction, you can often substitute the COPY instruction with a bind mount instead. For example, to temporarily add a requirements.txt file for a RUN pip install instruction:\n\nRUN --mount = type = bind,source = requirements.txt,target = /tmp/requirements.txt \\ pip install --requirement /tmp/requirements.txt\n\nBind mounts are more efficient than COPY for including files from the build context in the container. Note that bind-mounted files are only added temporarily for a single RUN instruction, and don't persist in the final image. If you need to include files from the build context in the final image, use COPY .\n\nThe ADD instruction is best for when you need to download a remote artifact as part of your build. ADD is better than manually adding files using something like wget and tar , because it ensures a more precise build cache. ADD also has built-in support for checksum validation of the remote resources, and a protocol for parsing branches, tags, and subdirectories from Git URLs .\n\nThe following example uses ADD to download a .NET installer. Combined with multi-stage builds, only the .NET runtime remains in the final stage, no intermediate files.\n\n# syntax=docker/dockerfile:1 FROM scratch AS src ARG DOTNET_VERSION = 8 .0.0-preview.6.23329.7 ADD --checksum = sha256:270d731bd08040c6a3228115de1f74b91cf441c584139ff8f8f6503447cebdbb \\ https://dotnetcli.azureedge.net/dotnet/Runtime/ $DOTNET_VERSION /dotnet-runtime- $DOTNET_VERSION -linux-arm64.tar.gz /dotnet.tar.gz FROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8 AS installer # Retrieve .NET Runtime RUN --mount = from = src,target = /src <<EOF mkdir -p /dotnet tar -oxzf /src/dotnet.tar.gz -C /dotnet EOF FROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8 COPY --from = installer /dotnet /usr/share/dotnet RUN ln -s /usr/share/dotnet/dotnet /usr/bin/dotnet\n\nFor more information about ADD or COPY , see the following:\n\nDockerfile reference for the ADD instruction Dockerfile reference for the COPY instruction\n\nENTRYPOINT\n\nThe best use for ENTRYPOINT is to set the image's main command, allowing that image to be run as though it was that command, and then use CMD as the default flags.\n\nThe following is an example of an image for the command line tool s3cmd :\n\nENTRYPOINT [ \"s3cmd\" ] CMD [ \"--help\" ]\n\nYou can use the following command to run the image and show the command's help:\n\n$ docker run s3cmd\n\nOr, you can use the right parameters to execute a command, like in the following example:\n\n$ docker run s3cmd ls s3://mybucket\n\nThis is useful because the image name can double as a reference to the binary as shown in the command above.\n\nThe ENTRYPOINT instruction can also be used in combination with a helper script, allowing it to function in a similar way to the command above, even when starting the tool may require more than one step.\n\nFor example, the Postgres Official Image uses the following script as its ENTRYPOINT :\n\n#!/bin/bash set -e if [ \" $1 \" = 'postgres' ] ; then chown -R postgres \" $PGDATA \" if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then gosu postgres initdb fi exec gosu postgres \" $@ \" fi exec \" $@ \"\n\nThis script uses the exec Bash command so that the final running application becomes the container's PID 1. This allows the application to receive any Unix signals sent to the container. For more information, see the ENTRYPOINT reference .\n\nIn the following example, a helper script is copied into the container and run via ENTRYPOINT on container start:\n\nCOPY ./docker-entrypoint.sh / ENTRYPOINT [ \"/docker-entrypoint.sh\" ] CMD [ \"postgres\" ]\n\nThis script lets you interact with Postgres in several ways.\n\nIt can simply start Postgres:\n\n$ docker run postgres\n\nOr, you can use it to run Postgres and pass parameters to the server:\n\n$ docker run postgres postgres --help\n\nLastly, you can use it to start a totally different tool, such as Bash:\n\n$ docker run --rm -it postgres bash\n\nFor more information about ENTRYPOINT , see Dockerfile reference for the ENTRYPOINT instruction .\n\nVOLUME\n\nYou should use the VOLUME instruction to expose any database storage area, configuration storage, or files and folders created by your Docker container. You are strongly encouraged to use VOLUME for any combination of mutable or user-serviceable parts of your image.\n\nFor more information about VOLUME , see Dockerfile reference for the VOLUME instruction .\n\nUSER\n\nIf a service can run without privileges, use USER to change to a non-root user. Start by creating the user and group in the Dockerfile with something like the following example:\n\nRUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres\n\nNote Consider an explicit UID/GID. Users and groups in an image are assigned a non-deterministic UID/GID in that the \"next\" UID/GID is assigned regardless of image rebuilds. So, if itâs critical, you should assign an explicit UID/GID.\n\nNote Due to an unresolved bug in the Go archive/tar package's handling of sparse files, attempting to create a user with a significantly large UID inside a Docker container can lead to disk exhaustion because /var/log/faillog in the container layer is filled with NULL (\\0) characters. A workaround is to pass the --no-log-init flag to useradd . The Debian/Ubuntu adduser wrapper does not support this flag.\n\nAvoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to sudo , such as initializing the daemon as root but running it as non- root , consider using âgosuâ .\n\nLastly, to reduce layers and complexity, avoid switching USER back and forth frequently.\n\nFor more information about USER , see Dockerfile reference for the USER instruction .\n\nWORKDIR\n\nFor clarity and reliability, you should always use absolute paths for your WORKDIR . Also, you should use WORKDIR instead of proliferating instructions like RUN cd â¦ && do-something , which are hard to read, troubleshoot, and maintain.\n\nFor more information about WORKDIR , see Dockerfile reference for the WORKDIR instruction .\n\nONBUILD\n\nAn ONBUILD command executes after the current Dockerfile build completes. ONBUILD executes in any child image derived FROM the current image. Think of the ONBUILD command as an instruction that the parent Dockerfile gives to the child Dockerfile.\n\nA Docker build executes ONBUILD commands before any command in a child Dockerfile.\n\nONBUILD is useful for images that are going to be built FROM a given image. For example, you would use ONBUILD for a language stack image that builds arbitrary user software written in that language within the Dockerfile, as you can see in Rubyâs ONBUILD variants .\n\nImages built with ONBUILD should get a separate tag. For example, ruby:1.9-onbuild or ruby:2.0-onbuild .\n\nBe careful when putting ADD or COPY in ONBUILD . The image fails catastrophically if the new build's context is missing the resource being added. Adding a separate tag, as recommended above, helps mitigate this by allowing the Dockerfile author to make a choice.\n\nFor more information about ONBUILD , see Dockerfile reference for the ONBUILD instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 74,
          "content_length": 18756
        }
      },
      {
        "header": "FROM",
        "content": "Whenever possible, use current official images as the basis for your images. Docker recommends the Alpine image as it is tightly controlled and small in size (currently under 6 MB), while still being a full Linux distribution.\n\nFor more information about the FROM instruction, see Dockerfile reference for the FROM instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 328
        }
      },
      {
        "header": "LABEL",
        "content": "You can add labels to your image to help organize images by project, record licensing information, to aid in automation, or for other reasons. For each label, add a line beginning with LABEL with one or more key-value pairs. The following examples show the different acceptable formats. Explanatory comments are included inline.\n\nStrings with spaces must be quoted or the spaces must be escaped. Inner quote characters ( \" ), must also be escaped. For example:\n\n# Set one or more individual labels LABEL com.example.version = \"0.0.1-beta\" LABEL vendor1 = \"ACME Incorporated\" LABEL vendor2 = ZENITH \\ Incorporated LABEL com.example.release-date = \"2015-02-12\" LABEL com.example.version.is-production = \"\"\n\nAn image can have more than one label. Prior to Docker 1.10, it was recommended to combine all labels into a single LABEL instruction, to prevent extra layers from being created. This is no longer necessary, but combining labels is still supported. For example:\n\n# Set multiple labels on one line LABEL com.example.version = \"0.0.1-beta\" com.example.release-date = \"2015-02-12\"\n\nThe above example can also be written as:\n\n# Set multiple labels at once, using line-continuation characters to break long lines LABEL vendor = ACME \\ Incorporated \\ com.example.is-beta = \\ com.example.is-production = \"\" \\ com.example.version = \"0.0.1-beta\" \\ com.example.release-date = \"2015-02-12\"\n\nSee Understanding object labels for guidelines about acceptable label keys and values. For information about querying labels, refer to the items related to filtering in Managing labels on objects . See also LABEL in the Dockerfile reference.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1626
        }
      },
      {
        "header": "RUN",
        "content": "Split long or complex RUN statements on multiple lines separated with backslashes to make your Dockerfile more readable, understandable, and maintainable.\n\nFor example, you can chain commands with the && operator, and use escape characters to break long commands into multiple lines.\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo\n\nBy default, backslash escapes a newline character, but you can change it with the escape directive .\n\nYou can also use here documents to run multiple commands without chaining them with a pipeline operator:\n\nRUN <<EOF apt-get update apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo EOF\n\nFor more information about RUN , see Dockerfile reference for the RUN instruction .\n\napt-get\n\nOne common use case for RUN instructions in Debian-based images is to install software using apt-get . Because apt-get installs packages, the RUN apt-get command has several counter-intuitive behaviors to look out for.\n\nAlways combine RUN apt-get update with apt-get install in the same RUN statement. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo\n\nUsing apt-get update alone in a RUN statement causes caching issues and subsequent apt-get install instructions to fail. For example, this issue will occur in the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM ubuntu:22.04 RUN apt-get update RUN apt-get install -y --no-install-recommends curl\n\nAfter building the image, all layers are in the Docker cache. Suppose you later modify apt-get install by adding an extra package as shown in the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM ubuntu:22.04 RUN apt-get update RUN apt-get install -y --no-install-recommends curl nginx\n\nDocker sees the initial and modified instructions as identical and reuses the cache from previous steps. As a result the apt-get update isn't executed because the build uses the cached version. Because the apt-get update isn't run, your build can potentially get an outdated version of the curl and nginx packages.\n\nUsing RUN apt-get update && apt-get install -y --no-install-recommends ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting. You can also achieve cache busting by specifying a package version. This is known as version pinning. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo = 1.3.*\n\nVersion pinning forces the build to retrieve a particular version regardless of whatâs in the cache. This technique can also reduce failures due to unanticipated changes in required packages.\n\nBelow is a well-formed RUN instruction that demonstrates all the apt-get recommendations.\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ aufs-tools \\ automake \\ build-essential \\ curl \\ dpkg-sig \\ libcap-dev \\ libsqlite3-dev \\ mercurial \\ reprepro \\ ruby1.9.1 \\ ruby1.9.1-dev \\ s3cmd = 1.1.* \\ && rm -rf /var/lib/apt/lists/*\n\nThe s3cmd argument specifies a version 1.1.* . If the image previously used an older version, specifying the new one causes a cache bust of apt-get update and ensures the installation of the new version. Listing packages on each line can also prevent mistakes in package duplication.\n\nIn addition, when you clean up the apt cache by removing /var/lib/apt/lists it reduces the image size, since the apt cache isn't stored in a layer. Since the RUN statement starts with apt-get update , the package cache is always refreshed prior to apt-get install .\n\nOfficial Debian and Ubuntu images automatically run apt-get clean , so explicit invocation is not required.\n\nUsing pipes\n\nSome RUN commands depend on the ability to pipe the output of one command into another, using the pipe character ( | ), as in the following example:\n\nRUN wget -O - https://some.site | wc -l > /number\n\nDocker executes these commands using the /bin/sh -c interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above, this build step succeeds and produces a new image so long as the wc -l command succeeds, even if the wget command fails.\n\nIf you want the command to fail due to an error at any stage in the pipe, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\n\nRUN set -o pipefail && wget -O - https://some.site | wc -l > /number\n\nNote Not all shells support the -o pipefail option. In cases such as the dash shell on Debian-based images, consider using the exec form of RUN to explicitly choose a shell that does support the pipefail option. For example: RUN [ \"/bin/bash\" , \"-c\" , \"set -o pipefail && wget -O - https://some.site | wc -l > /number\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 4909
        }
      },
      {
        "header": "apt-get",
        "content": "One common use case for RUN instructions in Debian-based images is to install software using apt-get . Because apt-get installs packages, the RUN apt-get command has several counter-intuitive behaviors to look out for.\n\nAlways combine RUN apt-get update with apt-get install in the same RUN statement. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo\n\nUsing apt-get update alone in a RUN statement causes caching issues and subsequent apt-get install instructions to fail. For example, this issue will occur in the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM ubuntu:22.04 RUN apt-get update RUN apt-get install -y --no-install-recommends curl\n\nAfter building the image, all layers are in the Docker cache. Suppose you later modify apt-get install by adding an extra package as shown in the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM ubuntu:22.04 RUN apt-get update RUN apt-get install -y --no-install-recommends curl nginx\n\nDocker sees the initial and modified instructions as identical and reuses the cache from previous steps. As a result the apt-get update isn't executed because the build uses the cached version. Because the apt-get update isn't run, your build can potentially get an outdated version of the curl and nginx packages.\n\nUsing RUN apt-get update && apt-get install -y --no-install-recommends ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting. You can also achieve cache busting by specifying a package version. This is known as version pinning. For example:\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ package-bar \\ package-baz \\ package-foo = 1.3.*\n\nVersion pinning forces the build to retrieve a particular version regardless of whatâs in the cache. This technique can also reduce failures due to unanticipated changes in required packages.\n\nBelow is a well-formed RUN instruction that demonstrates all the apt-get recommendations.\n\nRUN apt-get update && apt-get install -y --no-install-recommends \\ aufs-tools \\ automake \\ build-essential \\ curl \\ dpkg-sig \\ libcap-dev \\ libsqlite3-dev \\ mercurial \\ reprepro \\ ruby1.9.1 \\ ruby1.9.1-dev \\ s3cmd = 1.1.* \\ && rm -rf /var/lib/apt/lists/*\n\nThe s3cmd argument specifies a version 1.1.* . If the image previously used an older version, specifying the new one causes a cache bust of apt-get update and ensures the installation of the new version. Listing packages on each line can also prevent mistakes in package duplication.\n\nIn addition, when you clean up the apt cache by removing /var/lib/apt/lists it reduces the image size, since the apt cache isn't stored in a layer. Since the RUN statement starts with apt-get update , the package cache is always refreshed prior to apt-get install .\n\nOfficial Debian and Ubuntu images automatically run apt-get clean , so explicit invocation is not required.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2993
        }
      },
      {
        "header": "Using pipes",
        "content": "Some RUN commands depend on the ability to pipe the output of one command into another, using the pipe character ( | ), as in the following example:\n\nRUN wget -O - https://some.site | wc -l > /number\n\nDocker executes these commands using the /bin/sh -c interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above, this build step succeeds and produces a new image so long as the wc -l command succeeds, even if the wget command fails.\n\nIf you want the command to fail due to an error at any stage in the pipe, prepend set -o pipefail && to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\n\nRUN set -o pipefail && wget -O - https://some.site | wc -l > /number\n\nNote Not all shells support the -o pipefail option. In cases such as the dash shell on Debian-based images, consider using the exec form of RUN to explicitly choose a shell that does support the pipefail option. For example: RUN [ \"/bin/bash\" , \"-c\" , \"set -o pipefail && wget -O - https://some.site | wc -l > /number\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1091
        }
      },
      {
        "header": "CMD",
        "content": "The CMD instruction should be used to run the software contained in your image, along with any arguments. CMD should almost always be used in the form of CMD [\"executable\", \"param1\", \"param2\"] . Thus, if the image is for a service, such as Apache and Rails, you would run something like CMD [\"apache2\",\"-DFOREGROUND\"] . Indeed, this form of the instruction is recommended for any service-based image.\n\nIn most other cases, CMD should be given an interactive shell, such as bash, Python and perl. For example, CMD [\"perl\", \"-de0\"] , CMD [\"python\"] , or CMD [\"php\", \"-a\"] . Using this form means that when you execute something like docker run -it python , youâll get dropped into a usable shell, ready to go. CMD should rarely be used in the manner of CMD [\"param\", \"param\"] in conjunction with ENTRYPOINT , unless you and your expected users are already quite familiar with how ENTRYPOINT works.\n\nFor more information about CMD , see Dockerfile reference for the CMD instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 982
        }
      },
      {
        "header": "EXPOSE",
        "content": "The EXPOSE instruction indicates the ports on which a container listens for connections. Consequently, you should use the common, traditional port for your application. For example, an image containing the Apache web server would use EXPOSE 80 , while an image containing MongoDB would use EXPOSE 27017 and so on.\n\nFor external access, your users can execute docker run with a flag indicating how to map the specified port to the port of their choice. For container linking, Docker provides environment variables for the path from the recipient container back to the source (for example, MYSQL_PORT_3306_TCP ).\n\nFor more information about EXPOSE , see Dockerfile reference for the EXPOSE instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 701
        }
      },
      {
        "header": "ENV",
        "content": "To make new software easier to run, you can use ENV to update the PATH environment variable for the software your container installs. For example, ENV PATH=/usr/local/nginx/bin:$PATH ensures that CMD [\"nginx\"] just works.\n\nThe ENV instruction is also useful for providing the required environment variables specific to services you want to containerize, such as Postgresâs PGDATA .\n\nLastly, ENV can also be used to set commonly used version numbers so that version bumps are easier to maintain, as seen in the following example:\n\nENV PG_MAJOR = 9 .3 ENV PG_VERSION = 9 .3.4 RUN curl -SL https://example.com/postgres- $PG_VERSION .tar.xz | tar -xJC /usr/src/postgres && â¦ ENV PATH = /usr/local/postgres- $PG_MAJOR /bin: $PATH\n\nSimilar to having constant variables in a program, as opposed to hard-coding values, this approach lets you change a single ENV instruction to automatically bump the version of the software in your container.\n\nEach ENV line creates a new intermediate layer, just like RUN commands. This means that even if you unset the environment variable in a future layer, it still persists in this layer and its value can be dumped. You can test this by creating a Dockerfile like the following, and then building it.\n\n# syntax=docker/dockerfile:1 FROM alpine ENV ADMIN_USER = \"mark\" RUN echo $ADMIN_USER > ./mark RUN unset ADMIN_USER\n\n$ docker run --rm test sh -c 'echo $ADMIN_USER' mark\n\nTo prevent this and unset the environment variable, use a RUN command with shell commands, to set, use, and unset the variable all in a single layer. You can separate your commands with ; or && . If you use the second method, and one of the commands fails, the docker build also fails. This is usually a good idea. Using \\ as a line continuation character for Linux Dockerfiles improves readability. You could also put all of the commands into a shell script and have the RUN command just run that shell script.\n\n# syntax=docker/dockerfile:1 FROM alpine RUN export ADMIN_USER = \"mark\" \\ && echo $ADMIN_USER > ./mark \\ && unset ADMIN_USER CMD sh\n\n$ docker run --rm test sh -c 'echo $ADMIN_USER'\n\nFor more information about ENV , see Dockerfile reference for the ENV instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 2186
        }
      },
      {
        "header": "ADD or COPY",
        "content": "ADD and COPY are functionally similar. COPY supports basic copying of files into the container, from the build context or from a stage in a multi-stage build . ADD supports features for fetching files from remote HTTPS and Git URLs, and extracting tar files automatically when adding files from the build context.\n\nYou'll mostly want to use COPY for copying files from one stage to another in a multi-stage build. If you need to add files from the build context to the container temporarily to execute a RUN instruction, you can often substitute the COPY instruction with a bind mount instead. For example, to temporarily add a requirements.txt file for a RUN pip install instruction:\n\nRUN --mount = type = bind,source = requirements.txt,target = /tmp/requirements.txt \\ pip install --requirement /tmp/requirements.txt\n\nBind mounts are more efficient than COPY for including files from the build context in the container. Note that bind-mounted files are only added temporarily for a single RUN instruction, and don't persist in the final image. If you need to include files from the build context in the final image, use COPY .\n\nThe ADD instruction is best for when you need to download a remote artifact as part of your build. ADD is better than manually adding files using something like wget and tar , because it ensures a more precise build cache. ADD also has built-in support for checksum validation of the remote resources, and a protocol for parsing branches, tags, and subdirectories from Git URLs .\n\nThe following example uses ADD to download a .NET installer. Combined with multi-stage builds, only the .NET runtime remains in the final stage, no intermediate files.\n\n# syntax=docker/dockerfile:1 FROM scratch AS src ARG DOTNET_VERSION = 8 .0.0-preview.6.23329.7 ADD --checksum = sha256:270d731bd08040c6a3228115de1f74b91cf441c584139ff8f8f6503447cebdbb \\ https://dotnetcli.azureedge.net/dotnet/Runtime/ $DOTNET_VERSION /dotnet-runtime- $DOTNET_VERSION -linux-arm64.tar.gz /dotnet.tar.gz FROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8 AS installer # Retrieve .NET Runtime RUN --mount = from = src,target = /src <<EOF mkdir -p /dotnet tar -oxzf /src/dotnet.tar.gz -C /dotnet EOF FROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8 COPY --from = installer /dotnet /usr/share/dotnet RUN ln -s /usr/share/dotnet/dotnet /usr/bin/dotnet\n\nFor more information about ADD or COPY , see the following:\n\nDockerfile reference for the ADD instruction Dockerfile reference for the COPY instruction",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2556
        }
      },
      {
        "header": "ENTRYPOINT",
        "content": "The best use for ENTRYPOINT is to set the image's main command, allowing that image to be run as though it was that command, and then use CMD as the default flags.\n\nThe following is an example of an image for the command line tool s3cmd :\n\nENTRYPOINT [ \"s3cmd\" ] CMD [ \"--help\" ]\n\nYou can use the following command to run the image and show the command's help:\n\n$ docker run s3cmd\n\nOr, you can use the right parameters to execute a command, like in the following example:\n\n$ docker run s3cmd ls s3://mybucket\n\nThis is useful because the image name can double as a reference to the binary as shown in the command above.\n\nThe ENTRYPOINT instruction can also be used in combination with a helper script, allowing it to function in a similar way to the command above, even when starting the tool may require more than one step.\n\nFor example, the Postgres Official Image uses the following script as its ENTRYPOINT :\n\n#!/bin/bash set -e if [ \" $1 \" = 'postgres' ] ; then chown -R postgres \" $PGDATA \" if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then gosu postgres initdb fi exec gosu postgres \" $@ \" fi exec \" $@ \"\n\nThis script uses the exec Bash command so that the final running application becomes the container's PID 1. This allows the application to receive any Unix signals sent to the container. For more information, see the ENTRYPOINT reference .\n\nIn the following example, a helper script is copied into the container and run via ENTRYPOINT on container start:\n\nCOPY ./docker-entrypoint.sh / ENTRYPOINT [ \"/docker-entrypoint.sh\" ] CMD [ \"postgres\" ]\n\nThis script lets you interact with Postgres in several ways.\n\nIt can simply start Postgres:\n\n$ docker run postgres\n\nOr, you can use it to run Postgres and pass parameters to the server:\n\n$ docker run postgres postgres --help\n\nLastly, you can use it to start a totally different tool, such as Bash:\n\n$ docker run --rm -it postgres bash\n\nFor more information about ENTRYPOINT , see Dockerfile reference for the ENTRYPOINT instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 1984
        }
      },
      {
        "header": "VOLUME",
        "content": "You should use the VOLUME instruction to expose any database storage area, configuration storage, or files and folders created by your Docker container. You are strongly encouraged to use VOLUME for any combination of mutable or user-serviceable parts of your image.\n\nFor more information about VOLUME , see Dockerfile reference for the VOLUME instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 357
        }
      },
      {
        "header": "USER",
        "content": "If a service can run without privileges, use USER to change to a non-root user. Start by creating the user and group in the Dockerfile with something like the following example:\n\nRUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres\n\nNote Consider an explicit UID/GID. Users and groups in an image are assigned a non-deterministic UID/GID in that the \"next\" UID/GID is assigned regardless of image rebuilds. So, if itâs critical, you should assign an explicit UID/GID.\n\nNote Due to an unresolved bug in the Go archive/tar package's handling of sparse files, attempting to create a user with a significantly large UID inside a Docker container can lead to disk exhaustion because /var/log/faillog in the container layer is filled with NULL (\\0) characters. A workaround is to pass the --no-log-init flag to useradd . The Debian/Ubuntu adduser wrapper does not support this flag.\n\nAvoid installing or using sudo as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to sudo , such as initializing the daemon as root but running it as non- root , consider using âgosuâ .\n\nLastly, to reduce layers and complexity, avoid switching USER back and forth frequently.\n\nFor more information about USER , see Dockerfile reference for the USER instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1345
        }
      },
      {
        "header": "WORKDIR",
        "content": "For clarity and reliability, you should always use absolute paths for your WORKDIR . Also, you should use WORKDIR instead of proliferating instructions like RUN cd â¦ && do-something , which are hard to read, troubleshoot, and maintain.\n\nFor more information about WORKDIR , see Dockerfile reference for the WORKDIR instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 330
        }
      },
      {
        "header": "ONBUILD",
        "content": "An ONBUILD command executes after the current Dockerfile build completes. ONBUILD executes in any child image derived FROM the current image. Think of the ONBUILD command as an instruction that the parent Dockerfile gives to the child Dockerfile.\n\nA Docker build executes ONBUILD commands before any command in a child Dockerfile.\n\nONBUILD is useful for images that are going to be built FROM a given image. For example, you would use ONBUILD for a language stack image that builds arbitrary user software written in that language within the Dockerfile, as you can see in Rubyâs ONBUILD variants .\n\nImages built with ONBUILD should get a separate tag. For example, ruby:1.9-onbuild or ruby:2.0-onbuild .\n\nBe careful when putting ADD or COPY in ONBUILD . The image fails catastrophically if the new build's context is missing the resource being added. Adding a separate tag, as recommended above, helps mitigate this by allowing the Dockerfile author to make a choice.\n\nFor more information about ONBUILD , see Dockerfile reference for the ONBUILD instruction .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1062
        }
      }
    ],
    "url": "https://docs.docker.com/build/building/best-practices/",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "Multi-stage builds",
    "summary": "Back Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference\n\nBack Ask AI Start typing to search or try Ask AI.Contact support Manuals Get startedGuidesReference",
    "sections": [
      {
        "header": "",
        "content": "Multi-stage builds are useful to anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain.\n\n• Get started",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 142
        }
      },
      {
        "header": "Use multi-stage builds",
        "content": "With multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don't want in the final image.\n\nThe following Dockerfile has two separate stages: one for building a binary, and another where the binary gets copied from the first stage into the next stage.\n\nYou only need the single Dockerfile. No need for a separate build script. Just run docker build.\n\nThe end result is a tiny production image with nothing but the binary inside. None of the build tools required to build the application are included in the resulting image.\n\nHow does it work? The second FROM instruction starts a new build stage with the scratch image as its base. The COPY --from=0 line copies just the built artifact from the previous stage into this new stage. The Go SDK and any intermediate artifacts are left behind, and not saved in the final image.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMgolang:1.24WORKDIR/srcCOPY<<EOF ./main.gopackage mainimport\"fmt\"func main(){fmt.Println(\"hello, world\")}EOFRUNgo build -o /bin/hello ./main.goFROMscratchCOPY--from=0/bin/hello /bin/helloCMD[\"/bin/hello\"]\n```"
        ],
        "usage_examples": [
          "```\n$docker build -t hello .\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1031
        }
      },
      {
        "header": "Name your build stages",
        "content": "By default, the stages aren't named, and you refer to them by their integer number, starting with 0 for the first FROM instruction. However, you can name your stages, by adding an AS <NAME> to the FROM instruction. This example improves the previous one by naming the stages and using the name in the COPY instruction. This means that even if the instructions in your Dockerfile are re-ordered later, the COPY doesn't break.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMgolang:1.24 AS buildWORKDIR/srcCOPY<<EOF /src/main.gopackage mainimport\"fmt\"func main(){fmt.Println(\"hello, world\")}EOFRUNgo build -o /bin/hello ./main.goFROMscratchCOPY--from=build /bin/hello /bin/helloCMD[\"/bin/hello\"]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 424
        }
      },
      {
        "header": "Stop at a specific build stage",
        "content": "When you build your image, you don't necessarily need to build the entire Dockerfile including every stage. You can specify a target build stage. The following command assumes you are using the previous Dockerfile but stops at the stage named build:\n\nA few scenarios where this might be useful are:\n\n• Debugging a specific build stage\n• Using a debug stage with all debugging symbols or tools enabled, and a lean production stage\n• Using a testing stage in which your app gets populated with test data, but building for production using a different stage which uses real data",
        "code_examples": [],
        "usage_examples": [
          "```\n$docker build --target build -t hello .\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 575
        }
      },
      {
        "header": "Use an external image as a stage",
        "content": "When using multi-stage builds, you aren't limited to copying from stages you created earlier in your Dockerfile. You can use the COPY --from instruction to copy from a separate image, either using the local image name, a tag available locally or on a Docker registry, or a tag ID. The Docker client pulls the image if necessary and copies the artifact from there. The syntax is:",
        "code_examples": [
          "```\nCOPY--from=nginx:latest /etc/nginx/nginx.conf /nginx.conf\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 378
        }
      },
      {
        "header": "Use a previous stage as a new stage",
        "content": "You can pick up where a previous stage left off by referring to it when using the FROM directive. For example:",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMalpine:latest AS builderRUNapk --no-cache add build-baseFROMbuilder AS build1COPYsource1.cpp source.cppRUNg++ -o /binary source.cppFROMbuilder AS build2COPYsource2.cpp source.cppRUNg++ -o /binary source.cpp\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 110
        }
      },
      {
        "header": "Differences between legacy builder and BuildKit",
        "content": "The legacy Docker Engine builder processes all stages of a Dockerfile leading up to the selected --target. It will build a stage even if the selected target doesn't depend on that stage.\n\nBuildKit only builds the stages that the target stage depends on.\n\nFor example, given the following Dockerfile:\n\nWith BuildKit enabled, building the stage2 target in this Dockerfile means only base and stage2 are processed. There is no dependency on stage1, so it's skipped.\n\nOn the other hand, building the same target without BuildKit results in all stages being processed:\n\nThe legacy builder processes stage1, even if stage2 doesn't depend on it.",
        "code_examples": [
          "```\n# syntax=docker/dockerfile:1FROMubuntu AS baseRUNecho\"base\"FROMbase AS stage1RUNecho\"stage1\"FROMbase AS stage2RUNecho\"stage2\"\n```"
        ],
        "usage_examples": [
          "```\n$DOCKER_BUILDKIT=1docker build --no-cache -f Dockerfile --target stage2 .[+] Building 0.4s (7/7) FINISHED=> [internal] load build definition from Dockerfile                                            0.0s=> => transferring dockerfile: 36B                                                             0.0s=> [internal] load .dockerignore                                                               0.0s=> => transferring context: 2B                                                                 0.0s=> [internal] load metadata for docker.io/library/ubuntu:latest                                0.0s=> CACHED [base 1/2] FROM docker.io/library/ubuntu                                             0.0s=> [base 2/2] RUN echo \"base\"                                                                  0.1s=> [stage2 1/1] RUN echo \"stage2\"                                                              0.2s=> exporting to image                                                                          0.0s=> => exporting layers                                                                         0.0s=> => writing image sha256:f55003b607cef37614f607f0728e6fd4d113a4bf7ef12210da338c716f2cfd15    0.0s\n```",
          "```\n$DOCKER_BUILDKIT=0docker build --no-cache -f Dockerfile --target stage2 .Sending build context to Docker daemon  219.1kBStep 1/6 : FROM ubuntu AS base---> a7870fd478f4Step 2/6 : RUN echo \"base\"---> Running in e850d0e42ecabaseRemoving intermediate container e850d0e42eca---> d9f69f23cac8Step 3/6 : FROM base AS stage1---> d9f69f23cac8Step 4/6 : RUN echo \"stage1\"---> Running in 758ba6c1a9a3stage1Removing intermediate container 758ba6c1a9a3---> 396baa55b8c3Step 5/6 : FROM base AS stage2---> d9f69f23cac8Step 6/6 : RUN echo \"stage2\"---> Running in bbc025b93175stage2Removing intermediate container bbc025b93175---> 09fc3770a9c4Successfully built 09fc3770a9c4\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 638
        }
      }
    ],
    "url": "https://docs.docker.com/build/building/multi-stage/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "First Alpine Linux Containers",
    "summary": "In this lab you will run a popular, free, lightweight container and explore the basics of how containers work, how the Docker Engine executes and isolates containers from each other. If you already have experience running containers and basic Docker commands you can probably skip this intro exercise. Concepts in this exercise: Docker engine Containers & images Image registries and Docker Hub Container isolation Tips: Code snippets are shown in one of three ways throughout this environment: Code ",
    "sections": [
      {
        "header": "",
        "content": "In this lab you will run a popular, free, lightweight container and explore the basics of how containers work, how the Docker Engine executes and isolates containers from each other. If you already have experience running containers and basic Docker commands you can probably skip this intro exercise.\n\nConcepts in this exercise:\n\nCode snippets are shown in one of three ways throughout this environment:\n\n• Docker engine\n• Containers & images\n• Image registries and Docker Hub\n• Container isolation\n\n• Code that looks like this is sample code snippets that is usually part of an explanation.\n• Code that appears in box like the one below can be clicked on and it will automatically be typed in to the appropriate terminal window: uname -a\n• Code appearing in windows like the one below is code that you should type in yourself. Usually there will be a unique ID or other bit your need to enter which we cannot supply. Items appearing in <> are the pieces you should substitute based on the instructions. docker container start <container ID>",
        "code_examples": [
          "```\nuname -a\n```",
          "```\ndocker container start <container ID>\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1042
        }
      },
      {
        "header": "1.0 Running your first container",
        "content": "It’s time to get your hands dirty! As with all things technical, a “hello world” app is good place to start. Type or click the code below to run your first Docker container:\n\nThat’s it: your first container. The hello-world container output tells you a bit about what just happened. Essentially, the Docker engine running in your terminal tried to find an image named hello-world. Since you just got started there are no images stored locally (Unable to find image...) so Docker engine goes to its default Docker registry, which is Docker Hub, to look for an image named “hello-world”. It finds the image there, pulls it down, and then runs it in a container. And hello-world’s only function is to output the text you see in your terminal, after which the container exits.\n\nIf you are familiar with VMs, you may be thinking this is pretty much just like running a virtual machine, except with a central repository of VM images. And in this simple example, that is basically true. But as you go through these exercises you will start to see important ways that Docker and containers differ from VMs. For now, the simple explanation is this:\n\n• The VM is a hardware abstraction: it takes physical CPUs and RAM from a host, and divides and shares it across several smaller virtual machines. There is an OS and application running inside the VM, but the virtualization software usually has no real knowledge of that.\n• A container is an application abstraction: the focus is really on the OS and the application, and not so much the hardware abstraction. Many customers actually use both VMs and containers today in their environments and, in fact, may run containers inside of VMs.",
        "code_examples": [
          "```\ndocker container run hello-world\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1678
        }
      },
      {
        "header": "1.1 Docker Images",
        "content": "In this rest of this lab, you are going to run an Alpine Linux container. Alpine is a lightweight Linux distribution so it is quick to pull down and run, making it a popular starting point for many other images.\n\nTo get started, let’s run the following in our terminal:\n\nThe pull command fetches the alpine image from the Docker registry and saves it in our system. In this case the registry is Docker Hub. You can change the registry, but that’s a different lab.\n\nYou can use the docker image command to see a list of all images on your system.",
        "code_examples": [
          "```\ndocker image pull alpine\n```",
          "```\ndocker image ls\n```",
          "```\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nalpine                 latest              c51f86c28340        4 weeks ago         1.109 MB\nhello-world             latest              690ed74de00f        5 months ago        960 B\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 545
        }
      },
      {
        "header": "1.1 Docker Container Run",
        "content": "Great! Let’s now run a Docker container based on this image. To do that you are going to use the docker container run command.\n\nWhile the output of the ls command may not be all that exciting, behind the scenes quite a few things just took place. When you call run, the Docker client finds the image (alpine in this case), creates the container and then runs a command in that container. When you run docker container run alpine, you provided a command (ls -l), so Docker executed this command inside the container for which you saw the directory listing. After the ls command finished, the container shut down.\n\nThe fact that the container exited after running our command is important, as you will start to see. Let’s try something more exciting. Type in the following:\n\nAnd you should get the following output:\n\nIn this case, the Docker client dutifully ran the echo command inside our alpine container and then exited. If you noticed, all of that happened pretty quickly and again our container exited. As you will see in a few more steps, the echo command ran in a separate container instance. Imagine booting up a virtual machine (VM), running a command and then killing it; it would take a minute or two just to boot the VM before running the command. A VM has to emulate a full hardware stack, boot an operating system, and then launch your app - it’s a virtualized hardware environment. Docker containers function at the application layer so they skip most of the steps VMs require and just run what is required for the app. Now you know why they say containers are fast!\n\nTry another command.\n\nWait, nothing happened! Is that a bug? No! In fact, something did happen. You started a 3rd instance of the alpine container and it ran the command /bin/sh and then exited. You did not supply any additional commands to /bin/sh so it just launched the shell, exited the shell, and then stopped the container. What you might have expected was an interactive shell where you could type some commands. Docker has a facility for that by adding a flag to run the container in an interactive terminal. For this example, type the following:\n\nYou are now inside the container running a Linux shell and you can try out a few commands like ls -l, uname -a and others. Note that Alpine is a small Linux OS so several commands might be missing. Exit out of the shell and container by typing the exit command.\n\nOk, we said that we had run each of our commands above in a separate container instance. We can see these instances using the docker container ls command. The docker container ls command by itself shows you all containers that are currently running:\n\nSince no containers are running, you see a blank line. Let’s try a more useful variant: docker container ls -a\n\nWhat you see now is a list of all containers that you ran. Notice that the STATUS column shows that these containers exited some time ago.\n\nHere is the same output of the docker container ls -a command, shown diagrammatically (note that your container IDs and names will be different):\n\nIt makes sense to spend some time getting comfortable with the docker run commands. To find out more about run, use docker container run --help to see a list of all flags it supports. As you proceed further, we’ll see a few more variants of docker container run but feel free to experiment here before proceeding.",
        "code_examples": [
          "```\ndocker container run alpine ls -l\n```",
          "```\ntotal 48\ndrwxr-xr-x    2 root     root          4096 Mar  2 16:20 bin\ndrwxr-xr-x    5 root     root           360 Mar 18 09:47 dev\ndrwxr-xr-x   13 root     root          4096 Mar 18 09:47 etc\ndrwxr-xr-x    2 root     root          4096 Mar  2 16:20 home\ndrwxr-xr-x    5 root     root          4096 Mar  2 16:20 lib\n......\n......\n```",
          "```\ndocker container run alpine echo \"hello from alpine\"\n```",
          "```\nhello from alpine\n```",
          "```\ndocker container run alpine /bin/sh\n```",
          "```\ndocker container run -it alpine /bin/sh\n```",
          "```\ndocker container ls\n```",
          "```\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n```",
          "```\ndocker container ls -a\n```",
          "```\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n36171a5da744        alpine              \"/bin/sh\"                5 minutes ago       Exited (0) 2 minutes ago                        fervent_newton\na6a9d46d0b2f        alpine             \"echo 'hello from alp\"    6 minutes ago       Exited (0) 6 minutes ago                        lonely_kilby\nff0a5c3750b9        alpine             \"ls -l\"                   8 minutes ago       Exited (0) 8 minutes ago                        elated_ramanujan\nc317d0a9e3d2        hello-world         \"/hello\"                 34 seconds ago      Exited (0) 12 minutes ago                       stupefied_mcclintock\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 13,
          "content_length": 3364
        }
      },
      {
        "header": "1.2 Container Isolation",
        "content": "In the steps above we ran several commands via container instances with the help of docker container run. The docker container ls -a command showed us that there were several containers listed. Why are there so many containers listed if they are all from the alpine image?\n\nThis is a critical security concept in the world of Docker containers! Even though each docker container run command used the same alpine image, each execution was a separate, isolated container. Each container has a separate filesystem and runs in a different namespace; by default a container has no way of interacting with other containers, even those from the same image. Let’s try another exercise to learn more about isolation.\n\nThe /bin/ash is another type of shell available in the alpine image. Once the container launches and you are at the container’s command prompt type the following commands:\n\nThe first echo command creates a file called “hello.txt” with the words “hello world” inside it. The second command gives you a directory listing of the files and should show your newly created “hello.txt” file. Now type exit to leave this container.\n\nTo show how isolation works, run the following:\n\nIt is the same ls command we used inside the container’s interactive ash shell, but this time, did you notice that your “hello.txt” file is missing? That’s isolation! Your command ran in a new and separate instance, even though it is based on the same image. The 2nd instance has no way of interacting with the 1st instance because the Docker Engine keeps them separated and we have not setup any extra parameters that would enable these two instances to interact.\n\nIn every day work, Docker users take advantage of this feature not only for security, but to test the effects of making application changes. Isolation allows users to quickly create separate, isolated test copies of an application or service and have them run side-by-side without interfering with one another. In fact, there is a whole lifecycle where users take their changes and move them up to production using this basic concept and the built-in capabilities of Docker Enteprise. We will explore more of that in later exercises.\n\nRight now, the obvious question is “how do I get back to the container that has my ‘hello.txt’ file?”\n\ncommand again and you should see output similar to the following:\n\nGraphically this is what happened on our Docker Engine:\n\nThe container in which we created the “hello.txt” file is the same one where we used the /bin/ash shell, which we can see listed in the “COMMAND” column. The Container ID number from the first column uniquely identifies that particular container instance. In the sample output above the container ID is 3030c9c91e12. We can use a slightly different command to tell Docker to run this specific container instance. Try typing:\n\nNow use the docker container ls command again to list the running containers.\n\nNotice this time that our container instance is still running. We used the ash shell this time so the rather than simply exiting the way /bin/sh did earlier, ash waits for a command. We can send a command in to the container to run by using the exec command, as follows:\n\nThis time we get a directory listing and it shows our “hello.txt” file because we used the container instance where we created that file.\n\nNow you are starting to see some of the important concepts of containers. In the next exercise we will start to see how you can create your own Docker images and how to use a Dockerfile to standardize images such that you can create larger, more complex images in a simple, automated manner.\n\n• Pro tip: Instead of using the full container ID you can use just the first few characters, as long as they are enough to uniquely ID a container. So we could simply use “3030” to identify the container instance in the example above, since no other containers in this list start with these characters.",
        "code_examples": [
          "```\ndocker container run -it alpine /bin/ash\n```",
          "```\necho \"hello world\" > hello.txt\n\n ls\n```",
          "```\ndocker container run alpine ls\n```",
          "```\ndocker container ls -a\n```",
          "```\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n36171a5da744        alpine              \"ls\"                     2 minutes ago       Exited (0) 2 minutes ago                        distracted_bhaskara\n3030c9c91e12        alpine              \"/bin/ash\"               5 minutes ago       Exited (0) 2 minutes ago                        fervent_newton\na6a9d46d0b2f        alpine             \"echo 'hello from alp\"    6 minutes ago       Exited (0) 6 minutes ago                        lonely_kilby\nff0a5c3750b9        alpine             \"ls -l\"                   8 minutes ago       Exited (0) 8 minutes ago                        elated_ramanujan\nc317d0a9e3d2        hello-world         \"/hello\"                 34 seconds ago      Exited (0) 12 minutes ago                       stupefied_mcclintock\n```",
          "```\ndocker container start <container ID>\n```",
          "```\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n3030c9c91e12        alpine              \"/bin/ash\"                2 minutes ago       Up 14 seconds                        distracted_bhaskara\n```",
          "```\ndocker container exec <container ID> ls\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 3922
        }
      },
      {
        "header": "1.3 Terminology",
        "content": "In the last section, you saw a lot of Docker-specific jargon which might be confusing to some. So before you go further, let’s clarify some terminology that is used frequently in the Docker ecosystem.\n\nWhere do images get pulled from by default when not found locally?\n\nWhich command lists your Docker images?\n\n• Images - The file system and configuration of our application which are used to create containers. To find out more about a Docker image, run docker image inspect alpine. In the demo above, you used the docker image pull command to download the alpine image. When you executed the command docker container run hello-world, it also did a docker image pull behind the scenes to download the hello-world image.\n• Containers - Running instances of Docker images — containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. You created a container using docker run which you did using the alpine image that you downloaded. A list of running containers can be seen using the docker container ls command.\n• Docker daemon - The background service running on the host that manages building, running and distributing Docker containers.\n• Docker client - The command line tool that allows the user to interact with the Docker daemon.\n• Docker Hub - Store is, among other things, a registry of Docker images. You can think of the registry as a directory of all available Docker images. You’ll be using this later in this tutorial.\n\n• ( ) Docker Trusted Registry\n• (x) Docker Hub\n• ( ) There is no default\n• ( ) Docker Store\n\n• (x) docker image ls\n• ( ) docker run\n• ( ) docker container ls",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1742
        }
      }
    ],
    "url": "https://training.play-with-docker.com/ops-s1-hello/",
    "doc_type": "docker",
    "total_sections": 6
  },
  {
    "title": "Doing More With Docker Images",
    "summary": "In the previous exercise you pulled down images from Docker Store to run in your containers. Then you ran multiple instances and noted how each instance was isolated from the others. We hinted that this is used in many production IT environments every day but obviously we need a few more tools in our belt to get to the point where Docker can become a true time & money saver. First thing you may want to do is figure out how to create our own images. While there are over 700K images on Docker Stor",
    "sections": [
      {
        "header": "",
        "content": "In the previous exercise you pulled down images from Docker Store to run in your containers. Then you ran multiple instances and noted how each instance was isolated from the others. We hinted that this is used in many production IT environments every day but obviously we need a few more tools in our belt to get to the point where Docker can become a true time & money saver.\n\nFirst thing you may want to do is figure out how to create our own images. While there are over 700K images on Docker Store it is almost certain that none of them are exactly what you run in your data center today. Even something as common as a Windows OS image would get its own tweaks before you actually run it in production. In the first lab, we created a file called “hello.txt” in one of our container instances. If that instance of our Alpine container was something we wanted to re-use in future containers and share with others, we would need to create a custom image that everyone could use.\n\nWe will start with the simplest form of image creation, in which we simply commit one of our container instances as an image. Then we will explore a much more powerful and useful method for creating images: the Dockerfile.\n\nWe will then see how to get the details of an image through the inspection and explore the filesystem to have a better understanding of what happens under the hood.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1370
        }
      },
      {
        "header": "Image creation from a container",
        "content": "Let’s start by running an interactive shell in a ubuntu container:\n\nAs you know from earlier labs, you just grabbed the image called “ubuntu” from Docker Store and are now running the bash shell inside that container.1\n\nTo customize things a little bit we will install a package called figlet in this container. Your container should still be running so type the following commands at your ubuntu container command line:\n\nYou should see the words “hello docker” printed out in large ascii characters on the screen. Go ahead and exit from this container\n\nNow let us pretend this new figlet application is quite useful and you want to share it with the rest of your team. You could tell them to do exactly what you did above and install figlet in to their own container, which is simple enough in this example. But if this was a real world application where you had just installed several packages and run through a number of configuration steps the process could get cumbersome and become quite error prone. Instead, it would be easier to create an image you can share with your team.\n\nTo start, we need to get the ID of this container using the ls command (do not forget the -a option as the non running container are not returned by the ls command).\n\nBefore we create our own image, we might want to inspect all the changes we made. Try typing the command docker container diff <container ID> for the container you just created. You should see a list of all the files that were added to or changed in the container when you installed figlet. Docker keeps track of all of this information for us. This is part of the layer concept we will explore in a few minutes.\n\nNow, to create an image we need to “commit” this container. Commit creates an image locally on the system running the Docker engine. Run the following command, using the container ID you retrieved, in order to commit the container and create an image out of it.\n\nThat’s it - you have created your first image! Once it has been commited, we can see the newly created image in the list of available images.\n\nYou should see something like this:\n\nNote that the image we pulled down in the first step (ubuntu) is listed here along with our own custom image. Except our custom image has no information in the REPOSITORY or TAG columns, which would make it tough to identify exactly what was in this container if we wanted to share amongst multiple team members.\n\nAdding this information to an image is known as tagging an image. From the previous command, get the ID of the newly created image and tag it so it’s named ourfiglet:\n\nNow we have the more friendly name “ourfiglet” that we can use to identify our image.\n\nHere is a graphical view of what we just completed:\n\nNow we will run a container based on the newly created ourfiglet image:\n\nAs the figlet package is present in our ourfiglet image, the command returns the following output:\n\nThis example shows that we can create a container, add all the libraries and binaries in it and then commit it in order to create an image. We can then use that image just as we would for images pulled down from the Docker Store. We still have a slight issue in that our image is only stored locally. To share the image we would want to push the image to a registry somewhere. This is beyond the scope of this lab (and you should not enter any personal login information in these labs) but you can get a free Docker ID, run these labs, and push to the Docker Community Hub from your own system using Docker for Windows or Docker for Mac if you want to try this out.\n\nAs mentioned above, this approach of manually installing software in a container and then committing it to a custom image is just one way to create an image. It works fine and is quite common. However, there is a more powerful way to create images. In the following exercise we will see how images are created using a Dockerfile, which is a text file that contains all the instructions to build an image.",
        "code_examples": [
          "```\ndocker container run -ti ubuntu bash\n```",
          "```\napt-get update\napt-get install -y figlet\nfiglet \"hello docker\"\n```",
          "```\ndocker container ls -a\n```",
          "```\ndocker container commit CONTAINER_ID\n```",
          "```\ndocker image ls\n```",
          "```\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n<none>              <none>              a104f9ae9c37        46 seconds ago      160MB\nubuntu              latest              14f60031763d        4 days ago          120MB\n```",
          "```\ndocker image tag <IMAGE_ID> ourfiglet\ndocker image ls\n```",
          "```\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nourfiglet           latest              a104f9ae9c37        5 minutes ago       160MB\nubuntu              latest              14f60031763d        4 days ago          120MB\n```",
          "```\ndocker container run ourfiglet figlet hello\n```",
          "```\n_          _ _\n| |__   ___| | | ___\n| '_ \\ / _ \\ | |/ _ \\\n| | | |  __/ | | (_) |\n|_| |_|\\___|_|_|\\___/\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 3975
        }
      },
      {
        "header": "Image creation using a Dockerfile",
        "content": "Instead of creating a static binary image, we can use a file called a Dockerfile to create an image. The final result is essentially the same, but with a Dockerfile we are supplying the instructions for building the image, rather than just the raw binary files. This is useful because it becomes much easier to manage changes, especially as your images get bigger and more complex.\n\nFor example, if a new version of figlet is released we would either have to re-create our image from scratch, or run our image and upgrade the installed version of figlet. In contrast, a Dockerfile would include the apt-get commands we used to install figlet so that we - or anybody using the Dockerfile - could simply recompose the image using those instructions.\n\nIt is kind of like the old adage:\n\nGive a sysadmin an image and their app will be up-to-date for a day, give a sysadmin a Dockerfile and their app will always be up-to-date.\n\nOk, maybe that’s a bit of a stretch but Dockerfiles are powerful because they allow us to manage how an image is built, rather than just managing binaries. In practice, Dockerfiles can be managed the same way you might manage source code: they are simply text files so almost any version control system can be used to manage Dockerfiles over time.\n\nWe will use a simple example in this section and build a “hello world” application in Node.js. Do not be concerned if you are not familiar with Node.js: Docker (and this exercise) does not require you to know all these details.\n\nWe will start by creating a file in which we retrieve the hostname and display it. NOTE: You should be at the Docker host’s command line ($). If you see a command line that looks similar to root@abcd1234567:/# then you are probably still inside your ubuntu container from the previous exercise. Type exit to return to the host command line.\n\nType the following content into a file named index.js. You can use vi, vim or several other Linux editors in this exercise. If you need assistance with the Linux editor commands to do this follow this footnote2.\n\nThe file we just created is the javascript code for our server. As you can probably guess, Node.js will simply print out a “hello” message. We will Docker-ize this application by creating a Dockerfile. We will use alpine as the base OS image, add a Node.js runtime and then copy our source code in to the container. We will also specify the default command to be run upon container creation.\n\nCreate a file named Dockerfile and copy the following content into it. Again, help creating this file with Linux editors is here 3.\n\nLet’s build our first image out of this Dockerfile and name it hello:v0.1:\n\nThis is what you just completed:\n\nWe then start a container to check that our applications runs correctly:\n\nYou should then have an output similar to the following one (the ID will be different though).\n\nWhat just happened? We created two files: our application code (index.js) is a simple bit of javascript code that prints out a message. And the Dockerfile is the instructions for Docker engine to create our custom container. This Dockerfile does the following:\n\nRecall that in previous labs we put commands like echo \"hello world\" on the command line. With a Dockerfile we can specify precise commands to run for everyone who uses this container. Other users do not have to build the container themselves once you push your container up to a repository (which we will cover later) or even know what commands are used. The Dockerfile allows us to specify how to build a container so that we can repeat those steps precisely everytime and we can specify what the container should do when it runs. There are actually multiple methods for specifying the commands and accepting parameters a container will use, but for now it is enough to know that you have the tools to create some pretty powerful containers.\n\n• Specifies a base image to pull FROM - the alpine image we used in earlier labs.\n• Then it RUNs two commands (apk update and apk add) inside that container which installs the Node.js server.\n• Then we told it to COPY files from our working directory in to the container. The only file we have right now is our index.js.\n• Next we specify the WORKDIR - the directory the container should use when it starts up\n• And finally, we gave our container a command (CMD) to run when the container starts.",
        "code_examples": [
          "```\nvar os = require(\"os\");\nvar hostname = os.hostname();\nconsole.log(\"hello from \" + hostname);\n```",
          "```\nFROM alpine\nRUN apk update && apk add nodejs\nCOPY . /app\nWORKDIR /app\nCMD [\"node\",\"index.js\"]\n```",
          "```\ndocker image build -t hello:v0.1 .\n```",
          "```\ndocker container run hello:v0.1\n```",
          "```\nhello from 92d79b6de29f\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 16,
          "content_length": 4366
        }
      },
      {
        "header": "Image layers",
        "content": "There is something else interesting about the images we build with Docker. When running they appear to be a single OS and application. But the images themselves are actually built in layers. If you scroll back and look at the output from your docker image build command you will notice that there were 5 steps and each step had several tasks. You should see several “fetch” and “pull” tasks where Docker is grabbing various bits from Docker Store or other places. These bits were used to create one or more container layers. Layers are an important concept. To explore this, we will go through another set of exercises.\n\nFirst, check out the image you created earlier by using the history command (remember to use the docker image ls command from earlier exercises to find your image IDs):\n\nWhat you see is the list of intermediate container images that were built along the way to creating your final Node.js app image. Some of these intermediate images will become layers in your final container image. In the history command output, the original Alpine layers are at the bottom of the list and then each customization we added in our Dockerfile is its own step in the output. This is a powerful concept because it means that if we need to make a change to our application, it may only affect a single layer! To see this, we will modify our app a bit and create a new image.\n\nType the following in to your console window:\n\nThis will add a new line to the bottom of your index.js file from earlier so your application will output one additional line of text. Now we will build a new image using our updated code. We will also tag our new image to mark it as a new version so that anybody consuming our images later can identify the correct version to use:\n\nYou should see output similar to this:\n\nNotice something interesting in the build steps this time. In the output it goes through the same five steps, but notice that in some steps it says Using cache.\n\nDocker recognized that we had already built some of these layers in our earlier image builds and since nothing had changed in those layers it could simply use a cached version of the layer, rather than pulling down code a second time and running those steps. Docker’s layer management is very useful to IT teams when patching systems, updating or upgrading to the latest version of code, or making configuration changes to applications. Docker is intelligent enough to build the container in the most efficient way possible, as opposed to repeatedly building an image from the ground up each and every time.",
        "code_examples": [
          "```\ndocker image history <image ID>\n```",
          "```\necho \"console.log(\\\"this is v0.2\\\");\" >> index.js\n```",
          "```\ndocker image build -t hello:v0.2 .\n```",
          "```\nSending build context to Docker daemon  86.15MB\nStep 1/5 : FROM alpine\n ---> 7328f6f8b418\nStep 2/5 : RUN apk update && apk add nodejs\n ---> Using cache\n ---> 2707762fca63\nStep 3/5 : COPY . /app\n ---> 07b2e2127db4\nRemoving intermediate container 84eb9c31320d\nStep 4/5 : WORKDIR /app\n ---> 6630eb76312c\nRemoving intermediate container ee6c9e7a5337\nStep 5/5 : CMD node index.js\n ---> Running in e079fb6000a3\n ---> e536b9dadd2f\nRemoving intermediate container e079fb6000a3\nSuccessfully built e536b9dadd2f\nSuccessfully tagged hello:v0.2\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2567
        }
      },
      {
        "header": "Image Inspection",
        "content": "Now let us reverse our thinking a bit. What if we get a container from Docker Store or another registry and want to know a bit about what is inside the container we are consuming? Docker has an inspect command for images and it returns details on the container image, the commands it runs, the OS and more.\n\nThe alpine image should already be present locally from the exercises above (use docker image ls to confirm), if it’s not, run the following command to pull it down:\n\nOnce we are sure it is there let’s inspect it.\n\nThere is a lot of information in there:\n\nWe will not go into all the details here but we can use some filters to just inspect particular details about the image. You may have noticed that the image information is in JSON format. We can take advantage of that to use the inspect command with some filtering info to just get specific data from the image.\n\nLet’s get the list of layers:\n\nAlpine is just a small base OS image so there’s just one layer:\n\nNow let’s look at our custom Hello image. You will need the image ID (use docker image ls if you need to look it up):\n\nOur Hello image is a bit more interesting (your sha256 hashes will vary):\n\nWe have three layers in our application. Recall that we had the base Alpine image (the FROM command in our Dockerfile), then we had a RUN command to install some packages, then we had a COPY command to add in our javascript code. Those are our layers! If you look closely, you can even see that both alpine and hello are using the same base layer, which we know because they have the same sha256 hash.\n\nThe tools and commands we explored in this lab are just the beginning. Docker Enterprise Edition includes private Trusted Registries with Security Scanning and Image Signing capabilities so you can further inspect and authenticate your images. In addition, there are policy controls to specify which users have access to various images, who can push and pull images, and much more.\n\nAnother important note about layers: each layer is immutable. As an image is created and successive layers are added, the new layers keep track of the changes from the layer below. When you start the container running there is an additional layer used to keep track of any changes that occur as the application runs (like the “hello.txt” file we created in the earlier exercises). This design principle is important for both security and data management. If someone mistakenly or maliciously changes something in a running container, you can very easily revert back to its original state because the base layers cannot be changed. Or you can simply start a new container instance which will start fresh from your pristine image. And applications that create and store data (databases, for example) can store their data in a special kind of Docker object called a volume, so that data can persist and be shared with other containers. We will explore volumes in a later lab.\n\nUp next, we will look at more sophisticated applications that run across several containers and use Docker Compose and Docker Swarm to define our architecture and manage it.\n\n• the layers the image is composed of\n• the driver used to store the layers\n• the architecture / OS it has been created for\n• metadata of the image",
        "code_examples": [
          "```\ndocker image pull alpine\n```",
          "```\ndocker image inspect alpine\n```",
          "```\ndocker image inspect --format \"{{ json .RootFS.Layers }}\" alpine\n```",
          "```\n[\"sha256:60ab55d3379d47c1ba6b6225d59d10e1f52096ee9d5c816e42c635ccc57a5a2b\"]\n```",
          "```\ndocker image inspect --format \"{{ json .RootFS.Layers }}\" <image ID>\n```",
          "```\n[\"sha256:5bef08742407efd622d243692b79ba0055383bbce12900324f75e56f589aedb0\",\"sha256:5ac283aaea742f843c869d28bbeaf5000c08685b5f7ba01431094a207b8a1df9\",\"sha256:2ecb254be0603a2c76880be45a5c2b028f6208714aec770d49c9eff4cbc3cf25\"]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 13,
          "content_length": 3249
        }
      },
      {
        "header": "Terminology",
        "content": "• Layers - A Docker image is built up from a series of layers. Each layer represents an instruction in the image’s Dockerfile. Each layer except the last one is read-only.\n• Dockerfile - A text file that contains all the commands, in order, needed to build a given image. The Dockerfile reference page lists the various commands and format details for Dockerfiles.\n• Volumes - A special Docker container layer that allows data to persist and be shared separately from the container itself. Think of volumes as a way to abstract and manage your persistent data separately from the application itself.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 599
        }
      },
      {
        "header": "Footnotes",
        "content": "A note on images and the public Docker Store (AKA Docker Hub): Docker registries are subdivided in to many repositories. This is the same for both our public registries like Docker Store / Docker Hub, as well as Docker Trusted Registries that you might run in your own environment. Image names must be unique and are specified in the format <repository>/<image>:<tag>. In our exercises, we pulled images called “ubuntu” and “alpine”. Since there is no repository specified we pulled from a default public repository called “library” which is maintained by us at Docker. And since we did not specify a tag, the default is to look for a tag named “latest” and use that. The tags generally specify versions (although this is not a requirement). ↩\n\nType vi index.js then once the editor loads hit the i key. You can now type each of the commands as shown in the example. When you are finished hit the <esc> key then type :wq and that will save the file and take you back to the command prompt. You can type ls at the command prompt to ensure your index.js file is there or type cat index.js to make sure all the code is in the file. If you make a mistake in the editor and you have a hard time navigating the editor it might be easier to start fresh: simply type <esc> and then :wq if you are in the editor and then when you are back to the command line type rm index.js to delete the file and then start again. ↩\n\nType vi Dockerfile then once the editor loads hit the i key. Type in each line of the Dockerfile code as shown in the example - capitalization is important! - then hit the <esc> key followed by :wq. To verify your Dockerfile exists and is correct type cat Dockerfile. If you make a mistake in the editor and you have a hard time navigating the editor it might be easier to start fresh: simply type <esc> and then :wq if you are in the editor and then when you are back to the command line type rm Dockerfile and then start again. ↩\n\n• A note on images and the public Docker Store (AKA Docker Hub): Docker registries are subdivided in to many repositories. This is the same for both our public registries like Docker Store / Docker Hub, as well as Docker Trusted Registries that you might run in your own environment. Image names must be unique and are specified in the format <repository>/<image>:<tag>. In our exercises, we pulled images called “ubuntu” and “alpine”. Since there is no repository specified we pulled from a default public repository called “library” which is maintained by us at Docker. And since we did not specify a tag, the default is to look for a tag named “latest” and use that. The tags generally specify versions (although this is not a requirement). ↩\n• Type vi index.js then once the editor loads hit the i key. You can now type each of the commands as shown in the example. When you are finished hit the <esc> key then type :wq and that will save the file and take you back to the command prompt. You can type ls at the command prompt to ensure your index.js file is there or type cat index.js to make sure all the code is in the file. If you make a mistake in the editor and you have a hard time navigating the editor it might be easier to start fresh: simply type <esc> and then :wq if you are in the editor and then when you are back to the command line type rm index.js to delete the file and then start again. ↩\n• Type vi Dockerfile then once the editor loads hit the i key. Type in each line of the Dockerfile code as shown in the example - capitalization is important! - then hit the <esc> key followed by :wq. To verify your Dockerfile exists and is correct type cat Dockerfile. If you make a mistake in the editor and you have a hard time navigating the editor it might be easier to start fresh: simply type <esc> and then :wq if you are in the editor and then when you are back to the command line type rm Dockerfile and then start again. ↩\n\n[Note] A note on images and the public Docker Store (AKA Docker Hub): Docker registries are subdivided in to many repositories. This is the same for both our public registries like Docker Store / Docker Hub, as well as Docker Trusted Registries that you might run in your own environment. Image names must be unique and are specified in the format <repository>/<image>:<tag>. In our exercises, we pulled images called “ubuntu” and “alpine”. Since there is no repository specified we pulled from a default public repository called “library” which is maintained by us at Docker. And since we did not specify a tag, the default is to look for a tag named “latest” and use that. The tags generally specify versions (although this is not a requirement). ↩ Type vi index.js then once the editor loads hit the i key. You can now type each of the commands as shown in the example. When you are finished hit the <esc> key then type :wq and that will save the file and take you back to the command prompt. You can type ls at the command prompt to ensure your index.js file is there or type cat index.js to make sure all the code is in the file. If you make a mistake in the editor and you have a hard time navigating the editor it might be easier to start fresh: simply type <esc> and then :wq if you are in the editor and then when you are back to the command line type rm index.js to delete the file and then start again. ↩ Type vi Dockerfile then once the editor loads hit the i key. Type in each line of the Dockerfile code as shown in the example - capitalization is important! - then hit the <esc> key followed by :wq. To verify your Dockerfile exists and is correct type cat Dockerfile. If you make a mistake in the editor and you have a hard time navigating the editor it might be easier to start fresh: simply type <esc> and then :wq if you are in the editor and then when you are back to the command line type rm Dockerfile and then start again. ↩",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 5839
        }
      }
    ],
    "url": "https://training.play-with-docker.com/ops-s1-images/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "Docker for Beginners",
    "summary": "In this lab, we will look at some basic Docker commands and a simple build-ship-run workflow. We’ll start by running some simple containers, then we’ll use a Dockerfile to build a custom app. Finally, we’ll look at how to use bind mounts to modify a running container as you might if you were actively developing using Docker. Difficulty: Beginner (assumes no familiarity with Docker) Time: Approximately 30 minutes Tasks: Task 0: Prerequisites Task 1: Run some simple Docker containers Task 2: Packa",
    "sections": [
      {
        "header": "",
        "content": "In this lab, we will look at some basic Docker commands and a simple build-ship-run workflow. We’ll start by running some simple containers, then we’ll use a Dockerfile to build a custom app. Finally, we’ll look at how to use bind mounts to modify a running container as you might if you were actively developing using Docker.\n\nDifficulty: Beginner (assumes no familiarity with Docker)\n\nTime: Approximately 30 minutes\n\n• Task 0: Prerequisites\n• Task 1: Run some simple Docker containers\n• Task 2: Package and run a custom app using Docker\n• Task 3: Modify a Running Website",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 573
        }
      },
      {
        "header": "Task 0: Prerequisites",
        "content": "You will need all of the following to complete this lab:\n\n• A clone of the lab’s GitHub repo.\n• A DockerID.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 107
        }
      },
      {
        "header": "Clone the Lab’s GitHub Repo",
        "content": "Use the following command to clone the lab’s repo from GitHub (you can click the command or manually type it). This will make a copy of the lab’s repo in a new sub-directory called linux_tweet_app.",
        "code_examples": [],
        "usage_examples": [
          "```\ngit clone https://github.com/dockersamples/linux_tweet_app\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 197
        }
      },
      {
        "header": "Make sure you have a DockerID",
        "content": "If you do not have a DockerID (a free login used to access Docker Hub), please visit Docker Hub and register for one. You will need this for later steps.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 153
        }
      },
      {
        "header": "Task 1: Run some simple Docker containers",
        "content": "There are different ways to use containers. These include:\n\nIn this section you’ll try each of those options and see how Docker manages the workload.\n\n• To run a single task: This could be a shell script or a custom app.\n• Interactively: This connects you to the container similar to the way you SSH into a remote server.\n• In the background: For long-running services like websites and databases.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 397
        }
      },
      {
        "header": "Run a single task in an Alpine Linux container",
        "content": "In this step we’re going to start a new container and tell it to run the hostname command. The container will start, execute the hostname command, then exit.\n\nRun the following command in your Linux console.\n\nThe output below shows that the alpine:latest image could not be found locally. When this happens, Docker automatically pulls it from Docker Hub.\n\nAfter the image is pulled, the container’s hostname is displayed (888e89a3b36b in the example below).\n\nDocker keeps a container running as long as the process it started inside the container is still running. In this case the hostname process exits as soon as the output is written. This means the container stops. However, Docker doesn’t delete resources by default, so the container still exists in the Exited state.\n\nList all containers.\n\nNotice that your Alpine Linux container is in the Exited state.\n\nNote: The container ID is the hostname that the container displayed. In the example above it’s 888e89a3b36b.\n\nContainers which do one task and then exit can be very useful. You could build a Docker image that executes a script to configure something. Anyone can execute that task just by running the container - they don’t need the actual scripts or configuration information.\n\n• Run the following command in your Linux console. docker container run alpine hostname The output below shows that the alpine:latest image could not be found locally. When this happens, Docker automatically pulls it from Docker Hub. After the image is pulled, the container’s hostname is displayed (888e89a3b36b in the example below). Unable to find image 'alpine:latest' locally latest: Pulling from library/alpine 88286f41530e: Pull complete Digest: sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d Status: Downloaded newer image for alpine:latest 888e89a3b36b\n• Docker keeps a container running as long as the process it started inside the container is still running. In this case the hostname process exits as soon as the output is written. This means the container stops. However, Docker doesn’t delete resources by default, so the container still exists in the Exited state. List all containers. docker container ls --all Notice that your Alpine Linux container is in the Exited state. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 888e89a3b36b alpine \"hostname\" 50 seconds ago Exited (0) 49 seconds ago awesome_elion Note: The container ID is the hostname that the container displayed. In the example above it’s 888e89a3b36b.",
        "code_examples": [
          "```\ndocker container run alpine hostname\n```",
          "```\nUnable to find image 'alpine:latest' locally\n latest: Pulling from library/alpine\n 88286f41530e: Pull complete\n Digest: sha256:f006ecbb824d87947d0b51ab8488634bf69fe4094959d935c0c103f4820a417d\n Status: Downloaded newer image for alpine:latest\n 888e89a3b36b\n```",
          "```\ndocker container ls --all\n```",
          "```\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS            PORTS               NAMES\n 888e89a3b36b        alpine              \"hostname\"          50 seconds ago      Exited (0) 49 seconds ago                       awesome_elion\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2505
        }
      },
      {
        "header": "Run an interactive Ubuntu container",
        "content": "You can run a container based on a different version of Linux than is running on your Docker host.\n\nIn the next example, we are going to run an Ubuntu Linux container on top of an Alpine Linux Docker host (Play With Docker uses Alpine Linux for its nodes).\n\nRun a Docker container and access its shell.\n\nIn this example, we’re giving Docker three parameters:\n\nThe first two parameters allow you to interact with the Docker container.\n\nWe’re also telling the container to run bash as its main process (PID 1).\n\nWhen the container starts you’ll drop into the bash shell with the default prompt root@<container id>:/#. Docker has attached to the shell in the container, relaying input and output between your local session and the shell session in the container.\n\nRun the following commands in the container.\n\nls / will list the contents of the root directory in the container, ps aux will show running processes in the container, cat /etc/issue will show which Linux distro the container is running, in this case Ubuntu 20.04.3 LTS.\n\nType exit to leave the shell session. This will terminate the bash process, causing the container to exit.\n\nNote: As we used the --rm flag when we started the container, Docker removed the container when it stopped. This means if you run another docker container ls --all you won’t see the Ubuntu container.\n\nFor fun, let’s check the version of our host VM.\n\nNotice that our host VM is running Alpine Linux, yet we were able to run an Ubuntu container. As previously mentioned, the distribution of Linux inside the container does not need to match the distribution of Linux running on the Docker host.\n\nHowever, Linux containers require the Docker host to be running a Linux kernel. For example, Linux containers cannot run directly on Windows Docker hosts. The same is true of Windows containers - they need to run on a Docker host with a Windows kernel.\n\nInteractive containers are useful when you are putting together your own image. You can run a container and verify all the steps you need to deploy your app, and capture them in a Dockerfile.\n\nYou can commit a container to make an image from it - but you should avoid that wherever possible. It’s much better to use a repeatable Dockerfile to build your image. You’ll see that shortly.\n\n• Run a Docker container and access its shell. docker container run --interactive --tty --rm ubuntu bash In this example, we’re giving Docker three parameters: --interactive says you want an interactive session. --tty allocates a pseudo-tty. --rm tells Docker to go ahead and remove the container when it’s done executing. The first two parameters allow you to interact with the Docker container. We’re also telling the container to run bash as its main process (PID 1). When the container starts you’ll drop into the bash shell with the default prompt root@<container id>:/#. Docker has attached to the shell in the container, relaying input and output between your local session and the shell session in the container.\n• Run the following commands in the container. ls / will list the contents of the root directory in the container, ps aux will show running processes in the container, cat /etc/issue will show which Linux distro the container is running, in this case Ubuntu 20.04.3 LTS. ls / ps aux cat /etc/issue\n• Type exit to leave the shell session. This will terminate the bash process, causing the container to exit. exit Note: As we used the --rm flag when we started the container, Docker removed the container when it stopped. This means if you run another docker container ls --all you won’t see the Ubuntu container.\n• For fun, let’s check the version of our host VM. cat /etc/issue You should see: Welcome to Alpine Linux 3.8 Kernel \\r on an \\m (\\l)\n\n• --interactive says you want an interactive session.\n• --tty allocates a pseudo-tty.\n• --rm tells Docker to go ahead and remove the container when it’s done executing.",
        "code_examples": [
          "```\ndocker container run --interactive --tty --rm ubuntu bash\n```",
          "```\nps aux\n```",
          "```\ncat /etc/issue\n```",
          "```\ncat /etc/issue\n```",
          "```\nWelcome to Alpine Linux 3.8\n Kernel \\r on an \\m (\\l)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 16,
          "content_length": 3912
        }
      },
      {
        "header": "Run a background MySQL container",
        "content": "Background containers are how you’ll run most applications. Here’s a simple example using MySQL.\n\nRun a new MySQL container with the following command.\n\nAs the MySQL image was not available locally, Docker automatically pulled it from Docker Hub.\n\nAs long as the MySQL process is running, Docker will keep the container running in the background.\n\nList the running containers.\n\nNotice your container is running.\n\nYou can check what’s happening in your containers by using a couple of built-in Docker commands: docker container logs and docker container top.\n\nThis shows the logs from the MySQL Docker container.\n\nLet’s look at the processes running inside the container.\n\nYou should see the MySQL daemon (mysqld) is running in the container.\n\nAlthough MySQL is running, it is isolated within the container because no network ports have been published to the host. Network traffic cannot reach containers from the host unless ports are explicitly published.\n\nList the MySQL version using docker container exec.\n\ndocker container exec allows you to run a command inside a container. In this example, we’ll use docker container exec to run the command-line equivalent of mysql --user=root --password=$MYSQL_ROOT_PASSWORD --version inside our MySQL container.\n\nYou will see the MySQL version number, as well as a handy warning.\n\nYou can also use docker container exec to connect to a new shell process inside an already-running container. Executing the command below will give you an interactive shell (sh) inside your MySQL container.\n\nNotice that your shell prompt has changed. This is because your shell is now connected to the sh process running inside of your container.\n\nLet’s check the version number by running the same command again, only this time from within the new shell session in the container.\n\nNotice the output is the same as before.\n\nType exit to leave the interactive shell session.\n\n• Run a new MySQL container with the following command. docker container run \\ --detach \\ --name mydb \\ -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ mysql:latest --detach will run the container in the background. --name will name it mydb. -e will use an environment variable to specify the root password (NOTE: This should never be done in production). As the MySQL image was not available locally, Docker automatically pulled it from Docker Hub. Unable to find image 'mysql:latest' locallylatest: Pulling from library/mysql aa18ad1a0d33: Pull complete fdb8d83dece3: Pull complete 75b6ce7b50d3: Pull complete ed1d0a3a64e4: Pull complete 8eb36a82c85b: Pull complete 41be6f1a1c40: Pull complete 0e1b414eac71: Pull complete 914c28654a91: Pull complete 587693eb988c: Pull complete b183c3585729: Pull complete 315e21657aa4: Pull complete Digest: sha256:0dc3dacb751ef46a6647234abdec2d47400f0dfbe77ab490b02bffdae57846ed Status: Downloaded newer image for mysql:latest 41d6157c9f7d1529a6c922acb8167ca66f167119df0fe3d86964db6c0d7ba4e0 As long as the MySQL process is running, Docker will keep the container running in the background.\n• List the running containers. docker container ls Notice your container is running. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3f4e8da0caf7 mysql:latest \"docker-entrypoint...\" 52 seconds ago Up 51 seconds 3306/tcp mydb\n• You can check what’s happening in your containers by using a couple of built-in Docker commands: docker container logs and docker container top. docker container logs mydb This shows the logs from the MySQL Docker container. <output truncated> 2017-09-29T16:02:58.605004Z 0 [Note] Executing 'SELECT * FROM INFORMATION_SCHEMA.TABLES;' to get a list of tables using the deprecated partition engine. You may use the startup option '--disable-partition-engine-check' to skip this check. 2017-09-29T16:02:58.605026Z 0 [Note] Beginning of list of non-natively partitioned tables 2017-09-29T16:02:58.616575Z 0 [Note] End of list of non-natively partitioned tables Let’s look at the processes running inside the container. docker container top mydb You should see the MySQL daemon (mysqld) is running in the container. PID USER TIME COMMAND 2876 999 0:00 mysqld Although MySQL is running, it is isolated within the container because no network ports have been published to the host. Network traffic cannot reach containers from the host unless ports are explicitly published.\n• List the MySQL version using docker container exec. docker container exec allows you to run a command inside a container. In this example, we’ll use docker container exec to run the command-line equivalent of mysql --user=root --password=$MYSQL_ROOT_PASSWORD --version inside our MySQL container. docker exec -it mydb \\ mysql --user=root --password=$MYSQL_ROOT_PASSWORD --version You will see the MySQL version number, as well as a handy warning. mysql: [Warning] Using a password on the command line interface can be insecure. mysql Ver 14.14 Distrib 5.7.19, for Linux (x86_64) using EditLine wrapper\n• You can also use docker container exec to connect to a new shell process inside an already-running container. Executing the command below will give you an interactive shell (sh) inside your MySQL container. docker exec -it mydb sh Notice that your shell prompt has changed. This is because your shell is now connected to the sh process running inside of your container.\n• Let’s check the version number by running the same command again, only this time from within the new shell session in the container. mysql --user=root --password=$MYSQL_ROOT_PASSWORD --version Notice the output is the same as before.\n• Type exit to leave the interactive shell session. exit\n\n• --detach will run the container in the background.\n• --name will name it mydb.\n• -e will use an environment variable to specify the root password (NOTE: This should never be done in production).",
        "code_examples": [
          "```\ndocker container run \\\n --detach \\\n --name mydb \\\n -e MYSQL_ROOT_PASSWORD=my-secret-pw \\\n mysql:latest\n```",
          "```\nUnable to find image 'mysql:latest' locallylatest: Pulling from library/mysql\n aa18ad1a0d33: Pull complete\n fdb8d83dece3: Pull complete\n 75b6ce7b50d3: Pull complete\n ed1d0a3a64e4: Pull complete\n 8eb36a82c85b: Pull complete\n 41be6f1a1c40: Pull complete\n 0e1b414eac71: Pull complete\n 914c28654a91: Pull complete\n 587693eb988c: Pull complete\n b183c3585729: Pull complete\n 315e21657aa4: Pull complete\n Digest: sha256:0dc3dacb751ef46a6647234abdec2d47400f0dfbe77ab490b02bffdae57846ed\n Status: Downloaded newer image for mysql:latest\n 41d6157c9f7d1529a6c922acb8167ca66f167119df0fe3d86964db6c0d7ba4e0\n```",
          "```\ndocker container ls\n```",
          "```\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS            NAMES\n 3f4e8da0caf7        mysql:latest        \"docker-entrypoint...\"   52 seconds ago      Up 51 seconds       3306/tcp            mydb\n```",
          "```\ndocker container logs mydb\n```",
          "```\n<output truncated>\n   2017-09-29T16:02:58.605004Z 0 [Note] Executing 'SELECT * FROM INFORMATION_SCHEMA.TABLES;' to get a list of tables using the deprecated partition engine. You may use the startup option '--disable-partition-engine-check' to skip this check.\n   2017-09-29T16:02:58.605026Z 0 [Note] Beginning of list of non-natively partitioned tables\n   2017-09-29T16:02:58.616575Z 0 [Note] End of list of non-natively partitioned tables\n```",
          "```\ndocker container top mydb\n```",
          "```\nPID                 USER                TIME                COMMAND\n 2876                999                 0:00                mysqld\n```",
          "```\nmysql: [Warning] Using a password on the command line interface can be insecure.\n mysql  Ver 14.14 Distrib 5.7.19, for Linux (x86_64) using  EditLine wrapper\n```",
          "```\ndocker exec -it mydb sh\n```"
        ],
        "usage_examples": [
          "```\ndocker exec -it mydb \\\n mysql --user=root --password=$MYSQL_ROOT_PASSWORD --version\n```",
          "```\nmysql --user=root --password=$MYSQL_ROOT_PASSWORD --version\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 5785
        }
      },
      {
        "header": "Task 2: Package and run a custom app using Docker",
        "content": "In this step you’ll learn how to package your own apps as Docker images using a Dockerfile.\n\nThe Dockerfile syntax is straightforward. In this task, we’re going to create a simple NGINX website from a Dockerfile.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 212
        }
      },
      {
        "header": "Build a simple website image",
        "content": "Let’s have a look at the Dockerfile we’ll be using, which builds a simple website that allows you to send a tweet.\n\nMake sure you’re in the linux_tweet_app directory.\n\nDisplay the contents of the Dockerfile.\n\nLet’s see what each of these lines in the Dockerfile do.\n\nIn order to make the following commands more copy/paste friendly, export an environment variable containing your DockerID (if you don’t have a DockerID you can get one for free via Docker Hub).\n\nYou will have to manually type this command as it requires your unique DockerID.\n\nexport DOCKERID=<your docker id>\n\nEcho the value of the variable back to the terminal to ensure it was stored correctly.\n\nUse the docker image build command to create a new Docker image using the instructions in the Dockerfile.\n\nBe sure to include period (.) at the end of the command.\n\nThe output below shows the Docker daemon executing each line in the Dockerfile\n\nUse the docker container run command to start a new container from the image you created.\n\nAs this container will be running an NGINX web server, we’ll use the --publish flag to publish port 80 inside the container onto port 80 on the host. This will allow traffic coming in to the Docker host on port 80 to be directed to port 80 in the container. The format of the --publish flag is host_port:container_port.\n\nAny external traffic coming into the server on port 80 will now be directed into the container on port 80.\n\nIn a later step you will see how to map traffic from two different ports - this is necessary when two containers use the same port to communicate since you can only expose the port once on the host.\n\nClick here to load the website which should be running.\n\nOnce you’ve accessed your website, shut it down and remove it.\n\nNote: We used the --force parameter to remove the running container without shutting it down. This will ungracefully shutdown the container and permanently remove it from the Docker host.\n\nIn a production environment you may want to use docker container stop to gracefully stop the container and leave it on the host. You can then use docker container rm to permanently remove it.\n\n• Make sure you’re in the linux_tweet_app directory. cd ~/linux_tweet_app\n• Display the contents of the Dockerfile. cat Dockerfile FROM nginx:latest COPY index.html /usr/share/nginx/html COPY linux.png /usr/share/nginx/html EXPOSE 80 443 CMD [\"nginx\", \"-g\", \"daemon off;\"] Let’s see what each of these lines in the Dockerfile do. FROM specifies the base image to use as the starting point for this new image you’re creating. For this example we’re starting from nginx:latest. COPY copies files from the Docker host into the image, at a known location. In this example, COPY is used to copy two files into the image: index.html. and a graphic that will be used on our webpage. EXPOSE documents which ports the application uses. CMD specifies what command to run when a container is started from the image. Notice that we can specify the command, as well as run-time arguments.\n• In order to make the following commands more copy/paste friendly, export an environment variable containing your DockerID (if you don’t have a DockerID you can get one for free via Docker Hub). You will have to manually type this command as it requires your unique DockerID. export DOCKERID=<your docker id>\n• Echo the value of the variable back to the terminal to ensure it was stored correctly. echo $DOCKERID\n• Use the docker image build command to create a new Docker image using the instructions in the Dockerfile. --tag allows us to give the image a custom name. In this case it’s comprised of our DockerID, the application name, and a version. Having the Docker ID attached to the name will allow us to store it on Docker Hub in a later step . tells Docker to use the current directory as the build context Be sure to include period (.) at the end of the command. docker image build --tag $DOCKERID/linux_tweet_app:1.0 . The output below shows the Docker daemon executing each line in the Dockerfile Sending build context to Docker daemon 32.77kB Step 1/5 : FROM nginx:latest latest: Pulling from library/nginx afeb2bfd31c0: Pull complete 7ff5d10493db: Pull complete d2562f1ae1d0: Pull complete Digest: sha256:af32e714a9cc3157157374e68c818b05ebe9e0737aac06b55a09da374209a8f9 Status: Downloaded newer image for nginx:latest ---> da5939581ac8 Step 2/5 : COPY index.html /usr/share/nginx/html ---> eba2eec2bea9 Step 3/5 : COPY linux.png /usr/share/nginx/html ---> 4d080f499b53 Step 4/5 : EXPOSE 80 443 ---> Running in 47232cb5699f ---> 74c968a9165f Removing intermediate container 47232cb5699f Step 5/5 : CMD nginx -g daemon off; ---> Running in 4623761274ac ---> 12045a0df899 Removing intermediate container 4623761274ac Successfully built 12045a0df899 Successfully tagged <your docker ID>/linux_tweet_app:latest\n• Use the docker container run command to start a new container from the image you created. As this container will be running an NGINX web server, we’ll use the --publish flag to publish port 80 inside the container onto port 80 on the host. This will allow traffic coming in to the Docker host on port 80 to be directed to port 80 in the container. The format of the --publish flag is host_port:container_port. docker container run \\ --detach \\ --publish 80:80 \\ --name linux_tweet_app \\ $DOCKERID/linux_tweet_app:1.0 Any external traffic coming into the server on port 80 will now be directed into the container on port 80. In a later step you will see how to map traffic from two different ports - this is necessary when two containers use the same port to communicate since you can only expose the port once on the host.\n• Click here to load the website which should be running.\n• Once you’ve accessed your website, shut it down and remove it. docker container rm --force linux_tweet_app Note: We used the --force parameter to remove the running container without shutting it down. This will ungracefully shutdown the container and permanently remove it from the Docker host. In a production environment you may want to use docker container stop to gracefully stop the container and leave it on the host. You can then use docker container rm to permanently remove it.\n\n• FROM specifies the base image to use as the starting point for this new image you’re creating. For this example we’re starting from nginx:latest.\n• COPY copies files from the Docker host into the image, at a known location. In this example, COPY is used to copy two files into the image: index.html. and a graphic that will be used on our webpage.\n• EXPOSE documents which ports the application uses.\n• CMD specifies what command to run when a container is started from the image. Notice that we can specify the command, as well as run-time arguments.\n\n• --tag allows us to give the image a custom name. In this case it’s comprised of our DockerID, the application name, and a version. Having the Docker ID attached to the name will allow us to store it on Docker Hub in a later step\n• . tells Docker to use the current directory as the build context",
        "code_examples": [
          "```\ncd ~/linux_tweet_app\n```",
          "```\ncat Dockerfile\n```",
          "```\nFROM nginx:latest\n\n COPY index.html /usr/share/nginx/html\n COPY linux.png /usr/share/nginx/html\n\n EXPOSE 80 443     \n\n CMD [\"nginx\", \"-g\", \"daemon off;\"]\n```",
          "```\nSending build context to Docker daemon  32.77kB\n Step 1/5 : FROM nginx:latest\n latest: Pulling from library/nginx\n afeb2bfd31c0: Pull complete\n 7ff5d10493db: Pull complete\n d2562f1ae1d0: Pull complete\n Digest: sha256:af32e714a9cc3157157374e68c818b05ebe9e0737aac06b55a09da374209a8f9\n Status: Downloaded newer image for nginx:latest\n ---> da5939581ac8\n Step 2/5 : COPY index.html /usr/share/nginx/html\n ---> eba2eec2bea9\n Step 3/5 : COPY linux.png /usr/share/nginx/html\n ---> 4d080f499b53\n Step 4/5 : EXPOSE 80 443\n ---> Running in 47232cb5699f\n ---> 74c968a9165f\n Removing intermediate container 47232cb5699f\n Step 5/5 : CMD nginx -g daemon off;\n ---> Running in 4623761274ac\n ---> 12045a0df899\n Removing intermediate container 4623761274ac\n Successfully built 12045a0df899\n Successfully tagged <your docker ID>/linux_tweet_app:latest\n```",
          "```\ndocker container rm --force linux_tweet_app\n```"
        ],
        "usage_examples": [
          "```\necho $DOCKERID\n```",
          "```\ndocker image build --tag $DOCKERID/linux_tweet_app:1.0 .\n```",
          "```\ndocker container run \\\n --detach \\\n --publish 80:80 \\\n --name linux_tweet_app \\\n $DOCKERID/linux_tweet_app:1.0\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 7055
        }
      },
      {
        "header": "Task 3: Modify a running website",
        "content": "When you’re actively working on an application it is inconvenient to have to stop the container, rebuild the image, and run a new version every time you make a change to your source code.\n\nOne way to streamline this process is to mount the source code directory on the local machine into the running container. This will allow any changes made to the files on the host to be immediately reflected in the container.\n\nWe do this using something called a bind mount.\n\nWhen you use a bind mount, a file or directory on the host machine is mounted into a container running on the same host.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 585
        }
      },
      {
        "header": "Start our web app with a bind mount",
        "content": "Let’s start the web app and mount the current directory into the container.\n\nIn this example we’ll use the --mount flag to mount the current directory on the host into /usr/share/nginx/html inside the container.\n\nBe sure to run this command from within the linux_tweet_app directory on your Docker host.\n\nRemember from the Dockerfile, /usr/share/nginx/html is where the html files are stored for the web app.\n\nThe website should be running.\n\n• Let’s start the web app and mount the current directory into the container. In this example we’ll use the --mount flag to mount the current directory on the host into /usr/share/nginx/html inside the container. Be sure to run this command from within the linux_tweet_app directory on your Docker host. docker container run \\ --detach \\ --publish 80:80 \\ --name linux_tweet_app \\ --mount type=bind,source=\"$(pwd)\",target=/usr/share/nginx/html \\ $DOCKERID/linux_tweet_app:1.0 Remember from the Dockerfile, /usr/share/nginx/html is where the html files are stored for the web app.\n• The website should be running.",
        "code_examples": [],
        "usage_examples": [
          "```\ndocker container run \\\n --detach \\\n --publish 80:80 \\\n --name linux_tweet_app \\\n --mount type=bind,source=\"$(pwd)\",target=/usr/share/nginx/html \\\n $DOCKERID/linux_tweet_app:1.0\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1054
        }
      },
      {
        "header": "Modify the running website",
        "content": "Bind mounts mean that any changes made to the local file system are immediately reflected in the running container.\n\nCopy a new index.html into the container.\n\nThe Git repo that you pulled earlier contains several different versions of an index.html file. You can manually run an ls command from within the ~/linux_tweet_app directory to see a list of them. In this step we’ll replace index.html with index-new.html.\n\nGo to the running website and refresh the page. Notice that the site has changed.\n\nIf you are comfortable with vi you can use it to load the local index.html file and make additional changes. Those too would be reflected when you reload the webpage. If you are really adventurous, why not try using exec to access the running container and modify the files stored there.\n\nEven though we’ve modified the index.html local filesystem and seen it reflected in the running container, we’ve not actually changed the Docker image that the container was started from.\n\nTo show this, stop the current container and re-run the 1.0 image without a bind mount.\n\nStop and remove the currently running container.\n\nRerun the current version without a bind mount.\n\nNotice the website is back to the original version.\n\nStop and remove the current container\n\n• Copy a new index.html into the container. The Git repo that you pulled earlier contains several different versions of an index.html file. You can manually run an ls command from within the ~/linux_tweet_app directory to see a list of them. In this step we’ll replace index.html with index-new.html. cp index-new.html index.html\n• Go to the running website and refresh the page. Notice that the site has changed. If you are comfortable with vi you can use it to load the local index.html file and make additional changes. Those too would be reflected when you reload the webpage. If you are really adventurous, why not try using exec to access the running container and modify the files stored there.\n\n• Stop and remove the currently running container. docker rm --force linux_tweet_app\n• Rerun the current version without a bind mount. docker container run \\ --detach \\ --publish 80:80 \\ --name linux_tweet_app \\ $DOCKERID/linux_tweet_app:1.0\n• Notice the website is back to the original version.\n• Stop and remove the current container docker rm --force linux_tweet_app",
        "code_examples": [
          "```\ncp index-new.html index.html\n```",
          "```\ndocker rm --force linux_tweet_app\n```",
          "```\ndocker rm --force linux_tweet_app\n```"
        ],
        "usage_examples": [
          "```\ndocker container run \\\n --detach \\\n --publish 80:80 \\\n --name linux_tweet_app \\\n $DOCKERID/linux_tweet_app:1.0\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2331
        }
      },
      {
        "header": "Update the image",
        "content": "To persist the changes you made to the index.html file into the image, you need to build a new version of the image.\n\nBuild a new image and tag it as 2.0\n\nRemember that you previously modified the index.html file on the Docker hosts local filesystem. This means that running another docker image build command will build a new image with the updated index.html\n\nBe sure to include the period (.) at the end of the command.\n\nNotice how fast that built! This is because Docker only modified the portion of the image that changed vs. rebuilding the whole image.\n\nLet’s look at the images on the system.\n\nYou now have both versions of the web app on your host.\n\n• Build a new image and tag it as 2.0 Remember that you previously modified the index.html file on the Docker hosts local filesystem. This means that running another docker image build command will build a new image with the updated index.html Be sure to include the period (.) at the end of the command. docker image build --tag $DOCKERID/linux_tweet_app:2.0 . Notice how fast that built! This is because Docker only modified the portion of the image that changed vs. rebuilding the whole image.\n• Let’s look at the images on the system. docker image ls You now have both versions of the web app on your host. REPOSITORY TAG IMAGE ID CREATED SIZE <docker id>/linux_tweet_app 2.0 01612e05312b 16 seconds ago 108MB <docker id>/linux_tweet_app 1.0 bb32b5783cd3 4 minutes ago 108MB mysql latest b4e78b89bcf3 2 weeks ago 412MB ubuntu latest 2d696327ab2e 2 weeks ago 122MB nginx latest da5939581ac8 3 weeks ago 108MB alpine latest 76da55c8019d 3 weeks ago 3.97MB",
        "code_examples": [
          "```\ndocker image ls\n```",
          "```\nREPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\n <docker id>/linux_tweet_app    2.0                 01612e05312b        16 seconds ago      108MB\n <docker id>/linux_tweet_app    1.0                 bb32b5783cd3        4 minutes ago       108MB\n mysql                          latest              b4e78b89bcf3        2 weeks ago         412MB\n ubuntu                         latest              2d696327ab2e        2 weeks ago         122MB\n nginx                          latest              da5939581ac8        3 weeks ago         108MB\n alpine                         latest              76da55c8019d        3 weeks ago         3.97MB\n```"
        ],
        "usage_examples": [
          "```\ndocker image build --tag $DOCKERID/linux_tweet_app:2.0 .\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1615
        }
      },
      {
        "header": "Test the new version",
        "content": "Run a new container from the new version of the image.\n\nCheck the new version of the website (You may need to refresh your browser to get the new version to load).\n\nThe web page will have an orange background.\n\nWe can run both versions side by side. The only thing we need to be aware of is that we cannot have two containers using port 80 on the same host.\n\nAs we’re already using port 80 for the container running from the 2.0 version of the image, we will start a new container and publish it on port 8080. Additionally, we need to give our container a unique name (old_linux_tweet_app)\n\nRun another new container, this time from the old version of the image.\n\nNotice that this command maps the new container to port 8080 on the host. This is because two containers cannot map to the same port on a single Docker host.\n\nView the old version of the website.\n\n• Run a new container from the new version of the image. docker container run \\ --detach \\ --publish 80:80 \\ --name linux_tweet_app \\ $DOCKERID/linux_tweet_app:2.0\n• Check the new version of the website (You may need to refresh your browser to get the new version to load). The web page will have an orange background. We can run both versions side by side. The only thing we need to be aware of is that we cannot have two containers using port 80 on the same host. As we’re already using port 80 for the container running from the 2.0 version of the image, we will start a new container and publish it on port 8080. Additionally, we need to give our container a unique name (old_linux_tweet_app)\n• Run another new container, this time from the old version of the image. Notice that this command maps the new container to port 8080 on the host. This is because two containers cannot map to the same port on a single Docker host. docker container run \\ --detach \\ --publish 8080:80 \\ --name old_linux_tweet_app \\ $DOCKERID/linux_tweet_app:1.0\n• View the old version of the website.",
        "code_examples": [],
        "usage_examples": [
          "```\ndocker container run \\\n --detach \\\n --publish 80:80 \\\n --name linux_tweet_app \\\n $DOCKERID/linux_tweet_app:2.0\n```",
          "```\ndocker container run \\\n --detach \\\n --publish 8080:80 \\\n --name old_linux_tweet_app \\\n $DOCKERID/linux_tweet_app:1.0\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1941
        }
      },
      {
        "header": "Push your images to Docker Hub",
        "content": "List the images on your Docker host.\n\nYou will see that you now have two linux_tweet_app images - one tagged as 1.0 and the other as 2.0.\n\nThese images are only stored in your Docker hosts local repository. Your Docker host will be deleted after the workshop. In this step we’ll push the images to a public repository so you can run them from any Linux machine with Docker.\n\nDistribution is built into the Docker platform. You can build images locally and push them to a public or private registry, making them available to other users. Anyone with access can pull that image and run a container from it. The behavior of the app in the container will be the same for everyone, because the image contains the fully-configured app - the only requirements to run it are Linux and Docker.\n\nDocker Hub is the default public registry for Docker images.\n\nBefore you can push your images, you will need to log into Docker Hub.\n\nYou will need to supply your Docker ID credentials when prompted.\n\nPush version 1.0 of your web app using docker image push.\n\nYou’ll see the progress as the image is pushed up to Docker Hub.\n\nNow push version 2.0.\n\nNotice that several lines of the output say Layer already exists. This is because Docker will leverage read-only layers that are the same as any previously uploaded image layers.\n\nYou can browse to https://hub.docker.com/r/<your docker id>/ and see your newly-pushed Docker images. These are public repositories, so anyone can pull the image - you don’t even need a Docker ID to pull public images. Docker Hub also supports private repositories.\n\n• List the images on your Docker host. docker image ls -f reference=\"$DOCKERID/*\" You will see that you now have two linux_tweet_app images - one tagged as 1.0 and the other as 2.0. REPOSITORY TAG IMAGE ID CREATED SIZE <docker id>/linux_tweet_app 2.0 01612e05312b 3 minutes ago 108MB <docker id>/linux_tweet_app 1.0 bb32b5783cd3 7 minutes ago 108MB These images are only stored in your Docker hosts local repository. Your Docker host will be deleted after the workshop. In this step we’ll push the images to a public repository so you can run them from any Linux machine with Docker. Distribution is built into the Docker platform. You can build images locally and push them to a public or private registry, making them available to other users. Anyone with access can pull that image and run a container from it. The behavior of the app in the container will be the same for everyone, because the image contains the fully-configured app - the only requirements to run it are Linux and Docker. Docker Hub is the default public registry for Docker images.\n• Before you can push your images, you will need to log into Docker Hub. docker login You will need to supply your Docker ID credentials when prompted. Username: <your docker id> Password: <your docker id password> Login Succeeded\n• Push version 1.0 of your web app using docker image push. docker image push $DOCKERID/linux_tweet_app:1.0 You’ll see the progress as the image is pushed up to Docker Hub. The push refers to a repository [docker.io/<your docker id>/linux_tweet_app] 910e84bcef7a: Pushed 1dee161c8ba4: Pushed 110566462efa: Pushed 305e2b6ef454: Pushed 24e065a5f328: Pushed 1.0: digest: sha256:51e937ec18c7757879722f15fa1044cbfbf2f6b7eaeeb578c7c352baba9aa6dc size: 1363\n• Now push version 2.0. docker image push $DOCKERID/linux_tweet_app:2.0 Notice that several lines of the output say Layer already exists. This is because Docker will leverage read-only layers that are the same as any previously uploaded image layers. The push refers to a repository [docker.io/<your docker id>/linux_tweet_app] 0b171f8fbe22: Pushed 70d38c767c00: Pushed 110566462efa: Layer already exists 305e2b6ef454: Layer already exists 24e065a5f328: Layer already exists 2.0: digest: sha256:7c51f77f90b81e5a598a13f129c95543172bae8f5850537225eae0c78e4f3add size: 1363",
        "code_examples": [
          "```\nREPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\n <docker id>/linux_tweet_app    2.0                 01612e05312b        3 minutes ago       108MB\n <docker id>/linux_tweet_app    1.0                 bb32b5783cd3        7 minutes ago       108MB\n```",
          "```\ndocker login\n```",
          "```\nUsername: <your docker id>\n Password: <your docker id password>\n Login Succeeded\n```",
          "```\nThe push refers to a repository [docker.io/<your docker id>/linux_tweet_app]\n 910e84bcef7a: Pushed\n 1dee161c8ba4: Pushed\n 110566462efa: Pushed\n 305e2b6ef454: Pushed\n 24e065a5f328: Pushed\n 1.0: digest: sha256:51e937ec18c7757879722f15fa1044cbfbf2f6b7eaeeb578c7c352baba9aa6dc size: 1363\n```",
          "```\nThe push refers to a repository [docker.io/<your docker id>/linux_tweet_app]\n 0b171f8fbe22: Pushed\n 70d38c767c00: Pushed\n 110566462efa: Layer already exists\n 305e2b6ef454: Layer already exists\n 24e065a5f328: Layer already exists\n 2.0: digest: sha256:7c51f77f90b81e5a598a13f129c95543172bae8f5850537225eae0c78e4f3add size: 1363\n```"
        ],
        "usage_examples": [
          "```\ndocker image ls -f reference=\"$DOCKERID/*\"\n```",
          "```\ndocker image push $DOCKERID/linux_tweet_app:1.0\n```",
          "```\ndocker image push $DOCKERID/linux_tweet_app:2.0\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 3889
        }
      },
      {
        "header": "Next Step",
        "content": "Check out the introduction to a multi-service application stack orchestration in the Application Containerization and Microservice Orchestration tutorial.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 154
        }
      }
    ],
    "url": "https://training.play-with-docker.com/beginner-linux/",
    "doc_type": "docker",
    "total_sections": 17
  },
  {
    "title": "Swarm mode introduction",
    "summary": "This tutorial will show you how to setup a swarm and deploy your first services. Init your swarm docker swarm init --advertise-addr $(hostname -i) Copy the join command (watch out for newlines) output and paste it in the other terminal. Show members of swarm Type the below command in the first terminal: docker node ls That last line will show you a list of all the nodes, something like this: ID HOSTNAME STATUS AVAILABILITY MANAGE R STATUS kytp4gq5mrvmdbb0qpifdxeiv * node1 Ready Active Leader lz1",
    "sections": [
      {
        "header": "",
        "content": "This tutorial will show you how to setup a swarm and deploy your first services.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 80
        }
      },
      {
        "header": "Init your swarm",
        "content": "Copy the join command (watch out for newlines) output and paste it in the other terminal.",
        "code_examples": [],
        "usage_examples": [
          "```\ndocker swarm init --advertise-addr $(hostname -i)\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 89
        }
      },
      {
        "header": "Show members of swarm",
        "content": "Type the below command in the first terminal:\n\nThat last line will show you a list of all the nodes, something like this:\n\nIf you try to execute an administrative command in a non-leader node worker, you’ll get an error. Try it here:",
        "code_examples": [
          "```\ndocker node ls\n```",
          "```\nID                           HOSTNAME  STATUS  AVAILABILITY  MANAGE\nR STATUS\nkytp4gq5mrvmdbb0qpifdxeiv *  node1     Ready   Active        Leader\nlz1j4d6290j8lityk4w0cxls5    node2     Ready   Active\n```",
          "```\ndocker node ls\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 233
        }
      },
      {
        "header": "Creating services",
        "content": "The next step is to create a service and list out the services. This creates a single service called web that runs the latest nginx, type the below commands in the first terminal:\n\nYou can check that nginx is running by executing the following command:",
        "code_examples": [
          "```\ndocker service create -p 80:80 --name web nginx:latest\ndocker service ls\n```",
          "```\ncurl http://localhost:80\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 252
        }
      },
      {
        "header": "Scaling up",
        "content": "We will be performing these actions in the first terminal. Next let’s inspect the service:\n\nThat’s lots of info! Now, let’s scale the service:\n\nDocker has spread the 15 services evenly over all of the nodes",
        "code_examples": [
          "```\ndocker service inspect web\n```",
          "```\ndocker service scale web=15\n```",
          "```\ndocker service ps web\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 206
        }
      },
      {
        "header": "Updating nodes",
        "content": "You can also drain a particular node, that is remove all services from that node. The services will automatically be rescheduled on other nodes.\n\nYou can check out the nodes and see that node2 is still active but drained.",
        "code_examples": [
          "```\ndocker node update --availability drain node2\n```",
          "```\ndocker service ps web\n```",
          "```\ndocker node ls\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 221
        }
      },
      {
        "header": "Scaling down",
        "content": "You can also scale down the service\n\nLets check our service status\n\nNow bring node2 back online and show it’s new availability\n\nWhich of these can you do with Docker Swarm Mode?\n\n• start a service\n• end a service\n• list all service\n• scale up the number of replicas of a service\n• take a node out of the swarm",
        "code_examples": [
          "```\ndocker service scale web=10\n```",
          "```\ndocker service ps web\n```",
          "```\ndocker node update --availability active node2\n```",
          "```\ndocker node inspect node2 --pretty\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 309
        }
      }
    ],
    "url": "https://training.play-with-docker.com/swarm-mode-intro/",
    "doc_type": "docker",
    "total_sections": 7
  },
  {
    "title": "Dockerfile reference",
    "summary": "Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. This page describes the commands you can use in a Dockerfile. The Dockerfile supports the following instructions:",
    "sections": [
      {
        "header": "Overview",
        "content": "The Dockerfile supports the following instructions:\n\nInstruction Description ADD Add local or remote files and directories. ARG Use build-time variables. CMD Specify default commands. COPY Copy files and directories. ENTRYPOINT Specify default executable. ENV Set environment variables. EXPOSE Describe which ports your application is listening on. FROM Create a new build stage from a base image. HEALTHCHECK Check a container's health on startup. LABEL Add metadata to an image. MAINTAINER Specify the author of an image. ONBUILD Specify instructions for when the image is used in a build. RUN Execute build commands. SHELL Set the default shell of an image. STOPSIGNAL Specify the system call signal for exiting a container. USER Set user and group ID. VOLUME Create volume mounts. WORKDIR Change working directory.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 818
        }
      },
      {
        "header": "Format",
        "content": "Here is the format of the Dockerfile:\n\n# Comment INSTRUCTION arguments\n\nThe instruction is not case-sensitive. However, convention is for them to be UPPERCASE to distinguish them from arguments more easily.\n\nDocker runs instructions in a Dockerfile in order. A Dockerfile must begin with a FROM instruction . This may be after parser directives , comments , and globally scoped ARGs . The FROM instruction specifies the base image from which you are building. FROM may only be preceded by one or more ARG instructions, which declare arguments that are used in FROM lines in the Dockerfile.\n\nBuildKit treats lines that begin with # as a comment, unless the line is a valid parser directive . A # marker anywhere else in a line is treated as an argument. This allows statements like:\n\n# Comment RUN echo 'we are running some # of cool things'\n\nComment lines are removed before the Dockerfile instructions are executed. The comment in the following example is removed before the shell executes the echo command.\n\nRUN echo hello \\ # comment world\n\nThe following examples is equivalent.\n\nRUN echo hello \\ world\n\nComments don't support line continuation characters.\n\nNote Note on whitespace For backward compatibility, leading whitespace before comments ( # ) and instructions (such as RUN ) are ignored, but discouraged. Leading whitespace is not preserved in these cases, and the following examples are therefore equivalent: # this is a comment-line RUN echo hello RUN echo world # this is a comment-line RUN echo hello RUN echo world Whitespace in instruction arguments, however, isn't ignored. The following example prints hello world with leading whitespace as specified: RUN echo \"\\ hello\\ world\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1696
        }
      },
      {
        "header": "Parser directives",
        "content": "Parser directives are optional, and affect the way in which subsequent lines in a Dockerfile are handled. Parser directives don't add layers to the build, and don't show up as build steps. Parser directives are written as a special type of comment in the form # directive=value . A single directive may only be used once.\n\nThe following parser directives are supported:\n\nsyntax escape check (since Dockerfile v1.8.0)\n\nOnce a comment, empty line or builder instruction has been processed, BuildKit no longer looks for parser directives. Instead it treats anything formatted as a parser directive as a comment and doesn't attempt to validate if it might be a parser directive. Therefore, all parser directives must be at the top of a Dockerfile.\n\nParser directive keys, such as syntax or check , aren't case-sensitive, but they're lowercase by convention. Values for a directive are case-sensitive and must be written in the appropriate case for the directive. For example, #check=skip=jsonargsrecommended is invalid because the check name must use Pascal case, not lowercase. It's also conventional to include a blank line following any parser directives. Line continuation characters aren't supported in parser directives.\n\nDue to these rules, the following examples are all invalid:\n\nInvalid due to line continuation:\n\n# direc \\ tive = value\n\nInvalid due to appearing twice:\n\n# directive=value1 # directive=value2 FROM ImageName\n\nTreated as a comment because it appears after a builder instruction:\n\nFROM ImageName # directive=value\n\nTreated as a comment because it appears after a comment that isn't a parser directive:\n\n# About my dockerfile # directive=value FROM ImageName\n\nThe following unknowndirective is treated as a comment because it isn't recognized. The known syntax directive is treated as a comment because it appears after a comment that isn't a parser directive.\n\n# unknowndirective=value # syntax=value\n\nNon line-breaking whitespace is permitted in a parser directive. Hence, the following lines are all treated identically:\n\n#directive=value # directive =value # directive= value # directive = value # dIrEcTiVe=value\n\nsyntax\n\nUse the syntax parser directive to declare the Dockerfile syntax version to use for the build. If unspecified, BuildKit uses a bundled version of the Dockerfile frontend. Declaring a syntax version lets you automatically use the latest Dockerfile version without having to upgrade BuildKit or Docker Engine, or even use a custom Dockerfile implementation.\n\nMost users will want to set this parser directive to docker/dockerfile:1 , which causes BuildKit to pull the latest stable version of the Dockerfile syntax before the build.\n\n# syntax=docker/dockerfile:1\n\nFor more information about how the parser directive works, see Custom Dockerfile syntax .\n\nescape\n\n# escape=\\\n\nOr\n\n# escape=`\n\nThe escape directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is \\ .\n\nThe escape character is used both to escape characters in a line, and to escape a newline. This allows a Dockerfile instruction to span multiple lines. Note that regardless of whether the escape parser directive is included in a Dockerfile, escaping is not performed in a RUN command, except at the end of a line.\n\nSetting the escape character to ` is especially useful on Windows , where \\ is the directory path separator. ` is consistent with Windows PowerShell .\n\nConsider the following example which would fail in a non-obvious way on Windows. The second \\ at the end of the second line would be interpreted as an escape for the newline, instead of a target of the escape from the first \\ . Similarly, the \\ at the end of the third line would, assuming it was actually handled as an instruction, cause it be treated as a line continuation. The result of this Dockerfile is that second and third lines are considered a single instruction:\n\nFROM microsoft/nanoserver COPY testfile.txt c: \\\\ RUN dir c: \\\n\nResults in:\n\nPS E:\\myproject> docker build -t cmd . Sending build context to Docker daemon 3.072 kB Step 1/2 : FROM microsoft/nanoserver ---> 22738ff49c6d Step 2/2 : COPY testfile.txt c:\\RUN dir c: GetFileAttributesEx c:RUN: The system cannot find the file specified. PS E:\\myproject>\n\nOne solution to the above would be to use / as the target of both the COPY instruction, and dir . However, this syntax is, at best, confusing as it is not natural for paths on Windows, and at worst, error prone as not all commands on Windows support / as the path separator.\n\nBy adding the escape parser directive, the following Dockerfile succeeds as expected with the use of natural platform semantics for file paths on Windows:\n\n# escape=` FROM microsoft/nanoserver COPY testfile.txt c: \\ RUN dir c: \\\n\nResults in:\n\nPS E:\\myproject> docker build -t succeeds --no-cache=true . Sending build context to Docker daemon 3.072 kB Step 1/3 : FROM microsoft/nanoserver ---> 22738ff49c6d Step 2/3 : COPY testfile.txt c:\\ ---> 96655de338de Removing intermediate container 4db9acbb1682 Step 3/3 : RUN dir c:\\ ---> Running in a2c157f842f5 Volume in drive C has no label. Volume Serial Number is 7E6D-E0F7 Directory of c:\\ 10/05/2016 05:04 PM 1,894 License.txt 10/05/2016 02:22 PM <DIR> Program Files 10/05/2016 02:14 PM <DIR> Program Files (x86) 10/28/2016 11:18 AM 62 testfile.txt 10/28/2016 11:20 AM <DIR> Users 10/28/2016 11:20 AM <DIR> Windows 2 File(s) 1,956 bytes 4 Dir(s) 21,259,096,064 bytes free ---> 01c7f3bef04f Removing intermediate container a2c157f842f5 Successfully built 01c7f3bef04f PS E:\\myproject>\n\ncheck\n\n# check=skip=<checks|all> # check=error=<boolean>\n\nThe check directive is used to configure how build checks are evaluated. By default, all checks are run, and failures are treated as warnings.\n\nYou can disable specific checks using #check=skip=<check-name> . To specify multiple checks to skip, separate them with a comma:\n\n# check=skip=JSONArgsRecommended,StageNameCasing\n\nTo disable all checks, use #check=skip=all .\n\nBy default, builds with failing build checks exit with a zero status code despite warnings. To make the build fail on warnings, set #check=error=true .\n\n# check=error=true\n\nNote When using the check directive, with error=true option, it is recommended to pin the Dockerfile syntax to a specific version. Otherwise, your build may start to fail when new checks are added in the future versions.\n\nTo combine both the skip and error options, use a semi-colon to separate them:\n\n# check=skip=JSONArgsRecommended;error=true\n\nTo see all available checks, see the build checks reference . Note that the checks available depend on the Dockerfile syntax version. To make sure you're getting the most up-to-date checks, use the syntax directive to specify the Dockerfile syntax version to the latest stable version.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 6808
        }
      },
      {
        "header": "syntax",
        "content": "Use the syntax parser directive to declare the Dockerfile syntax version to use for the build. If unspecified, BuildKit uses a bundled version of the Dockerfile frontend. Declaring a syntax version lets you automatically use the latest Dockerfile version without having to upgrade BuildKit or Docker Engine, or even use a custom Dockerfile implementation.\n\nMost users will want to set this parser directive to docker/dockerfile:1 , which causes BuildKit to pull the latest stable version of the Dockerfile syntax before the build.\n\n# syntax=docker/dockerfile:1\n\nFor more information about how the parser directive works, see Custom Dockerfile syntax .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 651
        }
      },
      {
        "header": "escape",
        "content": "# escape=\\\n\nOr\n\n# escape=`\n\nThe escape directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is \\ .\n\nThe escape character is used both to escape characters in a line, and to escape a newline. This allows a Dockerfile instruction to span multiple lines. Note that regardless of whether the escape parser directive is included in a Dockerfile, escaping is not performed in a RUN command, except at the end of a line.\n\nSetting the escape character to ` is especially useful on Windows , where \\ is the directory path separator. ` is consistent with Windows PowerShell .\n\nConsider the following example which would fail in a non-obvious way on Windows. The second \\ at the end of the second line would be interpreted as an escape for the newline, instead of a target of the escape from the first \\ . Similarly, the \\ at the end of the third line would, assuming it was actually handled as an instruction, cause it be treated as a line continuation. The result of this Dockerfile is that second and third lines are considered a single instruction:\n\nFROM microsoft/nanoserver COPY testfile.txt c: \\\\ RUN dir c: \\\n\nResults in:\n\nPS E:\\myproject> docker build -t cmd . Sending build context to Docker daemon 3.072 kB Step 1/2 : FROM microsoft/nanoserver ---> 22738ff49c6d Step 2/2 : COPY testfile.txt c:\\RUN dir c: GetFileAttributesEx c:RUN: The system cannot find the file specified. PS E:\\myproject>\n\nOne solution to the above would be to use / as the target of both the COPY instruction, and dir . However, this syntax is, at best, confusing as it is not natural for paths on Windows, and at worst, error prone as not all commands on Windows support / as the path separator.\n\nBy adding the escape parser directive, the following Dockerfile succeeds as expected with the use of natural platform semantics for file paths on Windows:\n\n# escape=` FROM microsoft/nanoserver COPY testfile.txt c: \\ RUN dir c: \\\n\nResults in:\n\nPS E:\\myproject> docker build -t succeeds --no-cache=true . Sending build context to Docker daemon 3.072 kB Step 1/3 : FROM microsoft/nanoserver ---> 22738ff49c6d Step 2/3 : COPY testfile.txt c:\\ ---> 96655de338de Removing intermediate container 4db9acbb1682 Step 3/3 : RUN dir c:\\ ---> Running in a2c157f842f5 Volume in drive C has no label. Volume Serial Number is 7E6D-E0F7 Directory of c:\\ 10/05/2016 05:04 PM 1,894 License.txt 10/05/2016 02:22 PM <DIR> Program Files 10/05/2016 02:14 PM <DIR> Program Files (x86) 10/28/2016 11:18 AM 62 testfile.txt 10/28/2016 11:20 AM <DIR> Users 10/28/2016 11:20 AM <DIR> Windows 2 File(s) 1,956 bytes 4 Dir(s) 21,259,096,064 bytes free ---> 01c7f3bef04f Removing intermediate container a2c157f842f5 Successfully built 01c7f3bef04f PS E:\\myproject>",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2766
        }
      },
      {
        "header": "check",
        "content": "# check=skip=<checks|all> # check=error=<boolean>\n\nThe check directive is used to configure how build checks are evaluated. By default, all checks are run, and failures are treated as warnings.\n\nYou can disable specific checks using #check=skip=<check-name> . To specify multiple checks to skip, separate them with a comma:\n\n# check=skip=JSONArgsRecommended,StageNameCasing\n\nTo disable all checks, use #check=skip=all .\n\nBy default, builds with failing build checks exit with a zero status code despite warnings. To make the build fail on warnings, set #check=error=true .\n\n# check=error=true\n\nNote When using the check directive, with error=true option, it is recommended to pin the Dockerfile syntax to a specific version. Otherwise, your build may start to fail when new checks are added in the future versions.\n\nTo combine both the skip and error options, use a semi-colon to separate them:\n\n# check=skip=JSONArgsRecommended;error=true\n\nTo see all available checks, see the build checks reference . Note that the checks available depend on the Dockerfile syntax version. To make sure you're getting the most up-to-date checks, use the syntax directive to specify the Dockerfile syntax version to the latest stable version.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1226
        }
      },
      {
        "header": "Environment replacement",
        "content": "Environment variables (declared with the ENV statement ) can also be used in certain instructions as variables to be interpreted by the Dockerfile. Escapes are also handled for including variable-like syntax into a statement literally.\n\nEnvironment variables are notated in the Dockerfile either with $variable_name or ${variable_name} . They are treated equivalently and the brace syntax is typically used to address issues with variable names with no whitespace, like ${foo}_bar .\n\nThe ${variable_name} syntax also supports a few of the standard bash modifiers as specified below:\n\n${variable:-word} indicates that if variable is set then the result will be that value. If variable is not set then word will be the result. ${variable:+word} indicates that if variable is set then word will be the result, otherwise the result is the empty string.\n\nThe following variable replacements are supported in a pre-release version of Dockerfile syntax, when using the # syntax=docker/dockerfile-upstream:master syntax directive in your Dockerfile:\n\n${variable#pattern} removes the shortest match of pattern from variable , seeking from the start of the string. str = foobarbaz echo ${ str #f*b } # arbaz ${variable##pattern} removes the longest match of pattern from variable , seeking from the start of the string. str = foobarbaz echo ${ str ##f*b } # az ${variable%pattern} removes the shortest match of pattern from variable , seeking backwards from the end of the string. string = foobarbaz echo ${ string %b* } # foobar ${variable%%pattern} removes the longest match of pattern from variable , seeking backwards from the end of the string. string = foobarbaz echo ${ string %%b* } # foo ${variable/pattern/replacement} replace the first occurrence of pattern in variable with replacement string = foobarbaz echo ${ string /ba/fo } # fooforbaz ${variable//pattern/replacement} replaces all occurrences of pattern in variable with replacement string = foobarbaz echo ${ string //ba/fo } # fooforfoz\n\nIn all cases, word can be any string, including additional environment variables.\n\npattern is a glob pattern where ? matches any single character and * any number of characters (including zero). To match literal ? and * , use a backslash escape: \\? and \\* .\n\nYou can escape whole variable names by adding a \\ before the variable: \\$foo or \\${foo} , for example, will translate to $foo and ${foo} literals respectively.\n\nExample (parsed representation is displayed after the # ):\n\nFROM busybox ENV FOO = /bar WORKDIR ${FOO} # WORKDIR /bar ADD . $FOO # ADD . /bar COPY \\$ FOO /quux # COPY $FOO /quux\n\nEnvironment variables are supported by the following list of instructions in the Dockerfile:\n\nADD COPY ENV EXPOSE FROM LABEL STOPSIGNAL USER VOLUME WORKDIR ONBUILD (when combined with one of the supported instructions above)\n\nYou can also use environment variables with RUN , CMD , and ENTRYPOINT instructions, but in those cases the variable substitution is handled by the command shell, not the builder. Note that instructions using the exec form don't invoke a command shell automatically. See Variable substitution .\n\nEnvironment variable substitution use the same value for each variable throughout the entire instruction. Changing the value of a variable only takes effect in subsequent instructions. Consider the following example:\n\nENV abc = hello ENV abc = bye def = $abc ENV ghi = $abc\n\nThe value of def becomes hello The value of ghi becomes bye",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 3453
        }
      },
      {
        "header": ".dockerignore file",
        "content": "You can use .dockerignore file to exclude files and directories from the build context. For more information, see .dockerignore file .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 134
        }
      },
      {
        "header": "Shell and exec form",
        "content": "The RUN , CMD , and ENTRYPOINT instructions all have two possible forms:\n\nINSTRUCTION [\"executable\",\"param1\",\"param2\"] (exec form) INSTRUCTION command param1 param2 (shell form)\n\nThe exec form makes it possible to avoid shell string munging, and to invoke commands using a specific command shell, or any other executable. It uses a JSON array syntax, where each element in the array is a command, flag, or argument.\n\nThe shell form is more relaxed, and emphasizes ease of use, flexibility, and readability. The shell form automatically uses a command shell, whereas the exec form does not.\n\nExec form\n\nThe exec form is parsed as a JSON array, which means that you must use double-quotes (\") around words, not single-quotes (').\n\nENTRYPOINT [ \"/bin/bash\" , \"-c\" , \"echo hello\" ]\n\nThe exec form is best used to specify an ENTRYPOINT instruction, combined with CMD for setting default arguments that can be overridden at runtime. For more information, see ENTRYPOINT .\n\nVariable substitution\n\nUsing the exec form doesn't automatically invoke a command shell. This means that normal shell processing, such as variable substitution, doesn't happen. For example, RUN [ \"echo\", \"$HOME\" ] won't handle variable substitution for $HOME .\n\nIf you want shell processing then either use the shell form or execute a shell directly with the exec form, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ] . When using the exec form and executing a shell directly, as in the case for the shell form, it's the shell that's doing the environment variable substitution, not the builder.\n\nBackslashes\n\nIn exec form, you must escape backslashes. This is particularly relevant on Windows where the backslash is the path separator. The following line would otherwise be treated as shell form due to not being valid JSON, and fail in an unexpected way:\n\nRUN [ \"c:\\windows\\system32\\tasklist.exe\" ]\n\nThe correct syntax for this example is:\n\nRUN [ \"c:\\\\windows\\\\system32\\\\tasklist.exe\" ]\n\nShell form\n\nUnlike the exec form, instructions using the shell form always use a command shell. The shell form doesn't use the JSON array format, instead it's a regular string. The shell form string lets you escape newlines using the escape character (backslash by default) to continue a single instruction onto the next line. This makes it easier to use with longer commands, because it lets you split them up into multiple lines. For example, consider these two lines:\n\nRUN source $HOME /.bashrc && \\ echo $HOME\n\nThey're equivalent to the following line:\n\nRUN source $HOME /.bashrc && echo $HOME\n\nYou can also use heredocs with the shell form to break up supported commands.\n\nRUN <<EOF source $HOME /.bashrc echo $HOME EOF\n\nFor more information about heredocs, see Here-documents .\n\nUse a different shell\n\nYou can change the default shell using the SHELL command. For example:\n\nSHELL [ \"/bin/bash\" , \"-c\" ] RUN echo hello\n\nFor more information, see SHELL .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 2913
        }
      },
      {
        "header": "Exec form",
        "content": "The exec form is parsed as a JSON array, which means that you must use double-quotes (\") around words, not single-quotes (').\n\nENTRYPOINT [ \"/bin/bash\" , \"-c\" , \"echo hello\" ]\n\nThe exec form is best used to specify an ENTRYPOINT instruction, combined with CMD for setting default arguments that can be overridden at runtime. For more information, see ENTRYPOINT .\n\nVariable substitution\n\nUsing the exec form doesn't automatically invoke a command shell. This means that normal shell processing, such as variable substitution, doesn't happen. For example, RUN [ \"echo\", \"$HOME\" ] won't handle variable substitution for $HOME .\n\nIf you want shell processing then either use the shell form or execute a shell directly with the exec form, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ] . When using the exec form and executing a shell directly, as in the case for the shell form, it's the shell that's doing the environment variable substitution, not the builder.\n\nBackslashes\n\nIn exec form, you must escape backslashes. This is particularly relevant on Windows where the backslash is the path separator. The following line would otherwise be treated as shell form due to not being valid JSON, and fail in an unexpected way:\n\nRUN [ \"c:\\windows\\system32\\tasklist.exe\" ]\n\nThe correct syntax for this example is:\n\nRUN [ \"c:\\\\windows\\\\system32\\\\tasklist.exe\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1351
        }
      },
      {
        "header": "Variable substitution",
        "content": "Using the exec form doesn't automatically invoke a command shell. This means that normal shell processing, such as variable substitution, doesn't happen. For example, RUN [ \"echo\", \"$HOME\" ] won't handle variable substitution for $HOME .\n\nIf you want shell processing then either use the shell form or execute a shell directly with the exec form, for example: RUN [ \"sh\", \"-c\", \"echo $HOME\" ] . When using the exec form and executing a shell directly, as in the case for the shell form, it's the shell that's doing the environment variable substitution, not the builder.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 570
        }
      },
      {
        "header": "Backslashes",
        "content": "In exec form, you must escape backslashes. This is particularly relevant on Windows where the backslash is the path separator. The following line would otherwise be treated as shell form due to not being valid JSON, and fail in an unexpected way:\n\nRUN [ \"c:\\windows\\system32\\tasklist.exe\" ]\n\nThe correct syntax for this example is:\n\nRUN [ \"c:\\\\windows\\\\system32\\\\tasklist.exe\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 378
        }
      },
      {
        "header": "Shell form",
        "content": "Unlike the exec form, instructions using the shell form always use a command shell. The shell form doesn't use the JSON array format, instead it's a regular string. The shell form string lets you escape newlines using the escape character (backslash by default) to continue a single instruction onto the next line. This makes it easier to use with longer commands, because it lets you split them up into multiple lines. For example, consider these two lines:\n\nRUN source $HOME /.bashrc && \\ echo $HOME\n\nThey're equivalent to the following line:\n\nRUN source $HOME /.bashrc && echo $HOME\n\nYou can also use heredocs with the shell form to break up supported commands.\n\nRUN <<EOF source $HOME /.bashrc echo $HOME EOF\n\nFor more information about heredocs, see Here-documents .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 771
        }
      },
      {
        "header": "Use a different shell",
        "content": "You can change the default shell using the SHELL command. For example:\n\nSHELL [ \"/bin/bash\" , \"-c\" ] RUN echo hello\n\nFor more information, see SHELL .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 150
        }
      },
      {
        "header": "FROM",
        "content": "FROM [--platform=<platform>] <image> [AS <name>]\n\nOr\n\nFROM [--platform=<platform>] <image>[:<tag>] [AS <name>]\n\nOr\n\nFROM [--platform=<platform>] <image>[@<digest>] [AS <name>]\n\nThe FROM instruction initializes a new build stage and sets the base image for subsequent instructions. As such, a valid Dockerfile must start with a FROM instruction. The image can be any valid image.\n\nARG is the only instruction that may precede FROM in the Dockerfile. See Understand how ARG and FROM interact . FROM can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another. Simply make a note of the last image ID output by the commit before each new FROM instruction. Each FROM instruction clears any state created by previous instructions. Optionally a name can be given to a new build stage by adding AS name to the FROM instruction. The name can be used in subsequent FROM <name> , COPY --from=<name> , and RUN --mount=type=bind,from=<name> instructions to refer to the image built in this stage. The tag or digest values are optional. If you omit either of them, the builder assumes a latest tag by default. The builder returns an error if it can't find the tag value.\n\nThe optional --platform flag can be used to specify the platform of the image in case FROM references a multi-platform image. For example, linux/amd64 , linux/arm64 , or windows/amd64 . By default, the target platform of the build request is used. Global build arguments can be used in the value of this flag, for example automatic platform ARGs allow you to force a stage to native build platform ( --platform=$BUILDPLATFORM ), and use it to cross-compile to the target platform inside the stage.\n\nUnderstand how ARG and FROM interact\n\nFROM instructions support variables that are declared by any ARG instructions that occur before the first FROM .\n\nARG CODE_VERSION = latest FROM base:${CODE_VERSION} CMD /code/run-app FROM extras:${CODE_VERSION} CMD /code/run-extras\n\nAn ARG declared before a FROM is outside of a build stage, so it can't be used in any instruction after a FROM . To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage:\n\nARG VERSION = latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION > image_version",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2332
        }
      },
      {
        "header": "Understand how ARG and FROM interact",
        "content": "FROM instructions support variables that are declared by any ARG instructions that occur before the first FROM .\n\nARG CODE_VERSION = latest FROM base:${CODE_VERSION} CMD /code/run-app FROM extras:${CODE_VERSION} CMD /code/run-extras\n\nAn ARG declared before a FROM is outside of a build stage, so it can't be used in any instruction after a FROM . To use the default value of an ARG declared before the first FROM use an ARG instruction without a value inside of a build stage:\n\nARG VERSION = latest FROM busybox:$VERSION ARG VERSION RUN echo $VERSION > image_version",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 566
        }
      },
      {
        "header": "RUN",
        "content": "The RUN instruction will execute any commands to create a new layer on top of the current image. The added layer is used in the next step in the Dockerfile. RUN has two forms:\n\n# Shell form: RUN [ OPTIONS ] <command> ... # Exec form: RUN [ OPTIONS ] [ \"<command>\" , ... ]\n\nFor more information about the differences between these two forms, see shell or exec forms .\n\nThe shell form is most commonly used, and lets you break up longer instructions into multiple lines, either using newline escapes , or with heredocs :\n\nRUN <<EOF apt-get update apt-get install -y curl EOF\n\nThe available [OPTIONS] for the RUN instruction are:\n\nOption Minimum Dockerfile version --device 1.14-labs --mount 1.2 --network 1.3 --security 1.20\n\nCache invalidation for RUN instructions\n\nThe cache for RUN instructions isn't invalidated automatically during the next build. The cache for an instruction like RUN apt-get dist-upgrade -y will be reused during the next build. The cache for RUN instructions can be invalidated by using the --no-cache flag, for example docker build --no-cache .\n\nSee the Dockerfile Best Practices guide for more information.\n\nThe cache for RUN instructions can be invalidated by ADD and COPY instructions.\n\nRUN --device\n\nNote Not yet available in stable syntax, use docker/dockerfile:1-labs version. It also needs BuildKit 0.20.0 or later.\n\nRUN --device = name, [ required ]\n\nRUN --device allows build to request CDI devices to be available to the build step.\n\nWarning The use of --device is protected by the device entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement device flag or in buildkitd config , and for a build request with --allow device flag .\n\nThe device name is provided by the CDI specification registered in BuildKit.\n\nIn the following example, multiple devices are registered in the CDI specification for the vendor1.com/device vendor.\n\ncdiVersion : \"0.6.0\" kind : \"vendor1.com/device\" devices : - name : foo containerEdits : env : - FOO=injected - name : bar annotations : org.mobyproject.buildkit.device.class : class1 containerEdits : env : - BAR=injected - name : baz annotations : org.mobyproject.buildkit.device.class : class1 containerEdits : env : - BAZ=injected - name : qux annotations : org.mobyproject.buildkit.device.class : class2 containerEdits : env : - QUX=injected annotations : org.mobyproject.buildkit.device.autoallow : true\n\nThe device name format is flexible and accepts various patterns to support multiple device configurations:\n\nvendor1.com/device : request the first device found for this vendor vendor1.com/device=foo : request a specific device vendor1.com/device=* : request all devices for this vendor class1 : request devices by org.mobyproject.buildkit.device.class annotation\n\nNote Annotations are supported by the CDI specification since 0.6.0.\n\nNote To automatically allow all devices registered in the CDI specification, you can set the org.mobyproject.buildkit.device.autoallow annotation. You can also set this annotation for a specific device.\n\nExample: CUDA-Powered LLaMA Inference\n\nIn this example we use the --device flag to run llama.cpp inference using an NVIDIA GPU device through CDI:\n\n# syntax=docker/dockerfile:1-labs FROM scratch AS model ADD https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf /model.gguf FROM scratch AS prompt COPY <<EOF prompt.txt Q: Generate a list of 10 unique biggest countries by population in JSON with their estimated poulation in 1900 and 2024. Answer only newline formatted JSON with keys \"country\" , \"population_1900\" , \"population_2024\" with 10 items. A: [ { EOF FROM ghcr.io/ggml-org/llama.cpp:full-cuda-b5124 RUN --device = nvidia.com/gpu = all \\ --mount = from = model,target = /models \\ --mount = from = prompt,target = /tmp \\ ./llama-cli -m /models/model.gguf -no-cnv -ngl 99 -f /tmp/prompt.txt\n\nRUN --mount\n\nRUN --mount =[ type = <TYPE> ][ ,option = <value> [ ,option = <value> ] ... ]\n\nRUN --mount allows you to create filesystem mounts that the build can access. This can be used to:\n\nCreate bind mount to the host filesystem or other build stages Access build secrets or ssh-agent sockets Use a persistent package management cache to speed up your build\n\nThe supported mount types are:\n\nType Description bind (default) Bind-mount context directories (read-only). cache Mount a temporary directory to cache directories for compilers and package managers. tmpfs Mount a tmpfs in the build container. secret Allow the build container to access secure files such as private keys without baking them into the image or build cache. ssh Allow the build container to access SSH keys via SSH agents, with support for passphrases.\n\nRUN --mount=type=bind\n\nThis mount type allows binding files or directories to the build container. A bind mount is read-only by default.\n\nOption Description target , dst , destination 1 Mount path. source Source path in the from . Defaults to the root of the from . from Build stage, context, or image name for the root of the source. Defaults to the build context. rw , readwrite Allow writes on the mount. Written data will be discarded.\n\nRUN --mount=type=cache\n\nThis mount type allows the build container to cache directories for compilers and package managers.\n\nOption Description id Optional ID to identify separate/different caches. Defaults to value of target . target , dst , destination 1 Mount path. ro , readonly Read-only if set. sharing One of shared , private , or locked . Defaults to shared . A shared cache mount can be used concurrently by multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount. from Build stage, context, or image name to use as a base of the cache mount. Defaults to empty directory. source Subpath in the from to mount. Defaults to the root of the from . mode File mode for new cache directory in octal. Default 0755 . uid User ID for new cache directory. Default 0 . gid Group ID for new cache directory. Default 0 .\n\nContents of the cache directories persists between builder invocations without invalidating the instruction cache. Cache mounts should only be used for better performance. Your build should work with any contents of the cache directory as another build may overwrite the files or GC may clean it if more storage space is needed.\n\nExample: cache Go packages\n\n# syntax=docker/dockerfile:1 FROM golang RUN --mount = type = cache,target = /root/.cache/go-build \\ go build ...\n\nExample: cache apt packages\n\n# syntax=docker/dockerfile:1 FROM ubuntu RUN rm -f /etc/apt/apt.conf.d/docker-clean ; echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache RUN --mount = type = cache,target = /var/cache/apt,sharing = locked \\ --mount = type = cache,target = /var/lib/apt,sharing = locked \\ apt update && apt-get --no-install-recommends install -y gcc\n\nApt needs exclusive access to its data, so the caches use the option sharing=locked , which will make sure multiple parallel builds using the same cache mount will wait for each other and not access the same cache files at the same time. You could also use sharing=private if you prefer to have each build create another cache directory in this case.\n\nRUN --mount=type=tmpfs\n\nThis mount type allows mounting tmpfs in the build container.\n\nOption Description target , dst , destination 1 Mount path. size Specify an upper limit on the size of the filesystem.\n\nRUN --mount=type=secret\n\nThis mount type allows the build container to access secret values, such as tokens or private keys, without baking them into the image.\n\nBy default, the secret is mounted as a file. You can also mount the secret as an environment variable by setting the env option.\n\nOption Description id ID of the secret. Defaults to basename of the target path. target , dst , destination Mount the secret to the specified path. Defaults to /run/secrets/ + id if unset and if env is also unset. env Mount the secret to an environment variable instead of a file, or both. (since Dockerfile v1.10.0) required If set to true , the instruction errors out when the secret is unavailable. Defaults to false . mode File mode for secret file in octal. Default 0400 . uid User ID for secret file. Default 0 . gid Group ID for secret file. Default 0 .\n\nExample: access to S3\n\n# syntax=docker/dockerfile:1 FROM python:3 RUN pip install awscli RUN --mount = type = secret,id = aws,target = /root/.aws/credentials \\ aws s3 cp s3://... ...\n\n$ docker buildx build --secret id = aws,src = $HOME /.aws/credentials .\n\nExample: Mount as environment variable\n\nThe following example takes the secret API_KEY and mounts it as an environment variable with the same name.\n\n# syntax=docker/dockerfile:1 FROM alpine RUN --mount = type = secret,id = API_KEY,env = API_KEY \\ some-command --token-from-env $API_KEY\n\nAssuming that the API_KEY environment variable is set in the build environment, you can build this with the following command:\n\n$ docker buildx build --secret id = API_KEY .\n\nRUN --mount=type=ssh\n\nThis mount type allows the build container to access SSH keys via SSH agents, with support for passphrases.\n\nOption Description id ID of SSH agent socket or key. Defaults to \"default\". target , dst , destination SSH agent socket path. Defaults to /run/buildkit/ssh_agent.${N} . required If set to true , the instruction errors out when the key is unavailable. Defaults to false . mode File mode for socket in octal. Default 0600 . uid User ID for socket. Default 0 . gid Group ID for socket. Default 0 .\n\nExample: access to GitLab\n\n# syntax=docker/dockerfile:1 FROM alpine RUN apk add --no-cache openssh-client RUN mkdir -p -m 0700 ~/.ssh && ssh-keyscan gitlab.com >> ~/.ssh/known_hosts RUN --mount = type = ssh \\ ssh -q -T git@gitlab.com 2> & 1 | tee /hello # \"Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY\" should be printed here # with the type of build progress is defined as `plain`.\n\n$ eval $( ssh-agent ) $ ssh-add ~/.ssh/id_rsa (Input your passphrase here) $ docker buildx build --ssh default = $SSH_AUTH_SOCK .\n\nYou can also specify a path to *.pem file on the host directly instead of $SSH_AUTH_SOCK . However, pem files with passphrases are not supported.\n\nRUN --network\n\nRUN --network = <TYPE>\n\nRUN --network allows control over which networking environment the command is run in.\n\nThe supported network types are:\n\nType Description default (default) Run in the default network. none Run with no network access. host Run in the host's network environment.\n\nRUN --network=default\n\nEquivalent to not supplying a flag at all, the command is run in the default network for the build.\n\nRUN --network=none\n\nThe command is run with no network access ( lo is still available, but is isolated to this process)\n\nExample: isolating external effects\n\n# syntax=docker/dockerfile:1 FROM python:3.6 ADD mypackage.tgz wheels/ RUN --network = none pip install --find-links wheels mypackage\n\npip will only be able to install the packages provided in the tarfile, which can be controlled by an earlier build stage.\n\nRUN --network=host\n\nThe command is run in the host's network environment (similar to docker build --network=host , but on a per-instruction basis)\n\nWarning The use of --network=host is protected by the network.host entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement network.host flag or in buildkitd config , and for a build request with --allow network.host flag .\n\nRUN --security\n\nRUN --security = <sandbox | insecure>\n\nThe default security mode is sandbox . With --security=insecure , the builder runs the command without sandbox in insecure mode, which allows to run flows requiring elevated privileges (e.g. containerd). This is equivalent to running docker run --privileged .\n\nWarning In order to access this feature, entitlement security.insecure should be enabled when starting the buildkitd daemon with --allow-insecure-entitlement security.insecure flag or in buildkitd config , and for a build request with --allow security.insecure flag .\n\nDefault sandbox mode can be activated via --security=sandbox , but that is no-op.\n\nExample: check entitlements\n\n# syntax=docker/dockerfile:1 FROM ubuntu RUN --security = insecure cat /proc/self/status | grep CapEff\n\n#84 0.093 CapEff: 0000003fffffffff",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 33,
          "content_length": 12407
        }
      },
      {
        "header": "Cache invalidation for RUN instructions",
        "content": "The cache for RUN instructions isn't invalidated automatically during the next build. The cache for an instruction like RUN apt-get dist-upgrade -y will be reused during the next build. The cache for RUN instructions can be invalidated by using the --no-cache flag, for example docker build --no-cache .\n\nSee the Dockerfile Best Practices guide for more information.\n\nThe cache for RUN instructions can be invalidated by ADD and COPY instructions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 447
        }
      },
      {
        "header": "RUN --device",
        "content": "Note Not yet available in stable syntax, use docker/dockerfile:1-labs version. It also needs BuildKit 0.20.0 or later.\n\nRUN --device = name, [ required ]\n\nRUN --device allows build to request CDI devices to be available to the build step.\n\nWarning The use of --device is protected by the device entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement device flag or in buildkitd config , and for a build request with --allow device flag .\n\nThe device name is provided by the CDI specification registered in BuildKit.\n\nIn the following example, multiple devices are registered in the CDI specification for the vendor1.com/device vendor.\n\ncdiVersion : \"0.6.0\" kind : \"vendor1.com/device\" devices : - name : foo containerEdits : env : - FOO=injected - name : bar annotations : org.mobyproject.buildkit.device.class : class1 containerEdits : env : - BAR=injected - name : baz annotations : org.mobyproject.buildkit.device.class : class1 containerEdits : env : - BAZ=injected - name : qux annotations : org.mobyproject.buildkit.device.class : class2 containerEdits : env : - QUX=injected annotations : org.mobyproject.buildkit.device.autoallow : true\n\nThe device name format is flexible and accepts various patterns to support multiple device configurations:\n\nvendor1.com/device : request the first device found for this vendor vendor1.com/device=foo : request a specific device vendor1.com/device=* : request all devices for this vendor class1 : request devices by org.mobyproject.buildkit.device.class annotation\n\nNote Annotations are supported by the CDI specification since 0.6.0.\n\nNote To automatically allow all devices registered in the CDI specification, you can set the org.mobyproject.buildkit.device.autoallow annotation. You can also set this annotation for a specific device.\n\nExample: CUDA-Powered LLaMA Inference\n\nIn this example we use the --device flag to run llama.cpp inference using an NVIDIA GPU device through CDI:\n\n# syntax=docker/dockerfile:1-labs FROM scratch AS model ADD https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf /model.gguf FROM scratch AS prompt COPY <<EOF prompt.txt Q: Generate a list of 10 unique biggest countries by population in JSON with their estimated poulation in 1900 and 2024. Answer only newline formatted JSON with keys \"country\" , \"population_1900\" , \"population_2024\" with 10 items. A: [ { EOF FROM ghcr.io/ggml-org/llama.cpp:full-cuda-b5124 RUN --device = nvidia.com/gpu = all \\ --mount = from = model,target = /models \\ --mount = from = prompt,target = /tmp \\ ./llama-cli -m /models/model.gguf -no-cnv -ngl 99 -f /tmp/prompt.txt",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2687
        }
      },
      {
        "header": "Example: CUDA-Powered LLaMA Inference",
        "content": "In this example we use the --device flag to run llama.cpp inference using an NVIDIA GPU device through CDI:\n\n# syntax=docker/dockerfile:1-labs FROM scratch AS model ADD https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf /model.gguf FROM scratch AS prompt COPY <<EOF prompt.txt Q: Generate a list of 10 unique biggest countries by population in JSON with their estimated poulation in 1900 and 2024. Answer only newline formatted JSON with keys \"country\" , \"population_1900\" , \"population_2024\" with 10 items. A: [ { EOF FROM ghcr.io/ggml-org/llama.cpp:full-cuda-b5124 RUN --device = nvidia.com/gpu = all \\ --mount = from = model,target = /models \\ --mount = from = prompt,target = /tmp \\ ./llama-cli -m /models/model.gguf -no-cnv -ngl 99 -f /tmp/prompt.txt",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 811
        }
      },
      {
        "header": "RUN --mount",
        "content": "RUN --mount =[ type = <TYPE> ][ ,option = <value> [ ,option = <value> ] ... ]\n\nRUN --mount allows you to create filesystem mounts that the build can access. This can be used to:\n\nCreate bind mount to the host filesystem or other build stages Access build secrets or ssh-agent sockets Use a persistent package management cache to speed up your build\n\nThe supported mount types are:\n\nType Description bind (default) Bind-mount context directories (read-only). cache Mount a temporary directory to cache directories for compilers and package managers. tmpfs Mount a tmpfs in the build container. secret Allow the build container to access secure files such as private keys without baking them into the image or build cache. ssh Allow the build container to access SSH keys via SSH agents, with support for passphrases.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 815
        }
      },
      {
        "header": "RUN --mount=type=bind",
        "content": "This mount type allows binding files or directories to the build container. A bind mount is read-only by default.\n\nOption Description target , dst , destination 1 Mount path. source Source path in the from . Defaults to the root of the from . from Build stage, context, or image name for the root of the source. Defaults to the build context. rw , readwrite Allow writes on the mount. Written data will be discarded.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 416
        }
      },
      {
        "header": "RUN --mount=type=cache",
        "content": "This mount type allows the build container to cache directories for compilers and package managers.\n\nOption Description id Optional ID to identify separate/different caches. Defaults to value of target . target , dst , destination 1 Mount path. ro , readonly Read-only if set. sharing One of shared , private , or locked . Defaults to shared . A shared cache mount can be used concurrently by multiple writers. private creates a new mount if there are multiple writers. locked pauses the second writer until the first one releases the mount. from Build stage, context, or image name to use as a base of the cache mount. Defaults to empty directory. source Subpath in the from to mount. Defaults to the root of the from . mode File mode for new cache directory in octal. Default 0755 . uid User ID for new cache directory. Default 0 . gid Group ID for new cache directory. Default 0 .\n\nContents of the cache directories persists between builder invocations without invalidating the instruction cache. Cache mounts should only be used for better performance. Your build should work with any contents of the cache directory as another build may overwrite the files or GC may clean it if more storage space is needed.\n\nExample: cache Go packages\n\n# syntax=docker/dockerfile:1 FROM golang RUN --mount = type = cache,target = /root/.cache/go-build \\ go build ...\n\nExample: cache apt packages\n\n# syntax=docker/dockerfile:1 FROM ubuntu RUN rm -f /etc/apt/apt.conf.d/docker-clean ; echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache RUN --mount = type = cache,target = /var/cache/apt,sharing = locked \\ --mount = type = cache,target = /var/lib/apt,sharing = locked \\ apt update && apt-get --no-install-recommends install -y gcc\n\nApt needs exclusive access to its data, so the caches use the option sharing=locked , which will make sure multiple parallel builds using the same cache mount will wait for each other and not access the same cache files at the same time. You could also use sharing=private if you prefer to have each build create another cache directory in this case.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 2110
        }
      },
      {
        "header": "Example: cache Go packages",
        "content": "# syntax=docker/dockerfile:1 FROM golang RUN --mount = type = cache,target = /root/.cache/go-build \\ go build ...",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 113
        }
      },
      {
        "header": "Example: cache apt packages",
        "content": "# syntax=docker/dockerfile:1 FROM ubuntu RUN rm -f /etc/apt/apt.conf.d/docker-clean ; echo 'Binary::apt::APT::Keep-Downloaded-Packages \"true\";' > /etc/apt/apt.conf.d/keep-cache RUN --mount = type = cache,target = /var/cache/apt,sharing = locked \\ --mount = type = cache,target = /var/lib/apt,sharing = locked \\ apt update && apt-get --no-install-recommends install -y gcc\n\nApt needs exclusive access to its data, so the caches use the option sharing=locked , which will make sure multiple parallel builds using the same cache mount will wait for each other and not access the same cache files at the same time. You could also use sharing=private if you prefer to have each build create another cache directory in this case.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 723
        }
      },
      {
        "header": "RUN --mount=type=tmpfs",
        "content": "This mount type allows mounting tmpfs in the build container.\n\nOption Description target , dst , destination 1 Mount path. size Specify an upper limit on the size of the filesystem.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 181
        }
      },
      {
        "header": "RUN --mount=type=secret",
        "content": "This mount type allows the build container to access secret values, such as tokens or private keys, without baking them into the image.\n\nBy default, the secret is mounted as a file. You can also mount the secret as an environment variable by setting the env option.\n\nOption Description id ID of the secret. Defaults to basename of the target path. target , dst , destination Mount the secret to the specified path. Defaults to /run/secrets/ + id if unset and if env is also unset. env Mount the secret to an environment variable instead of a file, or both. (since Dockerfile v1.10.0) required If set to true , the instruction errors out when the secret is unavailable. Defaults to false . mode File mode for secret file in octal. Default 0400 . uid User ID for secret file. Default 0 . gid Group ID for secret file. Default 0 .\n\nExample: access to S3\n\n# syntax=docker/dockerfile:1 FROM python:3 RUN pip install awscli RUN --mount = type = secret,id = aws,target = /root/.aws/credentials \\ aws s3 cp s3://... ...\n\n$ docker buildx build --secret id = aws,src = $HOME /.aws/credentials .\n\nExample: Mount as environment variable\n\nThe following example takes the secret API_KEY and mounts it as an environment variable with the same name.\n\n# syntax=docker/dockerfile:1 FROM alpine RUN --mount = type = secret,id = API_KEY,env = API_KEY \\ some-command --token-from-env $API_KEY\n\nAssuming that the API_KEY environment variable is set in the build environment, you can build this with the following command:\n\n$ docker buildx build --secret id = API_KEY .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1546
        }
      },
      {
        "header": "Example: access to S3",
        "content": "# syntax=docker/dockerfile:1 FROM python:3 RUN pip install awscli RUN --mount = type = secret,id = aws,target = /root/.aws/credentials \\ aws s3 cp s3://... ...\n\n$ docker buildx build --secret id = aws,src = $HOME /.aws/credentials .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 232
        }
      },
      {
        "header": "Example: Mount as environment variable",
        "content": "The following example takes the secret API_KEY and mounts it as an environment variable with the same name.\n\n# syntax=docker/dockerfile:1 FROM alpine RUN --mount = type = secret,id = API_KEY,env = API_KEY \\ some-command --token-from-env $API_KEY\n\nAssuming that the API_KEY environment variable is set in the build environment, you can build this with the following command:\n\n$ docker buildx build --secret id = API_KEY .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 420
        }
      },
      {
        "header": "RUN --mount=type=ssh",
        "content": "This mount type allows the build container to access SSH keys via SSH agents, with support for passphrases.\n\nOption Description id ID of SSH agent socket or key. Defaults to \"default\". target , dst , destination SSH agent socket path. Defaults to /run/buildkit/ssh_agent.${N} . required If set to true , the instruction errors out when the key is unavailable. Defaults to false . mode File mode for socket in octal. Default 0600 . uid User ID for socket. Default 0 . gid Group ID for socket. Default 0 .\n\nExample: access to GitLab\n\n# syntax=docker/dockerfile:1 FROM alpine RUN apk add --no-cache openssh-client RUN mkdir -p -m 0700 ~/.ssh && ssh-keyscan gitlab.com >> ~/.ssh/known_hosts RUN --mount = type = ssh \\ ssh -q -T git@gitlab.com 2> & 1 | tee /hello # \"Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY\" should be printed here # with the type of build progress is defined as `plain`.\n\n$ eval $( ssh-agent ) $ ssh-add ~/.ssh/id_rsa (Input your passphrase here) $ docker buildx build --ssh default = $SSH_AUTH_SOCK .\n\nYou can also specify a path to *.pem file on the host directly instead of $SSH_AUTH_SOCK . However, pem files with passphrases are not supported.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1179
        }
      },
      {
        "header": "Example: access to GitLab",
        "content": "# syntax=docker/dockerfile:1 FROM alpine RUN apk add --no-cache openssh-client RUN mkdir -p -m 0700 ~/.ssh && ssh-keyscan gitlab.com >> ~/.ssh/known_hosts RUN --mount = type = ssh \\ ssh -q -T git@gitlab.com 2> & 1 | tee /hello # \"Welcome to GitLab, @GITLAB_USERNAME_ASSOCIATED_WITH_SSHKEY\" should be printed here # with the type of build progress is defined as `plain`.\n\n$ eval $( ssh-agent ) $ ssh-add ~/.ssh/id_rsa (Input your passphrase here) $ docker buildx build --ssh default = $SSH_AUTH_SOCK .\n\nYou can also specify a path to *.pem file on the host directly instead of $SSH_AUTH_SOCK . However, pem files with passphrases are not supported.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 647
        }
      },
      {
        "header": "RUN --network",
        "content": "RUN --network = <TYPE>\n\nRUN --network allows control over which networking environment the command is run in.\n\nThe supported network types are:\n\nType Description default (default) Run in the default network. none Run with no network access. host Run in the host's network environment.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 284
        }
      },
      {
        "header": "RUN --network=default",
        "content": "Equivalent to not supplying a flag at all, the command is run in the default network for the build.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 99
        }
      },
      {
        "header": "RUN --network=none",
        "content": "The command is run with no network access ( lo is still available, but is isolated to this process)\n\nExample: isolating external effects\n\n# syntax=docker/dockerfile:1 FROM python:3.6 ADD mypackage.tgz wheels/ RUN --network = none pip install --find-links wheels mypackage\n\npip will only be able to install the packages provided in the tarfile, which can be controlled by an earlier build stage.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 394
        }
      },
      {
        "header": "Example: isolating external effects",
        "content": "# syntax=docker/dockerfile:1 FROM python:3.6 ADD mypackage.tgz wheels/ RUN --network = none pip install --find-links wheels mypackage\n\npip will only be able to install the packages provided in the tarfile, which can be controlled by an earlier build stage.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 256
        }
      },
      {
        "header": "RUN --network=host",
        "content": "The command is run in the host's network environment (similar to docker build --network=host , but on a per-instruction basis)\n\nWarning The use of --network=host is protected by the network.host entitlement, which needs to be enabled when starting the buildkitd daemon with --allow-insecure-entitlement network.host flag or in buildkitd config , and for a build request with --allow network.host flag .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 402
        }
      },
      {
        "header": "RUN --security",
        "content": "RUN --security = <sandbox | insecure>\n\nThe default security mode is sandbox . With --security=insecure , the builder runs the command without sandbox in insecure mode, which allows to run flows requiring elevated privileges (e.g. containerd). This is equivalent to running docker run --privileged .\n\nWarning In order to access this feature, entitlement security.insecure should be enabled when starting the buildkitd daemon with --allow-insecure-entitlement security.insecure flag or in buildkitd config , and for a build request with --allow security.insecure flag .\n\nDefault sandbox mode can be activated via --security=sandbox , but that is no-op.\n\nExample: check entitlements\n\n# syntax=docker/dockerfile:1 FROM ubuntu RUN --security = insecure cat /proc/self/status | grep CapEff\n\n#84 0.093 CapEff: 0000003fffffffff",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 819
        }
      },
      {
        "header": "Example: check entitlements",
        "content": "# syntax=docker/dockerfile:1 FROM ubuntu RUN --security = insecure cat /proc/self/status | grep CapEff\n\n#84 0.093 CapEff: 0000003fffffffff",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 138
        }
      },
      {
        "header": "CMD",
        "content": "The CMD instruction sets the command to be executed when running a container from an image.\n\nYou can specify CMD instructions using shell or exec forms :\n\nCMD [\"executable\",\"param1\",\"param2\"] (exec form) CMD [\"param1\",\"param2\"] (exec form, as default parameters to ENTRYPOINT ) CMD command param1 param2 (shell form)\n\nThere can only be one CMD instruction in a Dockerfile. If you list more than one CMD , only the last one takes effect.\n\nThe purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well.\n\nIf you would like your container to run the same executable every time, then you should consider using ENTRYPOINT in combination with CMD . See ENTRYPOINT . If the user specifies arguments to docker run then they will override the default specified in CMD , but still use the default ENTRYPOINT .\n\nIf CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified in the exec form .\n\nNote Don't confuse RUN with CMD . RUN actually runs a command and commits the result; CMD doesn't execute anything at build time, but specifies the intended command for the image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1286
        }
      },
      {
        "header": "LABEL",
        "content": "LABEL <key> = <value> [ <key> = <value>... ]\n\nThe LABEL instruction adds metadata to an image. A LABEL is a key-value pair. To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing. A few usage examples:\n\nLABEL \"com.example.vendor\" = \"ACME Incorporated\" LABEL com.example.label-with-value = \"foo\" LABEL version = \"1.0\" LABEL description = \"This text illustrates \\ that label-values can span multiple lines.\"\n\nAn image can have more than one label. You can specify multiple labels on a single line. Prior to Docker 1.10, this decreased the size of the final image, but this is no longer the case. You may still choose to specify multiple labels in a single instruction, in one of the following two ways:\n\nLABEL multi.label1 = \"value1\" multi.label2 = \"value2\" other = \"value3\"\n\nLABEL multi.label1 = \"value1\" \\ multi.label2 = \"value2\" \\ other = \"value3\"\n\nNote Be sure to use double quotes and not single quotes. Particularly when you are using string interpolation (e.g. LABEL example=\"foo-$ENV_VAR\" ), single quotes will take the string as is without unpacking the variable's value.\n\nLabels included in base images (images in the FROM line) are inherited by your image. If a label already exists but with a different value, the most-recently-applied value overrides any previously-set value.\n\nTo view an image's labels, use the docker image inspect command. You can use the --format option to show just the labels;\n\n$ docker image inspect --format = '{{json .Config.Labels}}' myimage\n\n{ \"com.example.vendor\" : \"ACME Incorporated\" , \"com.example.label-with-value\" : \"foo\" , \"version\" : \"1.0\" , \"description\" : \"This text illustrates that label-values can span multiple lines.\" , \"multi.label1\" : \"value1\" , \"multi.label2\" : \"value2\" , \"other\" : \"value3\" }",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1799
        }
      },
      {
        "header": "MAINTAINER (deprecated)",
        "content": "MAINTAINER <name>\n\nThe MAINTAINER instruction sets the Author field of the generated images. The LABEL instruction is a much more flexible version of this and you should use it instead, as it enables setting any metadata you require, and can be viewed easily, for example with docker inspect . To set a label corresponding to the MAINTAINER field you could use:\n\nLABEL org.opencontainers.image.authors = \"SvenDowideit@home.org.au\"\n\nThis will then be visible from docker inspect with the other labels.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 500
        }
      },
      {
        "header": "EXPOSE",
        "content": "EXPOSE <port> [<port>/<protocol>...]\n\nThe EXPOSE instruction informs Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if you don't specify a protocol.\n\nThe EXPOSE instruction doesn't actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To publish the port when running the container, use the -p flag on docker run to publish and map one or more ports, or the -P flag to publish all exposed ports and map them to high-order ports.\n\nBy default, EXPOSE assumes TCP. You can also specify UDP:\n\nEXPOSE 80/udp\n\nTo expose on both TCP and UDP, include two lines:\n\nEXPOSE 80/tcp EXPOSE 80/udp\n\nIn this case, if you use -P with docker run , the port will be exposed once for TCP and once for UDP. Remember that -P uses an ephemeral high-ordered host port on the host, so TCP and UDP doesn't use the same port.\n\nRegardless of the EXPOSE settings, you can override them at runtime by using the -p flag. For example\n\n$ docker run -p 80:80/tcp -p 80:80/udp ...\n\nTo set up port redirection on the host system, see using the -P flag . The docker network command supports creating networks for communication among containers without the need to expose or publish specific ports, because the containers connected to the network can communicate with each other over any port. For detailed information, see the overview of this feature .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1565
        }
      },
      {
        "header": "ENV",
        "content": "ENV <key> = <value> [ <key> = <value>... ]\n\nThe ENV instruction sets the environment variable <key> to the value <value> . This value will be in the environment for all subsequent instructions in the build stage and can be replaced inline in many as well. The value will be interpreted for other environment variables, so quote characters will be removed if they are not escaped. Like command line parsing, quotes and backslashes can be used to include spaces within values.\n\nExample:\n\nENV MY_NAME = \"John Doe\" ENV MY_DOG = Rex \\ The \\ Dog ENV MY_CAT = fluffy\n\nThe ENV instruction allows for multiple <key>=<value> ... variables to be set at one time, and the example below will yield the same net results in the final image:\n\nENV MY_NAME = \"John Doe\" MY_DOG = Rex \\ The \\ Dog \\ MY_CAT = fluffy\n\nThe environment variables set using ENV will persist when a container is run from the resulting image. You can view the values using docker inspect , and change them using docker run --env <key>=<value> .\n\nA stage inherits any environment variables that were set using ENV by its parent stage or any ancestor. Refer to the multi-stage builds section in the manual for more information.\n\nEnvironment variable persistence can cause unexpected side effects. For example, setting ENV DEBIAN_FRONTEND=noninteractive changes the behavior of apt-get , and may confuse users of your image.\n\nIf an environment variable is only needed during build, and not in the final image, consider setting a value for a single command instead:\n\nRUN DEBIAN_FRONTEND = noninteractive apt-get update && apt-get install -y ...\n\nOr using ARG , which is not persisted in the final image:\n\nARG DEBIAN_FRONTEND = noninteractive RUN apt-get update && apt-get install -y ...\n\nNote Alternative syntax The ENV instruction also allows an alternative syntax ENV <key> <value> , omitting the = . For example: ENV MY_VAR my-value This syntax does not allow for multiple environment-variables to be set in a single ENV instruction, and can be confusing. For example, the following sets a single environment variable ( ONE ) with value \"TWO= THREE=world\" : ENV ONE TWO = THREE = world The alternative syntax is supported for backward compatibility, but discouraged for the reasons outlined above, and may be removed in a future release.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2292
        }
      },
      {
        "header": "ADD",
        "content": "ADD has two forms. The latter form is required for paths containing whitespace.\n\nADD [ OPTIONS ] <src> ... <dest> ADD [ OPTIONS ] [ \"<src>\" , ... \"<dest>\" ]\n\nThe available [OPTIONS] are:\n\nOption Minimum Dockerfile version --keep-git-dir 1.1 --checksum 1.6 --chown --chmod 1.2 --link 1.4 --exclude 1.19\n\nThe ADD instruction copies new files or directories from <src> and adds them to the filesystem of the image at the path <dest> . Files and directories can be copied from the build context, a remote URL, or a Git repository.\n\nThe ADD and COPY instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between ADD and COPY .\n\nSource\n\nYou can specify multiple source files or directories with ADD . The last argument must always be the destination. For example, to add two files, file1.txt and file2.txt , from the build context to /usr/src/things/ in the build container:\n\nADD file1.txt file2.txt /usr/src/things/\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash / ).\n\nTo add files from a remote location, you can specify a URL or the address of a Git repository as the source. For example:\n\nADD https://example.com/archive.zip /usr/src/things/ ADD git@github.com:user/repo.git /usr/src/things/\n\nBuildKit detects the type of <src> and processes it accordingly.\n\nIf <src> is a local file or directory, the contents of the directory are copied to the specified destination. See Adding files from the build context . If <src> is a local tar archive, it is decompressed and extracted to the specified destination. See Adding local tar archives . If <src> is a URL, the contents of the URL are downloaded and placed at the specified destination. See Adding files from a URL . If <src> is a Git repository, the repository is cloned to the specified destination. See Adding files from a Git repository .\n\nAdding files from the build context\n\nAny relative or local path that doesn't begin with a http:// , https:// , or git@ protocol prefix is considered a local file path. The local file path is relative to the build context. For example, if the build context is the current directory, ADD file.txt / adds the file at ./file.txt to the root of the filesystem in the build container.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as ADD ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making ADD something/ /something equivalent to ADD something /something .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the ADD instruction to copy remote files. You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nPattern matching\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png :\n\nADD *.png /dest/\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts .\n\nADD index.?s /dest/\n\nWhen adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following;\n\nADD arr [[] 0 ] .txt /dest/\n\nAdding local tar archives\n\nWhen using a local tar archive as the source for ADD , and the archive is in a recognized compression format ( gzip , bzip2 or xz , or uncompressed), the archive is decompressed and extracted into the specified destination. Local tar archives are extracted by default, see the [ ADD --unpack flag].\n\nWhen a directory is extracted, it has the same behavior as tar -x . The result is the union of:\n\nWhatever existed at the destination path, and The contents of the source tree, with conflicts resolved in favor of the content being added, on a file-by-file basis.\n\nNote Whether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with .tar.gz this isn't recognized as a compressed file and doesn't generate any kind of decompression error message, rather the file will simply be copied to the destination.\n\nAdding files from a URL\n\nIn the case where source is a remote file URL, the destination will have permissions of 600. If the HTTP response contains a Last-Modified header, the timestamp from that header will be used to set the mtime on the destination file. However, like any other file processed during an ADD , mtime isn't included in the determination of whether or not the file has changed and the cache should be updated.\n\nIf remote file is a tar archive, the archive is not extracted by default. To download and extract the archive, use the [ ADD --unpack flag].\n\nIf the destination ends with a trailing slash, then the filename is inferred from the URL path. For example, ADD http://example.com/foobar / would create the file /foobar . The URL must have a nontrivial path so that an appropriate filename can be discovered ( http://example.com doesn't work).\n\nIf the destination doesn't end with a trailing slash, the destination path becomes the filename of the file downloaded from the URL. For example, ADD http://example.com/foo /bar creates the file /bar .\n\nIf your URL files are protected using authentication, you need to use RUN wget , RUN curl or use another tool from within the container as the ADD instruction doesn't support authentication.\n\nAdding files from a Git repository\n\nTo use a Git repository as the source for ADD , you can reference the repository's HTTP or SSH address as the source. The repository is cloned to the specified destination in the image.\n\nADD https://github.com/user/repo.git /mydir/\n\nYou can use URL fragments to specify a specific branch, tag, commit, or subdirectory. For example, to add the docs directory of the v0.14.1 tag of the buildkit repository:\n\nADD git@github.com:moby/buildkit.git#v0.14.1:docs /buildkit-docs\n\nFor more information about Git URL fragments, see URL fragments .\n\nWhen adding from a Git repository, the permissions bits for files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nWhen using a Git repository as the source, the repository must be accessible from the build context. To add a repository via SSH, whether public or private, you must pass an SSH key for authentication. For example, given the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM alpine ADD git@git.example.com:foo/bar.git /bar\n\nTo build this Dockerfile, pass the --ssh flag to the docker build to mount the SSH agent socket to the build. For example:\n\n$ docker build --ssh default .\n\nFor more information about building with secrets, see Build secrets .\n\nDestination\n\nIf the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\n# create /abs/test.txt ADD test.txt /abs/\n\nTrailing slashes are significant. For example, ADD test.txt /abs creates a file at /abs , whereas ADD test.txt /abs/ creates /abs/test.txt .\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nWORKDIR /usr/src/app # create /usr/src/app/rel/test.txt ADD test.txt rel/\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.\n\nADD --keep-git-dir\n\nADD [ --keep-git-dir = <boolean> ] <src> ... <dir>\n\nWhen <src> is the HTTP or SSH address of a remote Git repository, BuildKit adds the contents of the Git repository to the image excluding the .git directory by default.\n\nThe --keep-git-dir=true flag lets you preserve the .git directory.\n\n# syntax=docker/dockerfile:1 FROM alpine ADD --keep-git-dir = true https://github.com/moby/buildkit.git#v0.10.1 /buildkit\n\nADD --checksum\n\nADD [ --checksum = <hash> ] <src> ... <dir>\n\nThe --checksum flag lets you verify the checksum of a remote resource. The checksum is formatted as sha256:<hash> . SHA-256 is the only supported hash algorithm.\n\nADD --checksum = sha256:24454f830cdb571e2c4ad15481119c43b3cafd48dd869a9b2945d1036d1dc68d https://mirrors.edge.kernel.org/pub/linux/kernel/Historic/linux-0.01.tar.gz /\n\nThe --checksum flag only supports HTTP(S) sources.\n\nADD --chown --chmod\n\nSee COPY --chown --chmod .\n\nADD --link\n\nSee COPY --link .\n\nADD --exclude\n\nSee COPY --exclude .\n\nADD --unpack\n\nADD [ --unpack = <bool> ] <src> ... <dir>\n\nThe --unpack flag controls whether or not to automatically unpack tar archives (including compressed formats like gzip or bzip2 ) when adding them to the image. Local tar archives are unpacked by default, whereas remote tar archives (where src is a URL) are downloaded without unpacking.\n\n# syntax=docker/dockerfile:1 FROM alpine # Download and unpack archive.tar.gz into /download: ADD --unpack = true https://example.com/archive.tar.gz /download # Add local tar without unpacking: ADD --unpack = false my-archive.tar.gz .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 44,
          "content_length": 10375
        }
      },
      {
        "header": "Source",
        "content": "You can specify multiple source files or directories with ADD . The last argument must always be the destination. For example, to add two files, file1.txt and file2.txt , from the build context to /usr/src/things/ in the build container:\n\nADD file1.txt file2.txt /usr/src/things/\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash / ).\n\nTo add files from a remote location, you can specify a URL or the address of a Git repository as the source. For example:\n\nADD https://example.com/archive.zip /usr/src/things/ ADD git@github.com:user/repo.git /usr/src/things/\n\nBuildKit detects the type of <src> and processes it accordingly.\n\nIf <src> is a local file or directory, the contents of the directory are copied to the specified destination. See Adding files from the build context . If <src> is a local tar archive, it is decompressed and extracted to the specified destination. See Adding local tar archives . If <src> is a URL, the contents of the URL are downloaded and placed at the specified destination. See Adding files from a URL . If <src> is a Git repository, the repository is cloned to the specified destination. See Adding files from a Git repository .\n\nAdding files from the build context\n\nAny relative or local path that doesn't begin with a http:// , https:// , or git@ protocol prefix is considered a local file path. The local file path is relative to the build context. For example, if the build context is the current directory, ADD file.txt / adds the file at ./file.txt to the root of the filesystem in the build container.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as ADD ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making ADD something/ /something equivalent to ADD something /something .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the ADD instruction to copy remote files. You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nPattern matching\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png :\n\nADD *.png /dest/\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts .\n\nADD index.?s /dest/\n\nWhen adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following;\n\nADD arr [[] 0 ] .txt /dest/\n\nAdding local tar archives\n\nWhen using a local tar archive as the source for ADD , and the archive is in a recognized compression format ( gzip , bzip2 or xz , or uncompressed), the archive is decompressed and extracted into the specified destination. Local tar archives are extracted by default, see the [ ADD --unpack flag].\n\nWhen a directory is extracted, it has the same behavior as tar -x . The result is the union of:\n\nWhatever existed at the destination path, and The contents of the source tree, with conflicts resolved in favor of the content being added, on a file-by-file basis.\n\nNote Whether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with .tar.gz this isn't recognized as a compressed file and doesn't generate any kind of decompression error message, rather the file will simply be copied to the destination.\n\nAdding files from a URL\n\nIn the case where source is a remote file URL, the destination will have permissions of 600. If the HTTP response contains a Last-Modified header, the timestamp from that header will be used to set the mtime on the destination file. However, like any other file processed during an ADD , mtime isn't included in the determination of whether or not the file has changed and the cache should be updated.\n\nIf remote file is a tar archive, the archive is not extracted by default. To download and extract the archive, use the [ ADD --unpack flag].\n\nIf the destination ends with a trailing slash, then the filename is inferred from the URL path. For example, ADD http://example.com/foobar / would create the file /foobar . The URL must have a nontrivial path so that an appropriate filename can be discovered ( http://example.com doesn't work).\n\nIf the destination doesn't end with a trailing slash, the destination path becomes the filename of the file downloaded from the URL. For example, ADD http://example.com/foo /bar creates the file /bar .\n\nIf your URL files are protected using authentication, you need to use RUN wget , RUN curl or use another tool from within the container as the ADD instruction doesn't support authentication.\n\nAdding files from a Git repository\n\nTo use a Git repository as the source for ADD , you can reference the repository's HTTP or SSH address as the source. The repository is cloned to the specified destination in the image.\n\nADD https://github.com/user/repo.git /mydir/\n\nYou can use URL fragments to specify a specific branch, tag, commit, or subdirectory. For example, to add the docs directory of the v0.14.1 tag of the buildkit repository:\n\nADD git@github.com:moby/buildkit.git#v0.14.1:docs /buildkit-docs\n\nFor more information about Git URL fragments, see URL fragments .\n\nWhen adding from a Git repository, the permissions bits for files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nWhen using a Git repository as the source, the repository must be accessible from the build context. To add a repository via SSH, whether public or private, you must pass an SSH key for authentication. For example, given the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM alpine ADD git@git.example.com:foo/bar.git /bar\n\nTo build this Dockerfile, pass the --ssh flag to the docker build to mount the SSH agent socket to the build. For example:\n\n$ docker build --ssh default .\n\nFor more information about building with secrets, see Build secrets .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 27,
          "content_length": 7254
        }
      },
      {
        "header": "Adding files from the build context",
        "content": "Any relative or local path that doesn't begin with a http:// , https:// , or git@ protocol prefix is considered a local file path. The local file path is relative to the build context. For example, if the build context is the current directory, ADD file.txt / adds the file at ./file.txt to the root of the filesystem in the build container.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as ADD ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making ADD something/ /something equivalent to ADD something /something .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the ADD instruction to copy remote files. You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nPattern matching\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png :\n\nADD *.png /dest/\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts .\n\nADD index.?s /dest/\n\nWhen adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following;\n\nADD arr [[] 0 ] .txt /dest/",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2394
        }
      },
      {
        "header": "Adding local tar archives",
        "content": "When using a local tar archive as the source for ADD , and the archive is in a recognized compression format ( gzip , bzip2 or xz , or uncompressed), the archive is decompressed and extracted into the specified destination. Local tar archives are extracted by default, see the [ ADD --unpack flag].\n\nWhen a directory is extracted, it has the same behavior as tar -x . The result is the union of:\n\nWhatever existed at the destination path, and The contents of the source tree, with conflicts resolved in favor of the content being added, on a file-by-file basis.\n\nNote Whether a file is identified as a recognized compression format or not is done solely based on the contents of the file, not the name of the file. For example, if an empty file happens to end with .tar.gz this isn't recognized as a compressed file and doesn't generate any kind of decompression error message, rather the file will simply be copied to the destination.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 935
        }
      },
      {
        "header": "Adding files from a URL",
        "content": "In the case where source is a remote file URL, the destination will have permissions of 600. If the HTTP response contains a Last-Modified header, the timestamp from that header will be used to set the mtime on the destination file. However, like any other file processed during an ADD , mtime isn't included in the determination of whether or not the file has changed and the cache should be updated.\n\nIf remote file is a tar archive, the archive is not extracted by default. To download and extract the archive, use the [ ADD --unpack flag].\n\nIf the destination ends with a trailing slash, then the filename is inferred from the URL path. For example, ADD http://example.com/foobar / would create the file /foobar . The URL must have a nontrivial path so that an appropriate filename can be discovered ( http://example.com doesn't work).\n\nIf the destination doesn't end with a trailing slash, the destination path becomes the filename of the file downloaded from the URL. For example, ADD http://example.com/foo /bar creates the file /bar .\n\nIf your URL files are protected using authentication, you need to use RUN wget , RUN curl or use another tool from within the container as the ADD instruction doesn't support authentication.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1234
        }
      },
      {
        "header": "Adding files from a Git repository",
        "content": "To use a Git repository as the source for ADD , you can reference the repository's HTTP or SSH address as the source. The repository is cloned to the specified destination in the image.\n\nADD https://github.com/user/repo.git /mydir/\n\nYou can use URL fragments to specify a specific branch, tag, commit, or subdirectory. For example, to add the docs directory of the v0.14.1 tag of the buildkit repository:\n\nADD git@github.com:moby/buildkit.git#v0.14.1:docs /buildkit-docs\n\nFor more information about Git URL fragments, see URL fragments .\n\nWhen adding from a Git repository, the permissions bits for files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nWhen using a Git repository as the source, the repository must be accessible from the build context. To add a repository via SSH, whether public or private, you must pass an SSH key for authentication. For example, given the following Dockerfile:\n\n# syntax=docker/dockerfile:1 FROM alpine ADD git@git.example.com:foo/bar.git /bar\n\nTo build this Dockerfile, pass the --ssh flag to the docker build to mount the SSH agent socket to the build. For example:\n\n$ docker build --ssh default .\n\nFor more information about building with secrets, see Build secrets .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1305
        }
      },
      {
        "header": "Destination",
        "content": "If the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\n# create /abs/test.txt ADD test.txt /abs/\n\nTrailing slashes are significant. For example, ADD test.txt /abs creates a file at /abs , whereas ADD test.txt /abs/ creates /abs/test.txt .\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nWORKDIR /usr/src/app # create /usr/src/app/rel/test.txt ADD test.txt rel/\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 842
        }
      },
      {
        "header": "ADD --keep-git-dir",
        "content": "ADD [ --keep-git-dir = <boolean> ] <src> ... <dir>\n\nWhen <src> is the HTTP or SSH address of a remote Git repository, BuildKit adds the contents of the Git repository to the image excluding the .git directory by default.\n\nThe --keep-git-dir=true flag lets you preserve the .git directory.\n\n# syntax=docker/dockerfile:1 FROM alpine ADD --keep-git-dir = true https://github.com/moby/buildkit.git#v0.10.1 /buildkit",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 411
        }
      },
      {
        "header": "ADD --checksum",
        "content": "ADD [ --checksum = <hash> ] <src> ... <dir>\n\nThe --checksum flag lets you verify the checksum of a remote resource. The checksum is formatted as sha256:<hash> . SHA-256 is the only supported hash algorithm.\n\nADD --checksum = sha256:24454f830cdb571e2c4ad15481119c43b3cafd48dd869a9b2945d1036d1dc68d https://mirrors.edge.kernel.org/pub/linux/kernel/Historic/linux-0.01.tar.gz /\n\nThe --checksum flag only supports HTTP(S) sources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 426
        }
      },
      {
        "header": "ADD --chown --chmod",
        "content": "See COPY --chown --chmod .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 26
        }
      },
      {
        "header": "ADD --link",
        "content": "See COPY --link .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 17
        }
      },
      {
        "header": "ADD --exclude",
        "content": "See COPY --exclude .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 20
        }
      },
      {
        "header": "ADD --unpack",
        "content": "ADD [ --unpack = <bool> ] <src> ... <dir>\n\nThe --unpack flag controls whether or not to automatically unpack tar archives (including compressed formats like gzip or bzip2 ) when adding them to the image. Local tar archives are unpacked by default, whereas remote tar archives (where src is a URL) are downloaded without unpacking.\n\n# syntax=docker/dockerfile:1 FROM alpine # Download and unpack archive.tar.gz into /download: ADD --unpack = true https://example.com/archive.tar.gz /download # Add local tar without unpacking: ADD --unpack = false my-archive.tar.gz .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 566
        }
      },
      {
        "header": "COPY",
        "content": "COPY has two forms. The latter form is required for paths containing whitespace.\n\nCOPY [ OPTIONS ] <src> ... <dest> COPY [ OPTIONS ] [ \"<src>\" , ... \"<dest>\" ]\n\nThe available [OPTIONS] are:\n\nOption Minimum Dockerfile version --from --chown --chmod 1.2 --link 1.4 --parents 1.20 --exclude 1.19\n\nThe COPY instruction copies new files or directories from <src> and adds them to the filesystem of the image at the path <dest> . Files and directories can be copied from the build context, build stage, named context, or an image.\n\nThe ADD and COPY instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between ADD and COPY .\n\nSource\n\nYou can specify multiple source files or directories with COPY . The last argument must always be the destination. For example, to copy two files, file1.txt and file2.txt , from the build context to /usr/src/things/ in the build container:\n\nCOPY file1.txt file2.txt /usr/src/things/\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash / ).\n\nCOPY accepts a flag --from=<name> that lets you specify the source location to be a build stage, context, or image. The following example copies files from a stage named build :\n\nFROM golang AS build WORKDIR /app RUN --mount = type = bind,target = . go build -o /myapp ./cmd COPY --from = build /myapp /usr/bin/\n\nFor more information about copying from named sources, see the --from flag .\n\nCopying from the build context\n\nWhen copying source files from the build context, paths are interpreted as relative to the root of the context.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as COPY ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making COPY something/ /something equivalent to COPY something /something .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the COPY instruction to copy files from other stages, named contexts, or images, using the --from flag . You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nWhen using a Git repository as the build context, the permissions bits for copied files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nPattern matching\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png :\n\nCOPY *.png /dest/\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts .\n\nCOPY index.?s /dest/\n\nWhen adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following;\n\nCOPY arr [[] 0 ] .txt /dest/\n\nDestination\n\nIf the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\n# create /abs/test.txt COPY test.txt /abs/\n\nTrailing slashes are significant. For example, COPY test.txt /abs creates a file at /abs , whereas COPY test.txt /abs/ creates /abs/test.txt .\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nWORKDIR /usr/src/app # create /usr/src/app/rel/test.txt COPY test.txt rel/\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.\n\nCOPY --from\n\nBy default, the COPY instruction copies files from the build context. The COPY --from flag lets you copy files from an image, a build stage, or a named context instead.\n\nCOPY [ --from = <image | stage | context> ] <src> ... <dest>\n\nTo copy from a build stage in a multi-stage build , specify the name of the stage you want to copy from. You specify stage names using the AS keyword with the FROM instruction.\n\n# syntax=docker/dockerfile:1 FROM alpine AS build COPY . . RUN apk add clang RUN clang -o /hello hello.c FROM scratch COPY --from = build /hello /\n\nYou can also copy files directly from named contexts (specified with --build-context <name>=<source> ) or images. The following example copies an nginx.conf file from the official Nginx image.\n\nCOPY --from = nginx:latest /etc/nginx/nginx.conf /nginx.conf\n\nThe source path of COPY --from is always resolved from filesystem root of the image or stage that you specify.\n\nCOPY --chown --chmod\n\nNote Only octal notation is currently supported. Non-octal support is tracked in moby/buildkit#1951 .\n\nCOPY [ --chown = <user>:<group> ] [ --chmod = <perms> ... ] <src> ... <dest>\n\nThe --chown and --chmod features are only supported on Dockerfiles used to build Linux containers, and doesn't work on Windows containers. Since user and group ownership concepts do not translate between Linux and Windows, the use of /etc/passwd and /etc/group for translating user and group names to IDs restricts this feature to only be viable for Linux OS-based containers.\n\nAll files and directories copied from the build context are created with a UID and GID of 0 unless the optional --chown flag specifies a given username, groupname, or UID/GID combination to request specific ownership of the copied content. The format of the --chown flag allows for either username and groupname strings or direct integer UID and GID in any combination. Providing a username without groupname or a UID without GID will use the same numeric UID as the GID. If a username or groupname is provided, the container's root filesystem /etc/passwd and /etc/group files will be used to perform the translation from name to integer UID or GID respectively. The following examples show valid definitions for the --chown flag:\n\nCOPY --chown = 55:mygroup files* /somedir/ COPY --chown = bin files* /somedir/ COPY --chown = 1 files* /somedir/ COPY --chown = 10:11 files* /somedir/ COPY --chown = myuser:mygroup --chmod = 644 files* /somedir/\n\nIf the container root filesystem doesn't contain either /etc/passwd or /etc/group files and either user or group names are used in the --chown flag, the build will fail on the COPY operation. Using numeric IDs requires no lookup and does not depend on container root filesystem content.\n\nWith the Dockerfile syntax version 1.10.0 and later, the --chmod flag supports variable interpolation, which lets you define the permission bits using build arguments:\n\n# syntax=docker/dockerfile:1.10 FROM alpine WORKDIR /src ARG MODE = 440 COPY --chmod = $MODE . .\n\nCOPY --link\n\nCOPY [ --link [= <boolean> ]] <src> ... <dest>\n\nEnabling this flag in COPY or ADD commands allows you to copy files with enhanced semantics where your files remain independent on their own layer and don't get invalidated when commands on previous layers are changed.\n\nWhen --link is used your source files are copied into an empty destination directory. That directory is turned into a layer that is linked on top of your previous state.\n\n# syntax=docker/dockerfile:1 FROM alpine COPY --link /foo /bar\n\nIs equivalent of doing two builds:\n\nFROM alpine\n\nand\n\nFROM scratch COPY /foo /bar\n\nand merging all the layers of both images together.\n\nBenefits of using --link\n\nUse --link to reuse already built layers in subsequent builds with --cache-from even if the previous layers have changed. This is especially important for multi-stage builds where a COPY --from statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With --link the layer the previous build generated is reused and merged on top of the new layers. This also means you can easily rebase your images when the base images receive updates, without having to execute the whole build again. In backends that support it, BuildKit can do this rebase action without the need to push or pull any layers between the client and the registry. BuildKit will detect this case and only create new image manifest that contains the new layers and old layers in correct order.\n\nThe same behavior where BuildKit can avoid pulling down the base image can also happen when using --link and no other commands that would require access to the files in the base image. In that case BuildKit will only build the layers for the COPY commands and push them to the registry directly on top of the layers of the base image.\n\nIncompatibilities with --link=false\n\nWhen using --link the COPY/ADD commands are not allowed to read any files from the previous state. This means that if in previous state the destination directory was a path that contained a symlink, COPY/ADD can not follow it. In the final image the destination path created with --link will always be a path containing only directories.\n\nIf you don't rely on the behavior of following symlinks in the destination path, using --link is always recommended. The performance of --link is equivalent or better than the default behavior and, it creates much better conditions for cache reuse.\n\nCOPY --parents\n\nCOPY [ --parents [= <boolean> ]] <src> ... <dest>\n\nThe --parents flag preserves parent directories for src entries. This flag defaults to false .\n\n# syntax=docker/dockerfile:1 FROM scratch COPY ./x/a.txt ./y/a.txt /no_parents/ COPY --parents ./x/a.txt ./y/a.txt /parents/ # /no_parents/a.txt # /parents/x/a.txt # /parents/y/a.txt\n\nThis behavior is similar to the Linux cp utility's --parents or rsync --relative flag.\n\nAs with Rsync, it is possible to limit which parent directories are preserved by inserting a dot and a slash ( ./ ) into the source path. If such point exists, only parent directories after it will be preserved. This may be especially useful copies between stages with --from where the source paths need to be absolute.\n\n# syntax=docker/dockerfile:1 FROM scratch COPY --parents ./x/./y/*.txt /parents/ # Build context: # ./x/y/a.txt # ./x/y/b.txt # # Output: # /parents/y/a.txt # /parents/y/b.txt\n\nNote that, without the --parents flag specified, any filename collision will fail the Linux cp operation with an explicit error message ( cp: will not overwrite just-created './x/a.txt' with './y/a.txt' ), where the Buildkit will silently overwrite the target file at the destination.\n\nWhile it is possible to preserve the directory structure for COPY instructions consisting of only one src entry, usually it is more beneficial to keep the layer count in the resulting image as low as possible. Therefore, with the --parents flag, the Buildkit is capable of packing multiple COPY instructions together, keeping the directory structure intact.\n\nCOPY --exclude\n\nCOPY [ --exclude = <path> ... ] <src> ... <dest>\n\nThe --exclude flag lets you specify a path expression for files to be excluded.\n\nThe path expression follows the same format as <src> , supporting wildcards and matching using Go's filepath.Match rules. For example, to add all files starting with \"hom\", excluding files with a .txt extension:\n\n# syntax=docker/dockerfile:1 FROM scratch COPY --exclude = *.txt hom* /mydir/\n\nYou can specify the --exclude option multiple times for a COPY instruction. Multiple --excludes are files matching its patterns not to be copied, even if the files paths match the pattern specified in <src> . To add all files starting with \"hom\", excluding files with either .txt or .md extensions:\n\n# syntax=docker/dockerfile:1 FROM scratch COPY --exclude = *.txt --exclude = *.md hom* /mydir/",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 48,
          "content_length": 12794
        }
      },
      {
        "header": "Source",
        "content": "You can specify multiple source files or directories with COPY . The last argument must always be the destination. For example, to copy two files, file1.txt and file2.txt , from the build context to /usr/src/things/ in the build container:\n\nCOPY file1.txt file2.txt /usr/src/things/\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash / ).\n\nCOPY accepts a flag --from=<name> that lets you specify the source location to be a build stage, context, or image. The following example copies files from a stage named build :\n\nFROM golang AS build WORKDIR /app RUN --mount = type = bind,target = . go build -o /myapp ./cmd COPY --from = build /myapp /usr/bin/\n\nFor more information about copying from named sources, see the --from flag .\n\nCopying from the build context\n\nWhen copying source files from the build context, paths are interpreted as relative to the root of the context.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as COPY ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making COPY something/ /something equivalent to COPY something /something .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the COPY instruction to copy files from other stages, named contexts, or images, using the --from flag . You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nWhen using a Git repository as the build context, the permissions bits for copied files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nPattern matching\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png :\n\nCOPY *.png /dest/\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts .\n\nCOPY index.?s /dest/\n\nWhen adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following;\n\nCOPY arr [[] 0 ] .txt /dest/",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 3317
        }
      },
      {
        "header": "Copying from the build context",
        "content": "When copying source files from the build context, paths are interpreted as relative to the root of the context.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as COPY ../something /something , automatically removes any parent directory navigation ( ../ ). Trailing slashes in the source path are also disregarded, making COPY something/ /something equivalent to COPY something /something .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( docker build - < Dockerfile ), there is no build context. In this case, you can only use the COPY instruction to copy files from other stages, named contexts, or images, using the --from flag . You can also pass a tar archive through stdin: ( docker build - < archive.tar ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nWhen using a Git repository as the build context, the permissions bits for copied files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nPattern matching\n\nFor local files, each <src> may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with .png :\n\nCOPY *.png /dest/\n\nIn the following example, ? is a single-character wildcard, matching e.g. index.js and index.ts .\n\nCOPY index.?s /dest/\n\nWhen adding files or directories that contain special characters (such as [ and ] ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named arr[0].txt , use the following;\n\nCOPY arr [[] 0 ] .txt /dest/",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2465
        }
      },
      {
        "header": "Destination",
        "content": "If the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\n# create /abs/test.txt COPY test.txt /abs/\n\nTrailing slashes are significant. For example, COPY test.txt /abs creates a file at /abs , whereas COPY test.txt /abs/ creates /abs/test.txt .\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nWORKDIR /usr/src/app # create /usr/src/app/rel/test.txt COPY test.txt rel/\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 846
        }
      },
      {
        "header": "COPY --from",
        "content": "By default, the COPY instruction copies files from the build context. The COPY --from flag lets you copy files from an image, a build stage, or a named context instead.\n\nCOPY [ --from = <image | stage | context> ] <src> ... <dest>\n\nTo copy from a build stage in a multi-stage build , specify the name of the stage you want to copy from. You specify stage names using the AS keyword with the FROM instruction.\n\n# syntax=docker/dockerfile:1 FROM alpine AS build COPY . . RUN apk add clang RUN clang -o /hello hello.c FROM scratch COPY --from = build /hello /\n\nYou can also copy files directly from named contexts (specified with --build-context <name>=<source> ) or images. The following example copies an nginx.conf file from the official Nginx image.\n\nCOPY --from = nginx:latest /etc/nginx/nginx.conf /nginx.conf\n\nThe source path of COPY --from is always resolved from filesystem root of the image or stage that you specify.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 924
        }
      },
      {
        "header": "COPY --chown --chmod",
        "content": "Note Only octal notation is currently supported. Non-octal support is tracked in moby/buildkit#1951 .\n\nCOPY [ --chown = <user>:<group> ] [ --chmod = <perms> ... ] <src> ... <dest>\n\nThe --chown and --chmod features are only supported on Dockerfiles used to build Linux containers, and doesn't work on Windows containers. Since user and group ownership concepts do not translate between Linux and Windows, the use of /etc/passwd and /etc/group for translating user and group names to IDs restricts this feature to only be viable for Linux OS-based containers.\n\nAll files and directories copied from the build context are created with a UID and GID of 0 unless the optional --chown flag specifies a given username, groupname, or UID/GID combination to request specific ownership of the copied content. The format of the --chown flag allows for either username and groupname strings or direct integer UID and GID in any combination. Providing a username without groupname or a UID without GID will use the same numeric UID as the GID. If a username or groupname is provided, the container's root filesystem /etc/passwd and /etc/group files will be used to perform the translation from name to integer UID or GID respectively. The following examples show valid definitions for the --chown flag:\n\nCOPY --chown = 55:mygroup files* /somedir/ COPY --chown = bin files* /somedir/ COPY --chown = 1 files* /somedir/ COPY --chown = 10:11 files* /somedir/ COPY --chown = myuser:mygroup --chmod = 644 files* /somedir/\n\nIf the container root filesystem doesn't contain either /etc/passwd or /etc/group files and either user or group names are used in the --chown flag, the build will fail on the COPY operation. Using numeric IDs requires no lookup and does not depend on container root filesystem content.\n\nWith the Dockerfile syntax version 1.10.0 and later, the --chmod flag supports variable interpolation, which lets you define the permission bits using build arguments:\n\n# syntax=docker/dockerfile:1.10 FROM alpine WORKDIR /src ARG MODE = 440 COPY --chmod = $MODE . .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 2057
        }
      },
      {
        "header": "COPY --link",
        "content": "COPY [ --link [= <boolean> ]] <src> ... <dest>\n\nEnabling this flag in COPY or ADD commands allows you to copy files with enhanced semantics where your files remain independent on their own layer and don't get invalidated when commands on previous layers are changed.\n\nWhen --link is used your source files are copied into an empty destination directory. That directory is turned into a layer that is linked on top of your previous state.\n\n# syntax=docker/dockerfile:1 FROM alpine COPY --link /foo /bar\n\nIs equivalent of doing two builds:\n\nFROM alpine\n\nand\n\nFROM scratch COPY /foo /bar\n\nand merging all the layers of both images together.\n\nBenefits of using --link\n\nUse --link to reuse already built layers in subsequent builds with --cache-from even if the previous layers have changed. This is especially important for multi-stage builds where a COPY --from statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With --link the layer the previous build generated is reused and merged on top of the new layers. This also means you can easily rebase your images when the base images receive updates, without having to execute the whole build again. In backends that support it, BuildKit can do this rebase action without the need to push or pull any layers between the client and the registry. BuildKit will detect this case and only create new image manifest that contains the new layers and old layers in correct order.\n\nThe same behavior where BuildKit can avoid pulling down the base image can also happen when using --link and no other commands that would require access to the files in the base image. In that case BuildKit will only build the layers for the COPY commands and push them to the registry directly on top of the layers of the base image.\n\nIncompatibilities with --link=false\n\nWhen using --link the COPY/ADD commands are not allowed to read any files from the previous state. This means that if in previous state the destination directory was a path that contained a symlink, COPY/ADD can not follow it. In the final image the destination path created with --link will always be a path containing only directories.\n\nIf you don't rely on the behavior of following symlinks in the destination path, using --link is always recommended. The performance of --link is equivalent or better than the default behavior and, it creates much better conditions for cache reuse.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2482
        }
      },
      {
        "header": "Benefits of using --link",
        "content": "Use --link to reuse already built layers in subsequent builds with --cache-from even if the previous layers have changed. This is especially important for multi-stage builds where a COPY --from statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With --link the layer the previous build generated is reused and merged on top of the new layers. This also means you can easily rebase your images when the base images receive updates, without having to execute the whole build again. In backends that support it, BuildKit can do this rebase action without the need to push or pull any layers between the client and the registry. BuildKit will detect this case and only create new image manifest that contains the new layers and old layers in correct order.\n\nThe same behavior where BuildKit can avoid pulling down the base image can also happen when using --link and no other commands that would require access to the files in the base image. In that case BuildKit will only build the layers for the COPY commands and push them to the registry directly on top of the layers of the base image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1191
        }
      },
      {
        "header": "Incompatibilities with --link=false",
        "content": "When using --link the COPY/ADD commands are not allowed to read any files from the previous state. This means that if in previous state the destination directory was a path that contained a symlink, COPY/ADD can not follow it. In the final image the destination path created with --link will always be a path containing only directories.\n\nIf you don't rely on the behavior of following symlinks in the destination path, using --link is always recommended. The performance of --link is equivalent or better than the default behavior and, it creates much better conditions for cache reuse.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 587
        }
      },
      {
        "header": "COPY --parents",
        "content": "COPY [ --parents [= <boolean> ]] <src> ... <dest>\n\nThe --parents flag preserves parent directories for src entries. This flag defaults to false .\n\n# syntax=docker/dockerfile:1 FROM scratch COPY ./x/a.txt ./y/a.txt /no_parents/ COPY --parents ./x/a.txt ./y/a.txt /parents/ # /no_parents/a.txt # /parents/x/a.txt # /parents/y/a.txt\n\nThis behavior is similar to the Linux cp utility's --parents or rsync --relative flag.\n\nAs with Rsync, it is possible to limit which parent directories are preserved by inserting a dot and a slash ( ./ ) into the source path. If such point exists, only parent directories after it will be preserved. This may be especially useful copies between stages with --from where the source paths need to be absolute.\n\n# syntax=docker/dockerfile:1 FROM scratch COPY --parents ./x/./y/*.txt /parents/ # Build context: # ./x/y/a.txt # ./x/y/b.txt # # Output: # /parents/y/a.txt # /parents/y/b.txt\n\nNote that, without the --parents flag specified, any filename collision will fail the Linux cp operation with an explicit error message ( cp: will not overwrite just-created './x/a.txt' with './y/a.txt' ), where the Buildkit will silently overwrite the target file at the destination.\n\nWhile it is possible to preserve the directory structure for COPY instructions consisting of only one src entry, usually it is more beneficial to keep the layer count in the resulting image as low as possible. Therefore, with the --parents flag, the Buildkit is capable of packing multiple COPY instructions together, keeping the directory structure intact.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1560
        }
      },
      {
        "header": "COPY --exclude",
        "content": "COPY [ --exclude = <path> ... ] <src> ... <dest>\n\nThe --exclude flag lets you specify a path expression for files to be excluded.\n\nThe path expression follows the same format as <src> , supporting wildcards and matching using Go's filepath.Match rules. For example, to add all files starting with \"hom\", excluding files with a .txt extension:\n\n# syntax=docker/dockerfile:1 FROM scratch COPY --exclude = *.txt hom* /mydir/\n\nYou can specify the --exclude option multiple times for a COPY instruction. Multiple --excludes are files matching its patterns not to be copied, even if the files paths match the pattern specified in <src> . To add all files starting with \"hom\", excluding files with either .txt or .md extensions:\n\n# syntax=docker/dockerfile:1 FROM scratch COPY --exclude = *.txt --exclude = *.md hom* /mydir/",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 817
        }
      },
      {
        "header": "ENTRYPOINT",
        "content": "An ENTRYPOINT allows you to configure a container that will run as an executable.\n\nENTRYPOINT has two possible forms:\n\nThe exec form, which is the preferred form: ENTRYPOINT [ \"executable\" , \"param1\" , \"param2\" ] The shell form: ENTRYPOINT command param1 param2\n\nFor more information about the different forms, see Shell and exec form .\n\nThe following command starts a container from the nginx with its default content, listening on port 80:\n\n$ docker run -i -t --rm -p 80:80 nginx\n\nCommand line arguments to docker run <image> will be appended after all elements in an exec form ENTRYPOINT , and will override all elements specified using CMD .\n\nThis allows arguments to be passed to the entry point, i.e., docker run <image> -d will pass the -d argument to the entry point. You can override the ENTRYPOINT instruction using the docker run --entrypoint flag.\n\nThe shell form of ENTRYPOINT prevents any CMD command line arguments from being used. It also starts your ENTRYPOINT as a subcommand of /bin/sh -c , which does not pass signals. This means that the executable will not be the container's PID 1 , and will not receive Unix signals. In this case, your executable doesn't receive a SIGTERM from docker stop <container> .\n\nOnly the last ENTRYPOINT instruction in the Dockerfile will have an effect.\n\nExec form ENTRYPOINT example\n\nYou can use the exec form of ENTRYPOINT to set fairly stable default commands and arguments and then use either form of CMD to set additional defaults that are more likely to be changed.\n\nFROM ubuntu ENTRYPOINT [ \"top\" , \"-b\" ] CMD [ \"-c\" ]\n\nWhen you run the container, you can see that top is the only process:\n\n$ docker run -it --rm --name test top -H top - 08:25:00 up 7:27, 0 users, load average: 0.00, 0.01, 0.05 Threads: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie % Cpu ( s ) : 0.1 us, 0.1 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 2056668 total, 1616832 used, 439836 free, 99352 buffers KiB Swap: 1441840 total, 0 used, 1441840 free. 1324440 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 19744 2336 2080 R 0.0 0.1 0:00.04 top\n\nTo examine the result further, you can use docker exec :\n\n$ docker exec -it test ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 2.6 0.1 19752 2352 ? Ss+ 08:24 0:00 top -b -H root 7 0.0 0.1 15572 2164 ? R+ 08:25 0:00 ps aux\n\nAnd you can gracefully request top to shut down using docker stop test .\n\nThe following Dockerfile shows using the ENTRYPOINT to run Apache in the foreground (i.e., as PID 1 ):\n\nFROM debian:stable RUN apt-get update && apt-get install -y --force-yes apache2 EXPOSE 80 443 VOLUME [ \"/var/www\" , \"/var/log/apache2\" , \"/etc/apache2\" ] ENTRYPOINT [ \"/usr/sbin/apache2ctl\" , \"-D\" , \"FOREGROUND\" ]\n\nIf you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using exec and gosu commands:\n\n#!/usr/bin/env bash set -e if [ \" $1 \" = 'postgres' ] ; then chown -R postgres \" $PGDATA \" if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then gosu postgres initdb fi exec gosu postgres \" $@ \" fi exec \" $@ \"\n\nLastly, if you need to do some extra cleanup (or communicate with other containers) on shutdown, or are co-ordinating more than one executable, you may need to ensure that the ENTRYPOINT script receives the Unix signals, passes them on, and then does some more work:\n\n#!/bin/sh # Note: I've written this using sh so it works in the busybox container too # USE the trap if you need to also do manual cleanup after the service is stopped, # or need to start multiple services in the one container trap \"echo TRAPed signal\" HUP INT QUIT TERM # start service in background here /usr/sbin/apachectl start echo \"[hit enter key to exit] or run 'docker stop <container>'\" read # stop service and clean up here echo \"stopping apache\" /usr/sbin/apachectl stop echo \"exited $0 \"\n\nIf you run this image with docker run -it --rm -p 80:80 --name test apache , you can then examine the container's processes with docker exec , or docker top , and then ask the script to stop Apache:\n\n$ docker exec -it test ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.0 4448 692 ? Ss+ 00:42 0:00 /bin/sh /run.sh 123 cmd cmd2 root 19 0.0 0.2 71304 4440 ? Ss 00:42 0:00 /usr/sbin/apache2 -k start www-data 20 0.2 0.2 360468 6004 ? Sl 00:42 0:00 /usr/sbin/apache2 -k start www-data 21 0.2 0.2 360468 6000 ? Sl 00:42 0:00 /usr/sbin/apache2 -k start root 81 0.0 0.1 15572 2140 ? R+ 00:44 0:00 ps aux $ docker top test PID USER COMMAND 10035 root {run.sh} /bin/sh /run.sh 123 cmd cmd2 10054 root /usr/sbin/apache2 -k start 10055 33 /usr/sbin/apache2 -k start 10056 33 /usr/sbin/apache2 -k start $ /usr/bin/time docker stop test test real 0m 0.27s user 0m 0.03s sys 0m 0.03s\n\nNote You can override the ENTRYPOINT setting using --entrypoint , but this can only set the binary to exec (no sh -c will be used).\n\nShell form ENTRYPOINT example\n\nYou can specify a plain string for the ENTRYPOINT and it will execute in /bin/sh -c . This form will use shell processing to substitute shell environment variables, and will ignore any CMD or docker run command line arguments. To ensure that docker stop will signal any long running ENTRYPOINT executable correctly, you need to remember to start it with exec :\n\nFROM ubuntu ENTRYPOINT exec top -b\n\nWhen you run this image, you'll see the single PID 1 process:\n\n$ docker run -it --rm --name test top Mem: 1704520K used, 352148K free, 0K shrd, 0K buff, 140368121167873K cached CPU: 5% usr 0% sys 0% nic 94% idle 0% io 0% irq 0% sirq Load average: 0.08 0.03 0.05 2/98 6 PID PPID USER STAT VSZ %VSZ %CPU COMMAND 1 0 root R 3164 0% 0% top -b\n\nWhich exits cleanly on docker stop :\n\n$ /usr/bin/time docker stop test test real 0m 0.20s user 0m 0.02s sys 0m 0.04s\n\nIf you forget to add exec to the beginning of your ENTRYPOINT :\n\nFROM ubuntu ENTRYPOINT top -b CMD -- --ignored-param1\n\nYou can then run it (giving it a name for the next step):\n\n$ docker run -it --name test top --ignored-param2 top - 13:58:24 up 17 min, 0 users, load average: 0.00, 0.00, 0.00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie % Cpu ( s ) : 16.7 us, 33.3 sy, 0.0 ni, 50.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 1990.8 total, 1354.6 free, 231.4 used, 404.7 buff/cache MiB Swap: 1024.0 total, 1024.0 free, 0.0 used. 1639.8 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 2612 604 536 S 0.0 0.0 0:00.02 sh 6 root 20 0 5956 3188 2768 R 0.0 0.2 0:00.00 top\n\nYou can see from the output of top that the specified ENTRYPOINT is not PID 1 .\n\nIf you then run docker stop test , the container will not exit cleanly - the stop command will be forced to send a SIGKILL after the timeout:\n\n$ docker exec -it test ps waux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.4 0.0 2612 604 pts/0 Ss+ 13:58 0:00 /bin/sh -c top -b --ignored-param2 root 6 0.0 0.1 5956 3188 pts/0 S+ 13:58 0:00 top -b root 7 0.0 0.1 5884 2816 pts/1 Rs+ 13:58 0:00 ps waux $ /usr/bin/time docker stop test test real 0m 10.19s user 0m 0.04s sys 0m 0.03s\n\nUnderstand how CMD and ENTRYPOINT interact\n\nBoth CMD and ENTRYPOINT instructions define what command gets executed when running a container. There are few rules that describe their co-operation.\n\nDockerfile should specify at least one of CMD or ENTRYPOINT commands. ENTRYPOINT should be defined when using the container as an executable. CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container. CMD will be overridden when running the container with alternative arguments.\n\nThe table below shows what command is executed for different ENTRYPOINT / CMD combinations:\n\nNo ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [\"exec_entry\", \"p1_entry\"] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [\"exec_cmd\", \"p1_cmd\"] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd\n\nNote If CMD is defined from the base image, setting ENTRYPOINT will reset CMD to an empty value. In this scenario, CMD must be defined in the current image to have a value.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 25,
          "content_length": 8325
        }
      },
      {
        "header": "Exec form ENTRYPOINT example",
        "content": "You can use the exec form of ENTRYPOINT to set fairly stable default commands and arguments and then use either form of CMD to set additional defaults that are more likely to be changed.\n\nFROM ubuntu ENTRYPOINT [ \"top\" , \"-b\" ] CMD [ \"-c\" ]\n\nWhen you run the container, you can see that top is the only process:\n\n$ docker run -it --rm --name test top -H top - 08:25:00 up 7:27, 0 users, load average: 0.00, 0.01, 0.05 Threads: 1 total, 1 running, 0 sleeping, 0 stopped, 0 zombie % Cpu ( s ) : 0.1 us, 0.1 sy, 0.0 ni, 99.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 2056668 total, 1616832 used, 439836 free, 99352 buffers KiB Swap: 1441840 total, 0 used, 1441840 free. 1324440 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 19744 2336 2080 R 0.0 0.1 0:00.04 top\n\nTo examine the result further, you can use docker exec :\n\n$ docker exec -it test ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 2.6 0.1 19752 2352 ? Ss+ 08:24 0:00 top -b -H root 7 0.0 0.1 15572 2164 ? R+ 08:25 0:00 ps aux\n\nAnd you can gracefully request top to shut down using docker stop test .\n\nThe following Dockerfile shows using the ENTRYPOINT to run Apache in the foreground (i.e., as PID 1 ):\n\nFROM debian:stable RUN apt-get update && apt-get install -y --force-yes apache2 EXPOSE 80 443 VOLUME [ \"/var/www\" , \"/var/log/apache2\" , \"/etc/apache2\" ] ENTRYPOINT [ \"/usr/sbin/apache2ctl\" , \"-D\" , \"FOREGROUND\" ]\n\nIf you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using exec and gosu commands:\n\n#!/usr/bin/env bash set -e if [ \" $1 \" = 'postgres' ] ; then chown -R postgres \" $PGDATA \" if [ -z \" $( ls -A \" $PGDATA \" ) \" ] ; then gosu postgres initdb fi exec gosu postgres \" $@ \" fi exec \" $@ \"\n\nLastly, if you need to do some extra cleanup (or communicate with other containers) on shutdown, or are co-ordinating more than one executable, you may need to ensure that the ENTRYPOINT script receives the Unix signals, passes them on, and then does some more work:\n\n#!/bin/sh # Note: I've written this using sh so it works in the busybox container too # USE the trap if you need to also do manual cleanup after the service is stopped, # or need to start multiple services in the one container trap \"echo TRAPed signal\" HUP INT QUIT TERM # start service in background here /usr/sbin/apachectl start echo \"[hit enter key to exit] or run 'docker stop <container>'\" read # stop service and clean up here echo \"stopping apache\" /usr/sbin/apachectl stop echo \"exited $0 \"\n\nIf you run this image with docker run -it --rm -p 80:80 --name test apache , you can then examine the container's processes with docker exec , or docker top , and then ask the script to stop Apache:\n\n$ docker exec -it test ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.1 0.0 4448 692 ? Ss+ 00:42 0:00 /bin/sh /run.sh 123 cmd cmd2 root 19 0.0 0.2 71304 4440 ? Ss 00:42 0:00 /usr/sbin/apache2 -k start www-data 20 0.2 0.2 360468 6004 ? Sl 00:42 0:00 /usr/sbin/apache2 -k start www-data 21 0.2 0.2 360468 6000 ? Sl 00:42 0:00 /usr/sbin/apache2 -k start root 81 0.0 0.1 15572 2140 ? R+ 00:44 0:00 ps aux $ docker top test PID USER COMMAND 10035 root {run.sh} /bin/sh /run.sh 123 cmd cmd2 10054 root /usr/sbin/apache2 -k start 10055 33 /usr/sbin/apache2 -k start 10056 33 /usr/sbin/apache2 -k start $ /usr/bin/time docker stop test test real 0m 0.27s user 0m 0.03s sys 0m 0.03s\n\nNote You can override the ENTRYPOINT setting using --entrypoint , but this can only set the binary to exec (no sh -c will be used).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3596
        }
      },
      {
        "header": "Shell form ENTRYPOINT example",
        "content": "You can specify a plain string for the ENTRYPOINT and it will execute in /bin/sh -c . This form will use shell processing to substitute shell environment variables, and will ignore any CMD or docker run command line arguments. To ensure that docker stop will signal any long running ENTRYPOINT executable correctly, you need to remember to start it with exec :\n\nFROM ubuntu ENTRYPOINT exec top -b\n\nWhen you run this image, you'll see the single PID 1 process:\n\n$ docker run -it --rm --name test top Mem: 1704520K used, 352148K free, 0K shrd, 0K buff, 140368121167873K cached CPU: 5% usr 0% sys 0% nic 94% idle 0% io 0% irq 0% sirq Load average: 0.08 0.03 0.05 2/98 6 PID PPID USER STAT VSZ %VSZ %CPU COMMAND 1 0 root R 3164 0% 0% top -b\n\nWhich exits cleanly on docker stop :\n\n$ /usr/bin/time docker stop test test real 0m 0.20s user 0m 0.02s sys 0m 0.04s\n\nIf you forget to add exec to the beginning of your ENTRYPOINT :\n\nFROM ubuntu ENTRYPOINT top -b CMD -- --ignored-param1\n\nYou can then run it (giving it a name for the next step):\n\n$ docker run -it --name test top --ignored-param2 top - 13:58:24 up 17 min, 0 users, load average: 0.00, 0.00, 0.00 Tasks: 2 total, 1 running, 1 sleeping, 0 stopped, 0 zombie % Cpu ( s ) : 16.7 us, 33.3 sy, 0.0 ni, 50.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st MiB Mem : 1990.8 total, 1354.6 free, 231.4 used, 404.7 buff/cache MiB Swap: 1024.0 total, 1024.0 free, 0.0 used. 1639.8 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 2612 604 536 S 0.0 0.0 0:00.02 sh 6 root 20 0 5956 3188 2768 R 0.0 0.2 0:00.00 top\n\nYou can see from the output of top that the specified ENTRYPOINT is not PID 1 .\n\nIf you then run docker stop test , the container will not exit cleanly - the stop command will be forced to send a SIGKILL after the timeout:\n\n$ docker exec -it test ps waux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.4 0.0 2612 604 pts/0 Ss+ 13:58 0:00 /bin/sh -c top -b --ignored-param2 root 6 0.0 0.1 5956 3188 pts/0 S+ 13:58 0:00 top -b root 7 0.0 0.1 5884 2816 pts/1 Rs+ 13:58 0:00 ps waux $ /usr/bin/time docker stop test test real 0m 10.19s user 0m 0.04s sys 0m 0.03s",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 2145
        }
      },
      {
        "header": "Understand how CMD and ENTRYPOINT interact",
        "content": "Both CMD and ENTRYPOINT instructions define what command gets executed when running a container. There are few rules that describe their co-operation.\n\nDockerfile should specify at least one of CMD or ENTRYPOINT commands. ENTRYPOINT should be defined when using the container as an executable. CMD should be used as a way of defining default arguments for an ENTRYPOINT command or for executing an ad-hoc command in a container. CMD will be overridden when running the container with alternative arguments.\n\nThe table below shows what command is executed for different ENTRYPOINT / CMD combinations:\n\nNo ENTRYPOINT ENTRYPOINT exec_entry p1_entry ENTRYPOINT [\"exec_entry\", \"p1_entry\"] No CMD error, not allowed /bin/sh -c exec_entry p1_entry exec_entry p1_entry CMD [\"exec_cmd\", \"p1_cmd\"] exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry exec_cmd p1_cmd CMD exec_cmd p1_cmd /bin/sh -c exec_cmd p1_cmd /bin/sh -c exec_entry p1_entry exec_entry p1_entry /bin/sh -c exec_cmd p1_cmd\n\nNote If CMD is defined from the base image, setting ENTRYPOINT will reset CMD to an empty value. In this scenario, CMD must be defined in the current image to have a value.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1169
        }
      },
      {
        "header": "VOLUME",
        "content": "VOLUME [ \"/data\" ]\n\nThe VOLUME instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, VOLUME [\"/var/log/\"] , or a plain string with multiple arguments, such as VOLUME /var/log or VOLUME /var/log /var/db . For more information/examples and mounting instructions via the Docker client, refer to Share Directories via Volumes documentation.\n\nThe docker run command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet:\n\nFROM ubuntu RUN mkdir /myvol RUN echo \"hello world\" > /myvol/greeting VOLUME /myvol\n\nThis Dockerfile results in an image that causes docker run to create a new mount point at /myvol and copy the greeting file into the newly created volume.\n\nNotes about specifying volumes\n\nKeep the following things in mind about volumes in the Dockerfile.\n\nVolumes on Windows-based containers : When using Windows-based containers, the destination of a volume inside the container must be one of: a non-existing or empty directory a drive other than C: a non-existing or empty directory a drive other than C: Changing the volume from within the Dockerfile : If any build steps change the data within the volume after it has been declared, those changes will be discarded when using the legacy builder. When using Buildkit, the changes will instead be kept. JSON formatting : The list is parsed as a JSON array. You must enclose words with double quotes ( \" ) rather than single quotes ( ' ). The host directory is declared at container run-time : The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can't be guaranteed to be available on all hosts. For this reason, you can't mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 2094
        }
      },
      {
        "header": "Notes about specifying volumes",
        "content": "Keep the following things in mind about volumes in the Dockerfile.\n\nVolumes on Windows-based containers : When using Windows-based containers, the destination of a volume inside the container must be one of: a non-existing or empty directory a drive other than C: a non-existing or empty directory a drive other than C: Changing the volume from within the Dockerfile : If any build steps change the data within the volume after it has been declared, those changes will be discarded when using the legacy builder. When using Buildkit, the changes will instead be kept. JSON formatting : The list is parsed as a JSON array. You must enclose words with double quotes ( \" ) rather than single quotes ( ' ). The host directory is declared at container run-time : The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can't be guaranteed to be available on all hosts. For this reason, you can't mount a host directory from within the Dockerfile. The VOLUME instruction does not support specifying a host-dir parameter. You must specify the mountpoint when you create or run the container.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1167
        }
      },
      {
        "header": "USER",
        "content": "USER <user>[:<group>]\n\nor\n\nUSER <UID>[:<GID>]\n\nThe USER instruction sets the user name (or UID) and optionally the user group (or GID) to use as the default user and group for the remainder of the current stage. The specified user is used for RUN instructions and at runtime, runs the relevant ENTRYPOINT and CMD commands.\n\nNote that when specifying a group for the user, the user will have only the specified group membership. Any other configured group memberships will be ignored.\n\nWarning When the user doesn't have a primary group then the image (or the next instructions) will be run with the root group. On Windows, the user must be created first if it's not a built-in account. This can be done with the net user command called as part of a Dockerfile.\n\nFROM microsoft/windowsservercore # Create Windows user in the container RUN net user /add patrick # Set it for subsequent commands USER patrick",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 905
        }
      },
      {
        "header": "WORKDIR",
        "content": "WORKDIR /path/to/workdir\n\nThe WORKDIR instruction sets the working directory for any RUN , CMD , ENTRYPOINT , COPY and ADD instructions that follow it in the Dockerfile. If the WORKDIR doesn't exist, it will be created even if it's not used in any subsequent Dockerfile instruction.\n\nThe WORKDIR instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\nWORKDIR /a WORKDIR b WORKDIR c RUN pwd\n\nThe output of the final pwd command in this Dockerfile would be /a/b/c .\n\nThe WORKDIR instruction can resolve environment variables previously set using ENV . You can only use environment variables explicitly set in the Dockerfile. For example:\n\nENV DIRPATH = /path WORKDIR $DIRPATH/$DIRNAME RUN pwd\n\nThe output of the final pwd command in this Dockerfile would be /path/$DIRNAME\n\nIf not specified, the default working directory is / . In practice, if you aren't building a Dockerfile from scratch ( FROM scratch ), the WORKDIR may likely be set by the base image you're using.\n\nTherefore, to avoid unintended operations in unknown directories, it's best practice to set your WORKDIR explicitly.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1202
        }
      },
      {
        "header": "ARG",
        "content": "ARG <name> [= <default value> ] [ <name> [= <default value> ] ... ]\n\nThe ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg <varname>=<value> flag.\n\nWarning It isn't recommended to use build arguments for passing secrets such as user credentials, API tokens, etc. Build arguments are visible in the docker history command and in max mode provenance attestations, which are attached to the image by default if you use the Buildx GitHub Actions and your GitHub repository is public. Refer to the RUN --mount=type=secret section to learn about secure ways to use secrets when building images.\n\nA Dockerfile may include one or more ARG instructions. For example, the following is a valid Dockerfile:\n\nFROM busybox ARG user1 ARG buildno # ...\n\nDefault values\n\nAn ARG instruction can optionally include a default value:\n\nFROM busybox ARG user1 = someuser ARG buildno = 1 # ...\n\nIf an ARG instruction has a default value and if there is no value passed at build-time, the builder uses the default.\n\nScope\n\nAn ARG variable comes into effect from the line on which it is declared in the Dockerfile. For example, consider this Dockerfile:\n\nFROM busybox USER ${username:-some_user} ARG username USER $username # ...\n\nA user builds this file by calling:\n\n$ docker build --build-arg username = what_user .\n\nThe USER instruction on line 2 evaluates to the some_user fallback, because the username variable is not yet declared. The username variable is declared on line 3, and available for reference in Dockerfile instruction from that point onwards. The USER instruction on line 4 evaluates to what_user , since at that point the username argument has a value of what_user which was passed on the command line. Prior to its definition by an ARG instruction, any use of a variable results in an empty string.\n\nAn ARG variable declared within a build stage is automatically inherited by other stages based on that stage. Unrelated build stages do not have access to the variable. To use an argument in multiple distinct stages, each stage must include the ARG instruction, or they must both be based on a shared base stage in the same Dockerfile where the variable is declared.\n\nFor more information, refer to variable scoping .\n\nUsing ARG variables\n\nYou can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Consider this Dockerfile with an ENV and ARG instruction.\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = v1.0.0 RUN echo $CONT_IMG_VER\n\nThen, assume this image is built with this command:\n\n$ docker build --build-arg CONT_IMG_VER = v2.0.1 .\n\nIn this case, the RUN instruction uses v1.0.0 instead of the ARG setting passed by the user: v2.0.1 This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition.\n\nUsing the example above but a different ENV specification you can create more useful interactions between ARG and ENV instructions:\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = ${ CONT_IMG_VER :- v1 .0.0 } RUN echo $CONT_IMG_VER\n\nUnlike an ARG instruction, ENV values are always persisted in the built image. Consider a docker build without the --build-arg flag:\n\n$ docker build .\n\nUsing this Dockerfile example, CONT_IMG_VER is still persisted in the image but its value would be v1.0.0 as it is the default set in line 3 by the ENV instruction.\n\nThe variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the ENV instruction. Variable expansion is only supported for a limited set of Dockerfile instructions.\n\nPredefined ARGs\n\nDocker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile.\n\nHTTP_PROXY http_proxy HTTPS_PROXY https_proxy FTP_PROXY ftp_proxy NO_PROXY no_proxy ALL_PROXY all_proxy\n\nTo use these, pass them on the command line using the --build-arg flag, for example:\n\n$ docker build --build-arg HTTPS_PROXY = https://my-proxy.example.com .\n\nBy default, these pre-defined variables are excluded from the output of docker history . Excluding them reduces the risk of accidentally leaking sensitive authentication information in an HTTP_PROXY variable.\n\nFor example, consider building the following Dockerfile using --build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.com\n\nFROM ubuntu RUN echo \"Hello World\"\n\nIn this case, the value of the HTTP_PROXY variable is not available in the docker history and is not cached. If you were to change location, and your proxy server changed to http://user:pass@proxy.sfo.example.com , a subsequent build does not result in a cache miss.\n\nIf you need to override this behaviour then you may do so by adding an ARG statement in the Dockerfile as follows:\n\nFROM ubuntu ARG HTTP_PROXY RUN echo \"Hello World\"\n\nWhen building this Dockerfile, the HTTP_PROXY is preserved in the docker history , and changing its value invalidates the build cache.\n\nAutomatic platform ARGs in the global scope\n\nThis feature is only available when using the BuildKit backend.\n\nBuildKit supports a predefined set of ARG variables with information on the platform of the node performing the build (build platform) and on the platform of the resulting image (target platform). The target platform can be specified with the --platform flag on docker build .\n\nThe following ARG variables are set automatically:\n\nTARGETPLATFORM - platform of the build result. Eg linux/amd64 , linux/arm/v7 , windows/amd64 . TARGETOS - OS component of TARGETPLATFORM TARGETARCH - architecture component of TARGETPLATFORM TARGETVARIANT - variant component of TARGETPLATFORM BUILDPLATFORM - platform of the node performing the build. BUILDOS - OS component of BUILDPLATFORM BUILDARCH - architecture component of BUILDPLATFORM BUILDVARIANT - variant component of BUILDPLATFORM\n\nThese arguments are defined in the global scope so are not automatically available inside build stages or for your RUN commands. To expose one of these arguments inside the build stage redefine it without value.\n\nFor example:\n\nFROM alpine ARG TARGETPLATFORM RUN echo \"I'm building for $TARGETPLATFORM \"\n\nBuildKit built-in build args\n\nArg Type Description BUILDKIT_BUILD_NAME String Override the build name shown in buildx history command and Docker Desktop Builds view . BUILDKIT_CACHE_MOUNT_NS String Set optional cache ID namespace. BUILDKIT_CONTEXT_KEEP_GIT_DIR Bool Trigger Git context to keep the .git directory. BUILDKIT_HISTORY_PROVENANCE_V1 Bool Enable SLSA Provenance v1 for build history record. BUILDKIT_INLINE_CACHE 2 Bool Inline cache metadata to image config or not. BUILDKIT_MULTI_PLATFORM Bool Opt into deterministic output regardless of multi-platform output or not. BUILDKIT_SANDBOX_HOSTNAME String Set the hostname (default buildkitsandbox ) BUILDKIT_SYNTAX String Set frontend image SOURCE_DATE_EPOCH Int Set the Unix timestamp for created image and layers. More info from reproducible builds . Supported since Dockerfile 1.5, BuildKit 0.11\n\nExample: keep .git dir\n\nWhen using a Git context, .git dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build:\n\n# syntax=docker/dockerfile:1 FROM alpine WORKDIR /src RUN --mount = target = . \\ make REVISION = $( git rev-parse HEAD ) build\n\n$ docker build --build-arg BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1 https://github.com/user/repo.git#main\n\nImpact on build caching\n\nARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile.\n\nFor example, consider these two Dockerfile:\n\nFROM ubuntu ARG CONT_IMG_VER RUN echo $CONT_IMG_VER\n\nFROM ubuntu ARG CONT_IMG_VER RUN echo hello\n\nIf you specify --build-arg CONT_IMG_VER=<value> on the command line, in both cases, the specification on line 2 doesn't cause a cache miss; line 3 does cause a cache miss. ARG CONT_IMG_VER causes the RUN line to be identified as the same as running CONT_IMG_VER=<value> echo hello , so if the <value> changes, you get a cache miss.\n\nConsider another example under the same command line:\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = $CONT_IMG_VER RUN echo $CONT_IMG_VER\n\nIn this example, the cache miss occurs on line 3. The miss happens because the variable's value in the ENV references the ARG variable and that variable is changed through the command line. In this example, the ENV command causes the image to include the value.\n\nIf an ENV instruction overrides an ARG instruction of the same name, like this Dockerfile:\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = hello RUN echo $CONT_IMG_VER\n\nLine 3 doesn't cause a cache miss because the value of CONT_IMG_VER is a constant ( hello ). As a result, the environment variables and values used on the RUN (line 4) doesn't change between builds.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 35,
          "content_length": 9507
        }
      },
      {
        "header": "Default values",
        "content": "An ARG instruction can optionally include a default value:\n\nFROM busybox ARG user1 = someuser ARG buildno = 1 # ...\n\nIf an ARG instruction has a default value and if there is no value passed at build-time, the builder uses the default.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 235
        }
      },
      {
        "header": "Scope",
        "content": "An ARG variable comes into effect from the line on which it is declared in the Dockerfile. For example, consider this Dockerfile:\n\nFROM busybox USER ${username:-some_user} ARG username USER $username # ...\n\nA user builds this file by calling:\n\n$ docker build --build-arg username = what_user .\n\nThe USER instruction on line 2 evaluates to the some_user fallback, because the username variable is not yet declared. The username variable is declared on line 3, and available for reference in Dockerfile instruction from that point onwards. The USER instruction on line 4 evaluates to what_user , since at that point the username argument has a value of what_user which was passed on the command line. Prior to its definition by an ARG instruction, any use of a variable results in an empty string.\n\nAn ARG variable declared within a build stage is automatically inherited by other stages based on that stage. Unrelated build stages do not have access to the variable. To use an argument in multiple distinct stages, each stage must include the ARG instruction, or they must both be based on a shared base stage in the same Dockerfile where the variable is declared.\n\nFor more information, refer to variable scoping .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1214
        }
      },
      {
        "header": "Using ARG variables",
        "content": "You can use an ARG or an ENV instruction to specify variables that are available to the RUN instruction. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Consider this Dockerfile with an ENV and ARG instruction.\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = v1.0.0 RUN echo $CONT_IMG_VER\n\nThen, assume this image is built with this command:\n\n$ docker build --build-arg CONT_IMG_VER = v2.0.1 .\n\nIn this case, the RUN instruction uses v1.0.0 instead of the ARG setting passed by the user: v2.0.1 This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition.\n\nUsing the example above but a different ENV specification you can create more useful interactions between ARG and ENV instructions:\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = ${ CONT_IMG_VER :- v1 .0.0 } RUN echo $CONT_IMG_VER\n\nUnlike an ARG instruction, ENV values are always persisted in the built image. Consider a docker build without the --build-arg flag:\n\n$ docker build .\n\nUsing this Dockerfile example, CONT_IMG_VER is still persisted in the image but its value would be v1.0.0 as it is the default set in line 3 by the ENV instruction.\n\nThe variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the ENV instruction. Variable expansion is only supported for a limited set of Dockerfile instructions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1543
        }
      },
      {
        "header": "Predefined ARGs",
        "content": "Docker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile.\n\nHTTP_PROXY http_proxy HTTPS_PROXY https_proxy FTP_PROXY ftp_proxy NO_PROXY no_proxy ALL_PROXY all_proxy\n\nTo use these, pass them on the command line using the --build-arg flag, for example:\n\n$ docker build --build-arg HTTPS_PROXY = https://my-proxy.example.com .\n\nBy default, these pre-defined variables are excluded from the output of docker history . Excluding them reduces the risk of accidentally leaking sensitive authentication information in an HTTP_PROXY variable.\n\nFor example, consider building the following Dockerfile using --build-arg HTTP_PROXY=http://user:pass@proxy.lon.example.com\n\nFROM ubuntu RUN echo \"Hello World\"\n\nIn this case, the value of the HTTP_PROXY variable is not available in the docker history and is not cached. If you were to change location, and your proxy server changed to http://user:pass@proxy.sfo.example.com , a subsequent build does not result in a cache miss.\n\nIf you need to override this behaviour then you may do so by adding an ARG statement in the Dockerfile as follows:\n\nFROM ubuntu ARG HTTP_PROXY RUN echo \"Hello World\"\n\nWhen building this Dockerfile, the HTTP_PROXY is preserved in the docker history , and changing its value invalidates the build cache.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1326
        }
      },
      {
        "header": "Automatic platform ARGs in the global scope",
        "content": "This feature is only available when using the BuildKit backend.\n\nBuildKit supports a predefined set of ARG variables with information on the platform of the node performing the build (build platform) and on the platform of the resulting image (target platform). The target platform can be specified with the --platform flag on docker build .\n\nThe following ARG variables are set automatically:\n\nTARGETPLATFORM - platform of the build result. Eg linux/amd64 , linux/arm/v7 , windows/amd64 . TARGETOS - OS component of TARGETPLATFORM TARGETARCH - architecture component of TARGETPLATFORM TARGETVARIANT - variant component of TARGETPLATFORM BUILDPLATFORM - platform of the node performing the build. BUILDOS - OS component of BUILDPLATFORM BUILDARCH - architecture component of BUILDPLATFORM BUILDVARIANT - variant component of BUILDPLATFORM\n\nThese arguments are defined in the global scope so are not automatically available inside build stages or for your RUN commands. To expose one of these arguments inside the build stage redefine it without value.\n\nFor example:\n\nFROM alpine ARG TARGETPLATFORM RUN echo \"I'm building for $TARGETPLATFORM \"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1142
        }
      },
      {
        "header": "BuildKit built-in build args",
        "content": "Arg Type Description BUILDKIT_BUILD_NAME String Override the build name shown in buildx history command and Docker Desktop Builds view . BUILDKIT_CACHE_MOUNT_NS String Set optional cache ID namespace. BUILDKIT_CONTEXT_KEEP_GIT_DIR Bool Trigger Git context to keep the .git directory. BUILDKIT_HISTORY_PROVENANCE_V1 Bool Enable SLSA Provenance v1 for build history record. BUILDKIT_INLINE_CACHE 2 Bool Inline cache metadata to image config or not. BUILDKIT_MULTI_PLATFORM Bool Opt into deterministic output regardless of multi-platform output or not. BUILDKIT_SANDBOX_HOSTNAME String Set the hostname (default buildkitsandbox ) BUILDKIT_SYNTAX String Set frontend image SOURCE_DATE_EPOCH Int Set the Unix timestamp for created image and layers. More info from reproducible builds . Supported since Dockerfile 1.5, BuildKit 0.11\n\nExample: keep .git dir\n\nWhen using a Git context, .git dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build:\n\n# syntax=docker/dockerfile:1 FROM alpine WORKDIR /src RUN --mount = target = . \\ make REVISION = $( git rev-parse HEAD ) build\n\n$ docker build --build-arg BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1 https://github.com/user/repo.git#main",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1234
        }
      },
      {
        "header": "Example: keep .git dir",
        "content": "When using a Git context, .git dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build:\n\n# syntax=docker/dockerfile:1 FROM alpine WORKDIR /src RUN --mount = target = . \\ make REVISION = $( git rev-parse HEAD ) build\n\n$ docker build --build-arg BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1 https://github.com/user/repo.git#main",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 382
        }
      },
      {
        "header": "Impact on build caching",
        "content": "ARG variables are not persisted into the built image as ENV variables are. However, ARG variables do impact the build cache in similar ways. If a Dockerfile defines an ARG variable whose value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition. In particular, all RUN instructions following an ARG instruction use the ARG variable implicitly (as an environment variable), thus can cause a cache miss. All predefined ARG variables are exempt from caching unless there is a matching ARG statement in the Dockerfile.\n\nFor example, consider these two Dockerfile:\n\nFROM ubuntu ARG CONT_IMG_VER RUN echo $CONT_IMG_VER\n\nFROM ubuntu ARG CONT_IMG_VER RUN echo hello\n\nIf you specify --build-arg CONT_IMG_VER=<value> on the command line, in both cases, the specification on line 2 doesn't cause a cache miss; line 3 does cause a cache miss. ARG CONT_IMG_VER causes the RUN line to be identified as the same as running CONT_IMG_VER=<value> echo hello , so if the <value> changes, you get a cache miss.\n\nConsider another example under the same command line:\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = $CONT_IMG_VER RUN echo $CONT_IMG_VER\n\nIn this example, the cache miss occurs on line 3. The miss happens because the variable's value in the ENV references the ARG variable and that variable is changed through the command line. In this example, the ENV command causes the image to include the value.\n\nIf an ENV instruction overrides an ARG instruction of the same name, like this Dockerfile:\n\nFROM ubuntu ARG CONT_IMG_VER ENV CONT_IMG_VER = hello RUN echo $CONT_IMG_VER\n\nLine 3 doesn't cause a cache miss because the value of CONT_IMG_VER is a constant ( hello ). As a result, the environment variables and values used on the RUN (line 4) doesn't change between builds.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1816
        }
      },
      {
        "header": "ONBUILD",
        "content": "ONBUILD <INSTRUCTION>\n\nThe ONBUILD instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile.\n\nThis is useful if you are building an image which will be used as a base to build other images, for example an application build environment or a daemon which may be customized with user-specific configuration.\n\nFor example, if your image is a reusable Python application builder, it will require application source code to be added in a particular directory, and it might require a build script to be called after that. You can't just call ADD and RUN now, because you don't yet have access to the application source code, and it will be different for each application build. You could simply provide application developers with a boilerplate Dockerfile to copy-paste into their application, but that's inefficient, error-prone and difficult to update because it mixes with application-specific code.\n\nThe solution is to use ONBUILD to register advance instructions to run later, during the next build stage.\n\nHere's how it works:\n\nWhen it encounters an ONBUILD instruction, the builder adds a trigger to the metadata of the image being built. The instruction doesn't otherwise affect the current build. At the end of the build, a list of all triggers is stored in the image manifest, under the key OnBuild . They can be inspected with the docker inspect command. Later the image may be used as a base for a new build, using the FROM instruction. As part of processing the FROM instruction, the downstream builder looks for ONBUILD triggers, and executes them in the same order they were registered. If any of the triggers fail, the FROM instruction is aborted which in turn causes the build to fail. If all triggers succeed, the FROM instruction completes and the build continues as usual. Triggers are cleared from the final image after being executed. In other words they aren't inherited by \"grand-children\" builds.\n\nFor example you might add something like this:\n\nONBUILD ADD . /app/src ONBUILD RUN /usr/local/bin/python-build --dir /app/src\n\nCopy or mount from stage, image, or context\n\nAs of Dockerfile syntax 1.11, you can use ONBUILD with instructions that copy or mount files from other stages, images, or build contexts. For example:\n\n# syntax=docker/dockerfile:1.11 FROM alpine AS baseimage ONBUILD COPY --from = build /usr/bin/app /app ONBUILD RUN --mount = from = config,target = /opt/appconfig ...\n\nIf the source of from is a build stage, the stage must be defined in the Dockerfile where ONBUILD gets triggered. If it's a named context, that context must be passed to the downstream build.\n\nONBUILD limitations\n\nChaining ONBUILD instructions using ONBUILD ONBUILD isn't allowed. The ONBUILD instruction may not trigger FROM or MAINTAINER instructions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3007
        }
      },
      {
        "header": "Copy or mount from stage, image, or context",
        "content": "As of Dockerfile syntax 1.11, you can use ONBUILD with instructions that copy or mount files from other stages, images, or build contexts. For example:\n\n# syntax=docker/dockerfile:1.11 FROM alpine AS baseimage ONBUILD COPY --from = build /usr/bin/app /app ONBUILD RUN --mount = from = config,target = /opt/appconfig ...\n\nIf the source of from is a build stage, the stage must be defined in the Dockerfile where ONBUILD gets triggered. If it's a named context, that context must be passed to the downstream build.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 512
        }
      },
      {
        "header": "ONBUILD limitations",
        "content": "Chaining ONBUILD instructions using ONBUILD ONBUILD isn't allowed. The ONBUILD instruction may not trigger FROM or MAINTAINER instructions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 139
        }
      },
      {
        "header": "STOPSIGNAL",
        "content": "STOPSIGNAL signal\n\nThe STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit. This signal can be a signal name in the format SIG<NAME> , for instance SIGKILL , or an unsigned number that matches a position in the kernel's syscall table, for instance 9 . The default is SIGTERM if not defined.\n\nThe image's default stopsignal can be overridden per container, using the --stop-signal flag on docker run and docker create .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 460
        }
      },
      {
        "header": "HEALTHCHECK",
        "content": "The HEALTHCHECK instruction has two forms:\n\nHEALTHCHECK [OPTIONS] CMD command (check container health by running a command inside the container) HEALTHCHECK NONE (disable any healthcheck inherited from the base image)\n\nThe HEALTHCHECK instruction tells Docker how to test a container to check that it's still working. This can detect cases such as a web server stuck in an infinite loop and unable to handle new connections, even though the server process is still running.\n\nWhen a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially starting . Whenever a health check passes, it becomes healthy (whatever state it was previously in). After a certain number of consecutive failures, it becomes unhealthy .\n\nThe options that can appear before CMD are:\n\n--interval=DURATION (default: 30s ) --timeout=DURATION (default: 30s ) --start-period=DURATION (default: 0s ) --start-interval=DURATION (default: 5s ) --retries=N (default: 3 )\n\nThe health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes.\n\nIf a single run of the check takes longer than timeout seconds then the check is considered to have failed. The process performing the check is abruptly stopped with a SIGKILL .\n\nIt takes retries consecutive failures of the health check for the container to be considered unhealthy .\n\nstart period provides initialization time for containers that need time to bootstrap. Probe failure during that period will not be counted towards the maximum number of retries. However, if a health check succeeds during the start period, the container is considered started and all consecutive failures will be counted towards the maximum number of retries.\n\nstart interval is the time between health checks during the start period. This option requires Docker Engine version 25.0 or later.\n\nThere can only be one HEALTHCHECK instruction in a Dockerfile. If you list more than one then only the last HEALTHCHECK will take effect.\n\nThe command after the CMD keyword can be either a shell command (e.g. HEALTHCHECK CMD /bin/check-running ) or an exec array (as with other Dockerfile commands; see e.g. ENTRYPOINT for details).\n\nThe command's exit status indicates the health status of the container. The possible values are:\n\n0: success - the container is healthy and ready for use 1: unhealthy - the container isn't working correctly 2: reserved - don't use this exit code\n\nFor example, to check every five minutes or so that a web-server is able to serve the site's main page within three seconds:\n\nHEALTHCHECK --interval=5m --timeout=3s \\ CMD curl -f http://localhost/ || exit 1\n\nTo help debug failing probes, any output text (UTF-8 encoded) that the command writes on stdout or stderr will be stored in the health status and can be queried with docker inspect . Such output should be kept short (only the first 4096 bytes are stored currently).\n\nWhen the health status of a container changes, a health_status event is generated with the new status.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 3086
        }
      },
      {
        "header": "SHELL",
        "content": "SHELL [ \"executable\" , \"parameters\" ]\n\nThe SHELL instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is [\"/bin/sh\", \"-c\"] , and on Windows is [\"cmd\", \"/S\", \"/C\"] . The SHELL instruction must be written in JSON form in a Dockerfile.\n\nThe SHELL instruction is particularly useful on Windows where there are two commonly used and quite different native shells: cmd and powershell , as well as alternate shells available including sh .\n\nThe SHELL instruction can appear multiple times. Each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent instructions. For example:\n\nFROM microsoft/windowsservercore # Executed as cmd /S /C echo default RUN echo default # Executed as cmd /S /C powershell -command Write-Host default RUN powershell -command Write-Host default # Executed as powershell -command Write-Host hello SHELL [ \"powershell\" , \"-command\" ] RUN Write-Host hello # Executed as cmd /S /C echo hello SHELL [ \"cmd\" , \"/S\" , \"/C\" ] RUN echo hello\n\nThe following instructions can be affected by the SHELL instruction when the shell form of them is used in a Dockerfile: RUN , CMD and ENTRYPOINT .\n\nThe following example is a common pattern found on Windows which can be streamlined by using the SHELL instruction:\n\nRUN powershell -command Execute-MyCmdlet -param1 \"c:\\foo.txt\"\n\nThe command invoked by the builder will be:\n\ncmd / S / C powershell -command Execute-MyCmdlet -param1 \"c:\\foo.txt\"\n\nThis is inefficient for two reasons. First, there is an unnecessary cmd.exe command processor (aka shell) being invoked. Second, each RUN instruction in the shell form requires an extra powershell -command prefixing the command.\n\nTo make this more efficient, one of two mechanisms can be employed. One is to use the JSON form of the RUN command such as:\n\nRUN [ \"powershell\" , \"-command\" , \"Execute-MyCmdlet\" , \"-param1 \\\"c:\\\\foo.txt\\\"\" ]\n\nWhile the JSON form is unambiguous and does not use the unnecessary cmd.exe , it does require more verbosity through double-quoting and escaping. The alternate mechanism is to use the SHELL instruction and the shell form, making a more natural syntax for Windows users, especially when combined with the escape parser directive:\n\n# escape=` FROM microsoft/nanoserver SHELL [ \"powershell\" , \"-command\" ] RUN New-Item -ItemType Directory C: \\E xample ADD Execute-MyCmdlet.ps1 c: \\e xample \\ RUN c: \\e xample \\E xecute-MyCmdlet -sample 'hello world'\n\nResulting in:\n\nPS E:\\myproject> docker build -t shell . Sending build context to Docker daemon 4.096 kB Step 1/5 : FROM microsoft/nanoserver ---> 22738ff49c6d Step 2/5 : SHELL powershell -command ---> Running in 6fcdb6855ae2 ---> 6331462d4300 Removing intermediate container 6fcdb6855ae2 Step 3/5 : RUN New-Item -ItemType Directory C:\\Example ---> Running in d0eef8386e97 Directory: C:\\ Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 10/28/2016 11:26 AM Example ---> 3f2fbf1395d9 Removing intermediate container d0eef8386e97 Step 4/5 : ADD Execute-MyCmdlet.ps1 c:\\example\\ ---> a955b2621c31 Removing intermediate container b825593d39fc Step 5/5 : RUN c:\\example\\Execute-MyCmdlet 'hello world' ---> Running in be6d8e63fe75 hello world ---> 8e559e9bf424 Removing intermediate container be6d8e63fe75 Successfully built 8e559e9bf424 PS E:\\myproject>\n\nThe SHELL instruction could also be used to modify the way in which a shell operates. For example, using SHELL cmd /S /C /V:ON|OFF on Windows, delayed environment variable expansion semantics could be modified.\n\nThe SHELL instruction can also be used on Linux should an alternate shell be required such as zsh , csh , tcsh and others.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 3686
        }
      },
      {
        "header": "Here-Documents",
        "content": "Here-documents allow redirection of subsequent Dockerfile lines to the input of RUN or COPY commands. If such command contains a here-document the Dockerfile considers the next lines until the line only containing a here-doc delimiter as part of the same command.\n\nExample: Running a multi-line script\n\n# syntax=docker/dockerfile:1 FROM debian RUN <<EOT bash set -ex apt-get update apt-get install -y vim EOT\n\nIf the command only contains a here-document, its contents is evaluated with the default shell.\n\n# syntax=docker/dockerfile:1 FROM debian RUN <<EOT mkdir -p foo/bar EOT\n\nAlternatively, shebang header can be used to define an interpreter.\n\n# syntax=docker/dockerfile:1 FROM python:3.6 RUN <<EOT #!/usr/bin/env python print ( \"hello world\" ) EOT\n\nMore complex examples may use multiple here-documents.\n\n# syntax=docker/dockerfile:1 FROM alpine RUN <<FILE1 cat > file1 && <<FILE 2 cat > file2 I am first FILE1 I am second FILE2\n\nExample: Creating inline files\n\nWith COPY instructions, you can replace the source parameter with a here-doc indicator to write the contents of the here-document directly to a file. The following example creates a greeting.txt file containing hello world using a COPY instruction.\n\n# syntax=docker/dockerfile:1 FROM alpine COPY <<EOF greeting.txt hello world EOF\n\nRegular here-doc variable expansion and tab stripping rules apply. The following example shows a small Dockerfile that creates a hello.sh script file using a COPY instruction with a here-document.\n\n# syntax=docker/dockerfile:1 FROM alpine ARG FOO = bar COPY <<-EOT /script.sh echo \"hello ${ FOO } \" EOT ENTRYPOINT ash /script.sh\n\nIn this case, file script prints \"hello bar\", because the variable is expanded when the COPY instruction gets executed.\n\n$ docker build -t heredoc . $ docker run heredoc hello bar\n\nIf instead you were to quote any part of the here-document word EOT , the variable would not be expanded at build-time.\n\n# syntax=docker/dockerfile:1 FROM alpine ARG FOO = bar COPY <<- \"EOT\" /script.sh echo \"hello ${ FOO } \" EOT ENTRYPOINT ash /script.sh\n\nNote that ARG FOO=bar is excessive here, and can be removed. The variable gets interpreted at runtime, when the script is invoked:\n\n$ docker build -t heredoc . $ docker run -e FOO = world heredoc hello world",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2274
        }
      },
      {
        "header": "Example: Running a multi-line script",
        "content": "# syntax=docker/dockerfile:1 FROM debian RUN <<EOT bash set -ex apt-get update apt-get install -y vim EOT\n\nIf the command only contains a here-document, its contents is evaluated with the default shell.\n\n# syntax=docker/dockerfile:1 FROM debian RUN <<EOT mkdir -p foo/bar EOT\n\nAlternatively, shebang header can be used to define an interpreter.\n\n# syntax=docker/dockerfile:1 FROM python:3.6 RUN <<EOT #!/usr/bin/env python print ( \"hello world\" ) EOT\n\nMore complex examples may use multiple here-documents.\n\n# syntax=docker/dockerfile:1 FROM alpine RUN <<FILE1 cat > file1 && <<FILE 2 cat > file2 I am first FILE1 I am second FILE2",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 631
        }
      },
      {
        "header": "Example: Creating inline files",
        "content": "With COPY instructions, you can replace the source parameter with a here-doc indicator to write the contents of the here-document directly to a file. The following example creates a greeting.txt file containing hello world using a COPY instruction.\n\n# syntax=docker/dockerfile:1 FROM alpine COPY <<EOF greeting.txt hello world EOF\n\nRegular here-doc variable expansion and tab stripping rules apply. The following example shows a small Dockerfile that creates a hello.sh script file using a COPY instruction with a here-document.\n\n# syntax=docker/dockerfile:1 FROM alpine ARG FOO = bar COPY <<-EOT /script.sh echo \"hello ${ FOO } \" EOT ENTRYPOINT ash /script.sh\n\nIn this case, file script prints \"hello bar\", because the variable is expanded when the COPY instruction gets executed.\n\n$ docker build -t heredoc . $ docker run heredoc hello bar\n\nIf instead you were to quote any part of the here-document word EOT , the variable would not be expanded at build-time.\n\n# syntax=docker/dockerfile:1 FROM alpine ARG FOO = bar COPY <<- \"EOT\" /script.sh echo \"hello ${ FOO } \" EOT ENTRYPOINT ash /script.sh\n\nNote that ARG FOO=bar is excessive here, and can be removed. The variable gets interpreted at runtime, when the script is invoked:\n\n$ docker build -t heredoc . $ docker run -e FOO = world heredoc hello world",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1306
        }
      },
      {
        "header": "Dockerfile examples",
        "content": "For examples of Dockerfiles, refer to:\n\nThe building best practices page The \"get started\" tutorials The language-specific getting started guides\n\nValue required ↩︎ ↩︎ ↩︎ For Docker-integrated BuildKit and docker buildx build ↩︎",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 228
        }
      }
    ],
    "url": "https://docs.docker.com/reference/dockerfile",
    "doc_type": "docker",
    "total_sections": 94
  },
  {
    "title": "Docker Engine security",
    "summary": "There are four major areas to consider when reviewing Docker security: Docker containers are very similar to LXC containers, and they have similar security features. When you start a container with docker run , behind the scenes Docker creates a set of namespaces and control groups for the container.",
    "sections": [
      {
        "header": "Kernel namespaces",
        "content": "Docker containers are very similar to LXC containers, and they have similar security features. When you start a container with docker run , behind the scenes Docker creates a set of namespaces and control groups for the container.\n\nNamespaces provide the first and most straightforward form of isolation. Processes running within a container cannot see, and even less affect, processes running in another container, or in the host system.\n\nEach container also gets its own network stack, meaning that a container doesn't get privileged access to the sockets or interfaces of another container. Of course, if the host system is setup accordingly, containers can interact with each other through their respective network interfaces â just like they can interact with external hosts. When you specify public ports for your containers or use links then IP traffic is allowed between containers. They can ping each other, send/receive UDP packets, and establish TCP connections, but that can be restricted if necessary. From a network architecture point of view, all containers on a given Docker host are sitting on bridge interfaces. This means that they are just like physical machines connected through a common Ethernet switch; no more, no less.\n\nHow mature is the code providing kernel namespaces and private networking? Kernel namespaces were introduced between kernel version 2.6.15 and 2.6.26 . This means that since July 2008 (date of the 2.6.26 release ), namespace code has been exercised and scrutinized on a large number of production systems. And there is more: the design and inspiration for the namespaces code are even older. Namespaces are actually an effort to reimplement the features of OpenVZ in such a way that they could be merged within the mainstream kernel. And OpenVZ was initially released in 2005, so both the design and the implementation are pretty mature.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1885
        }
      },
      {
        "header": "Control groups",
        "content": "Control Groups are another key component of Linux containers. They implement resource accounting and limiting. They provide many useful metrics, but they also help ensure that each container gets its fair share of memory, CPU, disk I/O; and, more importantly, that a single container cannot bring the system down by exhausting one of those resources.\n\nSo while they do not play a role in preventing one container from accessing or affecting the data and processes of another container, they are essential to fend off some denial-of-service attacks. They are particularly important on multi-tenant platforms, like public and private PaaS, to guarantee a consistent uptime (and performance) even when some applications start to misbehave.\n\nControl Groups have been around for a while as well: the code was started in 2006, and initially merged in kernel 2.6.24.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 859
        }
      },
      {
        "header": "Docker daemon attack surface",
        "content": "Running containers (and applications) with Docker implies running the Docker daemon. This daemon requires root privileges unless you opt-in to Rootless mode , and you should therefore be aware of some important details.\n\nFirst of all, only trusted users should be allowed to control your Docker daemon. This is a direct consequence of some powerful Docker features. Specifically, Docker allows you to share a directory between the Docker host and a guest container; and it allows you to do so without limiting the access rights of the container. This means that you can start a container where the /host directory is the / directory on your host; and the container can alter your host filesystem without any restriction. This is similar to how virtualization systems allow filesystem resource sharing. Nothing prevents you from sharing your root filesystem (or even your root block device) with a virtual machine.\n\nThis has a strong security implication: for example, if you instrument Docker from a web server to provision containers through an API, you should be even more careful than usual with parameter checking, to make sure that a malicious user cannot pass crafted parameters causing Docker to create arbitrary containers.\n\nFor this reason, the REST API endpoint (used by the Docker CLI to communicate with the Docker daemon) changed in Docker 0.5.2, and now uses a Unix socket instead of a TCP socket bound on 127.0.0.1 (the latter being prone to cross-site request forgery attacks if you happen to run Docker directly on your local machine, outside of a VM). You can then use traditional Unix permission checks to limit access to the control socket.\n\nYou can also expose the REST API over HTTP if you explicitly decide to do so. However, if you do that, be aware of the above mentioned security implications. Note that even if you have a firewall to limit accesses to the REST API endpoint from other hosts in the network, the endpoint can be still accessible from containers, and it can easily result in the privilege escalation. Therefore it is mandatory to secure API endpoints with HTTPS and certificates . Exposing the daemon API over HTTP without TLS is not permitted, and such a configuration causes the daemon to fail early on startup, see Unauthenticated TCP connections . It is also recommended to ensure that it is reachable only from a trusted network or VPN.\n\nYou can also use DOCKER_HOST=ssh://USER@HOST or ssh -L /path/to/docker.sock:/var/run/docker.sock instead if you prefer SSH over TLS.\n\nThe daemon is also potentially vulnerable to other inputs, such as image loading from either disk with docker load , or from the network with docker pull . As of Docker 1.3.2, images are now extracted in a chrooted subprocess on Linux/Unix platforms, being the first-step in a wider effort toward privilege separation. As of Docker 1.10.0, all images are stored and accessed by the cryptographic checksums of their contents, limiting the possibility of an attacker causing a collision with an existing image.\n\nFinally, if you run Docker on a server, it is recommended to run exclusively Docker on the server, and move all other services within containers controlled by Docker. Of course, it is fine to keep your favorite admin tools (probably at least an SSH server), as well as existing monitoring/supervision processes, such as NRPE and collectd.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3366
        }
      },
      {
        "header": "Linux kernel capabilities",
        "content": "By default, Docker starts containers with a restricted set of capabilities. What does that mean?\n\nCapabilities turn the binary \"root/non-root\" dichotomy into a fine-grained access control system. Processes (like web servers) that just need to bind on a port below 1024 do not need to run as root: they can just be granted the net_bind_service capability instead. And there are many other capabilities, for almost all the specific areas where root privileges are usually needed. This means a lot for container security.\n\nTypical servers run several processes as root , including the SSH daemon, cron daemon, logging daemons, kernel modules, network configuration tools, and more. A container is different, because almost all of those tasks are handled by the infrastructure around the container:\n\nSSH access are typically managed by a single server running on the Docker host cron , when necessary, should run as a user process, dedicated and tailored for the app that needs its scheduling service, rather than as a platform-wide facility Log management is also typically handed to Docker, or to third-party services like Loggly or Splunk Hardware management is irrelevant, meaning that you never need to run udevd or equivalent daemons within containers Network management happens outside of the containers, enforcing separation of concerns as much as possible, meaning that a container should never need to perform ifconfig , route , or ip commands (except when a container is specifically engineered to behave like a router or firewall, of course)\n\nThis means that in most cases, containers do not need \"real\" root privileges at all* And therefore, containers can run with a reduced capability set; meaning that \"root\" within a container has much less privileges than the real \"root\". For instance, it is possible to:\n\nDeny all \"mount\" operations Deny access to raw sockets (to prevent packet spoofing) Deny access to some filesystem operations, like creating new device nodes, changing the owner of files, or altering attributes (including the immutable flag) Deny module loading\n\nThis means that even if an intruder manages to escalate to root within a container, it is much harder to do serious damage, or to escalate to the host.\n\nThis doesn't affect regular web apps, but reduces the vectors of attack by malicious users considerably. By default Docker drops all capabilities except those needed , an allowlist instead of a denylist approach. You can see a full list of available capabilities in Linux manpages .\n\nOne primary risk with running Docker containers is that the default set of capabilities and mounts given to a container may provide incomplete isolation, either independently, or when used in combination with kernel vulnerabilities.\n\nDocker supports the addition and removal of capabilities, allowing use of a non-default profile. This may make Docker more secure through capability removal, or less secure through the addition of capabilities. The best practice for users would be to remove all capabilities except those explicitly required for their processes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3083
        }
      },
      {
        "header": "Docker Content Trust signature verification",
        "content": "Docker Engine can be configured to only run signed images. The Docker Content Trust signature verification feature is built directly into the dockerd binary. This is configured in the Dockerd configuration file.\n\nTo enable this feature, trustpinning can be configured in daemon.json , whereby only repositories signed with a user-specified root key can be pulled and run.\n\nThis feature provides more insight to administrators than previously available with the CLI for enforcing and performing image signature verification.\n\nFor more information on configuring Docker Content Trust Signature Verification, go to Content trust in Docker .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 637
        }
      },
      {
        "header": "Other kernel security features",
        "content": "Capabilities are just one of the many security features provided by modern Linux kernels. It is also possible to leverage existing, well-known systems like TOMOYO, AppArmor, SELinux, GRSEC, etc. with Docker.\n\nWhile Docker currently only enables capabilities, it doesn't interfere with the other systems. This means that there are many different ways to harden a Docker host. Here are a few examples.\n\nYou can run a kernel with GRSEC and PAX. This adds many safety checks, both at compile-time and run-time; it also defeats many exploits, thanks to techniques like address randomization. It doesn't require Docker-specific configuration, since those security features apply system-wide, independent of containers. If your distribution comes with security model templates for Docker containers, you can use them out of the box. For instance, we ship a template that works with AppArmor and Red Hat comes with SELinux policies for Docker. These templates provide an extra safety net (even though it overlaps greatly with capabilities). You can define your own policies using your favorite access control mechanism.\n\nJust as you can use third-party tools to augment Docker containers, including special network topologies or shared filesystems, tools exist to harden Docker containers without the need to modify Docker itself.\n\nAs of Docker 1.10 User Namespaces are supported directly by the docker daemon. This feature allows for the root user in a container to be mapped to a non uid-0 user outside the container, which can help to mitigate the risks of container breakout. This facility is available but not enabled by default.\n\nRefer to the daemon command in the command line reference for more information on this feature. Additional information on the implementation of User Namespaces in Docker can be found in this blog post .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1830
        }
      },
      {
        "header": "Conclusions",
        "content": "Docker containers are, by default, quite secure; especially if you run your processes as non-privileged users inside the container.\n\nYou can add an extra layer of safety by enabling AppArmor, SELinux, GRSEC, or another appropriate hardening system.\n\nIf you think of ways to make docker more secure, we welcome feature requests, pull requests, or comments on the Docker community forums.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 386
        }
      },
      {
        "header": "Related information",
        "content": "Use trusted images Seccomp security profiles for Docker AppArmor security profiles for Docker On the Security of Containers (2014) Docker swarm mode overlay network security model",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 179
        }
      }
    ],
    "url": "https://docs.docker.com/engine/security/",
    "doc_type": "docker",
    "total_sections": 8
  },
  {
    "title": "Swarm mode",
    "summary": "Swarm mode is an advanced feature for managing a cluster of Docker daemons. Use Swarm mode if you intend to use Swarm as a production runtime environment.",
    "sections": [
      {
        "header": "Feature highlights",
        "content": "Cluster management integrated with Docker Engine\n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don't need additional orchestration software to create or manage a swarm.\n\nDecentralized design\n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\nDeclarative service model\n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\nScaling\n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\nDesired state reconciliation\n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager creates two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\nMulti-host networking\n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\nService discovery\n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balance running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\nLoad balancing\n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\nSecure by default\n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\nRolling updates\n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2648
        }
      },
      {
        "header": "Cluster management integrated with Docker Engine",
        "content": "Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don't need additional orchestration software to create or manage a swarm.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 182
        }
      },
      {
        "header": "Decentralized design",
        "content": "Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 281
        }
      },
      {
        "header": "Declarative service model",
        "content": "Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 267
        }
      },
      {
        "header": "Scaling",
        "content": "For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 195
        }
      },
      {
        "header": "Desired state reconciliation",
        "content": "The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager creates two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 443
        }
      },
      {
        "header": "Multi-host networking",
        "content": "You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 192
        }
      },
      {
        "header": "Service discovery",
        "content": "Swarm manager nodes assign each service in the swarm a unique DNS name and load balance running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 202
        }
      },
      {
        "header": "Load balancing",
        "content": "You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 158
        }
      },
      {
        "header": "Secure by default",
        "content": "Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 229
        }
      },
      {
        "header": "Rolling updates",
        "content": "At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 249
        }
      },
      {
        "header": "What's next?",
        "content": "Learn Swarm mode key concepts . Get started with the Swarm mode tutorial . Explore Swarm mode CLI commands swarm init swarm join service create service inspect service ls service rm service scale service ps service update swarm init swarm join service create service inspect service ls service rm service scale service ps service update",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 336
        }
      }
    ],
    "url": "https://docs.docker.com/engine/swarm/",
    "doc_type": "docker",
    "total_sections": 12
  },
  {
    "title": "How To Install and Use Docker on Ubuntu",
    "summary": "Featured Products Compute",
    "sections": [
      {
        "header": "Table of contents",
        "content": "Prerequisites Step 1 Installing Docker Step 2 Executing the Docker Command Without Sudo Optional Step 3 Using the Docker Command Step 4 Working with Docker Images Step 5 Running a Docker Container Step 6 Managing Docker Containers Step 7 Committing Changes in a Container to a Docker Image Step 8 Pushing Docker Images to a Docker Repository Docker vs Docker Compose Troubleshooting Common Docker Installation Issues Docker Desktop on Ubuntu Beta Installing Docker Using a Dockerfile How to Uninstall Docker on Ubuntu Frequently Asked Questions Conclusion",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 555
        }
      },
      {
        "header": "Introduction",
        "content": "Docker is an application that simplifies the process of managing application processes in containers . Containers let you run your applications in resource-isolated processes. They’re similar to virtual machines, but containers are more portable, more resource-friendly, and more dependent on the host operating system.\n\nFor a detailed introduction to the different components of a Docker container, check out The Docker Ecosystem: An Introduction to Common Components .\n\nIn this tutorial, you’ll install and use Docker Community Edition (CE) on Ubuntu. You’ll install Docker itself, work with containers and images, and push an image to a Docker Repository. If you’re using Ubuntu 22.04, you can follow our guide on How To Install and Use Docker on Ubuntu 22.04 instead.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 771
        }
      },
      {
        "header": "Note",
        "content": "This article will walk you through installing Docker on an Ubuntu server. If you wanted a 1-click way to deploy a Docker application to a live server, take a look at DigitalOcean App Platform .\n\nPrerequisites To follow this tutorial, you will need the following: One Ubuntu server set up by following the Ubuntu initial server setup guide , including a sudo non-root user and a firewall. An account on Docker Hub if you wish to create your own images and push them to Docker Hub, as shown in Steps 7 and 8.\n\nStep 1 — Installing Docker The Docker installation package available in the official Ubuntu repository may not be the latest version. To ensure we get the latest version, we’ll install Docker from the official Docker repository. To do that, we’ll add a new package source, add the GPG key from Docker to ensure the downloads are valid, and then install the package. First, update your existing list of packages: sudo apt update Next, install a few prerequisite packages which let apt use packages over HTTPS: sudo apt install apt-transport-https ca-certificates curl software-properties-common Then add the GPG key for the official Docker repository to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Add the Docker repository to APT sources: sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\" This will also update our package database with the Docker packages from the newly added repo. Make sure you are about to install from the Docker repo instead of the default Ubuntu repo: apt-cache policy docker-ce You’ll see output like this, although the version number for Docker may be different: Output of apt-cache policy docker-ce docker-ce: Installed: ( none ) Candidate: 5 : 19.03.9~3-0~ubuntu-focal Version table: 5 : 19.03.9~3-0~ubuntu-focal 500 500 https://download.docker.com/linux/ubuntu focal/stable amd64 Packages Notice that docker-ce is not installed, but the candidate for installation is from the Docker repository for Ubuntu ( focal ). Finally, install Docker: sudo apt install docker-ce Docker should now be installed, the daemon started, and the process enabled to start on boot. Check that it’s running: sudo systemctl status docker The output should be similar to the following, showing that the service is active and running: Output ● docker.service - Docker Application Container Engine Loaded: loaded ( /lib/systemd/system/docker.service ; enabled ; vendor preset: enabled ) Active: active ( running ) since Tue 2020 -05-19 17 :00:41 UTC ; 17s ago TriggeredBy: ● docker.socket Docs: https://docs.docker.com Main PID: 24321 ( dockerd ) Tasks: 8 Memory: 46 .4M CGroup: /system.slice/docker.service └─24321 /usr/bin/dockerd -H fd:// --containerd = /run/containerd/containerd.sock Installing Docker now gives you not just the Docker service (daemon) but also the docker command line utility, or the Docker client. We’ll explore how to use the docker command later in this tutorial.\n\nStep 2 — Executing the Docker Command Without Sudo (Optional) By default, the docker command can only be run the root user or by a user in the docker group, which is automatically created during Docker’s installation process. If you attempt to run the docker command without prefixing it with sudo or without being in the docker group, you’ll get an output like this: Output docker: Cannot connect to the Docker daemon. Is the docker daemon running on this host?. See 'docker run --help' . If you want to avoid typing sudo whenever you run the docker command, add your username to the docker group: sudo usermod -aG docker ${ USER } To apply the new group membership, log out of the server and back in, or type the following: su - ${ USER } You will be prompted to enter your user’s password to continue. Confirm that your user is now added to the docker group by typing: groups Output sammy sudo docker If you need to add a user to the docker group that you’re not logged in as, declare that username explicitly using: sudo usermod -aG docker username The rest of this article assumes you are running the docker command as a user in the docker group. If you choose not to, please prepend the commands with sudo . Let’s explore the docker command next.\n\nStep 3 — Using the Docker Command Using docker consists of passing it a chain of options and commands followed by arguments. The syntax takes this form: docker [ option ] [ command ] [ arguments ] To view all available subcommands, type: docker As of Docker 19, the complete list of available subcommands includes: Output attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container 's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container' s filesystem events Get real time events from the server exec Run a command in a running container export Export a container's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive ( streamed to STDOUT by default ) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container ( s ) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes To view the options available to a specific command, type: docker docker-subcommand --help To view system-wide information about Docker, use: docker info Let’s explore some of these commands. We’ll start by working with images.\n\nStep 4 — Working with Docker Images Docker containers are built from Docker images. By default, Docker pulls these images from Docker Hub , a Docker registry managed by Docker, the company behind the Docker project. Anyone can host their Docker images on Docker Hub, so most applications and Linux distributions you’ll need will have images hosted there. To check whether you can access and download images from Docker Hub, type: docker run hello-world The output will indicate that Docker in working correctly: Output Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 0e03bdcc26d7: Pull complete Digest: sha256:6a65f928fb91fcfbc963f7aa6d57c8eeb426ad9a20c7ee045538ef34847f44f1 Status: Downloaded newer image for hello-world:latest Hello from Docker ! This message shows that your installation appears to be working correctly. .. . Docker was initially unable to find the hello-world image locally, so it downloaded the image from Docker Hub, which is the default repository. Once the image downloaded, Docker created a container from the image and the application within the container executed, displaying the message. You can search for images available on Docker Hub by using the docker command with the search subcommand. For example, to search for the Ubuntu image, type: docker search ubuntu The script will crawl Docker Hub and return a listing of all images whose name match the search string. In this case, the output will be similar to this: Output NAME DESCRIPTION STARS OFFICIAL AUTOMATED ubuntu Ubuntu is a Debian-based Linux operating sys… 10908 [ OK ] dorowu/ubuntu-desktop-lxde-vnc Docker image to provide HTML5 VNC interface … 428 [ OK ] rastasheep/ubuntu-sshd Dockerized SSH service, built on top of offi… 244 [ OK ] consol/ubuntu-xfce-vnc Ubuntu container with \"headless\" VNC session… 218 [ OK ] ubuntu-upstart Upstart is an event-based replacement for th… 108 [ OK ] ansible/ubuntu14.04-ansible Ubuntu 14.04 LTS with .. . In the OFFICIAL column, OK indicates an image built and supported by the company behind the project. Once you’ve identified the image that you would like to use, you can download it to your computer using the pull subcommand. Execute the following command to download the official ubuntu image to your computer: docker pull ubuntu You’ll see the following output: Output Using default tag: latest latest: Pulling from library/ubuntu d51af753c3d3: Pull complete fc878cd0a91c: Pull complete 6154df8ff988: Pull complete fee5db0ff82f: Pull complete Digest: sha256:747d2dbbaaee995098c9792d99bd333c6783ce56150d1b11e333bbceed5c54d7 Status: Downloaded newer image for ubuntu:latest docker.io/library/ubuntu:latest After an image has been downloaded, you can then run a container using the downloaded image with the run subcommand. As you saw with the hello-world example, if an image has not been downloaded when docker is executed with the run subcommand, the Docker client will first download the image, then run a container using it. To see the images that have been downloaded to your computer, type: docker images The output will look similar to the following: Output REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest 1d622ef86b13 3 weeks ago 73 .9MB hello-world latest bf756fb1ae65 4 months ago 13 .3kB As you’ll see later in this tutorial, images that you use to run containers can be modified and used to generate new images, which may then be uploaded ( pushed is the technical term) to Docker Hub or other Docker registries. Let’s look at how to run containers in more detail.\n\nStep 5 — Running a Docker Container The hello-world container you ran in the previous step is an example of a container that runs and exits after emitting a test message. Containers can be much more useful than that, and they can be interactive. After all, they are similar to virtual machines, only more resource-friendly. As an example, let’s run a container using the latest image of Ubuntu. The combination of the -i and -t switches gives you interactive shell access into the container: docker run -it ubuntu Your command prompt should change to reflect the fact that you’re now working inside the container and should take this form: Output root@d9b100f2f636:/ # Note the container id in the command prompt. In this example, it is d9b100f2f636 . You’ll need that container ID later to identify the container when you want to remove it. Now you can run any command inside the container. For example, let’s update the package database inside the container. You don’t need to prefix any command with sudo , because you’re operating inside the container as the root user: apt update Then install any application in it. Let’s install Node.js: apt install nodejs This installs Node.js in the container from the official Ubuntu repository. When the installation finishes, verify that Node.js is installed: node -v You’ll see the version number displayed in your terminal: Output v10.19.0 Any changes you make inside the container only apply to that container. To exit the container, type exit at the prompt. Let’s look at managing the containers on our system next.\n\nStep 6 — Managing Docker Containers After using Docker for a while, you’ll have many active (running) and inactive containers on your computer. To view the active ones , use: docker ps You will see output similar to the following: Output CONTAINER ID IMAGE COMMAND CREATED In this tutorial, you started two containers; one from the hello-world image and another from the ubuntu image. Both containers are no longer running, but they still exist on your system. To view all containers — active and inactive, run docker ps with the -a switch: docker ps -a You’ll see output similar to this: 1c08a7a0d0e4 ubuntu \"/bin/bash\" 2 minutes ago Exited ( 0 ) 8 seconds ago quizzical_mcnulty a707221a5f6c hello-world \"/hello\" 6 minutes ago Exited ( 0 ) 6 minutes ago youthful_curie To view the latest container you created, pass it the -l switch: docker ps -l CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1c08a7a0d0e4 ubuntu \"/bin/bash\" 2 minutes ago Exited ( 0 ) 40 seconds ago quizzical_mcnulty To start a stopped container, use docker start , followed by the container ID or the container’s name. Let’s start the Ubuntu-based container with the ID of 1c08a7a0d0e4 : docker start 1c08a7a0d0e4 The container will start, and you can use docker ps to see its status: Output CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1c08a7a0d0e4 ubuntu \"/bin/bash\" 3 minutes ago Up 5 seconds quizzical_mcnulty To stop a running container, use docker stop , followed by the container ID or name. This time, we’ll use the name that Docker assigned the container, which is quizzical_mcnulty : docker stop quizzical_mcnulty Once you’ve decided you no longer need a container anymore, remove it with the docker rm command, again using either the container ID or the name. Use the docker ps -a command to find the container ID or name for the container associated with the hello-world image and remove it. docker rm youthful_curie You can start a new container and give it a name using the --name switch. You can also use the --rm switch to create a container that removes itself when it’s stopped. See the docker run help command for more information on these options and others. Containers can be turned into images which you can use to build new containers. Let’s look at how that works.\n\nStep 7 — Committing Changes in a Container to a Docker Image When you start up a Docker image, you can create, modify, and delete files just like you can with a virtual machine. The changes that you make will only apply to that container. You can start and stop it, but once you destroy it with the docker rm command, the changes will be lost for good. This section shows you how to save the state of a container as a new Docker image. After installing Node.js inside the Ubuntu container, you now have a container running off an image, but the container is different from the image you used to create it. But you might want to reuse this Node.js container as the basis for new images later. Then commit the changes to a new Docker image instance using the following command. docker commit -m \"What you did to the image\" -a \"Author Name\" container_id repository / new_image_name The -m switch is for the commit message that helps you and others know what changes you made, while -a is used to specify the author. The container_id is the one you noted earlier in the tutorial when you started the interactive Docker session. Unless you created additional repositories on Docker Hub, the repository is usually your Docker Hub username. For example, for the user sammy , with the container ID of d9b100f2f636 , the command would be: docker commit -m \"added Node.js\" -a \" sammy \" d9b100f2f636 sammy /ubuntu-nodejs When you commit an image, the new image is saved locally on your computer. Later in this tutorial, you’ll learn how to push an image to a Docker registry like Docker Hub so others can access it. Listing the Docker images again will show the new image, as well as the old one that it was derived from: docker images You’ll see output like this: Output REPOSITORY TAG IMAGE ID CREATED SIZE sammy /ubuntu-nodejs latest 7c1f35226ca6 7 seconds ago 179MB .. . In this example, ubuntu-nodejs is the new image, which was derived from the existing ubuntu image from Docker Hub. The size difference reflects the changes that were made. And in this example, the change was that NodeJS was installed. So next time you need to run a container using Ubuntu with NodeJS pre-installed, you can just use the new image. You can also build Images from a Dockerfile , which lets you automate the installation of software in a new image. However, that’s outside the scope of this tutorial. Now let’s share the new image with others so they can create containers from it.\n\nStep 8 — Pushing Docker Images to a Docker Repository The next logical step after creating a new image from an existing image is to share it with a select few of your friends, the whole world on Docker Hub, or other Docker registry that you have access to. To push an image to Docker Hub or any other Docker registry, you must have an account there. This section shows you how to push a Docker image to Docker Hub. To learn how to create your own private Docker registry, check out How To Set Up a Private Docker Registry on Ubuntu or How To Set Up a Private Docker Registry on Ubuntu . To push your image, first log into Docker Hub. docker login -u docker-registry-username You’ll be prompted to authenticate using your Docker Hub password. If you specified the correct password, authentication should succeed. Note: If your Docker registry username is different from the local username you used to create the image, you will have to tag your image with your registry username. For the example given in the last step, you would type: docker tag sammy /ubuntu-nodejs docker-registry-username /ubuntu-nodejs Then you may push your own image using: docker push docker-registry-username / docker-image-name To push the ubuntu-nodejs image to the sammy repository, the command would be: docker push sammy / ubuntu-nodejs The process may take some time to complete as it uploads the images, but when completed, the output will look like this: Output The push refers to a repository [ docker.io/ sammy /ubuntu-nodejs ] e3fbbfb44187: Pushed 5f70bf18a086: Pushed a3b5c80a4eba: Pushed 7f18b442972b: Pushed 3ce512daaf78: Pushed 7aae4540b42d: Pushed .. . After pushing an image to a registry, it should be listed on your account’s dashboard, like that show in the image below. If a push attempt results in an error of this sort, then you likely did not log in: Output The push refers to a repository [ docker.io/ sammy /ubuntu-nodejs ] e3fbbfb44187: Preparing 5f70bf18a086: Preparing a3b5c80a4eba: Preparing 7f18b442972b: Preparing 3ce512daaf78: Preparing 7aae4540b42d: Waiting unauthorized: authentication required Log in with docker login and repeat the push attempt. Then verify that it exists on your Docker Hub repository page. You can now use docker pull sammy / ubuntu-nodejs to pull the image to a new machine and use it to run a new container.\n\nDocker vs Docker Compose While Docker lets you build and run containers, managing multi-container applications with just Docker can be tedious. That’s where Docker Compose comes in. Docker Compose is a tool for defining and running multi-container Docker applications using a YAML file. Instead of manually running separate containers for a web server, database, and caching layer, you can define them all in a docker-compose.yml file and run them with: docker-compose up For a detailed guide on using Docker Compose, including how to set up complex multi-container applications and manage their configurations, check out our tutorial on How To Install and Use Docker Compose on Ubuntu . This guide will walk you through creating and managing multi-container applications with Docker Compose, from basic setups to more complex configurations. Feature Docker CLI Docker Compose Usage Single container operations Multi-container orchestration Configuration CLI commands YAML configuration file Dependency handling Manual Handles linked services automatically Best use case Testing isolated containers Local development and staging setups For more, see How To Install and Use Docker Compose on Ubuntu .\n\nTroubleshooting Common Docker Installation Issues Problem: docker: command not found Fix: The Docker CLI isn’t in your $PATH . Reinstall Docker or ensure /usr/bin is included. sudo apt install docker-ce docker-ce-cli containerd.io Problem: Cannot connect to the Docker daemon Fix: Docker isn’t running, or your user isn’t in the docker group. sudo systemctl start docker sudo usermod -aG docker $USER Then log out and log back in. Problem: GPG key or repository error Fix: If the keyserver or Docker GPG key changes, refer to Docker’s official docs for the latest key and repository steps. If you’re using Ubuntu 22.04, you may want to check our guide on How To Install and Use Docker on Ubuntu 22.04 for version-specific instructions. For automated installation that can help avoid these issues, consider using our guide on How To Use Ansible to Install and Set Up Docker on Ubuntu .\n\nDocker Desktop on Ubuntu (Beta) Docker Desktop is now available in beta for Linux distributions like Ubuntu. It provides a GUI, bundled Docker Engine, and Kubernetes support. To install Docker Desktop on Ubuntu: sudo apt install ./docker-desktop- < version > - < arch > .deb Refer to Docker Desktop for Linux docs for prerequisites and download links. Note: Docker Desktop is suited for development environments. For server-side installs, use Docker CE.\n\nInstalling Docker Using a Dockerfile For automated environments, install Docker using a Dockerfile . Here’s an example: FROM ubuntu:20.04 RUN apt-get update && \\ apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release && \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - && \\ add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\" && \\ apt-get update && \\ apt-get install -y docker-ce docker-ce-cli containerd.io For more advanced automation options, you can also use Ansible to install and configure Docker. Check out our guide on How To Use Ansible to Install and Set Up Docker on Ubuntu . This is particularly useful if you need to manage Docker installations across multiple servers or want to maintain consistent configurations in your infrastructure.\n\nHow to Uninstall Docker on Ubuntu To remove Docker from your system, run: sudo apt purge docker-ce docker-ce-cli containerd.io sudo rm -rf /var/lib/docker sudo rm -rf /var/lib/containerd\n\nFrequently Asked Questions 1. What is the best way to install Docker on Ubuntu? The recommended and most reliable way to install Docker on Ubuntu is by using Docker’s official repository. This approach ensures you always receive the latest stable version, complete with security patches and updates directly from Docker. To do this, you first update your package index and install prerequisites like apt-transport-https , ca-certificates , curl , gnupg , and lsb-release . Next, you add Docker’s official GPG key and set up the stable repository. After updating your package index again, you can install Docker Engine and related components using sudo apt install docker-ce docker-ce-cli containerd.io This method is preferred over using the default Ubuntu repositories, which may contain outdated versions. By following Docker’s official installation steps, you ensure compatibility with the latest features, bug fixes, and security enhancements, making your Docker experience on Ubuntu both robust and secure. 2. Do I need sudo to run Docker commands? By default, Docker requires root privileges to run its commands, which means you need to prepend sudo to every Docker command. However, you can avoid this by adding your user to the docker group, which grants permission to run Docker commands without sudo . To do this, execute sudo usermod -aG docker $USER and then log out and log back in for the changes to take effect. This step modifies your user’s group memberships, allowing Docker to be run as a non-root user. Keep in mind that adding users to the docker group grants them root-level access to the Docker daemon, so only add trusted users. If you skip this step, you’ll encounter permission errors when running Docker commands without sudo . For security and convenience, it’s best to add your user to the docker group if you use Docker frequently. 3. How do I verify if Docker is running correctly? To confirm that Docker is installed and running properly on your Ubuntu system, you can use a couple of simple commands. First, run docker info to display detailed information about the Docker installation, including server version, storage driver, and running containers. If Docker is running, this command will return a wealth of information; if not, you’ll see an error. Another common test is to run docker run hello-world This command downloads a test image from Docker Hub and runs it in a container. If everything is set up correctly, you’ll see a message indicating that Docker is working as expected. If you encounter errors, check that the Docker service is running with sudo systemctl status docker and review any error messages for troubleshooting. These steps help ensure your Docker installation is functional and ready for use. 4. Can I install a specific version of Docker? Yes, you can install a specific version of Docker on Ubuntu, which is useful if you need compatibility with certain applications or want to avoid the latest changes. First, list all available Docker versions in the repository using apt-cache madison docker-ce This command will display a list of version strings. Once you identify the desired version, install it by running sudo apt install docker-ce = < VERSION_STRING > replacing <VERSION_STRING> with the exact version number (for example, 5:20.10.7~3-0~ubuntu-focal ). This approach ensures you have precise control over the Docker version on your system. Remember to also specify matching versions for docker-ce-cli and containerd.io if needed. Pinning a specific version can help maintain stability in production environments or when working with legacy projects that require a particular Docker release. 5. What’s the difference between Docker and Docker Compose? Docker and Docker Compose are related but serve different purposes. Docker is a platform that allows you to build, ship, and run individual containers, each encapsulating an application and its dependencies. It’s ideal for running single services or applications in isolation. Docker Compose, on the other hand, is a tool designed to define and manage multi-container applications. With Compose, you use a YAML file ( docker-compose.yml ) to specify how multiple containers should interact, their configurations, networks, and volumes. This makes it easy to orchestrate complex applications consisting of several interconnected services, such as a web server, database, and cache. While Docker alone is sufficient for simple use cases, Docker Compose streamlines the process of managing multi-container setups, making development, testing, and deployment of complex applications much more efficient and reproducible. 6. How do I remove Docker from my system? To completely uninstall Docker from your Ubuntu system, you need to remove the Docker packages and delete any associated data. Start by running sudo apt purge docker-ce docker-ce-cli containerd.io to remove the Docker Engine, CLI, and container runtime. This command deletes the installed packages but leaves configuration files and data directories intact. To fully clean up, remove Docker’s data directories with sudo rm -rf /var/lib/docker and sudo rm -rf /var/lib/containerd These directories store images, containers, volumes, and other persistent data. If you added your user to the docker group, you may also want to remove the group with sudo groupdel docker After these steps, Docker and all its data will be removed from your system. Always back up important data before uninstalling to avoid accidental loss. 7. Is Docker Desktop available for Ubuntu, and should I use it? Yes, Docker Desktop is now available in beta for Linux distributions, including Ubuntu. Docker Desktop provides a graphical user interface (GUI) for managing containers, images, volumes, and networks, making it easier for users who prefer not to work exclusively with the command line. It also bundles the Docker Engine and offers integrated Kubernetes support, which is useful for development and testing environments. To install Docker Desktop on Ubuntu, download the appropriate .deb package from Docker’s official website and install it using sudo apt install ./docker-desktop- < version > - < arch > .deb However, Docker Desktop is primarily intended for development and not recommended for production servers. For server-side or headless environments, it’s better to use Docker Engine Community Edition (CE) via the command line. Always check Docker’s documentation for prerequisites and the latest installation instructions before proceeding.\n\nConclusion In this comprehensive tutorial, you’ve successfully installed Docker on Ubuntu, mastered the fundamentals of container management, and learned how to work with Docker images and containers. You’ve also gained hands-on experience with Docker Hub by pushing a modified image to the registry. These skills form a solid foundation for container-based development and deployment. To further enhance your Docker expertise, we encourage you to explore the other Docker tutorials in the DigitalOcean Community, where you’ll find advanced topics like container orchestration, networking, and security best practices.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 29645
        }
      },
      {
        "header": "Problem: docker: command not found",
        "content": "Fix: The Docker CLI isn’t in your $PATH . Reinstall Docker or ensure /usr/bin is included.",
        "code_examples": [
          "sudo\napt\ninstall\ndocker-ce docker-ce-cli containerd.io"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 90
        }
      },
      {
        "header": "Problem: Cannot connect to the Docker daemon",
        "content": "Fix: Docker isn’t running, or your user isn’t in the docker group.\n\nThen log out and log back in.",
        "code_examples": [
          "sudo\nsystemctl start\ndocker\nsudo\nusermod\n-aG\ndocker\n$USER"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 97
        }
      },
      {
        "header": "Problem: GPG key or repository error",
        "content": "Fix: If the keyserver or Docker GPG key changes, refer to Docker’s official docs for the latest key and repository steps. If you’re using Ubuntu 22.04, you may want to check our guide on How To Install and Use Docker on Ubuntu 22.04 for version-specific instructions. For automated installation that can help avoid these issues, consider using our guide on How To Use Ansible to Install and Set Up Docker on Ubuntu .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 416
        }
      },
      {
        "header": "1. What is the best way to install Docker on Ubuntu?",
        "content": "The recommended and most reliable way to install Docker on Ubuntu is by using Docker’s official repository. This approach ensures you always receive the latest stable version, complete with security patches and updates directly from Docker. To do this, you first update your package index and install prerequisites like apt-transport-https , ca-certificates , curl , gnupg , and lsb-release . Next, you add Docker’s official GPG key and set up the stable repository. After updating your package index again, you can install Docker Engine and related components using\n\nThis method is preferred over using the default Ubuntu repositories, which may contain outdated versions. By following Docker’s official installation steps, you ensure compatibility with the latest features, bug fixes, and security enhancements, making your Docker experience on Ubuntu both robust and secure.",
        "code_examples": [
          "sudo\napt\ninstall\ndocker-ce docker-ce-cli containerd.io"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 877
        }
      },
      {
        "header": "2. Do I need sudo to run Docker commands?",
        "content": "By default, Docker requires root privileges to run its commands, which means you need to prepend sudo to every Docker command. However, you can avoid this by adding your user to the docker group, which grants permission to run Docker commands without sudo . To do this, execute\n\nand then log out and log back in for the changes to take effect. This step modifies your user’s group memberships, allowing Docker to be run as a non-root user. Keep in mind that adding users to the docker group grants them root-level access to the Docker daemon, so only add trusted users. If you skip this step, you’ll encounter permission errors when running Docker commands without sudo . For security and convenience, it’s best to add your user to the docker group if you use Docker frequently.",
        "code_examples": [
          "sudo\nusermod\n-aG\ndocker\n$USER"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 778
        }
      },
      {
        "header": "3. How do I verify if Docker is running correctly?",
        "content": "To confirm that Docker is installed and running properly on your Ubuntu system, you can use a couple of simple commands. First, run\n\nto display detailed information about the Docker installation, including server version, storage driver, and running containers. If Docker is running, this command will return a wealth of information; if not, you’ll see an error. Another common test is to run\n\nThis command downloads a test image from Docker Hub and runs it in a container. If everything is set up correctly, you’ll see a message indicating that Docker is working as expected. If you encounter errors, check that the Docker service is running with\n\nand review any error messages for troubleshooting. These steps help ensure your Docker installation is functional and ready for use.",
        "code_examples": [
          "docker\ninfo",
          "docker\nrun hello-world",
          "sudo\nsystemctl status\ndocker"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 781
        }
      },
      {
        "header": "4. Can I install a specific version of Docker?",
        "content": "Yes, you can install a specific version of Docker on Ubuntu, which is useful if you need compatibility with certain applications or want to avoid the latest changes. First, list all available Docker versions in the repository using\n\nThis command will display a list of version strings. Once you identify the desired version, install it by running\n\nreplacing <VERSION_STRING> with the exact version number (for example, 5:20.10.7~3-0~ubuntu-focal ). This approach ensures you have precise control over the Docker version on your system. Remember to also specify matching versions for\n\nand\n\nif needed. Pinning a specific version can help maintain stability in production environments or when working with legacy projects that require a particular Docker release.",
        "code_examples": [
          "apt-cache\nmadison docker-ce",
          "sudo\napt\ninstall\ndocker-ce\n=\n<\nVERSION_STRING\n>",
          "docker-ce-cli",
          "containerd.io"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 760
        }
      },
      {
        "header": "5. What’s the difference between Docker and Docker Compose?",
        "content": "Docker and Docker Compose are related but serve different purposes. Docker is a platform that allows you to build, ship, and run individual containers, each encapsulating an application and its dependencies. It’s ideal for running single services or applications in isolation. Docker Compose, on the other hand, is a tool designed to define and manage multi-container applications. With Compose, you use a YAML file ( docker-compose.yml ) to specify how multiple containers should interact, their configurations, networks, and volumes. This makes it easy to orchestrate complex applications consisting of several interconnected services, such as a web server, database, and cache. While Docker alone is sufficient for simple use cases, Docker Compose streamlines the process of managing multi-container setups, making development, testing, and deployment of complex applications much more efficient and reproducible.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 916
        }
      },
      {
        "header": "6. How do I remove Docker from my system?",
        "content": "To completely uninstall Docker from your Ubuntu system, you need to remove the Docker packages and delete any associated data. Start by running\n\nto remove the Docker Engine, CLI, and container runtime. This command deletes the installed packages but leaves configuration files and data directories intact. To fully clean up, remove Docker’s data directories with\n\nand\n\nThese directories store images, containers, volumes, and other persistent data. If you added your user to the docker group, you may also want to remove the group with\n\nAfter these steps, Docker and all its data will be removed from your system. Always back up important data before uninstalling to avoid accidental loss.",
        "code_examples": [
          "sudo\napt\npurge docker-ce docker-ce-cli containerd.io",
          "sudo\nrm\n-rf\n/var/lib/docker",
          "sudo\nrm\n-rf\n/var/lib/containerd",
          "sudo\ngroupdel\ndocker"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 689
        }
      },
      {
        "header": "7. Is Docker Desktop available for Ubuntu, and should I use it?",
        "content": "Yes, Docker Desktop is now available in beta for Linux distributions, including Ubuntu. Docker Desktop provides a graphical user interface (GUI) for managing containers, images, volumes, and networks, making it easier for users who prefer not to work exclusively with the command line. It also bundles the Docker Engine and offers integrated Kubernetes support, which is useful for development and testing environments. To install Docker Desktop on Ubuntu, download the appropriate .deb package from Docker’s official website and install it using\n\nHowever, Docker Desktop is primarily intended for development and not recommended for production servers. For server-side or headless environments, it’s better to use Docker Engine Community Edition (CE) via the command line. Always check Docker’s documentation for prerequisites and the latest installation instructions before proceeding.",
        "code_examples": [
          "sudo\napt\ninstall\n./docker-desktop-\n<\nversion\n>\n-\n<\narch\n>\n.deb"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 887
        }
      },
      {
        "header": "Still looking for an answer?",
        "content": "Ask a question Search for more help",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 35
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "The developer cloud",
        "content": "Scale up as you grow — whether you're running one virtual machine or ten thousand.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 82
        }
      },
      {
        "header": "Get started for free",
        "content": "Sign up and get $200 in credit for your first 60 days with DigitalOcean.*",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 73
        }
      },
      {
        "header": "Company",
        "content": "About Leadership Blog Careers Customers Partners Referral Program Affiliate Program Press Legal Privacy Policy Security Investor Relations",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 138
        }
      },
      {
        "header": "Products",
        "content": "Overview Droplets Kubernetes Functions App Platform Gradient™ AI GPU Droplets Gradient™ AI Bare Metal GPUs Gradient™ AI 1-Click Models Gradient™ AI Platform Load Balancers Managed Databases Spaces Block Storage Network File Storage API Uptime Identity and Access Management Cloudways",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 283
        }
      },
      {
        "header": "Resources",
        "content": "Community Tutorials Community Q&A CSS-Tricks Write for DOnations Currents Research DigitalOcean Startups Wavemakers Program Compass Council Open Source Newsletter Signup Marketplace Pricing Pricing Calculator Documentation Release Notes Code of Conduct Shop Swag",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 262
        }
      },
      {
        "header": "Solutions",
        "content": "Website Hosting VPS Hosting Web & Mobile Apps Game Development Streaming VPN SaaS Platforms Cloud Hosting for Blockchain Startup Resources Migration Assistance",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 159
        }
      },
      {
        "header": "Contact",
        "content": "Support Sales Report Abuse System Status Share your ideas",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 57
        }
      }
    ],
    "url": "https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "How to Install Docker Compose on Ubuntu (Step-by-Step Guide)",
    "summary": "Featured Products Compute",
    "sections": [
      {
        "header": "Table of contents",
        "content": "Prerequisites Step 1 Installing Docker Compose Step 2 Setting Up a dockercomposeyml File Step 3 Running Docker Compose Step 4 Getting Familiar with Docker Compose Commands Common Errors and Debugging FAQs Conclusion",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 215
        }
      },
      {
        "header": "Introduction",
        "content": "Docker simplifies the process of managing application processes in containers. While containers are similar to virtual machines in certain ways, they are more lightweight and resource-friendly. This allows developers to break down an application environment into multiple isolated services.\n\nFor applications depending on several services, orchestrating all the containers to start up, communicate, and shut down together can quickly become unwieldy. Docker Compose is a tool that allows you to run multi-container application environments based on definitions set in a YAML file. It uses service definitions to build fully customizable environments with multiple containers that can share networks and data volumes.\n\nIn this guide, you’ll demonstrate how to install Docker Compose on an Ubuntu 20.04 server and how to get started using this tool.\n\nSimplify deploying applications to servers with DigitalOcean App Platform . Deploy directly from GitHub in minutes.\n\nPrerequisites To follow this article, you will need: Access to an Ubuntu 20.04 local machine or development server as a non-root user with sudo privileges. If you’re using a remote server, it’s advisable to have an active firewall installed. To set these up, please refer to our Initial Server Setup Guide for Ubuntu 20.04 . Docker installed on your server or local machine, following Steps 1 and 2 of How To Install and Use Docker on Ubuntu 20.04 . Note: This tutorial will guide you through installing Docker Compose v1, which uses docker-compose . Starting with Docker Compose v2, Docker has migrated towards using the compose CLI plugin command as documented in our latest Ubuntu 22.04 version of this tutorial , and away from the original docker-compose . While the installation differs, in general the actual usage involves dropping the hyphen from docker-compose calls to become docker compose . For full compatibility details, check the official Docker documentation on command compatibility between the new compose and the old docker-compose .\n\nStep 1 — Installing Docker Compose To make sure you obtain the most updated stable version of Docker Compose, you’ll download this software from its official Github repository . First, confirm the latest version available in their releases page . At the time of this writing, the most current stable version is 1.29.2 . The following command will download the 1.29.2 release and save the executable file at /usr/local/bin/docker-compose , which will make this software globally accessible as docker-compose : sudo curl -L \"https://github.com/docker/compose/releases/download/ 1.29.2 /docker-compose- $( uname -s ) - $( uname -m ) \" -o /usr/local/bin/docker-compose Next, set the correct permissions so that the docker-compose command is executable: sudo chmod +x /usr/local/bin/docker-compose To verify that the installation was successful, you can run: docker-compose --version You’ll see output similar to this: Output docker-compose version 1.29 .2 , build 5becea4c Docker Compose is now successfully installed on your system. In the next section, you’ll see how to set up a docker-compose.yml file and get a containerized environment up and running with this tool.\n\nStep 2 — Setting Up a docker-compose.yml File To demonstrate how to set up a docker-compose.yml file and work with Docker Compose, you’ll create a web server environment using the official Nginx image from Docker Hub , the public Docker registry. This containerized environment will serve a single static HTML file. Start off by creating a new directory in your home folder, and then moving into it: mkdir ~/compose-demo cd ~/compose-demo In this directory, set up an application folder to serve as the document root for your Nginx environment: mkdir app Using your preferred text editor, create a new index.html file within the app folder: nano app/index.html Place the following content into this file: ~/compose-demo/app/index.html <! doctype html > < html lang = \" en \" > < head > < meta charset = \" utf-8 \" > < title > Docker Compose Demo </ title > < link rel = \" stylesheet \" href = \" https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/dark.min.css \" > </ head > < body > < h1 > This is a Docker Compose Demo Page. </ h1 > < p > This content is being served by an Nginx container. </ p > </ body > </ html > Save and close the file when you’re done. If you are using nano , you can do that by typing CTRL+X , then Y and ENTER to confirm. Next, create the docker-compose.yml file: nano docker-compose.yml Insert the following content in your docker-compose.yml file: docker-compose.yml version : '3.7' services : web : image : nginx : alpine ports : - \"8000:80\" volumes : - ./app : /usr/share/nginx/html The docker-compose.yml file typically starts off with the version definition. This will tell Docker Compose which configuration version you’re using. You then have the services block, where you set up the services that are part of this environment. In your case, you have a single service called web . This service uses the nginx:alpine image and sets up a port redirection with the ports directive. All requests on port 8000 of the host machine (the system from where you’re running Docker Compose) will be redirected to the web container on port 80 , where Nginx will be running. The volumes directive will create a shared volume between the host machine and the container. This will share the local app folder with the container, and the volume will be located at /usr/share/nginx/html inside the container, which will then overwrite the default document root for Nginx. Save and close the file. You have set up a demo page and a docker-compose.yml file to create a containerized web server environment that will serve it. In the next step, you’ll bring this environment up with Docker Compose.\n\nStep 3 — Running Docker Compose With the docker-compose.yml file in place, you can now execute Docker Compose to bring your environment up. The following command will download the necessary Docker images, create a container for the web service, and run the containerized environment in background mode: docker-compose up -d Docker Compose will first look for the defined image on your local system, and if it can’t locate the image it will download the image from Docker Hub. You’ll see output like this: Output Creating network \" compose-demo_default \" with the default driver Pulling web ( nginx:alpine ) .. . alpine: Pulling from library/nginx cbdbe7a5bc2a: Pull complete 10c113fb0c77: Pull complete 9ba64393807b: Pull complete c829a9c40ab2: Pull complete 61d685417b2f: Pull complete Digest: sha256:57254039c6313fe8c53f1acbf15657ec9616a813397b74b063e32443427c5502 Status: Downloaded newer image for nginx:alpine Creating compose-demo_web_1 .. . done Note: If you run into a permission error regarding the Docker socket, this means you skipped Step 2 of How To Install and Use Docker on Ubuntu 20.04 . Going back and completing that step will enable permissions to run docker commands without sudo . Your environment is now up and running in the background. To verify that the container is active, you can run: docker-compose ps This command will show you information about the running containers and their state, as well as any port redirections currently in place: Output Name Command State Ports ---------------------------------------------------------------------------------- compose-demo_web_1 /docker-entrypoint.sh ngin .. . Up 0.0 .0.0:8000- > 80 /tcp You can now access the demo application by pointing your browser to either localhost:8000 if you are running this demo on your local machine, or your_server_domain_or_IP :8000 if you are running this demo on a remote server. You’ll see a page like this: The shared volume you’ve set up within the docker-compose.yml file keeps your app folder files in sync with the container’s document root. If you make any changes to the index.html file, they will be automatically picked up by the container and thus reflected on your browser when you reload the page. In the next step, you’ll see how to manage your containerized environment with Docker Compose commands.\n\nStep 4 — Getting Familiar with Docker Compose Commands You’ve seen how to set up a docker-compose.yml file and bring your environment up with docker-compose up . You’ll now see how to use Docker Compose commands to manage and interact with your containerized environment. To check the logs produced by your Nginx container, you can use the logs command: docker-compose logs You’ll see output similar to this: Output Attaching to compose-demo_web_1 web_1 | /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration web_1 | /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ web_1 | /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh web_1 | 10 -listen-on-ipv6-by-default.sh: Getting the checksum of /etc/nginx/conf.d/default.conf web_1 | 10 -listen-on-ipv6-by-default.sh: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf web_1 | /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh web_1 | /docker-entrypoint.sh: Configuration complete ; ready for start up web_1 | 172.22 .0.1 - - [ 02/Jun/2020:10:47:13 +0000 ] \"GET / HTTP/1.1\" 200 353 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36\" \"-\" If you want to pause the environment execution without changing the current state of your containers, you can use: docker-compose pause Output Pausing compose-demo_web_1 .. . done To resume execution after issuing a pause: docker-compose unpause Output Unpausing compose-demo_web_1 .. . done The stop command will terminate the container execution, but it won’t destroy any data associated with your containers: docker-compose stop Output Stopping compose-demo_web_1 .. . done If you want to remove the containers, networks, and volumes associated with this containerized environment, use the down command: docker-compose down Output Removing compose-demo_web_1 .. . done Removing network compose-demo_default Notice that this won’t remove the base image used by Docker Compose to spin up your environment (in your case, nginx:alpine ). This way, whenever you bring your environment up again with a docker-compose up , the process will be much faster since the image is already on your system. In case you want to also remove the base image from your system, you can use: docker image rm nginx:alpine Output Untagged: nginx:alpine Untagged: nginx@sha256:b89a6ccbda39576ad23fd079978c967cecc6b170db6e7ff8a769bf2259a71912 Deleted: sha256:7d0cdcc60a96a5124763fddf5d534d058ad7d0d8d4c3b8be2aefedf4267d0270 Deleted: sha256:05a0eaca15d731e0029a7604ef54f0dda3b736d4e987e6ac87b91ac7aac03ab1 Deleted: sha256:c6bbc4bdac396583641cb44cd35126b2c195be8fe1ac5e6c577c14752bbe9157 Deleted: sha256:35789b1e1a362b0da8392ca7d5759ef08b9a6b7141cc1521570f984dc7905eb6 Deleted: sha256:a3efaa65ec344c882fe5d543a392a54c4ceacd1efd91662d06964211b1be4c08 Deleted: sha256:3e207b409db364b595ba862cdc12be96dcdad8e36c59a03b7b3b61c946a5741a Note : Please refer to our guide on How to Install and Use Docker for a more detailed reference on Docker commands.\n\nCommon Errors and Debugging docker-compose: command not found If you encounter this error, it means that the docker-compose command is not in your system’s PATH . This could be due to an incomplete installation or a misconfiguration. To fix this, you can either add the directory where docker-compose is located to your PATH, or you can run the command with the full path to the docker-compose executable. For example, if docker-compose is located in /usr/local/bin , you can run the command like this: /usr/local/bin/docker-compose Permission issues when running Docker Compose If you encounter permission issues when running Docker Compose, it could be due to the user not having the necessary permissions to run Docker commands. To fix this, you can either run the command with sudo to elevate your privileges, or you can add your user to the docker group. For example, to run the docker-compose command with sudo , you can use: sudo docker-compose To add your user to the docker group, you can use the following command: sudo usermod -aG docker $USER Remember to log out and log back in for the changes to take effect.\n\nFAQs What is Docker Compose used for? Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to create a YAML file to configure your application’s services, and then uses that configuration to create and start all the services from a single command. For example, you can define a service for a web server and another for a database, and Docker Compose will handle the creation and linking of these services. What is the latest version of Docker Compose? To find the latest version of Docker Compose, you can check the Docker Compose releases page . At the time of this writing, the latest version is v2.34.0 . How do I check if Docker Compose is installed? To check if Docker Compose is installed, you can run the following command: docker-compose --version This will display the version of Docker Compose installed on your system. Can I install Docker Compose without Docker? No, Docker Compose requires Docker to be installed on your system. Docker Compose is a tool that builds on top of Docker, so you need to have Docker installed and running before you can use Docker Compose. How do I uninstall Docker Compose from Ubuntu? To uninstall Docker Compose from Ubuntu, you can run the following command: sudo apt-get purge docker-compose This will remove Docker Compose from your system.\n\nConclusion In this guide, you’ve seen how to install Docker Compose and set up a containerized environment based on an Nginx web server image. You’ve also seen how to manage this environment using Compose commands. For a complete reference of all available docker-compose commands, check the official documentation . If you’re interested in more usages of Docker Compose, check out the following tutorials: How To Install WordPress With Docker Compose . How To Secure a Containerized Node.js Application with Nginx, Let’s Encrypt, and Docker Compose . How To Install LAMP Stack on Ubuntu . How To Install and Set Up Laravel with Docker Compose on Ubuntu . How To Install WordPress With Docker Compose .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 14400
        }
      },
      {
        "header": "docker-compose: command not found",
        "content": "If you encounter this error, it means that the docker-compose command is not in your system’s PATH . This could be due to an incomplete installation or a misconfiguration.\n\nTo fix this, you can either add the directory where docker-compose is located to your PATH, or you can run the command with the full path to the docker-compose executable.\n\nFor example, if docker-compose is located in /usr/local/bin , you can run the command like this:",
        "code_examples": [
          "/usr/local/bin/docker-compose"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 442
        }
      },
      {
        "header": "Permission issues when running Docker Compose",
        "content": "If you encounter permission issues when running Docker Compose, it could be due to the user not having the necessary permissions to run Docker commands.\n\nTo fix this, you can either run the command with sudo to elevate your privileges, or you can add your user to the docker group.\n\nFor example, to run the docker-compose command with sudo , you can use:\n\nTo add your user to the docker group, you can use the following command:\n\nRemember to log out and log back in for the changes to take effect.",
        "code_examples": [
          "sudo\ndocker-compose",
          "sudo\nusermod\n-aG\ndocker\n$USER"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 497
        }
      },
      {
        "header": "What is Docker Compose used for?",
        "content": "Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to create a YAML file to configure your application’s services, and then uses that configuration to create and start all the services from a single command.\n\nFor example, you can define a service for a web server and another for a database, and Docker Compose will handle the creation and linking of these services.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 416
        }
      },
      {
        "header": "What is the latest version of Docker Compose?",
        "content": "To find the latest version of Docker Compose, you can check the Docker Compose releases page . At the time of this writing, the latest version is v2.34.0 .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 155
        }
      },
      {
        "header": "How do I check if Docker Compose is installed?",
        "content": "To check if Docker Compose is installed, you can run the following command:\n\nThis will display the version of Docker Compose installed on your system.",
        "code_examples": [
          "docker-compose\n--version"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 150
        }
      },
      {
        "header": "Can I install Docker Compose without Docker?",
        "content": "No, Docker Compose requires Docker to be installed on your system. Docker Compose is a tool that builds on top of Docker, so you need to have Docker installed and running before you can use Docker Compose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 205
        }
      },
      {
        "header": "How do I uninstall Docker Compose from Ubuntu?",
        "content": "To uninstall Docker Compose from Ubuntu, you can run the following command:\n\nThis will remove Docker Compose from your system.",
        "code_examples": [
          "sudo\napt-get\npurge\ndocker-compose"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 126
        }
      },
      {
        "header": "Still looking for an answer?",
        "content": "Ask a question Search for more help",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 35
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "The developer cloud",
        "content": "Scale up as you grow — whether you're running one virtual machine or ten thousand.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 82
        }
      },
      {
        "header": "Get started for free",
        "content": "Sign up and get $200 in credit for your first 60 days with DigitalOcean.*",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 73
        }
      },
      {
        "header": "Company",
        "content": "About Leadership Blog Careers Customers Partners Referral Program Affiliate Program Press Legal Privacy Policy Security Investor Relations",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 138
        }
      },
      {
        "header": "Products",
        "content": "Overview Droplets Kubernetes Functions App Platform Gradient™ AI GPU Droplets Gradient™ AI Bare Metal GPUs Gradient™ AI 1-Click Models Gradient™ AI Platform Load Balancers Managed Databases Spaces Block Storage Network File Storage API Uptime Identity and Access Management Cloudways",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 283
        }
      },
      {
        "header": "Resources",
        "content": "Community Tutorials Community Q&A CSS-Tricks Write for DOnations Currents Research DigitalOcean Startups Wavemakers Program Compass Council Open Source Newsletter Signup Marketplace Pricing Pricing Calculator Documentation Release Notes Code of Conduct Shop Swag",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 262
        }
      },
      {
        "header": "Solutions",
        "content": "Website Hosting VPS Hosting Web & Mobile Apps Game Development Streaming VPN SaaS Platforms Cloud Hosting for Blockchain Startup Resources Migration Assistance",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 159
        }
      },
      {
        "header": "Contact",
        "content": "Support Sales Report Abuse System Status Share your ideas",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 57
        }
      }
    ],
    "url": "https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-20-04",
    "doc_type": "docker",
    "total_sections": 23
  },
  {
    "title": "How To Remove Docker Images, Containers, and Volumes",
    "summary": "Featured Products Compute",
    "sections": [
      {
        "header": "Table of contents",
        "content": "Purging All Unused or Dangling Images Containers Volumes and Networks Removing Docker Images Removing Containers Removing Volumes docker rm vs docker rmi vs docker prune Common Errors and Debugging FAQs Conclusion",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 213
        }
      },
      {
        "header": "Introduction",
        "content": "Docker makes it easy to wrap your applications and services in containers so you can run them anywhere. However, as you work with Docker, it’s also easy to accumulate an excessive number of unused images, containers, and data volumes that clutter the output and consume disk space.\n\nDocker gives you all the tools you need to clean up your system from the command line. This cheat sheet-style guide provides a quick reference to commands that are useful for freeing disk space and keeping your system organized by removing unused Docker images, containers, and volumes.\n\nHow to Use This Guide:\n\nThis guide is in cheat sheet format with self-contained command-line snippets. Jump to any section that is relevant to the task you are trying to complete.\n\nNote: The command substitution syntax, command $( command ) , used in the commands is available in many popular shells, such as bash , zsh , and Windows Powershell .\n\nIf you want a 1-click way to deploy a Docker application to a live server, take a look at DigitalOcean App Platform .\n\nPurging All Unused or Dangling Images, Containers, Volumes, and Networks Docker provides a single command that will clean up any resources — images, containers, volumes, and networks — that are dangling (not tagged or associated with a container): docker system prune To additionally remove any stopped containers and all unused images (not just dangling images), add the -a flag to the command: docker system prune -a\n\nRemoving Docker Images Remove one or more specific images Use the docker images command with the -a flag to locate the ID of the images you want to remove. This will show you every image, including intermediate image layers. When you’ve located the images you want to delete, you can pass their ID or tag to docker rmi : List: docker images -a Remove: docker rmi Image Image Note: The -a or --all flag in the docker images command displays all the Docker images, including intermediate ones that are not referenced by any tags. By default, docker images shows only the images with at least one tag. However, there may be some images without any tags that are still taking up disk space on the system. The -a flag can be helpful in identifying images that can be pruned to save disk space. When used with the docker rmi command, the -f or --force flag can also be used to remove images with no tags. Remove Dangling Docker Images Docker images consist of multiple layers. Dangling images are layers that have no relationship to any tagged images. They no longer serve a purpose and consume disk space. They can be located by adding the filter flag -f with a value of dangling=true to the docker images command. When you’re sure you want to delete them, you can use the docker image prune command: Note: If you build an image without tagging it, the image will appear on the list of dangling images because it has no association with a tagged image. You can avoid this situation by providing a tag when you build, and you can retroactively tag an image with the docker tag command. List: docker images -f dangling = true Remove: docker image prune Removing images according to a pattern You can find all the images that match a pattern using a combination of docker images and grep . Once you’re satisfied, you can delete them by using awk to pass the IDs to docker rmi . Note that these utilities are not supplied by Docker and are not necessarily available on all systems: List: docker images -a | grep \" pattern \" Remove: docker images -a | grep \" pattern \" | awk '{print $1\":\"$2}' | xargs docker rmi Remove all images All the Docker images on a system can be listed by adding -a to the docker images command. Once you’re sure you want to delete them all, you can add the -q flag to pass the image ID to docker rmi : List: docker images -a Remove: docker rmi $( docker images -a -q )\n\nRemoving Containers Remove one or more specific containers Use the docker ps command with the -a flag to locate the name or ID of the containers you want to remove: List: docker ps -a Remove: docker rm ID_or_Name ID_or_Name Remove a container upon exiting If you know when you’re creating a container that you won’t want to keep it around once you’re done, you can run docker run --rm to automatically delete it when it exits: Run and Remove: docker run --rm image_name Remove all exited containers You can locate containers using docker ps -a and filter them by their status: created , restarting , running , paused , or exited . To review the list of exited containers, use the -f flag to filter based on status. When you’ve verified you want to remove those containers, use -q to pass the IDs to the docker rm command: List: docker ps -a -f status = exited Remove: docker rm $( docker ps -a -f status = exited -q ) Remove containers using more than one filter Docker filters can be combined by repeating the filter flag with an additional value. This results in a list of containers that meet either condition. For example, if you want to delete all containers marked as either created (a state which can result when you run a container with an invalid command) or exited , you can use two filters: List: docker ps -a -f status = exited -f status = created Remove: docker rm $( docker ps -a -f status = exited -f status = created -q ) Remove containers according to a pattern You can find all the containers that match a pattern using a combination of docker ps and grep . When you’re satisfied that you have the list you want to delete, you can use awk and xargs to supply the ID to docker rm . Note that these utilities are not supplied by Docker and are not necessarily available on all systems: List: docker ps -a | grep \" pattern ” Remove: docker ps -a | grep \" pattern \" | awk '{print $1}' | xargs docker rm Stop and remove all containers You can review the containers on your system with docker ps . Adding the -a flag will show all containers. When you’re sure you want to delete them, you can add the -q flag to supply the IDs to the docker stop and docker rm commands: List: docker ps -a Remove: docker stop $( docker ps -a -q ) docker rm $( docker ps -a -q )\n\nRemoving Volumes Remove one or more specific volumes Use the docker volume ls command to locate the volume name or names you wish to delete. Then you can remove one or more volumes with the docker volume rm command: List: docker volume ls Remove: docker volume rm volume_name volume_name Remove dangling volumes Since the point of volumes is to exist independent from containers, when a container is removed, a volume is not automatically removed at the same time. When a volume exists and is no longer connected to any containers, it’s called a dangling volume . To locate them to confirm you want to remove them, you can use the docker volume ls command with a filter to limit the results to dangling volumes. When you’re satisfied with the list, you can remove them all with docker volume prune : List: docker volume ls -f dangling = true Remove: docker volume prune Remove a container and its volume If you create an unnamed volume, it can be deleted at the same time as the container with the -v flag. Note that this only works with unnamed volumes. When the container is successfully removed, its ID is displayed. Note that no reference is made to the removal of the volume. If it is unnamed, it is silently removed from the system. If it is named, it silently stays present. Remove: docker rm -v container_name\n\ndocker rm vs docker rmi vs docker prune Command Description Targets Flags docker rm Removes one or more containers Containers -f to force removal, -v to remove volumes docker rmi Removes one or more images Images -f to force removal docker prune Removes unused or dangling resources Images, Containers, Volumes, Networks -a to remove all unused resources, -f to force removal Note: The -f flag is used to force the removal of resources without prompting for confirmation. The -a flag is used to remove all unused resources, including dangling ones.\n\nCommon Errors and Debugging Handling thread synchronization issues when multiple containers share volumes When multiple containers share volumes, thread synchronization issues can arise, leading to data corruption or unexpected behavior. To handle these issues, you can use the following strategies: Use Named Volumes: Named volumes provide better control and management over shared data. Here’s an example of how to use named volumes in a Docker Compose file: version : '3.8' services : app : image : myapp volumes : - myvolume : /app/node_modules volumes : myvolume : Implement File Locks: Use file locking mechanisms to ensure that only one container can access a file at a time. This can be achieved using tools like flock or lockfile within your application code. Use Docker Compose: Docker Compose allows you to define and manage multi-container applications, ensuring proper synchronization and volume sharing. Here’s an example of a Docker Compose file that defines a service with a named volume: version : '3.8' services : app : image : myapp volumes : - myvolume : /app/node_modules depends_on : - db db : image : mydb volumes : - myvolume : /var/lib/mysql volumes : myvolume : Debugging performance bottlenecks caused by excess image layers Excess image layers can lead to performance bottlenecks, especially during the build and deployment process. To debug and resolve these issues, follow these steps: Analyze Image Layers: Use the docker history <image> command to inspect the layers of an image and identify unnecessary layers. For example: docker history myapp Optimize Dockerfile: Combine multiple RUN instructions into a single instruction to reduce the number of layers. For instance, instead of: RUN apt update && apt install -y python3 RUN apt install -y python3-pip Use: RUN apt update && apt install -y python3 python3-pip Use Multi-Stage Builds: Multi-stage builds allow you to create smaller and more efficient images by copying only the necessary artifacts from intermediate stages. Here’s an example: FROM scratch AS base WORKDIR /app COPY go.mod ./ COPY go.sum ./ RUN go mod download FROM base AS builder RUN go build -o myapp FROM scratch COPY --from = builder /app/myapp . CMD [ \"./myapp\" ] Fixing container is running errors when attempting to remove an active container When you try to remove an active container, you may encounter the container is running error. To fix this issue, you have several options: First stop the container, then remove it: docker stop < container_id > docker rm < container_id > Force remove the container in a single command: docker rm -f < container_id > Stop and remove all containers: docker stop $( docker ps -a -q ) docker rm $( docker ps -a -q ) Use Docker Compose to stop and remove containers: docker-compose down If you’re using Docker Desktop, you can also use the GUI to stop and remove containers by right-clicking on the running container and selecting “Stop” and then “Remove” .\n\nFAQs 1. How do I delete all stopped containers in Docker? To delete all stopped containers in Docker, use the following command: docker-compose down This command will stop and remove all containers defined in your docker-compose.yml file. If you want to remove all stopped containers without using docker-compose , you can use: docker system prune -a 2. What happens when I run docker system prune? When you run docker system prune , Docker will remove all stopped containers and all networks not used by at least one container. Additionally, if you use the -a flag, Docker will also remove all unused images. This command is useful for freeing up disk space and cleaning up your Docker environment. 3. Can I remove a running Docker container? Yes, you can remove a running Docker container using the -f flag with the docker rm command. This will force the removal of the container without stopping it first. Here’s an example: docker rm -f < container_id > 4. How do I free up disk space used by Docker? To free up disk space used by Docker, you can use the following commands: docker system prune -a to remove all unused images. docker system prune -a -v to remove all unused images and volumes. docker volume prune -a to remove all unused volumes. docker network prune -a to remove all unused networks. 5. What is the difference between docker rm and docker rmi? docker rm is used to remove a container, while docker rmi is used to remove an image. docker rm will delete a container and its associated resources, but it will not delete the image that the container was based on. docker rmi , on the other hand, will delete an image, but it will not delete any containers that are based on that image. 6. How do I completely remove Docker images? To completely remove a Docker image, use the following command: docker rmi < image-id > Replace <image-id> with the ID or name of the image. If the image is in use by a container, you must first remove the container before removing the image. 7. How do I remove unused Docker images? Unused images (dangling and untagged) can be removed using the following command: docker image prune To remove all unused images, use the --all flag: docker image prune --all 8. How do I clear all Docker images and cache? To remove all Docker images, containers, volumes, and networks, use this command: docker system prune --all --volumes Note: This command will delete everything related to Docker, including all stopped containers and volumes. 9. How do I remove files from a Docker image? You cannot directly modify a Docker image. Instead, create a new image without the unwanted files. Here’s how: Start and login inside the container from the image: docker run -it < image-id > /bin/bash Now, remove files within the container as needed. Next, commit the changes to a new image: docker commit < container-id > < new-image-name > 10. How to remove all stopped Docker containers? To remove all stopped containers, use: docker container prune You will be prompted for confirmation. To skip the prompt, use: docker container prune -f 11. How do I remove old Docker containers? To remove containers that have been inactive for a specified time, use the following: docker ps -a --filter \"status=exited\" --filter \"status=created\" docker rm $( docker ps -a -q --filter \"status=exited\" --filter \"status=created\" ) This removes containers with exited or created status. Adjust the filter based on your needs. 12. Where are Docker images stored? Docker images are stored in the Docker directory of your system. By default: Linux: /var/lib/docker MacOS: In Docker Desktop’s VM Windows: In Docker Desktop’s WSL2 VM or the C:\\ProgramData\\DockerDesktop You can verify the storage driver and path using: docker info | grep \"Docker Root Dir\" 13. How do I remove a container when Docker is finished? To automatically remove a container after it exits, use the --rm flag when starting the container: docker run --rm < image-id > This ensures the container is removed as soon as it stops.\n\nConclusion This guide covers some of the common commands used to remove images, containers, and volumes with Docker. There are many other combinations and flags that can be used with each. For a comprehensive guide to what’s available, see the Docker documentation for docker system prune , docker rmi , docker rm , and docker volume rm . If there are common cleanup tasks you’d like to see in the guide, please ask or make suggestions in the comments. For a detailed look at the different components of a Docker container, check out The Docker Ecosystem: An Introduction to Common Components . Additionally, explore more Docker-related topics with these tutorials: Working with Docker Containers . How to Install and Use Docker on Ubuntu . How to Automatically Update Docker Container Images with Watchtower on Ubuntu .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 15773
        }
      },
      {
        "header": "Remove one or more specific images",
        "content": "Use the docker images command with the -a flag to locate the ID of the images you want to remove. This will show you every image, including intermediate image layers. When you’ve located the images you want to delete, you can pass their ID or tag to docker rmi :\n\nList:\n\nRemove:\n\nNote: The -a or --all flag in the docker images command displays all the Docker images, including intermediate ones that are not referenced by any tags. By default, docker images shows only the images with at least one tag. However, there may be some images without any tags that are still taking up disk space on the system. The -a flag can be helpful in identifying images that can be pruned to save disk space. When used with the docker rmi command, the -f or --force flag can also be used to remove images with no tags.",
        "code_examples": [
          "docker\nimages\n-a",
          "docker\nrmi\nImage\nImage"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 803
        }
      },
      {
        "header": "Remove Dangling Docker Images",
        "content": "Docker images consist of multiple layers. Dangling images are layers that have no relationship to any tagged images. They no longer serve a purpose and consume disk space. They can be located by adding the filter flag -f with a value of dangling=true to the docker images command. When you’re sure you want to delete them, you can use the docker image prune command:\n\nNote: If you build an image without tagging it, the image will appear on the list of dangling images because it has no association with a tagged image. You can avoid this situation by providing a tag when you build, and you can retroactively tag an image with the docker tag command.\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nimages\n-f\ndangling\n=\ntrue",
          "docker\nimage prune"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 667
        }
      },
      {
        "header": "Removing images according to a pattern",
        "content": "You can find all the images that match a pattern using a combination of docker images and grep . Once you’re satisfied, you can delete them by using awk to pass the IDs to docker rmi . Note that these utilities are not supplied by Docker and are not necessarily available on all systems:\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nimages\n-a\n|\ngrep\n\"\npattern\n\"",
          "docker\nimages\n-a\n|\ngrep\n\"\npattern\n\"\n|\nawk\n'{print $1\":\"$2}'\n|\nxargs\ndocker\nrmi"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 303
        }
      },
      {
        "header": "Remove all images",
        "content": "All the Docker images on a system can be listed by adding -a to the docker images command. Once you’re sure you want to delete them all, you can add the -q flag to pass the image ID to docker rmi :\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nimages\n-a",
          "docker\nrmi\n$(\ndocker\nimages\n-a\n-q\n)"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 213
        }
      },
      {
        "header": "Remove one or more specific containers",
        "content": "Use the docker ps command with the -a flag to locate the name or ID of the containers you want to remove:\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nps\n-a",
          "docker\nrm\nID_or_Name\nID_or_Name"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 121
        }
      },
      {
        "header": "Remove a container upon exiting",
        "content": "If you know when you’re creating a container that you won’t want to keep it around once you’re done, you can run docker run --rm to automatically delete it when it exits:\n\nRun and Remove:",
        "code_examples": [
          "docker\nrun\n--rm\nimage_name"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 187
        }
      },
      {
        "header": "Remove all exited containers",
        "content": "You can locate containers using docker ps -a and filter them by their status: created , restarting , running , paused , or exited . To review the list of exited containers, use the -f flag to filter based on status. When you’ve verified you want to remove those containers, use -q to pass the IDs to the docker rm command:\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nps\n-a\n-f\nstatus\n=\nexited",
          "docker\nrm\n$(\ndocker\nps\n-a\n-f\nstatus\n=\nexited\n-q\n)"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 338
        }
      },
      {
        "header": "Remove containers using more than one filter",
        "content": "Docker filters can be combined by repeating the filter flag with an additional value. This results in a list of containers that meet either condition. For example, if you want to delete all containers marked as either created (a state which can result when you run a container with an invalid command) or exited , you can use two filters:\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nps\n-a\n-f\nstatus\n=\nexited\n-f\nstatus\n=\ncreated",
          "docker\nrm\n$(\ndocker\nps\n-a\n-f\nstatus\n=\nexited\n-f\nstatus\n=\ncreated\n-q\n)"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 354
        }
      },
      {
        "header": "Remove containers according to a pattern",
        "content": "You can find all the containers that match a pattern using a combination of docker ps and grep . When you’re satisfied that you have the list you want to delete, you can use awk and xargs to supply the ID to docker rm . Note that these utilities are not supplied by Docker and are not necessarily available on all systems:\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nps\n-a\n|\ngrep\n\"\npattern\n”",
          "docker\nps\n-a\n|\ngrep\n\"\npattern\n\"\n|\nawk\n'{print $1}'\n|\nxargs\ndocker\nrm"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 338
        }
      },
      {
        "header": "Stop and remove all containers",
        "content": "You can review the containers on your system with docker ps . Adding the -a flag will show all containers. When you’re sure you want to delete them, you can add the -q flag to supply the IDs to the docker stop and docker rm commands:\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nps\n-a",
          "docker\nstop\n$(\ndocker\nps\n-a\n-q\n)\ndocker\nrm\n$(\ndocker\nps\n-a\n-q\n)"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 249
        }
      },
      {
        "header": "Remove one or more specific volumes",
        "content": "Use the docker volume ls command to locate the volume name or names you wish to delete. Then you can remove one or more volumes with the docker volume rm command:\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nvolume\nls",
          "docker\nvolume\nrm\nvolume_name\nvolume_name"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 178
        }
      },
      {
        "header": "Remove dangling volumes",
        "content": "Since the point of volumes is to exist independent from containers, when a container is removed, a volume is not automatically removed at the same time. When a volume exists and is no longer connected to any containers, it’s called a dangling volume . To locate them to confirm you want to remove them, you can use the docker volume ls command with a filter to limit the results to dangling volumes. When you’re satisfied with the list, you can remove them all with docker volume prune :\n\nList:\n\nRemove:",
        "code_examples": [
          "docker\nvolume\nls\n-f\ndangling\n=\ntrue",
          "docker\nvolume prune"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 503
        }
      },
      {
        "header": "Remove a container and its volume",
        "content": "If you create an unnamed volume, it can be deleted at the same time as the container with the -v flag. Note that this only works with unnamed volumes. When the container is successfully removed, its ID is displayed. Note that no reference is made to the removal of the volume. If it is unnamed, it is silently removed from the system. If it is named, it silently stays present.\n\nRemove:",
        "code_examples": [
          "docker\nrm\n-v\ncontainer_name"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 386
        }
      },
      {
        "header": "Handling thread synchronization issues when multiple containers share volumes",
        "content": "When multiple containers share volumes, thread synchronization issues can arise, leading to data corruption or unexpected behavior. To handle these issues, you can use the following strategies:\n\nUse Named Volumes: Named volumes provide better control and management over shared data. Here’s an example of how to use named volumes in a Docker Compose file:\n\nImplement File Locks: Use file locking mechanisms to ensure that only one container can access a file at a time. This can be achieved using tools like flock or lockfile within your application code. Use Docker Compose: Docker Compose allows you to define and manage multi-container applications, ensuring proper synchronization and volume sharing. Here’s an example of a Docker Compose file that defines a service with a named volume:",
        "code_examples": [
          "version\n:\n'3.8'\nservices\n:\napp\n:\nimage\n:\nmyapp\nvolumes\n:\n-\nmyvolume\n:\n/app/node_modules\nvolumes\n:\nmyvolume\n:",
          "version\n:\n'3.8'\nservices\n:\napp\n:\nimage\n:\nmyapp\nvolumes\n:\n-\nmyvolume\n:\n/app/node_modules\ndepends_on\n:\n-\ndb\ndb\n:\nimage\n:\nmydb\nvolumes\n:\n-\nmyvolume\n:\n/var/lib/mysql\nvolumes\n:\nmyvolume\n:"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 791
        }
      },
      {
        "header": "Debugging performance bottlenecks caused by excess image layers",
        "content": "Excess image layers can lead to performance bottlenecks, especially during the build and deployment process. To debug and resolve these issues, follow these steps:\n\nAnalyze Image Layers: Use the docker history <image> command to inspect the layers of an image and identify unnecessary layers. For example:\n\nOptimize Dockerfile: Combine multiple RUN instructions into a single instruction to reduce the number of layers. For instance, instead of:\n\nUse:\n\nUse Multi-Stage Builds: Multi-stage builds allow you to create smaller and more efficient images by copying only the necessary artifacts from intermediate stages. Here’s an example:",
        "code_examples": [
          "docker\nhistory\nmyapp",
          "RUN\napt update && apt install -y python3\nRUN\napt install -y python3-pip",
          "RUN\napt update && apt install -y python3 python3-pip",
          "FROM\nscratch\nAS\nbase\nWORKDIR\n/app\nCOPY\ngo.mod ./\nCOPY\ngo.sum ./\nRUN\ngo mod download\nFROM\nbase\nAS\nbuilder\nRUN\ngo build -o myapp\nFROM\nscratch\nCOPY\n--from\n=\nbuilder\n/app/myapp .\nCMD\n[\n\"./myapp\"\n]"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 634
        }
      },
      {
        "header": "Fixing container is running errors when attempting to remove an active container",
        "content": "When you try to remove an active container, you may encounter the container is running error. To fix this issue, you have several options:\n\nFirst stop the container, then remove it: docker stop < container_id > docker rm < container_id > Force remove the container in a single command: docker rm -f < container_id > Stop and remove all containers: docker stop $( docker ps -a -q ) docker rm $( docker ps -a -q ) Use Docker Compose to stop and remove containers: docker-compose down If you’re using Docker Desktop, you can also use the GUI to stop and remove containers by right-clicking on the running container and selecting “Stop” and then “Remove” .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 652
        }
      },
      {
        "header": "Still looking for an answer?",
        "content": "Ask a question Search for more help",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 35
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "The developer cloud",
        "content": "Scale up as you grow — whether you're running one virtual machine or ten thousand.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 82
        }
      },
      {
        "header": "Get started for free",
        "content": "Sign up and get $200 in credit for your first 60 days with DigitalOcean.*",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 73
        }
      },
      {
        "header": "Company",
        "content": "About Leadership Blog Careers Customers Partners Referral Program Affiliate Program Press Legal Privacy Policy Security Investor Relations",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 138
        }
      },
      {
        "header": "Products",
        "content": "Overview Droplets Kubernetes Functions App Platform Gradient™ AI GPU Droplets Gradient™ AI Bare Metal GPUs Gradient™ AI 1-Click Models Gradient™ AI Platform Load Balancers Managed Databases Spaces Block Storage Network File Storage API Uptime Identity and Access Management Cloudways",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 283
        }
      },
      {
        "header": "Resources",
        "content": "Community Tutorials Community Q&A CSS-Tricks Write for DOnations Currents Research DigitalOcean Startups Wavemakers Program Compass Council Open Source Newsletter Signup Marketplace Pricing Pricing Calculator Documentation Release Notes Code of Conduct Shop Swag",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 262
        }
      },
      {
        "header": "Solutions",
        "content": "Website Hosting VPS Hosting Web & Mobile Apps Game Development Streaming VPN SaaS Platforms Cloud Hosting for Blockchain Startup Resources Migration Assistance",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 159
        }
      },
      {
        "header": "Contact",
        "content": "Support Sales Report Abuse System Status Share your ideas",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 57
        }
      }
    ],
    "url": "https://www.digitalocean.com/community/tutorials/how-to-remove-docker-images-containers-and-volumes",
    "doc_type": "docker",
    "total_sections": 32
  },
  {
    "title": "Docker Explained: Using Dockerfiles to Automate Building of Images",
    "summary": "Featured Products Compute",
    "sections": [
      {
        "header": "Table of contents",
        "content": "Status Deprecated Glossary Docker in Brief Dockerfiles Dockerfile Syntax Dockerfile Commands Instructions How to Use Dockerfiles Dockerfile Example Creating an Image to Install MongoDB",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 184
        }
      },
      {
        "header": "Reason",
        "content": "The techniques in this article are outdated and may no longer reflect Docker best-practices.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 92
        }
      },
      {
        "header": "See Instead",
        "content": "The Docker Ecosystem: An Introduction to Common Components Dockerfile reference Best practices for writing Dockerfiles",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 118
        }
      },
      {
        "header": "Introduction",
        "content": "Docker containers are created by using base images. An image can be basic, with nothing but the operating-system fundamentals, or it can consist of a sophisticated pre-built application stack ready for launch.\n\nWhen building your images with Docker, each action taken (i.e. a command executed such as apt-get install) forms a new layer on top of the previous one. These base images then can be used to create new containers.\n\nIn this DigitalOcean article, we will see about automating this process as much as possible, as well as demonstrate the best practices and methods to make most of Docker and containers via Dockerfiles : scripts to build containers, step-by-step, layer-by-layer, automatically from a base image.\n\nGlossary 1. Docker in Brief 2. Dockerfiles 3. Dockerfile Syntax What is Syntax? Dockerfile Syntax Example 4. Dockerfile Commands ADD CMD ENTRYPOINT ENV EXPOSE FROM MAINTAINER RUN USER VOLUME WORKDIR 5. How To Use Dockerfiles 6. Dockerfile Example: Creating an Image to Install MongoDB Creating the Empty Dockerfile Defining Our File and Its Purpose Setting The Base Image to Use Defining The Maintainer (Author) Updating The Application Repository List Setting Arguments and Commands for Downloading MongoDB Setting The Default Port For MongoDB Saving The Dockerfile Building Our First Image Running A MongoDB Instance\n\nDocker in Brief The Docker project offers higher-level tools which work together, built on top of some Linux kernel features. The goal is to help developers and system administrators port applications - with all of their dependencies conjointly - and get them running across systems and machines headache free . Docker achieves this by creating safe, LXC-based (i.e. Linux Containers) environments for applications called “Docker containers”. These containers are created using Docker images , which can be built either by executing commands manually or automatically through Dockerfiles . Note: To learn more about Docker and its parts (e.g. Docker daemon, CLI, images etc.), check out our introductory article to the project: How To Install and Use Docker on Ubuntu 16.04 .\n\nDockerfiles Each Dockerfile is a script, composed of various commands (instructions) and arguments listed successively to automatically perform actions on a base image in order to create (or form) a new one. They are used for organizing things and greatly help with deployments by simplifying the process start-to-finish. Dockerfiles begin with defining an image FROM which the build process starts. Followed by various other methods, commands and arguments (or conditions), in return, provide a new image which is to be used for creating docker containers. They can be used by providing a Dockerfile’s content - in various ways - to the docker daemon to build an image (as explained in the How To Use section).\n\nDockerfile Syntax Before we begin talking about Dockerfiles, let’s quickly go over its syntax and what that actually means. What is Syntax? Very simply, syntax in programming means a structure to order commands, arguments, and everything else that is required to program an application to perform a procedure (i.e. a function / collection of instructions). These structures are based on rules, clearly and explicitly defined, and they are to be followed by the programmer to interface with whichever computer application (e.g. interpreters, daemons etc.) uses or expects them. If a script (i.e. a file containing series of tasks to be performed) is not correctly structured (i.e. wrong syntax), the computer program will not be able to parse it. Parsing roughly can be understood as going over an input with the end goal of understanding what is meant. Dockerfiles use simple, clean, and clear syntax which makes them strikingly easy to create and use. They are designed to be self explanatory, especially because they allow commenting just like a good and properly written application source-code. Dockerfile Syntax Example Dockerfile syntax consists of two kind of main line blocks: comments and commands + arguments. # Line blocks used for commenting command argument argument .. A Simple Example: # Print \"Hello docker!\" RUN echo \"Hello docker!\"\n\nDockerfile Commands (Instructions) Currently there are about a dozen different set of commands which Dockerfiles can contain to have Docker build an image. In this section, we will go over all of them, individually, before working on a Dockerfile example. Note: As explained in the previous section (Dockerfile Syntax), all these commands are to be listed (i.e. written) successively, inside a single plain text file (i.e. Dockerfile), in the order you would like them performed (i.e. executed) by the docker daemon to build an image. However, some of these commands (e.g. MAINTAINER) can be placed anywhere you seem fit (but always after FROM command), as they do not constitute of any execution but rather value of a definition (i.e. just some additional information). ADD The ADD command gets two arguments: a source and a destination. It basically copies the files from the source on the host into the container’s own filesystem at the set destination. If, however, the source is a URL (e.g. http://github.com/user/file/ ), then the contents of the URL are downloaded and placed at the destination. Example: # Usage: ADD [source directory or URL] [destination directory] ADD /my_app_folder /my_app_folder CMD The command CMD, similarly to RUN, can be used for executing a specific command. However, unlike RUN it is not executed during build, but when a container is instantiated using the image being built. Therefore, it should be considered as an initial, default command that gets executed (i.e. run) with the creation of containers based on the image. To clarify: an example for CMD would be running an application upon creation of a container which is already installed using RUN (e.g. RUN apt-get install …) inside the image. This default application execution command that is set with CMD becomes the default and replaces any command which is passed during the creation. Example: # Usage 1: CMD application \"argument\", \"argument\", .. CMD \"echo\" \"Hello docker!\" ENTRYPOINT ENTRYPOINT argument sets the concrete default application that is used every time a container is created using the image. For example, if you have installed a specific application inside an image and you will use this image to only run that application, you can state it with ENTRYPOINT and whenever a container is created from that image, your application will be the target. If you couple ENTRYPOINT with CMD, you can remove “application” from CMD and just leave “arguments” which will be passed to the ENTRYPOINT. Example: # Usage: ENTRYPOINT application \"argument\", \"argument\", .. # Remember: arguments are optional. They can be provided by CMD # or during the creation of a container. ENTRYPOINT echo # Usage example with CMD: # Arguments set with CMD can be overridden during *run* CMD \"Hello docker!\" ENTRYPOINT echo ENV The ENV command is used to set the environment variables (one or more). These variables consist of “key value” pairs which can be accessed within the container by scripts and applications alike. This functionality of Docker offers an enormous amount of flexibility for running programs. Example: # Usage: ENV key value ENV SERVER_WORKS 4 EXPOSE The EXPOSE command is used to associate a specified port to enable networking between the running process inside the container and the outside world (i.e. the host). Example: # Usage: EXPOSE [port] EXPOSE 8080 To learn about Docker networking, check out the Docker container networking documentation . FROM FROM directive is probably the most crucial amongst all others for Dockerfiles. It defines the base image to use to start the build process. It can be any image, including the ones you have created previously. If a FROM image is not found on the host, Docker will try to find it (and download) from the Docker Hub or other container repository. It needs to be the first command declared inside a Dockerfile. Example: # Usage: FROM [image name] FROM ubuntu MAINTAINER One of the commands that can be set anywhere in the file - although it would be better if it was declared on top - is MAINTAINER. This non-executing command declares the author, hence setting the author field of the images. It should come nonetheless after FROM. Example: # Usage: MAINTAINER [name] MAINTAINER authors_name RUN The RUN command is the central executing directive for Dockerfiles. It takes a command as its argument and runs it to form the image. Unlike CMD, it actually is used to build the image (forming another layer on top of the previous one which is committed). Example: # Usage: RUN [command] RUN aptitude install -y riak USER The USER directive is used to set the UID (or username) which is to run the container based on the image being built. Example: # Usage: USER [UID] USER 751 VOLUME The VOLUME command is used to enable access from your container to a directory on the host machine (i.e. mounting it). Example: # Usage: VOLUME [\"/dir_1\", \"/dir_2\" ..] VOLUME [\"/my_files\"] WORKDIR The WORKDIR directive is used to set where the command defined with CMD is to be executed. Example: # Usage: WORKDIR /path WORKDIR ~/\n\nHow to Use Dockerfiles Using Dockerfiles is as simple as having the Docker daemon run one. The output after executing the script will be the ID of the new docker image. Usage: # Build an image using the Dockerfile at current location # Example: docker build -t [name] . docker build -t my_mongodb .\n\nDockerfile Example: Creating an Image to Install MongoDB In this final section for Dockerfiles, we will create a Dockerfile document and populate it step-by-step with the end result of having a Dockerfile, which can be used to create a docker image to run MongoDB containers. Note: After starting to edit the Dockerfile, all the content and arguments from the sections below are to be written (appended) inside of it successively, following our example and explanations from the Docker Syntax section. You can see what the end result will look like at the latest section of this walkthrough. Creating the Empty Dockerfile Using the nano text editor, let’s start editing our Dockerfile. nano Dockerfile Defining Our File and Its Purpose Albeit optional, it is always a good practice to let yourself and everybody figure out (when necessary) what this file is and what it is intended to do. For this, we will begin our Dockerfile with fancy comments (#) to describe it. ############################################################ # Dockerfile to build MongoDB container images # Based on Ubuntu ############################################################ Setting The Base Image to Use # Set the base image to Ubuntu FROM ubuntu Defining The Maintainer (Author) # File Author / Maintainer MAINTAINER Example McAuthor Setting Arguments and Commands for Downloading MongoDB ################## BEGIN INSTALLATION ###################### # Install MongoDB Following the Instructions at MongoDB Docs # Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/ # Add the package verification key RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10 # Add MongoDB to the repository sources list RUN echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | tee /etc/apt/sources.list.d/mongodb.list # Update the repository sources list RUN apt-get update # Install MongoDB package (.deb) RUN apt-get install -y mongodb-10gen # Create the default data directory RUN mkdir -p /data/db ##################### INSTALLATION END ##################### Setting The Default Port For MongoDB # Expose the default port EXPOSE 27017 # Default port to execute the entrypoint (MongoDB) CMD [\"--port 27017\"] # Set default container command ENTRYPOINT usr/bin/mongod Saving The Dockerfile After you have appended everything to the file, it is time to save and exit. Press CTRL+X and then Y to confirm and save the Dockerfile. This is what the final file should look like: ############################################################ # Dockerfile to build MongoDB container images # Based on Ubuntu ############################################################ # Set the base image to Ubuntu FROM ubuntu # File Author / Maintainer MAINTAINER Example McAuthor ################## BEGIN INSTALLATION ###################### # Install MongoDB Following the Instructions at MongoDB Docs # Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/ # Add the package verification key RUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10 # Add MongoDB to the repository sources list RUN echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | tee /etc/apt/sources.list.d/mongodb.list # Update the repository sources list RUN apt-get update # Install MongoDB package (.deb) RUN apt-get install -y mongodb-10gen # Create the default data directory RUN mkdir -p /data/db ##################### INSTALLATION END ##################### # Expose the default port EXPOSE 27017 # Default port to execute the entrypoint (MongoDB) CMD [\"--port 27017\"] # Set default container command ENTRYPOINT usr/bin/mongod Building Our First Image Using the explanations from before, we are ready to create our first MongoDB image with docker! docker build -t my_mongodb . Note: The -t [name] flag here is used to tag the image. To learn more about what else you can do during build, run docker build --help . Running A MongoDB Instance Using the image we have build, we can now proceed to the final step: creating a container running a MongoDB instance inside, using a name of our choice (if desired with -name [name] ). docker run -name my_first_mdb_instance -i -t my_mongodb Note: If a name is not set, we will need to deal with complex, alphanumeric IDs which can be obtained by listing all the containers using docker ps -l . Note: To detach yourself from the container, use the escape sequence CTRL+P followed by CTRL+Q . Enjoy! <div class=“author”>Submitted by: <a href=“ https://twitter.com/ostezer ”>O.S. Tezer</a></div>",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 14140
        }
      },
      {
        "header": "3. Dockerfile Syntax",
        "content": "What is Syntax? Dockerfile Syntax Example",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 41
        }
      },
      {
        "header": "4. Dockerfile Commands",
        "content": "ADD CMD ENTRYPOINT ENV EXPOSE FROM MAINTAINER RUN USER VOLUME WORKDIR",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 69
        }
      },
      {
        "header": "6. Dockerfile Example: Creating an Image to Install MongoDB",
        "content": "Creating the Empty Dockerfile Defining Our File and Its Purpose Setting The Base Image to Use Defining The Maintainer (Author) Updating The Application Repository List Setting Arguments and Commands for Downloading MongoDB Setting The Default Port For MongoDB Saving The Dockerfile Building Our First Image Running A MongoDB Instance",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 333
        }
      },
      {
        "header": "What is Syntax?",
        "content": "Very simply, syntax in programming means a structure to order commands, arguments, and everything else that is required to program an application to perform a procedure (i.e. a function / collection of instructions).\n\nThese structures are based on rules, clearly and explicitly defined, and they are to be followed by the programmer to interface with whichever computer application (e.g. interpreters, daemons etc.) uses or expects them. If a script (i.e. a file containing series of tasks to be performed) is not correctly structured (i.e. wrong syntax), the computer program will not be able to parse it. Parsing roughly can be understood as going over an input with the end goal of understanding what is meant.\n\nDockerfiles use simple, clean, and clear syntax which makes them strikingly easy to create and use. They are designed to be self explanatory, especially because they allow commenting just like a good and properly written application source-code.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 960
        }
      },
      {
        "header": "Dockerfile Syntax Example",
        "content": "Dockerfile syntax consists of two kind of main line blocks: comments and commands + arguments.\n\nA Simple Example:",
        "code_examples": [
          "# Line blocks used for commenting\ncommand argument argument ..",
          "# Print \"Hello docker!\"\nRUN echo \"Hello docker!\""
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 113
        }
      },
      {
        "header": "ADD",
        "content": "The ADD command gets two arguments: a source and a destination. It basically copies the files from the source on the host into the container’s own filesystem at the set destination. If, however, the source is a URL (e.g. http://github.com/user/file/ ), then the contents of the URL are downloaded and placed at the destination.\n\nExample:",
        "code_examples": [
          "# Usage: ADD [source directory or URL] [destination directory]\nADD /my_app_folder /my_app_folder"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 337
        }
      },
      {
        "header": "CMD",
        "content": "The command CMD, similarly to RUN, can be used for executing a specific command. However, unlike RUN it is not executed during build, but when a container is instantiated using the image being built. Therefore, it should be considered as an initial, default command that gets executed (i.e. run) with the creation of containers based on the image.\n\nTo clarify: an example for CMD would be running an application upon creation of a container which is already installed using RUN (e.g. RUN apt-get install …) inside the image. This default application execution command that is set with CMD becomes the default and replaces any command which is passed during the creation.\n\nExample:",
        "code_examples": [
          "# Usage 1: CMD application \"argument\", \"argument\", ..\nCMD \"echo\" \"Hello docker!\""
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 680
        }
      },
      {
        "header": "ENTRYPOINT",
        "content": "ENTRYPOINT argument sets the concrete default application that is used every time a container is created using the image. For example, if you have installed a specific application inside an image and you will use this image to only run that application, you can state it with ENTRYPOINT and whenever a container is created from that image, your application will be the target.\n\nIf you couple ENTRYPOINT with CMD, you can remove “application” from CMD and just leave “arguments” which will be passed to the ENTRYPOINT.\n\nExample:",
        "code_examples": [
          "# Usage: ENTRYPOINT application \"argument\", \"argument\", ..\n# Remember: arguments are optional. They can be provided by CMD\n#           or during the creation of a container.\nENTRYPOINT echo\n\n# Usage example with CMD:\n# Arguments set with CMD can be overridden during *run*\nCMD \"Hello docker!\"\nENTRYPOINT echo"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 527
        }
      },
      {
        "header": "ENV",
        "content": "The ENV command is used to set the environment variables (one or more). These variables consist of “key value” pairs which can be accessed within the container by scripts and applications alike. This functionality of Docker offers an enormous amount of flexibility for running programs.\n\nExample:",
        "code_examples": [
          "# Usage: ENV key value\nENV SERVER_WORKS 4"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 296
        }
      },
      {
        "header": "EXPOSE",
        "content": "The EXPOSE command is used to associate a specified port to enable networking between the running process inside the container and the outside world (i.e. the host).\n\nExample:\n\nTo learn about Docker networking, check out the Docker container networking documentation .",
        "code_examples": [
          "# Usage: EXPOSE [port]\nEXPOSE 8080"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 268
        }
      },
      {
        "header": "FROM",
        "content": "FROM directive is probably the most crucial amongst all others for Dockerfiles. It defines the base image to use to start the build process. It can be any image, including the ones you have created previously. If a FROM image is not found on the host, Docker will try to find it (and download) from the Docker Hub or other container repository. It needs to be the first command declared inside a Dockerfile.\n\nExample:",
        "code_examples": [
          "# Usage: FROM [image name]\nFROM ubuntu"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 417
        }
      },
      {
        "header": "MAINTAINER",
        "content": "One of the commands that can be set anywhere in the file - although it would be better if it was declared on top - is MAINTAINER. This non-executing command declares the author, hence setting the author field of the images. It should come nonetheless after FROM.\n\nExample:",
        "code_examples": [
          "# Usage: MAINTAINER [name]\nMAINTAINER authors_name"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 272
        }
      },
      {
        "header": "RUN",
        "content": "The RUN command is the central executing directive for Dockerfiles. It takes a command as its argument and runs it to form the image. Unlike CMD, it actually is used to build the image (forming another layer on top of the previous one which is committed).\n\nExample:",
        "code_examples": [
          "# Usage: RUN [command]\nRUN aptitude install -y riak"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 265
        }
      },
      {
        "header": "USER",
        "content": "The USER directive is used to set the UID (or username) which is to run the container based on the image being built.\n\nExample:",
        "code_examples": [
          "# Usage: USER [UID]\nUSER 751"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 127
        }
      },
      {
        "header": "VOLUME",
        "content": "The VOLUME command is used to enable access from your container to a directory on the host machine (i.e. mounting it).\n\nExample:",
        "code_examples": [
          "# Usage: VOLUME [\"/dir_1\", \"/dir_2\" ..]\nVOLUME [\"/my_files\"]"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 128
        }
      },
      {
        "header": "WORKDIR",
        "content": "The WORKDIR directive is used to set where the command defined with CMD is to be executed.\n\nExample:",
        "code_examples": [
          "# Usage: WORKDIR /path\nWORKDIR ~/"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 100
        }
      },
      {
        "header": "Creating the Empty Dockerfile",
        "content": "Using the nano text editor, let’s start editing our Dockerfile.",
        "code_examples": [
          "nano Dockerfile"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 63
        }
      },
      {
        "header": "Defining Our File and Its Purpose",
        "content": "Albeit optional, it is always a good practice to let yourself and everybody figure out (when necessary) what this file is and what it is intended to do. For this, we will begin our Dockerfile with fancy comments (#) to describe it.",
        "code_examples": [
          "############################################################\n# Dockerfile to build MongoDB container images\n# Based on Ubuntu\n############################################################"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 231
        }
      },
      {
        "header": "Saving The Dockerfile",
        "content": "After you have appended everything to the file, it is time to save and exit. Press CTRL+X and then Y to confirm and save the Dockerfile.\n\nThis is what the final file should look like:",
        "code_examples": [
          "############################################################\n# Dockerfile to build MongoDB container images\n# Based on Ubuntu\n############################################################\n\n# Set the base image to Ubuntu\nFROM ubuntu\n\n# File Author / Maintainer\nMAINTAINER Example McAuthor\n\n################## BEGIN INSTALLATION ######################\n# Install MongoDB Following the Instructions at MongoDB Docs\n# Ref: http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/\n\n# Add the package verification key\nRUN apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10\n\n# Add MongoDB to the repository sources list\nRUN echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | tee /etc/apt/sources.list.d/mongodb.list\n\n# Update the repository sources list\nRUN apt-get update\n\n# Install MongoDB package (.deb)\nRUN apt-get install -y mongodb-10gen\n\n# Create the default data directory\nRUN mkdir -p /data/db\n\n##################### INSTALLATION END #####################\n\n# Expose the default port\nEXPOSE 27017\n\n# Default port to execute the entrypoint (MongoDB)\nCMD [\"--port 27017\"]\n\n# Set default container command\nENTRYPOINT usr/bin/mongod"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 183
        }
      },
      {
        "header": "Building Our First Image",
        "content": "Using the explanations from before, we are ready to create our first MongoDB image with docker!\n\nNote: The -t [name] flag here is used to tag the image. To learn more about what else you can do during build, run docker build --help .",
        "code_examples": [
          "docker build -t my_mongodb ."
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 233
        }
      },
      {
        "header": "Running A MongoDB Instance",
        "content": "Using the image we have build, we can now proceed to the final step: creating a container running a MongoDB instance inside, using a name of our choice (if desired with -name [name] ).\n\nNote: If a name is not set, we will need to deal with complex, alphanumeric IDs which can be obtained by listing all the containers using docker ps -l .\n\nNote: To detach yourself from the container, use the escape sequence CTRL+P followed by CTRL+Q .\n\nEnjoy!\n\n<div class=“author”>Submitted by: <a href=“ https://twitter.com/ostezer ”>O.S. Tezer</a></div>",
        "code_examples": [
          "docker run -name my_first_mdb_instance -i -t my_mongodb"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 540
        }
      },
      {
        "header": "Still looking for an answer?",
        "content": "Ask a question Search for more help",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 35
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "The developer cloud",
        "content": "Scale up as you grow — whether you're running one virtual machine or ten thousand.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 82
        }
      },
      {
        "header": "Get started for free",
        "content": "Sign up and get $200 in credit for your first 60 days with DigitalOcean.*",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 73
        }
      },
      {
        "header": "Company",
        "content": "About Leadership Blog Careers Customers Partners Referral Program Affiliate Program Press Legal Privacy Policy Security Investor Relations",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 138
        }
      },
      {
        "header": "Products",
        "content": "Overview Droplets Kubernetes Functions App Platform Gradient™ AI GPU Droplets Gradient™ AI Bare Metal GPUs Gradient™ AI 1-Click Models Gradient™ AI Platform Load Balancers Managed Databases Spaces Block Storage Network File Storage API Uptime Identity and Access Management Cloudways",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 283
        }
      },
      {
        "header": "Resources",
        "content": "Community Tutorials Community Q&A CSS-Tricks Write for DOnations Currents Research DigitalOcean Startups Wavemakers Program Compass Council Open Source Newsletter Signup Marketplace Pricing Pricing Calculator Documentation Release Notes Code of Conduct Shop Swag",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 262
        }
      },
      {
        "header": "Solutions",
        "content": "Website Hosting VPS Hosting Web & Mobile Apps Game Development Streaming VPN SaaS Platforms Cloud Hosting for Blockchain Startup Resources Migration Assistance",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 159
        }
      },
      {
        "header": "Contact",
        "content": "Support Sales Report Abuse System Status Share your ideas",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 57
        }
      }
    ],
    "url": "https://www.digitalocean.com/community/tutorials/docker-explained-using-dockerfiles-to-automate-building-of-images",
    "doc_type": "docker",
    "total_sections": 39
  },
  {
    "title": "The Docker Ecosystem: An Introduction to Common Components",
    "summary": "Featured Products Compute",
    "sections": [
      {
        "header": "Table of contents",
        "content": "Docker and Containerization Service Discovery and Global Configuration Stores Networking Tools Scheduling Cluster Management and Orchestration Conclusion",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 153
        }
      },
      {
        "header": "Introduction",
        "content": "Containerization is the process of distributing and deploying applications in a portable and predictable way. It accomplishes this by packaging components and their dependencies into standardized, isolated, lightweight process environments called containers. Many organizations are now interested in designing applications and services that can be easily deployed to distributed systems, allowing the system to scale easily and survive machine and application failures. Docker, a containerization platform developed to simplify and standardize deployment in various environments, was largely instrumental in spurring the adoption of this style of service design and management. A large amount of software has been created to build on this ecosystem of distributed container management.\n\nDocker and Containerization Docker is the most common containerization software in use today. While other containerizing systems exist, Docker makes container creation and management simple and integrates with many open source projects. In this image, you can begin to see (in a simplified view) how containers relate to the host system. Containers isolate individual applications and use operating system resources that have been abstracted by Docker. In the exploded view on the right, we can see that containers can be built by “layering”, with multiple containers sharing underlying layers, decreasing resource usage. Docker’s main advantages are: Lightweight resource utilization : instead of virtualizing an entire operating system, containers isolate at the process level and use the host’s kernel. Portability : all of the dependencies for a containerized application are bundled inside of the container, allowing it to run on any Docker host. Predictability : The host does not care about what is running inside of the container and the container does not care about which host it is running on. The interfaces are standardized and the interactions are predictable. Typically, when designing an application or service to use Docker, it works best to break out functionality into individual containers, a design decision known as service-oriented architecture. This gives you the ability to easily scale or update components independently in the future. Having this flexibility is one of the many reasons that people are interested in Docker for development and deployment. To find out more about containerizing applications with Docker, click here .\n\nService Discovery and Global Configuration Stores Service discovery is one component of an overall strategy aimed at making container deployments scalable and flexible. Service discovery is used so that containers can find out about the environment they have been introduced to without administrator intervention. They can find connection information for the components they must interact with, and they can register themselves so that other tools know that they are available. These tools also typically function as globally distributed configuration stores where arbitrary config settings can be set for the services operating in your infrastructure. In the above image, you can see an example flow in which one application registers its connection information with the discovery service system. Once registered, other applications can query the discovery service to find out how to connect to the application. These tools are often implemented as simple key-value stores that are distributed among the hosts in a clustered environment. Generally, the key-value stores provide an HTTP API for accessing and setting values. Some include additional security measures like encrypted entries or access control mechanisms. The distributed stores are essential for managing the clustered Docker hosts in addition to their primary function of providing self-configuration details for new containers. Some of the responsibilities of service discovery stores are: Allowing applications to obtain the data needed to connect with to the services they depend on. Allowing services to register their connection information for the above purpose. Providing a globally accessible location to store arbitrary configuration data. Storing information about cluster members as needed by any cluster management software. Some popular service discovery tools and related projects are: etcd : service discovery / globally distributed key-value store consul : service discovery / globally distributed key-value store zookeeper : service discovery / globally distributed key-value store crypt : project to encrypt etcd entries confd : watches key-value store for changes and triggers reconfiguration of services with new values To learn more about service discovery with Docker, visit our guide here .\n\nNetworking Tools Containerized applications lend themselves to a service-oriented design that encourages breaking out functionality into discrete components. While this makes management and scaling easier, it requires even more assurance regarding the functionality and reliability of networking between the components. Docker itself provides the basic networking structures necessary for container-to-container and container-to-host communication. Docker’s native networking capabilities provide two mechanisms for hooking containers together. The first is to expose a container’s ports and optionally map to the host system for external routing. You can select the host port to map to or allow Docker to randomly choose a high, unused port. This is a generic way of providing access to a container that works well for most purposes. The other method is to allow containers to communicate by using Docker “links”. A linked container will get connection information about its counterpart, allowing it to automatically connect if it is configured to pay attention to those variables. This allows contact between containers on the same host without having to know beforehand the port or address where the service will be located. This basic level of networking is suitable for single-host or closely managed environments. However, the Docker ecosystem has produce a variety of projects that focus on expanding the networking functionality available to operators and developers. Some additional networking capabilities available through additional tools include: Overlay networking to simplify and unify the address space across multiple hosts. Virtual private networks adapted to provide secure communication between various components. Assigning per-host or per-application subnetting Establishing macvlan interfaces for communication Configuring custom MAC addresses, gateways, etc. for your containers Some projects that are involved with improving Docker networking are: flannel : Overlay network providing each host with a separate subnet. weave : Overlay network portraying all containers on a single network. pipework : Advanced networking toolkit for arbitrarily advanced networking configurations. For a more in-depth look at the different approaches to networking with Docker, click here .\n\nScheduling, Cluster Management, and Orchestration Another component needed when building a clustered container environment is a scheduler. Schedulers are responsible for starting containers on the available hosts. The image above demonstrates a simplified scheduling decision. The request is given through an API or management tool. From here, the scheduler evaluates the conditions of the request and the state of the available hosts. In this example, it pulls information about container density from a distributed data store / discovery service (as discussed above) so that it can place the new application on the least busy host. This host selection process is one of the core responsibilities of the scheduler. Usually, it has functions that automate this process with the administrator having the option to specify certain constraints. Some of these constraints may be: Schedule the container on the same host as another given container. Make sure that the container is not placed on the same host as another given container. Place the container on a host with a matching label or metadata. Place the container on the least busy host. Run the container on every host in the cluster. The scheduler is responsible for loading containers onto relevant hosts and starting, stopping, and managing the life cycle of the process. Because the scheduler must interact with each host in the group, cluster management functions are also typically included. These allow the scheduler to get information about the members and perform administration tasks. Orchestration in this context generally refers to the combination of container scheduling and managing hosts. Some popular projects that function as schedulers and fleet management tools are: fleet : scheduler and cluster management tool. marathon : scheduler and service management tool. Swarm : scheduler and service management tool. mesos : host abstraction service that consolidates host resources for the scheduler. kubernetes : advanced scheduler capable of managing container groups. compose : container orchestration tool for creating container groups. To find out more about basic scheduling, container grouping, and cluster management software for Docker, click here .\n\nConclusion By now, you should be familiar with the general function of most of the software associated with the Docker ecosystem. Docker itself, along with all of the supporting projects, provide a software management, design, and deployment strategy that enables massive scalability. By understanding and leveraging the capabilities of various projects, you can execute complex application deployments that are flexible enough to account for variable operating requirements.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 9731
        }
      },
      {
        "header": "Tutorial Series: The Docker Ecosystem",
        "content": "The Docker project has given many developers and administrators an easy platform with which to build and deploy scalable applications. In this series, we will be exploring how Docker and the components designed to integrate with it provide the tools needed to easily deliver highly available, distributed applications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 318
        }
      },
      {
        "header": "Still looking for an answer?",
        "content": "Ask a question Search for more help",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 35
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "Deploy on DigitalOcean",
        "content": "Click below to sign up for DigitalOcean's virtual machines, Databases, and AIML products.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Popular Topics",
        "content": "AI/ML Ubuntu Linux Basics JavaScript Python MySQL Docker Kubernetes All tutorials Talk to an expert",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 99
        }
      },
      {
        "header": "Featured tutorials",
        "content": "SOLID Design Principles Explained: Building Better Software Architecture How To Remove Docker Images, Containers, and Volumes How to Create a MySQL User and Grant Privileges (Step-by-Step)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 188
        }
      },
      {
        "header": "The developer cloud",
        "content": "Scale up as you grow — whether you're running one virtual machine or ten thousand.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 82
        }
      },
      {
        "header": "Get started for free",
        "content": "Sign up and get $200 in credit for your first 60 days with DigitalOcean.*",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 73
        }
      },
      {
        "header": "Company",
        "content": "About Leadership Blog Careers Customers Partners Referral Program Affiliate Program Press Legal Privacy Policy Security Investor Relations",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 138
        }
      },
      {
        "header": "Products",
        "content": "Overview Droplets Kubernetes Functions App Platform Gradient™ AI GPU Droplets Gradient™ AI Bare Metal GPUs Gradient™ AI 1-Click Models Gradient™ AI Platform Load Balancers Managed Databases Spaces Block Storage Network File Storage API Uptime Identity and Access Management Cloudways",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 283
        }
      },
      {
        "header": "Resources",
        "content": "Community Tutorials Community Q&A CSS-Tricks Write for DOnations Currents Research DigitalOcean Startups Wavemakers Program Compass Council Open Source Newsletter Signup Marketplace Pricing Pricing Calculator Documentation Release Notes Code of Conduct Shop Swag",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 262
        }
      },
      {
        "header": "Solutions",
        "content": "Website Hosting VPS Hosting Web & Mobile Apps Game Development Streaming VPN SaaS Platforms Cloud Hosting for Blockchain Startup Resources Migration Assistance",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 159
        }
      },
      {
        "header": "Contact",
        "content": "Support Sales Report Abuse System Status Share your ideas",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 57
        }
      }
    ],
    "url": "https://www.digitalocean.com/community/tutorials/the-docker-ecosystem-an-introduction-to-common-components",
    "doc_type": "docker",
    "total_sections": 17
  },
  {
    "title": "Introduction to Containers and Docker",
    "summary": "Note Access to this page requires authorization. You can try signing in or changing directories .",
    "sections": [
      {
        "header": "Share via",
        "content": "Facebook\n\nx.com\n\nLinkedIn\n\nEmail\n\nPrint",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 39
        }
      },
      {
        "header": "Feedback",
        "content": "Was this page helpful? Yes No No Need help with this topic? Want to try using Ask Learn to clarify or guide you through this topic? Ask Learn Ask Learn Suggest a fix?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 166
        }
      }
    ],
    "url": "https://learn.microsoft.com/en-us/dotnet/architecture/microservices/container-docker-introduction/",
    "doc_type": "docker",
    "total_sections": 2
  },
  {
    "title": "Docker containers, images, and registries",
    "summary": "Note Access to this page requires authorization. You can try signing in or changing directories .",
    "sections": [
      {
        "header": "Share via",
        "content": "Facebook\n\nx.com\n\nLinkedIn\n\nEmail\n\nPrint",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 39
        }
      },
      {
        "header": "Feedback",
        "content": "Was this page helpful? Yes No No Need help with this topic? Want to try using Ask Learn to clarify or guide you through this topic? Ask Learn Ask Learn Suggest a fix?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 166
        }
      }
    ],
    "url": "https://learn.microsoft.com/en-us/dotnet/architecture/microservices/container-docker-introduction/docker-containers-images-registries",
    "doc_type": "docker",
    "total_sections": 2
  },
  {
    "title": "What is Azure Container Instances?",
    "summary": "Note Access to this page requires authorization. You can try signing in or changing directories .",
    "sections": [
      {
        "header": "Share via",
        "content": "Facebook\n\nx.com\n\nLinkedIn\n\nEmail\n\nPrint",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 39
        }
      },
      {
        "header": "Fast startup times",
        "content": "Containers offer significant startup benefits over virtual machines (VMs). Azure Container Instances can start containers in Azure in seconds, without the need to provision and manage VMs.\n\nBring Linux or Windows container images from Docker Hub, a private Azure container registry , or another cloud-based Docker registry. Visit the FAQ to learn which registries ACI supports. Azure Container Instances caches several common base OS images, helping speed deployment of your custom application images.\n\nFor even faster startup times, ACI supports standby pools .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 562
        }
      },
      {
        "header": "Container access",
        "content": "Azure Container Instances enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN). When you create a container instance, you can specify a custom DNS name label so your application is reachable at customlabel . azureregion .azurecontainer.io.\n\nAzure Container Instances also supports executing a command in a running container by providing an interactive shell to help with application development and troubleshooting. Access takes places over HTTPS, using TLS to secure client connections.\n\nImportant Azure Container Instances requires all secure connections from servers and applications to use TLS 1.2. Support for TLS 1.0 and 1.1 has been retired.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 719
        }
      },
      {
        "header": "Compliant deployments",
        "content": "Hypervisor-level security\n\nHistorically, containers offered application dependency isolation and resource governance but were insufficiently hardened for hostile multitenant usage. Azure Container Instances guarantees your application is as isolated in a container as it would be in a VM.\n\nCustomer data\n\nThe Azure Container Instances service doesn't store customer data. It does, however, store the subscription IDs of the Azure subscription used to create resources. Storing subscription IDs is required to ensure your container groups continue running as expected.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 567
        }
      },
      {
        "header": "Hypervisor-level security",
        "content": "Historically, containers offered application dependency isolation and resource governance but were insufficiently hardened for hostile multitenant usage. Azure Container Instances guarantees your application is as isolated in a container as it would be in a VM.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 261
        }
      },
      {
        "header": "Customer data",
        "content": "The Azure Container Instances service doesn't store customer data. It does, however, store the subscription IDs of the Azure subscription used to create resources. Storing subscription IDs is required to ensure your container groups continue running as expected.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 262
        }
      },
      {
        "header": "Custom sizes",
        "content": "Containers are typically optimized to run just a single application, but the exact needs of those applications can differ greatly. Azure Container Instances provides optimum utilization by allowing exact specifications of CPU cores and memory. You pay based on what you need and get billed by the second, so you can fine-tune your spending based on actual need.\n\nFor compute-intensive jobs such as machine learning, Azure Container Instances can schedule Linux containers to use NVIDIA Tesla GPU resources (preview).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 516
        }
      },
      {
        "header": "Persistent storage",
        "content": "To retrieve and persist state with Azure Container Instances, we offer direct mounting of Azure Files shares backed by Azure Storage.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 133
        }
      },
      {
        "header": "Linux and Windows containers",
        "content": "Azure Container Instances can schedule both Windows and Linux containers with the same API. You can specify your OS type preference when you create your container groups .\n\nSome features are currently restricted to Linux containers:\n\nMultiple containers per container group Volume mounting ( Azure Files , emptyDir , GitRepo , secret ) Resource usage metrics with Azure Monitor GPU resources (preview)\n\nFor Windows container deployments, use images based on common Windows base images .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 486
        }
      },
      {
        "header": "Run multiple containers in a single container group",
        "content": "Azure Container Instances supports scheduling of multiple containers within a single container group that share the same container host, local network, storage, and lifecycle. This enables you to combine your main application container with other supporting role containers, such as logging sidecars.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 300
        }
      },
      {
        "header": "Virtual network deployment",
        "content": "Important If you deploy your container group into a virtual network, you must use a NAT gateway for outbound connectivity. This is the only supported configuration for outbound connectivity from your container group in a virtual network. See Configure a NAT gateway for static IP address for outbound traffic from a container group for more information on how to configure this.\n\nAzure Container Instances enables deployment of container instances into an Azure virtual network . When deployed into a subnet within your virtual network, container instances can communicate securely with other resources in the virtual network, including those that are on premises (through VPN gateway or ExpressRoute ).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 703
        }
      },
      {
        "header": "Availability zones support",
        "content": "Azure Container Instances supports zonal container group deployments , meaning the instance is pinned to a specific, self-selected availability zone. The availability zone can be specified per container group.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 209
        }
      },
      {
        "header": "Managed identity",
        "content": "Azure Container Instances supports using managed identity with your container group , which enables your container group to authenticate to any service that supports Microsoft Entra authentication without managing credentials in your container code.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 249
        }
      },
      {
        "header": "Managed identity authenticated image pull",
        "content": "Azure Container Instances can authenticate with an Azure Container Registry (ACR) instance using a managed identity , allowing you to pull the image without having to include a username and password directly in your container group definition.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 243
        }
      },
      {
        "header": "Confidential container deployment",
        "content": "Confidential containers on ACI enable you to run containers in a trusted execution environment (TEE) that provides hardware-based confidentiality and integrity protections for your container workloads. Confidential containers on ACI can protect data-in-use and encrypts data being processed in memory. Confidential containers on ACI are supported as a SKU that you can select when deploying your workload. For more information, see confidential container groups .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 463
        }
      },
      {
        "header": "Spot container deployment",
        "content": "ACI Spot containers allow customers to run interruptible, containerized workloads on unused Azure capacity at discounted prices of up to 70% compared to regular-priority ACI containers. ACI spot containers may be preempted when Azure encounters a shortage of surplus capacity, and they're suitable for workloads without strict availability requirements. Customers are billed for per-second memory and core usage. To utilize ACI Spot containers, you can deploy your workload with a specific property flag indicating that you want to use Spot container groups and take advantage of the discounted pricing model. For more information, see Spot container groups .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 659
        }
      },
      {
        "header": "NGroups",
        "content": "NGroups provides advanced capabilities for managing multiple related container groups. NGroups provides support for maintaining a specified number of container groups, performing rolling upgrades, deploying across multiple availability zones, using load balancers for ingress, and deploying confidential containers. For more information, see About NGroups .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 357
        }
      },
      {
        "header": "Virtual nodes on Azure Container Instances",
        "content": "Virtual nodes on Azure Container Instances allow you to deploy pods in your Azure Kubernetes Service (AKS) cluster that run as container groups in ACI. This allows you to orchestrate your container groups using familiar Kubernetes constructs. Since virtual nodes are backed by ACI's serverless infrastructure, you can quickly scale up your workload without needing to wait for the Kubernetes cluster autoscaler to deploy VM compute nodes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 438
        }
      },
      {
        "header": "Considerations",
        "content": "Userâs credentials passed via command line interface (CLI) are stored as plain text in the backend. Storing credentials in plain text is a security risk; Microsoft advises customers to store user credentials in CLI environment variables to ensure they're encrypted/transformed when stored in the backend.\n\nIf your container group stops working, we suggest trying to restart your container, checking your application code, or your local network configuration before opening a support request .\n\nContainer Images can't be larger than 15 GB, any images above this size may cause unexpected behavior: How large can my container image be?\n\nSome Windows Server base images are no longer compatible with Azure Container Instances: What Windows base OS images are supported?\n\nIf a container group restarts, the container groupâs IP may change. We advise against using a hard coded IP address in your scenario. If you need a static public IP address, use Application Gateway: Static IP address for container group - Azure Container Instances | Microsoft Learn\n\nThere are ports that are reserved for service functionality. We advise you not to use these ports because using them leads to unexpected behavior: Does the ACI service reserve ports for service functionality?\n\nIf youâre having trouble deploying or running your container, first check the Troubleshooting Guide for common mistakes and issues\n\nYour container groups may restart due to platform maintenance events. These maintenance events are done to ensure the continuous improvement of the underlying infrastructure: Container had an isolated restart without explicit user input\n\nACI doesn't allow privileged container operations . We advise you to not depend on using the root directory for your scenario.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1764
        }
      },
      {
        "header": "Next steps",
        "content": "Try deploying a container to Azure with a single command using our quickstart guide:\n\nAzure Container Instances Quickstart\n\nLINKS - External",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 140
        }
      },
      {
        "header": "Feedback",
        "content": "Was this page helpful? Yes No No Need help with this topic? Want to try using Ask Learn to clarify or guide you through this topic? Ask Learn Ask Learn Suggest a fix?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 166
        }
      }
    ],
    "url": "https://learn.microsoft.com/en-us/azure/container-instances/container-instances-overview",
    "doc_type": "docker",
    "total_sections": 21
  },
  {
    "title": "Quickstart: Deploy a container instance in Azure using the Azure CLI",
    "summary": "Note Access to this page requires authorization. You can try signing in or changing directories .",
    "sections": [
      {
        "header": "Share via",
        "content": "Facebook\n\nx.com\n\nLinkedIn\n\nEmail\n\nPrint",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 39
        }
      },
      {
        "header": "Prerequisites",
        "content": "Use the Bash environment in Azure Cloud Shell . For more information, see Get started with Azure Cloud Shell . If you prefer to run CLI reference commands locally, install the Azure CLI. If you're running on Windows or macOS, consider running Azure CLI in a Docker container. For more information, see How to run the Azure CLI in a Docker container . If you're using a local installation, sign in to the Azure CLI by using the az login command. To finish the authentication process, follow the steps displayed in your terminal. For other sign-in options, see Authenticate to Azure using Azure CLI . When you're prompted, install the Azure CLI extension on first use. For more information about extensions, see Use and manage extensions with the Azure CLI . Run az version to find the version and dependent libraries that are installed. To upgrade to the latest version, run az upgrade . If you're using a local installation, sign in to the Azure CLI by using the az login command. To finish the authentication process, follow the steps displayed in your terminal. For other sign-in options, see Authenticate to Azure using Azure CLI . When you're prompted, install the Azure CLI extension on first use. For more information about extensions, see Use and manage extensions with the Azure CLI . Run az version to find the version and dependent libraries that are installed. To upgrade to the latest version, run az upgrade .\n\nThis quickstart requires version 2.0.55 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed. Warning Best practice: Userâs credentials passed via command line interface (CLI) are stored as plain text in the backend. Storing credentials in plain text is a security risk; Microsoft advises customers to store user credentials in CLI environment variables to ensure they are encrypted/transformed when stored in the backend.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1890
        }
      },
      {
        "header": "Create a resource group",
        "content": "Azure container instances, like all Azure resources, must be deployed into a resource group. Resource groups allow you to organize and manage related Azure resources.\n\nFirst, create a resource group named myResourceGroup in the eastus location with the az group create command:",
        "code_examples": [
          "az group create --name myResourceGroup --location eastus"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 277
        }
      },
      {
        "header": "Create a container",
        "content": "Now that you have a resource group, you can run a container in Azure. To create a container instance with the Azure CLI, provide a resource group name, container instance name, and Docker container image to the az container create command. In this quickstart, you use the public mcr.microsoft.com/azuredocs/aci-helloworld image. This image packages a small web app written in Node.js that serves a static HTML page.\n\nYou can expose your containers to the internet by specifying one or more ports to open, a DNS name label, or both. In this quickstart, you deploy a container with a DNS name label so that the web app is publicly reachable.\n\nExecute a command similar to the following to start a container instance. Set a --dns-name-label value that's unique within the Azure region where you create the instance. If you receive a \"DNS name label not available\" error message, try a different DNS name label.\n\nTo deploy the container into a specific availability zone , use the --zone argument and specify the logical zone number:\n\nImportant Zonal deployments are only available in regions that support availability zones. To see if your region supports availability zones, see Azure Regions List .\n\nWithin a few seconds, you should get a response from the Azure CLI indicating the deployment completed. Check its status with the az container show command:\n\nWhen you run the command, the container's fully qualified domain name (FQDN) and its provisioning state are displayed.\n\nIf the container's ProvisioningState is Succeeded , go to its FQDN in your browser. If you see a web page similar to the following, congratulations! You successfully deployed an application running in a Docker container to Azure.\n\nIf at first the application isn't displayed, you might need to wait a few seconds while DNS propagates, then try refreshing your browser.",
        "code_examples": [
          "az container create --resource-group myResourceGroup --name mycontainer --image mcr.microsoft.com/azuredocs/aci-helloworld --dns-name-label aci-demo --ports 80 --os-type linux --memory 1.5 --cpu 1",
          "az container create --resource-group myResourceGroup --name mycontainer --image mcr.microsoft.com/azuredocs/aci-helloworld --dns-name-label aci-demo --ports 80 --os-type linux --memory 1.5 --cpu 1 --zone 1",
          "az container show --resource-group myResourceGroup --name mycontainer --query \"{FQDN:ipAddress.fqdn,ProvisioningState:provisioningState}\" --out table",
          "FQDN                               ProvisioningState\n---------------------------------  -------------------\naci-demo.eastus.azurecontainer.io  Succeeded"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1845
        }
      },
      {
        "header": "Pull the container logs",
        "content": "When you need to troubleshoot a container or the application it runs (or just see its output), start by viewing the container instance's logs.\n\nPull the container instance logs with the az container logs command:\n\nThe output displays the logs for the container, and should show the HTTP GET requests generated when you viewed the application in your browser.",
        "code_examples": [
          "az container logs --resource-group myResourceGroup --name mycontainer",
          "listening on port 80\n::ffff:10.240.255.55 - - [21/Mar/2019:17:43:53 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"\n::ffff:10.240.255.55 - - [21/Mar/2019:17:44:36 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"\n::ffff:10.240.255.55 - - [21/Mar/2019:17:44:36 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\""
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 358
        }
      },
      {
        "header": "Attach output streams",
        "content": "In addition to viewing the logs, you can attach your local standard out and standard error streams to that of the container.\n\nFirst, execute the az container attach command to attach your local console to the container's output streams:\n\nOnce attached, refresh your browser a few times to generate some more output. When you're done, detach your console with Control+C . You should see output similar to the following sample:",
        "code_examples": [
          "az container attach --resource-group myResourceGroup --name mycontainer",
          "Container 'mycontainer' is in state 'Running'...\n(count: 1) (last timestamp: 2019-03-21 17:27:20+00:00) pulling image \"mcr.microsoft.com/azuredocs/aci-helloworld\"\n(count: 1) (last timestamp: 2019-03-21 17:27:24+00:00) Successfully pulled image \"mcr.microsoft.com/azuredocs/aci-helloworld\"\n(count: 1) (last timestamp: 2019-03-21 17:27:27+00:00) Created container\n(count: 1) (last timestamp: 2019-03-21 17:27:27+00:00) Started container\n\nStart streaming logs:\nlistening on port 80\n\n::ffff:10.240.255.55 - - [21/Mar/2019:17:43:53 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"\n::ffff:10.240.255.55 - - [21/Mar/2019:17:44:36 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"\n::ffff:10.240.255.55 - - [21/Mar/2019:17:44:36 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"\n::ffff:10.240.255.55 - - [21/Mar/2019:17:47:01 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"\n::ffff:10.240.255.56 - - [21/Mar/2019:17:47:12 +0000] \"GET / HTTP/1.1\" 304 - \"-\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\""
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 425
        }
      },
      {
        "header": "Clean up resources",
        "content": "When you're done with the container, remove it using the az container delete command:\n\nTo verify that the container deleted, execute the az container list command:\n\nThe mycontainer container shouldn't appear in the command's output. If you have no other containers in the resource group, no output is displayed.\n\nIf you're done with the myResourceGroup resource group and all the resources it contains, delete it with the az group delete command:",
        "code_examples": [
          "az container delete --resource-group myResourceGroup --name mycontainer",
          "az container list --resource-group myResourceGroup --output table",
          "az group delete --name myResourceGroup"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 446
        }
      },
      {
        "header": "Next steps",
        "content": "In this quickstart, you created an Azure container instance by using a public Microsoft image. If you'd like to build a container image and deploy it from a private Azure container registry, continue to the Azure Container Instances tutorial.\n\nAzure Container Instances tutorial\n\nTo try out options for running containers in an orchestration system on Azure, see the Azure Kubernetes Service (AKS) quickstarts.\n\nIMAGES\n\nLINKS - External\n\nLINKS - Internal",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 454
        }
      },
      {
        "header": "Feedback",
        "content": "Was this page helpful? Yes No No Need help with this topic? Want to try using Ask Learn to clarify or guide you through this topic? Ask Learn Ask Learn Suggest a fix?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 166
        }
      }
    ],
    "url": "https://learn.microsoft.com/en-us/azure/container-instances/container-instances-quickstart",
    "doc_type": "docker",
    "total_sections": 9
  },
  {
    "title": "Introduction to Azure Container Registry",
    "summary": "Note Access to this page requires authorization. You can try signing in or changing directories .",
    "sections": [
      {
        "header": "Share via",
        "content": "Facebook\n\nx.com\n\nLinkedIn\n\nEmail\n\nPrint",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 39
        }
      },
      {
        "header": "Use cases",
        "content": "Pull images from an Azure container registry to various deployment targets:\n\nScalable orchestration systems that manage containerized applications across clusters of hosts, including Kubernetes , DC/OS , and Docker Swarm . Azure services that support building and running applications at scale, such as Azure Kubernetes Service (AKS) , App Service , Batch , and Service Fabric .\n\nDevelopers can also push to a container registry as part of a container development workflow. For example, you can target a container registry from a continuous integration and continuous delivery (CI/CD) tool such as Azure Pipelines or Jenkins .\n\nConfigure Azure Container Registry tasks to automatically rebuild application images when their base images are updated, or automate image builds when your team commits code to a Git repository. Create multi-step tasks to automate building, testing, and patching container images in parallel in the cloud.\n\nAzure provides tooling like the Azure CLI, the Azure portal, and API support to manage your container registries. Optionally, install the Docker extension and the Azure Account extension for Visual Studio Code. You can use these extensions to pull images from a container registry, push images to a container registry, or run Azure Container Registry tasks, all within Visual Studio Code.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1323
        }
      },
      {
        "header": "Key features",
        "content": "Registry service tiers : Create one or more container registries in your Azure subscription. Registries are available in three tiers: Basic, Standard, and Premium . Each tier supports webhook integration, registry authentication with Microsoft Entra ID, and delete functionality. Take advantage of local, network-close storage of your container images by creating a registry in the same Azure location as your deployments. Use the geo-replication feature of Premium registries for advanced replication and container image distribution. Security and access : You log in to a registry by using the Azure CLI or the standard docker login command. Azure Container Registry transfers container images over HTTPS, and it supports TLS to help secure client connections. Important As of January 13, 2020, Azure Container Registry requires all secure connections from servers and applications to use TLS 1.2. Enable TLS 1.2 by using any recent Docker client (version 18.03.0 or later). You control access to a container registry by using an Azure identity, a Microsoft Entra service principal , or a provided admin account. Use Azure role-based access control (RBAC) to assign specific registry permissions to users or systems. Security features of the Premium service tier include content trust for image tag signing, and firewalls and virtual networks (preview) to restrict access to the registry. Microsoft Defender for Cloud optionally integrates with Azure Container Registry to scan images whenever you push an image to a registry. Supported images and artifacts : When images are grouped in a repository, each image is a read-only snapshot of a Docker-compatible container. Azure container registries can include both Windows and Linux images. You control image names for all your container deployments. Use standard Docker commands to push images into a repository or pull an image from a repository. In addition to Docker container images, Azure Container Registry stores related content formats such as Helm charts and images built to the Open Container Initiative (OCI) Image Format Specification . Automated image builds : Use Azure Container Registry tasks to streamline building, testing, pushing, and deploying images in Azure. For example, use Azure Container Registry tasks to extend your development inner loop to the cloud by offloading docker build operations to Azure. Configure build tasks to automate your container OS and framework patching pipeline, and build images automatically when your team commits code to source control. Multi-step tasks provide step-based task definition and execution for building, testing, and patching container images in the cloud. Task steps define individual build and push operations for container images. They can also define the execution of one or more containers, in which each step uses a container as its execution environment.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 2881
        }
      },
      {
        "header": "Related content",
        "content": "Create a container registry by using the Azure portal Create a container registry by using the Azure CLI Automate container builds and maintenance by using Azure Container Registry tasks",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 186
        }
      },
      {
        "header": "Feedback",
        "content": "Was this page helpful? Yes No No Need help with this topic? Want to try using Ask Learn to clarify or guide you through this topic? Ask Learn Ask Learn Suggest a fix?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 166
        }
      }
    ],
    "url": "https://learn.microsoft.com/en-us/azure/container-registry/container-registry-intro",
    "doc_type": "docker",
    "total_sections": 5
  },
  {
    "title": "Application development resources Stay organized with collections Save and categorize content based on your preferences.",
    "summary": "The Architecture Center provides content resources across a wide variety of application development subjects. The documents that are listed in the \"Application development\" section of the left navigation can help you make decisions about your application development.",
    "sections": [
      {
        "header": "",
        "content": "Home Technology areas Cloud Architecture Center Send feedback Application development resources Stay organized with collections Save and categorize content based on your preferences. The Architecture Center provides content resources across a wide variety of application development subjects. The documents that are listed in the \"Application development\" section of the left navigation can help you make decisions about your application development. Send feedback",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 464
        }
      }
    ],
    "url": "https://docs.cloud.google.com/architecture/application-development",
    "doc_type": "docker",
    "total_sections": 1
  },
  {
    "title": "Use Docker to build Docker images",
    "summary": "You can use GitLab CI/CD with Docker to create Docker images. For example, you can create a Docker image of your application, test it, and push it to a container registry. To run Docker commands in your CI/CD jobs, you must configure GitLab Runner to support docker commands. This method requires privileged mode.",
    "sections": [
      {
        "header": "Enable Docker commands in your CI/CD jobs",
        "content": "To enable Docker commands for your CI/CD jobs, you can use:\n\nThe shell executor Docker-in-Docker Docker socket binding Docker pipe binding\n\nUse the shell executor\n\nTo include Docker commands in your CI/CD jobs, you can configure your runner to use the shell executor. In this configuration, the gitlab-runner user runs the Docker commands, but needs permission to do so.\n\nInstall GitLab Runner. Register a runner. Select the shell executor. For example: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor shell \\ --description \"My Runner\" On the server where GitLab Runner is installed, install Docker Engine. View a list of supported platforms . Add the gitlab-runner user to the docker group: sudo usermod -aG docker gitlab-runner Verify that gitlab-runner has access to Docker: sudo -u gitlab-runner -H docker info In GitLab, add docker info to .gitlab-ci.yml to verify that Docker is working: default : before_script : - docker info build_image : script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nYou can now use docker commands (and install Docker Compose if needed).\n\nWhen you add gitlab-runner to the docker group, you effectively grant gitlab-runner full root permissions. For more information, see security of the docker group .\n\nUse Docker-in-Docker\n\n“Docker-in-Docker” ( dind ) means:\n\nYour registered runner uses the Docker executor or the Kubernetes executor . The executor uses a container image of Docker , provided by Docker, to run your CI/CD jobs.\n\nThe Docker image includes all of the docker tools and can run the job script in context of the image in privileged mode.\n\nYou should use Docker-in-Docker with TLS enabled, which is supported by GitLab.com instance runners .\n\nYou should always pin a specific version of the image, like docker:24.0.5 . If you use a tag like docker:latest , you have no control over which version is used. This can cause incompatibility problems when new versions are released.\n\nUse the Docker executor with Docker-in-Docker\n\nYou can use the Docker executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in the Docker executor\n\nThe Docker daemon supports connections over TLS. TLS is the default in Docker 19.03.12 and later.\n\nThis task enables --docker-privileged , which effectively disables the container’s security mechanisms and exposes your host to privilege escalation. This action can cause container breakout. For more information, see runtime privilege and Linux capabilities .\n\nTo use Docker-in-Docker with TLS enabled:\n\nInstall GitLab Runner . Register GitLab Runner from the command line. Use docker and privileged mode: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor docker \\ --description \"My Docker Runner\" \\ --tag-list \"tls-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-privileged \\ --docker-volumes \"/certs/client\" This command registers a new runner to use the docker:24.0.5-cli image (if none is specified at the job level). To start the build and service containers, it uses the privileged mode. If you want to use Docker-in-Docker, you must always use privileged = true in your Docker containers. This command mounts /certs/client for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation . The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = true disable_cache = false volumes = [ \"/certs/client\" , \"/cache\" ] [ runners . cache ] [ runners . cache . s3 ] [ runners . cache . gcs ] This command registers a new runner to use the docker:24.0.5-cli image (if none is specified at the job level). To start the build and service containers, it uses the privileged mode. If you want to use Docker-in-Docker, you must always use privileged = true in your Docker containers. This command mounts /certs/client for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation . You can now use docker in the job script. You should include the docker:24.0.5-dind service: default : image : docker:24.0.5-cli services : - docker:24.0.5-dind before_script : - docker info variables : # When you use the dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. Docker 19.03 does this automatically # by setting the DOCKER_HOST in # https://github.com/docker-library/docker/blob/d45051476babc297257df490d22cbd806f1b11e4/19.03/docker-entrypoint.sh#L23-L29 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml DOCKER_TLS_CERTDIR : \"/certs\" build : stage : build tags : - tls-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nUse a Unix socket on a shared volume between Docker-in-Docker and build container\n\nDirectories defined in volumes = [\"/certs/client\", \"/cache\"] in the Docker-in-Docker with TLS enabled in the Docker executor approach are persistent between builds . If multiple CI/CD jobs using a Docker executor runner have Docker-in-Docker services enabled, then each job writes to the directory path. This approach might result in a conflict.\n\nTo address this conflict, use a Unix socket on a volume shared between the Docker-in-Docker service and the build container. This approach improves performance and establishes a secure connection between the service and client.\n\nThe following is a sample config.toml with temporary volume shared between build and service containers:\n\n[[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] image = \"docker:24.0.5-cli\" privileged = true volumes = [ \"/runner/services/docker\" ] # Temporary volume shared between build and service containers.\n\nThe Docker-in-Docker service creates a docker.sock . The Docker client connects to docker.sock through a Docker Unix socket volume.\n\njob : variables : # This variable is shared by both the DinD service and Docker client. # For the service, it will instruct DinD to create `docker.sock` here. # For the client, it tells the Docker client which Docker Unix socket to connect to. DOCKER_HOST : \"unix:///runner/services/docker/docker.sock\" services : - docker:24.0.5-dind image : docker:24.0.5-cli script : - docker version\n\nDocker-in-Docker with TLS disabled in the Docker executor\n\nSometimes there are legitimate reasons to disable TLS. For example, you have no control over the GitLab Runner configuration that you are using.\n\nRegister GitLab Runner from command line. Use docker and privileged mode: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor docker \\ --description \"My Docker Runner\" \\ --tag-list \"no-tls-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-privileged The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = true disable_cache = false volumes = [ \"/cache\" ] [ runners . cache ] [ runners . cache . s3 ] [ runners . cache . gcs ] Include the docker:24.0.5-dind service in the job script: default : image : docker:24.0.5-cli services : - docker:24.0.5-dind before_script : - docker info variables : # When using dind service, you must instruct docker to talk with the # daemon started inside of the service. The daemon is available with # a network connection instead of the default /var/run/docker.sock socket. # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#accessing-the-services # DOCKER_HOST : tcp://docker:2375 # # This instructs Docker not to start over TLS. DOCKER_TLS_CERTDIR : \"\" build : stage : build tags : - no - tls-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nDocker-in-Docker with proxy enabled in the Docker executor\n\nYou might need to configure proxy settings to use the docker push command.\n\nFor more information, see Proxy settings when using dind service .\n\nUse the Kubernetes executor with Docker-in-Docker\n\nYou can use the Kubernetes executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in Kubernetes\n\nTo use Docker-in-Docker with TLS enabled in Kubernetes:\n\nUsing the Helm chart , update the values.yml file to specify a volume mount. runners : tags : \"tls-dind-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = true [[runners.kubernetes.volumes.empty_dir]] name = \"docker-certs\" mount_path = \"/certs/client\" medium = \"Memory\" Include the docker:24.0.5-dind service in the job: default : image : docker:24.0.5-cli services : - name : docker:24.0.5-dind variables : HEALTHCHECK_TCP_PORT : \"2376\" before_script : - docker info variables : # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. DOCKER_HOST : tcp://docker:2376 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml DOCKER_TLS_CERTDIR : \"/certs\" # These are usually specified by the entrypoint, however the # Kubernetes executor doesn't run entrypoints # https://gitlab.com/gitlab-org/gitlab-runner/-/issues/4125 DOCKER_TLS_VERIFY : 1 DOCKER_CERT_PATH : \"$DOCKER_TLS_CERTDIR/client\" build : stage : build tags : - tls-dind-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nDocker-in-Docker with TLS disabled in Kubernetes\n\nTo use Docker-in-Docker with TLS disabled in Kubernetes, you must adapt the previous example to:\n\nRemove the [[runners.kubernetes.volumes.empty_dir]] section from the values.yml file. Change the port from 2376 to 2375 with DOCKER_HOST: tcp://docker:2375 . Instruct Docker to start with TLS disabled with DOCKER_TLS_CERTDIR: \"\" .\n\nFor example:\n\nUsing the Helm chart , update the values.yml file : runners : tags : \"no-tls-dind-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = true You can now use docker in the job script. You should include the docker:24.0.5-dind service: default : image : docker:24.0.5-cli services : - name : docker:24.0.5-dind variables : HEALTHCHECK_TCP_PORT : \"2375\" before_script : - docker info variables : # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. DOCKER_HOST : tcp://docker:2375 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # This instructs Docker not to start over TLS. DOCKER_TLS_CERTDIR : \"\" build : stage : build tags : - no - tls-dind-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nKnown issues with Docker-in-Docker\n\nDocker-in-Docker is the recommended configuration, but you should be aware of the following issues:\n\nThe docker-compose command : This command is not available in this configuration by default. To use docker-compose in your job scripts, follow the Docker Compose installation instructions . Cache : Each job runs in a new environment. Because every build gets its own instance of the Docker engine, concurrent jobs do not cause conflicts. However, jobs can be slower because there’s no caching of layers. See Docker layer caching . Storage drivers : By default, earlier versions of Docker use the vfs storage driver, which copies the file system for each job. Docker 17.09 and later use --storage-driver overlay2 , which is the recommended storage driver. See Using the OverlayFS driver for details. Root file system : Because the docker:24.0.5-dind container and the runner container do not share their root file system, you can use the job’s working directory as a mount point for child containers. For example, if you have files you want to share with a child container, you could create a subdirectory under /builds/$CI_PROJECT_PATH and use it as your mount point. For a more detailed explanation, see issue #41227 . variables : MOUNT_POINT : /builds/$CI_PROJECT_PATH/mnt script : - mkdir -p \"$MOUNT_POINT\" - docker run -v \"$MOUNT_POINT:/mnt\" my-docker-image\n\nUse Docker socket binding\n\nTo use Docker commands in your CI/CD jobs, you can bind-mount /var/run/docker.sock into the build container. Docker is then available in the context of the image.\n\nIf you bind the Docker socket you can’t use docker:24.0.5-dind as a service. Volume bindings also affect services, making them incompatible.\n\nUse the Docker executor with Docker socket binding\n\nTo mount the Docker socket with the Docker executor, add \"/var/run/docker.sock:/var/run/docker.sock\" to the Volumes in the [runners.docker] section .\n\nTo mount /var/run/docker.sock while registering your runner, include the following options: sudo gitlab-runner register \\ --non-interactive \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor \"docker\" \\ --description \"docker-runner\" \\ --tag-list \"socket-binding-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-volumes \"/var/run/docker.sock:/var/run/docker.sock\" The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = RUNNER_TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = false disable_cache = false volumes = [ \"/var/run/docker.sock:/var/run/docker.sock\" , \"/cache\" ] [ runners . cache ] Insecure = false Use Docker in the job script: default : image : docker:24.0.5-cli before_script : - docker info build : stage : build tags : - socket-binding-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nUse the Kubernetes executor with Docker socket binding\n\nTo mount the Docker socket with the Kubernetes executor, add \"/var/run/docker.sock\" to the Volumes in the [[runners.kubernetes.volumes.host_path]] section .\n\nTo specify a volume mount, update the values.yml file by using Helm chart . runners : tags : \"socket-binding-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = false [runners.kubernetes] [[runners.kubernetes.volumes.host_path]] host_path = '/var/run/docker.sock' mount_path = '/var/run/docker.sock' name = 'docker-sock' read_only = true Use Docker in the job script: default : image : docker:24.0.5-cli before_script : - docker info build : stage : build tags : - socket-binding-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nKnown issues with Docker socket binding\n\nWhen you use Docker socket binding, you avoid running Docker in privileged mode. However, the implications of this method are:\n\nWhen you share the Docker daemon, you effectively disable the container’s security mechanisms and expose your host to privilege escalation. This can cause container breakout. For example, if you run docker rm -f $(docker ps -a -q) in a project, it removes the GitLab Runner containers. Concurrent jobs might not work. If your tests create containers with specific names, they might conflict with each other. Any containers created by Docker commands are siblings of the runner, rather than children of the runner. This might cause complications for your workflow. Sharing files and directories from the source repository into containers might not work as expected. Volume mounting is done in the context of the host machine, not the build container. For example: docker run --rm -t -i -v $( pwd ) /src:/home/app/src test-image:latest run_app_tests\n\nYou do not need to include the docker:24.0.5-dind service, like you do when you use the Docker-in-Docker executor:\n\ndefault : image : docker:24.0.5-cli before_script : - docker info build : stage : build script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nFor complex Docker-in-Docker setups like Code Quality scanning using CodeClimate , you must match host and container paths for proper execution. For more details, see Use private runners for CodeClimate-based scanning .\n\nUse Docker pipe binding\n\nWindows Containers run Windows executables compiled for the Windows Server kernel and userland (either windowsservercore or nanoserver). To build and run Windows containers, a Windows system with container support is required. For more information, see Windows Containers .\n\nTo use Docker pipe binding, you must install and run a Docker Engine on the host Windows Server operating system. For more information, see Install Docker Community Edition (CE) on Windows Server .\n\nTo use Docker commands in your Windows-based container CI/CD jobs, you can bind-mount \\\\.\\pipe\\docker_engine into the launched executor container. Docker is then available in the context of the image.\n\nThe Docker pipe binding in Windows is similar to Docker socket binding in Linux and have similar Known issues as Known issues with Docker socket binding .\n\nA mandatory prerequisite for usage of Docker pipe binding is a Docker Engine installed and running on the host Windows Server operating system. See: Install Docker Community Edition (CE) on Windows Server\n\nUse the Docker executor with Docker pipe binding\n\nYou can use the Docker executor to run jobs in a Windows-based container.\n\nTo mount the Docker pipe with the Docker executor, add \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" to the Volumes in the [runners.docker] section .\n\nTo mount \"\\\\.\\pipe\\docker_engine while registering your runner, include the following options: .\\ gitlab-runner . exe register \\ - -non-interactive \\ - -url \"https://gitlab.com/\" \\ - -registration-token REGISTRATION_TOKEN \\ - -executor \"docker-windows\" \\ - -description \"docker-windows-runner\" - -tag-list \"docker-windows-runner\" \\ - -docker-image \"docker:25-windowsservercore-ltsc2022\" \\ - -docker-volumes \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = RUNNER_TOKEN executor = \"docker-windows\" [ runners . docker ] tls_verify = false image = \"docker:25-windowsservercore-ltsc2022\" privileged = false disable_cache = false volumes = [ \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" ] [ runners . cache ] Insecure = false Use Docker in the job script: default : image : docker:25-windowsservercore-ltsc2022 before_script : - docker version - docker info build : stage : build tags : - docker-windows-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nUse the Kubernetes executor with Docker pipe binding\n\nYou can use the Kubernetes executor to run jobs in a Windows-based container.\n\nTo use Kubernetes executor for Windows-based containers, you must include Windows nodes in your Kubernetes cluster. For more information, see Windows containers in Kubernetes .\n\nYou can use Runner operating in a Linux environment but targeting Windows nodes\n\nTo mount the Docker pipe with the Kubernetes executor, add \"\\\\.\\pipe\\docker_engine\" to the Volumes in the [[runners.kubernetes.volumes.host_path]] section .\n\nTo specify a volume mount, update the values.yml file by using Helm chart . runners : tags : \"kubernetes-windows-runner\" config : | [[runners]] executor = \"kubernetes\" # The FF_USE_POWERSHELL_PATH_RESOLVER feature flag has to be enabled for PowerShell # to resolve paths for Windows correctly when Runner is operating in a Linux environment # but targeting Windows nodes. [runners.feature_flags] FF_USE_POWERSHELL_PATH_RESOLVER = true [runners.kubernetes] [[runners.kubernetes.volumes.host_path]] host_path = '\\\\.\\pipe\\docker_engine' mount_path = '\\\\.\\pipe\\docker_engine' name = 'docker-pipe' read_only = true [runners.kubernetes.node_selector] \"kubernetes.io/arch\" = \"amd64\" \"kubernetes.io/os\" = \"windows\" \"node.kubernetes.io/windows-build\" = \"10.0.20348\" Use Docker in the job script: default : image : docker:25-windowsservercore-ltsc2022 before_script : - docker version - docker info build : stage : build tags : - kubernetes-windows-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nKnown issues with AWS EKS Kubernetes cluster\n\nWhen you migrate from dockerd to containerd , the AWS EKS bootstrapping script Start-EKSBootstrap.ps1 stops and disables the Docker Service. To work around this issue, rename the Docker Service after you Install Docker Community Edition (CE) on Windows Server with this script:\n\nWrite-Output \"Rename the just installed Docker Engine Service from docker to dockerd\" Write-Output \"because the Start-EKSBootstrap.ps1 stops and disables the docker Service as part of migration from dockerd to containerd\" Stop-Service -Name docker dockerd - -register-service - -service-name dockerd Start-Service -Name dockerd Write-Output \"Ready to do Docker pipe binding on Windows EKS Node! :-)\"\n\nKnown issues with Docker pipe binding\n\nDocker pipe binding has the same set of security and isolation issues as the Known issues with Docker socket binding .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 43,
          "content_length": 22545
        }
      },
      {
        "header": "Use the shell executor",
        "content": "To include Docker commands in your CI/CD jobs, you can configure your runner to use the shell executor. In this configuration, the gitlab-runner user runs the Docker commands, but needs permission to do so.\n\nInstall GitLab Runner. Register a runner. Select the shell executor. For example: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor shell \\ --description \"My Runner\" On the server where GitLab Runner is installed, install Docker Engine. View a list of supported platforms . Add the gitlab-runner user to the docker group: sudo usermod -aG docker gitlab-runner Verify that gitlab-runner has access to Docker: sudo -u gitlab-runner -H docker info In GitLab, add docker info to .gitlab-ci.yml to verify that Docker is working: default : before_script : - docker info build_image : script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nYou can now use docker commands (and install Docker Compose if needed).\n\nWhen you add gitlab-runner to the docker group, you effectively grant gitlab-runner full root permissions. For more information, see security of the docker group .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1184
        }
      },
      {
        "header": "Use Docker-in-Docker",
        "content": "“Docker-in-Docker” ( dind ) means:\n\nYour registered runner uses the Docker executor or the Kubernetes executor . The executor uses a container image of Docker , provided by Docker, to run your CI/CD jobs.\n\nThe Docker image includes all of the docker tools and can run the job script in context of the image in privileged mode.\n\nYou should use Docker-in-Docker with TLS enabled, which is supported by GitLab.com instance runners .\n\nYou should always pin a specific version of the image, like docker:24.0.5 . If you use a tag like docker:latest , you have no control over which version is used. This can cause incompatibility problems when new versions are released.\n\nUse the Docker executor with Docker-in-Docker\n\nYou can use the Docker executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in the Docker executor\n\nThe Docker daemon supports connections over TLS. TLS is the default in Docker 19.03.12 and later.\n\nThis task enables --docker-privileged , which effectively disables the container’s security mechanisms and exposes your host to privilege escalation. This action can cause container breakout. For more information, see runtime privilege and Linux capabilities .\n\nTo use Docker-in-Docker with TLS enabled:\n\nInstall GitLab Runner . Register GitLab Runner from the command line. Use docker and privileged mode: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor docker \\ --description \"My Docker Runner\" \\ --tag-list \"tls-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-privileged \\ --docker-volumes \"/certs/client\" This command registers a new runner to use the docker:24.0.5-cli image (if none is specified at the job level). To start the build and service containers, it uses the privileged mode. If you want to use Docker-in-Docker, you must always use privileged = true in your Docker containers. This command mounts /certs/client for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation . The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = true disable_cache = false volumes = [ \"/certs/client\" , \"/cache\" ] [ runners . cache ] [ runners . cache . s3 ] [ runners . cache . gcs ] This command registers a new runner to use the docker:24.0.5-cli image (if none is specified at the job level). To start the build and service containers, it uses the privileged mode. If you want to use Docker-in-Docker, you must always use privileged = true in your Docker containers. This command mounts /certs/client for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation . You can now use docker in the job script. You should include the docker:24.0.5-dind service: default : image : docker:24.0.5-cli services : - docker:24.0.5-dind before_script : - docker info variables : # When you use the dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. Docker 19.03 does this automatically # by setting the DOCKER_HOST in # https://github.com/docker-library/docker/blob/d45051476babc297257df490d22cbd806f1b11e4/19.03/docker-entrypoint.sh#L23-L29 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml DOCKER_TLS_CERTDIR : \"/certs\" build : stage : build tags : - tls-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nUse a Unix socket on a shared volume between Docker-in-Docker and build container\n\nDirectories defined in volumes = [\"/certs/client\", \"/cache\"] in the Docker-in-Docker with TLS enabled in the Docker executor approach are persistent between builds . If multiple CI/CD jobs using a Docker executor runner have Docker-in-Docker services enabled, then each job writes to the directory path. This approach might result in a conflict.\n\nTo address this conflict, use a Unix socket on a volume shared between the Docker-in-Docker service and the build container. This approach improves performance and establishes a secure connection between the service and client.\n\nThe following is a sample config.toml with temporary volume shared between build and service containers:\n\n[[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] image = \"docker:24.0.5-cli\" privileged = true volumes = [ \"/runner/services/docker\" ] # Temporary volume shared between build and service containers.\n\nThe Docker-in-Docker service creates a docker.sock . The Docker client connects to docker.sock through a Docker Unix socket volume.\n\njob : variables : # This variable is shared by both the DinD service and Docker client. # For the service, it will instruct DinD to create `docker.sock` here. # For the client, it tells the Docker client which Docker Unix socket to connect to. DOCKER_HOST : \"unix:///runner/services/docker/docker.sock\" services : - docker:24.0.5-dind image : docker:24.0.5-cli script : - docker version\n\nDocker-in-Docker with TLS disabled in the Docker executor\n\nSometimes there are legitimate reasons to disable TLS. For example, you have no control over the GitLab Runner configuration that you are using.\n\nRegister GitLab Runner from command line. Use docker and privileged mode: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor docker \\ --description \"My Docker Runner\" \\ --tag-list \"no-tls-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-privileged The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = true disable_cache = false volumes = [ \"/cache\" ] [ runners . cache ] [ runners . cache . s3 ] [ runners . cache . gcs ] Include the docker:24.0.5-dind service in the job script: default : image : docker:24.0.5-cli services : - docker:24.0.5-dind before_script : - docker info variables : # When using dind service, you must instruct docker to talk with the # daemon started inside of the service. The daemon is available with # a network connection instead of the default /var/run/docker.sock socket. # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#accessing-the-services # DOCKER_HOST : tcp://docker:2375 # # This instructs Docker not to start over TLS. DOCKER_TLS_CERTDIR : \"\" build : stage : build tags : - no - tls-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nDocker-in-Docker with proxy enabled in the Docker executor\n\nYou might need to configure proxy settings to use the docker push command.\n\nFor more information, see Proxy settings when using dind service .\n\nUse the Kubernetes executor with Docker-in-Docker\n\nYou can use the Kubernetes executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in Kubernetes\n\nTo use Docker-in-Docker with TLS enabled in Kubernetes:\n\nUsing the Helm chart , update the values.yml file to specify a volume mount. runners : tags : \"tls-dind-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = true [[runners.kubernetes.volumes.empty_dir]] name = \"docker-certs\" mount_path = \"/certs/client\" medium = \"Memory\" Include the docker:24.0.5-dind service in the job: default : image : docker:24.0.5-cli services : - name : docker:24.0.5-dind variables : HEALTHCHECK_TCP_PORT : \"2376\" before_script : - docker info variables : # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. DOCKER_HOST : tcp://docker:2376 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml DOCKER_TLS_CERTDIR : \"/certs\" # These are usually specified by the entrypoint, however the # Kubernetes executor doesn't run entrypoints # https://gitlab.com/gitlab-org/gitlab-runner/-/issues/4125 DOCKER_TLS_VERIFY : 1 DOCKER_CERT_PATH : \"$DOCKER_TLS_CERTDIR/client\" build : stage : build tags : - tls-dind-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nDocker-in-Docker with TLS disabled in Kubernetes\n\nTo use Docker-in-Docker with TLS disabled in Kubernetes, you must adapt the previous example to:\n\nRemove the [[runners.kubernetes.volumes.empty_dir]] section from the values.yml file. Change the port from 2376 to 2375 with DOCKER_HOST: tcp://docker:2375 . Instruct Docker to start with TLS disabled with DOCKER_TLS_CERTDIR: \"\" .\n\nFor example:\n\nUsing the Helm chart , update the values.yml file : runners : tags : \"no-tls-dind-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = true You can now use docker in the job script. You should include the docker:24.0.5-dind service: default : image : docker:24.0.5-cli services : - name : docker:24.0.5-dind variables : HEALTHCHECK_TCP_PORT : \"2375\" before_script : - docker info variables : # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. DOCKER_HOST : tcp://docker:2375 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # This instructs Docker not to start over TLS. DOCKER_TLS_CERTDIR : \"\" build : stage : build tags : - no - tls-dind-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nKnown issues with Docker-in-Docker\n\nDocker-in-Docker is the recommended configuration, but you should be aware of the following issues:\n\nThe docker-compose command : This command is not available in this configuration by default. To use docker-compose in your job scripts, follow the Docker Compose installation instructions . Cache : Each job runs in a new environment. Because every build gets its own instance of the Docker engine, concurrent jobs do not cause conflicts. However, jobs can be slower because there’s no caching of layers. See Docker layer caching . Storage drivers : By default, earlier versions of Docker use the vfs storage driver, which copies the file system for each job. Docker 17.09 and later use --storage-driver overlay2 , which is the recommended storage driver. See Using the OverlayFS driver for details. Root file system : Because the docker:24.0.5-dind container and the runner container do not share their root file system, you can use the job’s working directory as a mount point for child containers. For example, if you have files you want to share with a child container, you could create a subdirectory under /builds/$CI_PROJECT_PATH and use it as your mount point. For a more detailed explanation, see issue #41227 . variables : MOUNT_POINT : /builds/$CI_PROJECT_PATH/mnt script : - mkdir -p \"$MOUNT_POINT\" - docker run -v \"$MOUNT_POINT:/mnt\" my-docker-image",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 12245
        }
      },
      {
        "header": "Use the Docker executor with Docker-in-Docker",
        "content": "You can use the Docker executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in the Docker executor\n\nThe Docker daemon supports connections over TLS. TLS is the default in Docker 19.03.12 and later.\n\nThis task enables --docker-privileged , which effectively disables the container’s security mechanisms and exposes your host to privilege escalation. This action can cause container breakout. For more information, see runtime privilege and Linux capabilities .\n\nTo use Docker-in-Docker with TLS enabled:\n\nInstall GitLab Runner . Register GitLab Runner from the command line. Use docker and privileged mode: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor docker \\ --description \"My Docker Runner\" \\ --tag-list \"tls-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-privileged \\ --docker-volumes \"/certs/client\" This command registers a new runner to use the docker:24.0.5-cli image (if none is specified at the job level). To start the build and service containers, it uses the privileged mode. If you want to use Docker-in-Docker, you must always use privileged = true in your Docker containers. This command mounts /certs/client for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation . The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = true disable_cache = false volumes = [ \"/certs/client\" , \"/cache\" ] [ runners . cache ] [ runners . cache . s3 ] [ runners . cache . gcs ] This command registers a new runner to use the docker:24.0.5-cli image (if none is specified at the job level). To start the build and service containers, it uses the privileged mode. If you want to use Docker-in-Docker, you must always use privileged = true in your Docker containers. This command mounts /certs/client for the service and build container, which is needed for the Docker client to use the certificates in that directory. For more information, see the Docker image documentation . You can now use docker in the job script. You should include the docker:24.0.5-dind service: default : image : docker:24.0.5-cli services : - docker:24.0.5-dind before_script : - docker info variables : # When you use the dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. Docker 19.03 does this automatically # by setting the DOCKER_HOST in # https://github.com/docker-library/docker/blob/d45051476babc297257df490d22cbd806f1b11e4/19.03/docker-entrypoint.sh#L23-L29 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml DOCKER_TLS_CERTDIR : \"/certs\" build : stage : build tags : - tls-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nUse a Unix socket on a shared volume between Docker-in-Docker and build container\n\nDirectories defined in volumes = [\"/certs/client\", \"/cache\"] in the Docker-in-Docker with TLS enabled in the Docker executor approach are persistent between builds . If multiple CI/CD jobs using a Docker executor runner have Docker-in-Docker services enabled, then each job writes to the directory path. This approach might result in a conflict.\n\nTo address this conflict, use a Unix socket on a volume shared between the Docker-in-Docker service and the build container. This approach improves performance and establishes a secure connection between the service and client.\n\nThe following is a sample config.toml with temporary volume shared between build and service containers:\n\n[[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] image = \"docker:24.0.5-cli\" privileged = true volumes = [ \"/runner/services/docker\" ] # Temporary volume shared between build and service containers.\n\nThe Docker-in-Docker service creates a docker.sock . The Docker client connects to docker.sock through a Docker Unix socket volume.\n\njob : variables : # This variable is shared by both the DinD service and Docker client. # For the service, it will instruct DinD to create `docker.sock` here. # For the client, it tells the Docker client which Docker Unix socket to connect to. DOCKER_HOST : \"unix:///runner/services/docker/docker.sock\" services : - docker:24.0.5-dind image : docker:24.0.5-cli script : - docker version\n\nDocker-in-Docker with TLS disabled in the Docker executor\n\nSometimes there are legitimate reasons to disable TLS. For example, you have no control over the GitLab Runner configuration that you are using.\n\nRegister GitLab Runner from command line. Use docker and privileged mode: sudo gitlab-runner register -n \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor docker \\ --description \"My Docker Runner\" \\ --tag-list \"no-tls-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-privileged The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = true disable_cache = false volumes = [ \"/cache\" ] [ runners . cache ] [ runners . cache . s3 ] [ runners . cache . gcs ] Include the docker:24.0.5-dind service in the job script: default : image : docker:24.0.5-cli services : - docker:24.0.5-dind before_script : - docker info variables : # When using dind service, you must instruct docker to talk with the # daemon started inside of the service. The daemon is available with # a network connection instead of the default /var/run/docker.sock socket. # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#accessing-the-services # DOCKER_HOST : tcp://docker:2375 # # This instructs Docker not to start over TLS. DOCKER_TLS_CERTDIR : \"\" build : stage : build tags : - no - tls-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nDocker-in-Docker with proxy enabled in the Docker executor\n\nYou might need to configure proxy settings to use the docker push command.\n\nFor more information, see Proxy settings when using dind service .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 6883
        }
      },
      {
        "header": "Use the Kubernetes executor with Docker-in-Docker",
        "content": "You can use the Kubernetes executor to run jobs in a Docker container.\n\nDocker-in-Docker with TLS enabled in Kubernetes\n\nTo use Docker-in-Docker with TLS enabled in Kubernetes:\n\nUsing the Helm chart , update the values.yml file to specify a volume mount. runners : tags : \"tls-dind-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = true [[runners.kubernetes.volumes.empty_dir]] name = \"docker-certs\" mount_path = \"/certs/client\" medium = \"Memory\" Include the docker:24.0.5-dind service in the job: default : image : docker:24.0.5-cli services : - name : docker:24.0.5-dind variables : HEALTHCHECK_TCP_PORT : \"2376\" before_script : - docker info variables : # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. DOCKER_HOST : tcp://docker:2376 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # Specify to Docker where to create the certificates. Docker # creates them automatically on boot, and creates # `/certs/client` to share between the service and job # container, thanks to volume mount from config.toml DOCKER_TLS_CERTDIR : \"/certs\" # These are usually specified by the entrypoint, however the # Kubernetes executor doesn't run entrypoints # https://gitlab.com/gitlab-org/gitlab-runner/-/issues/4125 DOCKER_TLS_VERIFY : 1 DOCKER_CERT_PATH : \"$DOCKER_TLS_CERTDIR/client\" build : stage : build tags : - tls-dind-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nDocker-in-Docker with TLS disabled in Kubernetes\n\nTo use Docker-in-Docker with TLS disabled in Kubernetes, you must adapt the previous example to:\n\nRemove the [[runners.kubernetes.volumes.empty_dir]] section from the values.yml file. Change the port from 2376 to 2375 with DOCKER_HOST: tcp://docker:2375 . Instruct Docker to start with TLS disabled with DOCKER_TLS_CERTDIR: \"\" .\n\nFor example:\n\nUsing the Helm chart , update the values.yml file : runners : tags : \"no-tls-dind-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = true You can now use docker in the job script. You should include the docker:24.0.5-dind service: default : image : docker:24.0.5-cli services : - name : docker:24.0.5-dind variables : HEALTHCHECK_TCP_PORT : \"2375\" before_script : - docker info variables : # When using dind service, you must instruct Docker to talk with # the daemon started inside of the service. The daemon is available # with a network connection instead of the default # /var/run/docker.sock socket. DOCKER_HOST : tcp://docker:2375 # # The 'docker' hostname is the alias of the service container as described at # https://docs.gitlab.com/ee/ci/services/#accessing-the-services. # # This instructs Docker not to start over TLS. DOCKER_TLS_CERTDIR : \"\" build : stage : build tags : - no - tls-dind-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 3196
        }
      },
      {
        "header": "Known issues with Docker-in-Docker",
        "content": "Docker-in-Docker is the recommended configuration, but you should be aware of the following issues:\n\nThe docker-compose command : This command is not available in this configuration by default. To use docker-compose in your job scripts, follow the Docker Compose installation instructions . Cache : Each job runs in a new environment. Because every build gets its own instance of the Docker engine, concurrent jobs do not cause conflicts. However, jobs can be slower because there’s no caching of layers. See Docker layer caching . Storage drivers : By default, earlier versions of Docker use the vfs storage driver, which copies the file system for each job. Docker 17.09 and later use --storage-driver overlay2 , which is the recommended storage driver. See Using the OverlayFS driver for details. Root file system : Because the docker:24.0.5-dind container and the runner container do not share their root file system, you can use the job’s working directory as a mount point for child containers. For example, if you have files you want to share with a child container, you could create a subdirectory under /builds/$CI_PROJECT_PATH and use it as your mount point. For a more detailed explanation, see issue #41227 . variables : MOUNT_POINT : /builds/$CI_PROJECT_PATH/mnt script : - mkdir -p \"$MOUNT_POINT\" - docker run -v \"$MOUNT_POINT:/mnt\" my-docker-image",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1362
        }
      },
      {
        "header": "Use Docker socket binding",
        "content": "To use Docker commands in your CI/CD jobs, you can bind-mount /var/run/docker.sock into the build container. Docker is then available in the context of the image.\n\nIf you bind the Docker socket you can’t use docker:24.0.5-dind as a service. Volume bindings also affect services, making them incompatible.\n\nUse the Docker executor with Docker socket binding\n\nTo mount the Docker socket with the Docker executor, add \"/var/run/docker.sock:/var/run/docker.sock\" to the Volumes in the [runners.docker] section .\n\nTo mount /var/run/docker.sock while registering your runner, include the following options: sudo gitlab-runner register \\ --non-interactive \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor \"docker\" \\ --description \"docker-runner\" \\ --tag-list \"socket-binding-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-volumes \"/var/run/docker.sock:/var/run/docker.sock\" The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = RUNNER_TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = false disable_cache = false volumes = [ \"/var/run/docker.sock:/var/run/docker.sock\" , \"/cache\" ] [ runners . cache ] Insecure = false Use Docker in the job script: default : image : docker:24.0.5-cli before_script : - docker info build : stage : build tags : - socket-binding-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nUse the Kubernetes executor with Docker socket binding\n\nTo mount the Docker socket with the Kubernetes executor, add \"/var/run/docker.sock\" to the Volumes in the [[runners.kubernetes.volumes.host_path]] section .\n\nTo specify a volume mount, update the values.yml file by using Helm chart . runners : tags : \"socket-binding-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = false [runners.kubernetes] [[runners.kubernetes.volumes.host_path]] host_path = '/var/run/docker.sock' mount_path = '/var/run/docker.sock' name = 'docker-sock' read_only = true Use Docker in the job script: default : image : docker:24.0.5-cli before_script : - docker info build : stage : build tags : - socket-binding-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nKnown issues with Docker socket binding\n\nWhen you use Docker socket binding, you avoid running Docker in privileged mode. However, the implications of this method are:\n\nWhen you share the Docker daemon, you effectively disable the container’s security mechanisms and expose your host to privilege escalation. This can cause container breakout. For example, if you run docker rm -f $(docker ps -a -q) in a project, it removes the GitLab Runner containers. Concurrent jobs might not work. If your tests create containers with specific names, they might conflict with each other. Any containers created by Docker commands are siblings of the runner, rather than children of the runner. This might cause complications for your workflow. Sharing files and directories from the source repository into containers might not work as expected. Volume mounting is done in the context of the host machine, not the build container. For example: docker run --rm -t -i -v $( pwd ) /src:/home/app/src test-image:latest run_app_tests\n\nYou do not need to include the docker:24.0.5-dind service, like you do when you use the Docker-in-Docker executor:\n\ndefault : image : docker:24.0.5-cli before_script : - docker info build : stage : build script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nFor complex Docker-in-Docker setups like Code Quality scanning using CodeClimate , you must match host and container paths for proper execution. For more details, see Use private runners for CodeClimate-based scanning .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 3944
        }
      },
      {
        "header": "Use the Docker executor with Docker socket binding",
        "content": "To mount the Docker socket with the Docker executor, add \"/var/run/docker.sock:/var/run/docker.sock\" to the Volumes in the [runners.docker] section .\n\nTo mount /var/run/docker.sock while registering your runner, include the following options: sudo gitlab-runner register \\ --non-interactive \\ --url \"https://gitlab.com/\" \\ --registration-token REGISTRATION_TOKEN \\ --executor \"docker\" \\ --description \"docker-runner\" \\ --tag-list \"socket-binding-docker-runner\" \\ --docker-image \"docker:24.0.5-cli\" \\ --docker-volumes \"/var/run/docker.sock:/var/run/docker.sock\" The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = RUNNER_TOKEN executor = \"docker\" [ runners . docker ] tls_verify = false image = \"docker:24.0.5-cli\" privileged = false disable_cache = false volumes = [ \"/var/run/docker.sock:/var/run/docker.sock\" , \"/cache\" ] [ runners . cache ] Insecure = false Use Docker in the job script: default : image : docker:24.0.5-cli before_script : - docker info build : stage : build tags : - socket-binding-docker-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1192
        }
      },
      {
        "header": "Use the Kubernetes executor with Docker socket binding",
        "content": "To mount the Docker socket with the Kubernetes executor, add \"/var/run/docker.sock\" to the Volumes in the [[runners.kubernetes.volumes.host_path]] section .\n\nTo specify a volume mount, update the values.yml file by using Helm chart . runners : tags : \"socket-binding-kubernetes-runner\" config : | [[runners]] [runners.kubernetes] image = \"ubuntu:20.04\" privileged = false [runners.kubernetes] [[runners.kubernetes.volumes.host_path]] host_path = '/var/run/docker.sock' mount_path = '/var/run/docker.sock' name = 'docker-sock' read_only = true Use Docker in the job script: default : image : docker:24.0.5-cli before_script : - docker info build : stage : build tags : - socket-binding-kubernetes-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 797
        }
      },
      {
        "header": "Known issues with Docker socket binding",
        "content": "When you use Docker socket binding, you avoid running Docker in privileged mode. However, the implications of this method are:\n\nWhen you share the Docker daemon, you effectively disable the container’s security mechanisms and expose your host to privilege escalation. This can cause container breakout. For example, if you run docker rm -f $(docker ps -a -q) in a project, it removes the GitLab Runner containers. Concurrent jobs might not work. If your tests create containers with specific names, they might conflict with each other. Any containers created by Docker commands are siblings of the runner, rather than children of the runner. This might cause complications for your workflow. Sharing files and directories from the source repository into containers might not work as expected. Volume mounting is done in the context of the host machine, not the build container. For example: docker run --rm -t -i -v $( pwd ) /src:/home/app/src test-image:latest run_app_tests\n\nYou do not need to include the docker:24.0.5-dind service, like you do when you use the Docker-in-Docker executor:\n\ndefault : image : docker:24.0.5-cli before_script : - docker info build : stage : build script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nFor complex Docker-in-Docker setups like Code Quality scanning using CodeClimate , you must match host and container paths for proper execution. For more details, see Use private runners for CodeClimate-based scanning .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1496
        }
      },
      {
        "header": "Use Docker pipe binding",
        "content": "Windows Containers run Windows executables compiled for the Windows Server kernel and userland (either windowsservercore or nanoserver). To build and run Windows containers, a Windows system with container support is required. For more information, see Windows Containers .\n\nTo use Docker pipe binding, you must install and run a Docker Engine on the host Windows Server operating system. For more information, see Install Docker Community Edition (CE) on Windows Server .\n\nTo use Docker commands in your Windows-based container CI/CD jobs, you can bind-mount \\\\.\\pipe\\docker_engine into the launched executor container. Docker is then available in the context of the image.\n\nThe Docker pipe binding in Windows is similar to Docker socket binding in Linux and have similar Known issues as Known issues with Docker socket binding .\n\nA mandatory prerequisite for usage of Docker pipe binding is a Docker Engine installed and running on the host Windows Server operating system. See: Install Docker Community Edition (CE) on Windows Server\n\nUse the Docker executor with Docker pipe binding\n\nYou can use the Docker executor to run jobs in a Windows-based container.\n\nTo mount the Docker pipe with the Docker executor, add \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" to the Volumes in the [runners.docker] section .\n\nTo mount \"\\\\.\\pipe\\docker_engine while registering your runner, include the following options: .\\ gitlab-runner . exe register \\ - -non-interactive \\ - -url \"https://gitlab.com/\" \\ - -registration-token REGISTRATION_TOKEN \\ - -executor \"docker-windows\" \\ - -description \"docker-windows-runner\" - -tag-list \"docker-windows-runner\" \\ - -docker-image \"docker:25-windowsservercore-ltsc2022\" \\ - -docker-volumes \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = RUNNER_TOKEN executor = \"docker-windows\" [ runners . docker ] tls_verify = false image = \"docker:25-windowsservercore-ltsc2022\" privileged = false disable_cache = false volumes = [ \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" ] [ runners . cache ] Insecure = false Use Docker in the job script: default : image : docker:25-windowsservercore-ltsc2022 before_script : - docker version - docker info build : stage : build tags : - docker-windows-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nUse the Kubernetes executor with Docker pipe binding\n\nYou can use the Kubernetes executor to run jobs in a Windows-based container.\n\nTo use Kubernetes executor for Windows-based containers, you must include Windows nodes in your Kubernetes cluster. For more information, see Windows containers in Kubernetes .\n\nYou can use Runner operating in a Linux environment but targeting Windows nodes\n\nTo mount the Docker pipe with the Kubernetes executor, add \"\\\\.\\pipe\\docker_engine\" to the Volumes in the [[runners.kubernetes.volumes.host_path]] section .\n\nTo specify a volume mount, update the values.yml file by using Helm chart . runners : tags : \"kubernetes-windows-runner\" config : | [[runners]] executor = \"kubernetes\" # The FF_USE_POWERSHELL_PATH_RESOLVER feature flag has to be enabled for PowerShell # to resolve paths for Windows correctly when Runner is operating in a Linux environment # but targeting Windows nodes. [runners.feature_flags] FF_USE_POWERSHELL_PATH_RESOLVER = true [runners.kubernetes] [[runners.kubernetes.volumes.host_path]] host_path = '\\\\.\\pipe\\docker_engine' mount_path = '\\\\.\\pipe\\docker_engine' name = 'docker-pipe' read_only = true [runners.kubernetes.node_selector] \"kubernetes.io/arch\" = \"amd64\" \"kubernetes.io/os\" = \"windows\" \"node.kubernetes.io/windows-build\" = \"10.0.20348\" Use Docker in the job script: default : image : docker:25-windowsservercore-ltsc2022 before_script : - docker version - docker info build : stage : build tags : - kubernetes-windows-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nKnown issues with AWS EKS Kubernetes cluster\n\nWhen you migrate from dockerd to containerd , the AWS EKS bootstrapping script Start-EKSBootstrap.ps1 stops and disables the Docker Service. To work around this issue, rename the Docker Service after you Install Docker Community Edition (CE) on Windows Server with this script:\n\nWrite-Output \"Rename the just installed Docker Engine Service from docker to dockerd\" Write-Output \"because the Start-EKSBootstrap.ps1 stops and disables the docker Service as part of migration from dockerd to containerd\" Stop-Service -Name docker dockerd - -register-service - -service-name dockerd Start-Service -Name dockerd Write-Output \"Ready to do Docker pipe binding on Windows EKS Node! :-)\"\n\nKnown issues with Docker pipe binding\n\nDocker pipe binding has the same set of security and isolation issues as the Known issues with Docker socket binding .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 13,
          "content_length": 4928
        }
      },
      {
        "header": "Use the Docker executor with Docker pipe binding",
        "content": "You can use the Docker executor to run jobs in a Windows-based container.\n\nTo mount the Docker pipe with the Docker executor, add \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" to the Volumes in the [runners.docker] section .\n\nTo mount \"\\\\.\\pipe\\docker_engine while registering your runner, include the following options: .\\ gitlab-runner . exe register \\ - -non-interactive \\ - -url \"https://gitlab.com/\" \\ - -registration-token REGISTRATION_TOKEN \\ - -executor \"docker-windows\" \\ - -description \"docker-windows-runner\" - -tag-list \"docker-windows-runner\" \\ - -docker-image \"docker:25-windowsservercore-ltsc2022\" \\ - -docker-volumes \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" The previous command creates a config.toml entry similar to the following example: [[ runners ]] url = \"https://gitlab.com/\" token = RUNNER_TOKEN executor = \"docker-windows\" [ runners . docker ] tls_verify = false image = \"docker:25-windowsservercore-ltsc2022\" privileged = false disable_cache = false volumes = [ \"\\\\.\\pipe\\docker_engine:\\\\.\\pipe\\docker_engine\" ] [ runners . cache ] Insecure = false Use Docker in the job script: default : image : docker:25-windowsservercore-ltsc2022 before_script : - docker version - docker info build : stage : build tags : - docker-windows-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1363
        }
      },
      {
        "header": "Use the Kubernetes executor with Docker pipe binding",
        "content": "You can use the Kubernetes executor to run jobs in a Windows-based container.\n\nTo use Kubernetes executor for Windows-based containers, you must include Windows nodes in your Kubernetes cluster. For more information, see Windows containers in Kubernetes .\n\nYou can use Runner operating in a Linux environment but targeting Windows nodes\n\nTo mount the Docker pipe with the Kubernetes executor, add \"\\\\.\\pipe\\docker_engine\" to the Volumes in the [[runners.kubernetes.volumes.host_path]] section .\n\nTo specify a volume mount, update the values.yml file by using Helm chart . runners : tags : \"kubernetes-windows-runner\" config : | [[runners]] executor = \"kubernetes\" # The FF_USE_POWERSHELL_PATH_RESOLVER feature flag has to be enabled for PowerShell # to resolve paths for Windows correctly when Runner is operating in a Linux environment # but targeting Windows nodes. [runners.feature_flags] FF_USE_POWERSHELL_PATH_RESOLVER = true [runners.kubernetes] [[runners.kubernetes.volumes.host_path]] host_path = '\\\\.\\pipe\\docker_engine' mount_path = '\\\\.\\pipe\\docker_engine' name = 'docker-pipe' read_only = true [runners.kubernetes.node_selector] \"kubernetes.io/arch\" = \"amd64\" \"kubernetes.io/os\" = \"windows\" \"node.kubernetes.io/windows-build\" = \"10.0.20348\" Use Docker in the job script: default : image : docker:25-windowsservercore-ltsc2022 before_script : - docker version - docker info build : stage : build tags : - kubernetes-windows-runner script : - docker build -t my-docker-image . - docker run my-docker-image /script/to/run/tests\n\nKnown issues with AWS EKS Kubernetes cluster\n\nWhen you migrate from dockerd to containerd , the AWS EKS bootstrapping script Start-EKSBootstrap.ps1 stops and disables the Docker Service. To work around this issue, rename the Docker Service after you Install Docker Community Edition (CE) on Windows Server with this script:\n\nWrite-Output \"Rename the just installed Docker Engine Service from docker to dockerd\" Write-Output \"because the Start-EKSBootstrap.ps1 stops and disables the docker Service as part of migration from dockerd to containerd\" Stop-Service -Name docker dockerd - -register-service - -service-name dockerd Start-Service -Name dockerd Write-Output \"Ready to do Docker pipe binding on Windows EKS Node! :-)\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2262
        }
      },
      {
        "header": "Known issues with Docker pipe binding",
        "content": "Docker pipe binding has the same set of security and isolation issues as the Known issues with Docker socket binding .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 118
        }
      },
      {
        "header": "Enable registry mirror for docker:dind service",
        "content": "When the Docker daemon starts inside the service container, it uses the default configuration. You might want to configure a registry mirror for performance improvements and to ensure you do not exceed Docker Hub rate limits.\n\nThe service in the .gitlab-ci.yml file\n\nYou can append extra CLI flags to the dind service to set the registry mirror:\n\nservices : - name : docker:24.0.5-dind command : [ \"--registry-mirror\" , \"https://registry-mirror.example.com\" ] # Specify the registry mirror to use\n\nThe service in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can specify the command to configure the registry mirror for the Docker daemon. The dind service must be defined for the Docker or Kubernetes executor .\n\nDocker:\n\n[[ runners ]] ... executor = \"docker\" [ runners . docker ] ... privileged = true [[ runners . docker . services ]] name = \"docker:24.0.5-dind\" command = [ \"--registry-mirror\" , \"https://registry-mirror.example.com\" ]\n\nKubernetes:\n\n[[ runners ]] ... name = \"kubernetes\" [ runners . kubernetes ] ... privileged = true [[ runners . kubernetes . services ]] name = \"docker:24.0.5-dind\" command = [ \"--registry-mirror\" , \"https://registry-mirror.example.com\" ]\n\nThe Docker executor in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can use the mirror for every dind service. Update the configuration to specify a volume mount .\n\nFor example, if you have a /opt/docker/daemon.json file with the following content:\n\n{ \"registry-mirrors\" : [ \"https://registry-mirror.example.com\" ] }\n\nUpdate the config.toml file to mount the file to /etc/docker/daemon.json . This mounts the file for every container created by GitLab Runner. The configuration is detected by the dind service.\n\n[[ runners ]] ... executor = \"docker\" [ runners . docker ] image = \"alpine:3.12\" privileged = true volumes = [ \"/opt/docker/daemon.json:/etc/docker/daemon.json:ro\" ]\n\nThe Kubernetes executor in the GitLab Runner configuration file\n\nIf you are a GitLab Runner administrator, you can use the mirror for every dind service. Update the configuration to specify a ConfigMap volume mount .\n\nFor example, if you have a /tmp/daemon.json file with the following content:\n\n{ \"registry-mirrors\" : [ \"https://registry-mirror.example.com\" ] }\n\nCreate a ConfigMap with the content of this file. You can do this with a command like:\n\nkubectl create configmap docker-daemon --namespace gitlab-runner --from-file /tmp/daemon.json\n\nYou must use the namespace that the Kubernetes executor for GitLab Runner uses to create job pods.\n\nAfter the ConfigMap is created, you can update the config.toml file to mount the file to /etc/docker/daemon.json . This update mounts the file for every container created by GitLab Runner. The dind service detects this configuration.\n\n[[ runners ]] ... executor = \"kubernetes\" [ runners . kubernetes ] image = \"alpine:3.12\" privileged = true [[ runners . kubernetes . volumes . config_map ]] name = \"docker-daemon\" mount_path = \"/etc/docker/daemon.json\" sub_path = \"daemon.json\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 3060
        }
      },
      {
        "header": "The service in the .gitlab-ci.yml file",
        "content": "You can append extra CLI flags to the dind service to set the registry mirror:\n\nservices : - name : docker:24.0.5-dind command : [ \"--registry-mirror\" , \"https://registry-mirror.example.com\" ] # Specify the registry mirror to use",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 229
        }
      },
      {
        "header": "The service in the GitLab Runner configuration file",
        "content": "If you are a GitLab Runner administrator, you can specify the command to configure the registry mirror for the Docker daemon. The dind service must be defined for the Docker or Kubernetes executor .\n\nDocker:\n\n[[ runners ]] ... executor = \"docker\" [ runners . docker ] ... privileged = true [[ runners . docker . services ]] name = \"docker:24.0.5-dind\" command = [ \"--registry-mirror\" , \"https://registry-mirror.example.com\" ]\n\nKubernetes:\n\n[[ runners ]] ... name = \"kubernetes\" [ runners . kubernetes ] ... privileged = true [[ runners . kubernetes . services ]] name = \"docker:24.0.5-dind\" command = [ \"--registry-mirror\" , \"https://registry-mirror.example.com\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 664
        }
      },
      {
        "header": "The Docker executor in the GitLab Runner configuration file",
        "content": "If you are a GitLab Runner administrator, you can use the mirror for every dind service. Update the configuration to specify a volume mount .\n\nFor example, if you have a /opt/docker/daemon.json file with the following content:\n\n{ \"registry-mirrors\" : [ \"https://registry-mirror.example.com\" ] }\n\nUpdate the config.toml file to mount the file to /etc/docker/daemon.json . This mounts the file for every container created by GitLab Runner. The configuration is detected by the dind service.\n\n[[ runners ]] ... executor = \"docker\" [ runners . docker ] image = \"alpine:3.12\" privileged = true volumes = [ \"/opt/docker/daemon.json:/etc/docker/daemon.json:ro\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 655
        }
      },
      {
        "header": "The Kubernetes executor in the GitLab Runner configuration file",
        "content": "If you are a GitLab Runner administrator, you can use the mirror for every dind service. Update the configuration to specify a ConfigMap volume mount .\n\nFor example, if you have a /tmp/daemon.json file with the following content:\n\n{ \"registry-mirrors\" : [ \"https://registry-mirror.example.com\" ] }\n\nCreate a ConfigMap with the content of this file. You can do this with a command like:\n\nkubectl create configmap docker-daemon --namespace gitlab-runner --from-file /tmp/daemon.json\n\nYou must use the namespace that the Kubernetes executor for GitLab Runner uses to create job pods.\n\nAfter the ConfigMap is created, you can update the config.toml file to mount the file to /etc/docker/daemon.json . This update mounts the file for every container created by GitLab Runner. The dind service detects this configuration.\n\n[[ runners ]] ... executor = \"kubernetes\" [ runners . kubernetes ] image = \"alpine:3.12\" privileged = true [[ runners . kubernetes . volumes . config_map ]] name = \"docker-daemon\" mount_path = \"/etc/docker/daemon.json\" sub_path = \"daemon.json\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1060
        }
      },
      {
        "header": "Authenticate with registry in Docker-in-Docker",
        "content": "When you use Docker-in-Docker, the standard authentication methods do not work, because a fresh Docker daemon is started with the service. You should authenticate with registry .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 178
        }
      },
      {
        "header": "Make Docker-in-Docker builds faster with Docker layer caching",
        "content": "When using Docker-in-Docker, Docker downloads all layers of your image every time you create a build. You can make your builds faster with Docker layer caching .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 161
        }
      },
      {
        "header": "Use the OverlayFS driver",
        "content": "The instance runners on GitLab.com use the overlay2 driver by default.\n\nBy default, when using docker:dind , Docker uses the vfs storage driver, which copies the file system on every run. You can avoid this disk-intensive operation by using a different driver, for example overlay2 .\n\nRequirements\n\nEnsure a recent kernel is used, preferably >= 4.2 . Check whether the overlay module is loaded: sudo lsmod | grep overlay If you see no result, then the module is not loaded. To load the module, use: sudo modprobe overlay If the module loaded, you must make sure the module loads on reboot. On Ubuntu systems, do this by adding the following line to /etc/modules : overlay\n\nUse the OverlayFS driver per project\n\nYou can enable the driver for each project individually by using the DOCKER_DRIVER CI/CD variable in .gitlab-ci.yml :\n\nvariables : DOCKER_DRIVER : overlay2\n\nUse the OverlayFS driver for every project\n\nIf you use your own runners , you can enable the driver for every project by setting the DOCKER_DRIVER environment variable in the [[runners]] section of the config.toml file :\n\nenvironment = [ \"DOCKER_DRIVER=overlay2\" ]\n\nIf you’re running multiple runners, you must modify all configuration files.\n\nRead more about the runner configuration and using the OverlayFS storage driver .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1293
        }
      },
      {
        "header": "Requirements",
        "content": "Ensure a recent kernel is used, preferably >= 4.2 . Check whether the overlay module is loaded: sudo lsmod | grep overlay If you see no result, then the module is not loaded. To load the module, use: sudo modprobe overlay If the module loaded, you must make sure the module loads on reboot. On Ubuntu systems, do this by adding the following line to /etc/modules : overlay",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 372
        }
      },
      {
        "header": "Use the OverlayFS driver per project",
        "content": "You can enable the driver for each project individually by using the DOCKER_DRIVER CI/CD variable in .gitlab-ci.yml :\n\nvariables : DOCKER_DRIVER : overlay2",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 155
        }
      },
      {
        "header": "Use the OverlayFS driver for every project",
        "content": "If you use your own runners , you can enable the driver for every project by setting the DOCKER_DRIVER environment variable in the [[runners]] section of the config.toml file :\n\nenvironment = [ \"DOCKER_DRIVER=overlay2\" ]\n\nIf you’re running multiple runners, you must modify all configuration files.\n\nRead more about the runner configuration and using the OverlayFS storage driver .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 381
        }
      },
      {
        "header": "Docker alternatives",
        "content": "You can build container images without enabling privileged mode on your runner:\n\nBuildKit : Includes rootless BuildKit options that eliminate Docker daemon dependency. Buildah : Build OCI-compliant images without requiring a Docker daemon.\n\nBuildah example\n\nTo use Buildah with GitLab CI/CD, you need a runner with one of the following executors:\n\nKubernetes . Docker . Docker Machine .\n\nIn this example, you use Buildah to:\n\nBuild a Docker image. Push it to GitLab container registry .\n\nIn the last step, Buildah uses the Dockerfile under the root directory of the project to build the Docker image. Finally, it pushes the image to the project’s container registry:\n\nbuild : stage : build image : quay.io/buildah/stable variables : # Use vfs with buildah. Docker offers overlayfs as a default, but Buildah # cannot stack overlayfs on top of another overlayfs filesystem. STORAGE_DRIVER : vfs # Write all image metadata in the docker format, not the standard OCI format. # Newer versions of docker can handle the OCI format, but older versions, like # the one shipped with Fedora 30, cannot handle the format. BUILDAH_FORMAT : docker FQ_IMAGE_NAME : \"$CI_REGISTRY_IMAGE/test\" before_script : # GitLab container registry credentials taken from the # [predefined CI/CD variables](../variables/_index.md#predefined-cicd-variables) # to authenticate to the registry. - echo \"$CI_REGISTRY_PASSWORD\" | buildah login -u \"$CI_REGISTRY_USER\" --password-stdin $CI_REGISTRY script : - buildah images - buildah build -t $FQ_IMAGE_NAME - buildah images - buildah push $FQ_IMAGE_NAME\n\nIf you are using GitLab Runner Operator deployed to an OpenShift cluster, try the tutorial for using Buildah to build images in rootless container .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1719
        }
      },
      {
        "header": "Buildah example",
        "content": "To use Buildah with GitLab CI/CD, you need a runner with one of the following executors:\n\nKubernetes . Docker . Docker Machine .\n\nIn this example, you use Buildah to:\n\nBuild a Docker image. Push it to GitLab container registry .\n\nIn the last step, Buildah uses the Dockerfile under the root directory of the project to build the Docker image. Finally, it pushes the image to the project’s container registry:\n\nbuild : stage : build image : quay.io/buildah/stable variables : # Use vfs with buildah. Docker offers overlayfs as a default, but Buildah # cannot stack overlayfs on top of another overlayfs filesystem. STORAGE_DRIVER : vfs # Write all image metadata in the docker format, not the standard OCI format. # Newer versions of docker can handle the OCI format, but older versions, like # the one shipped with Fedora 30, cannot handle the format. BUILDAH_FORMAT : docker FQ_IMAGE_NAME : \"$CI_REGISTRY_IMAGE/test\" before_script : # GitLab container registry credentials taken from the # [predefined CI/CD variables](../variables/_index.md#predefined-cicd-variables) # to authenticate to the registry. - echo \"$CI_REGISTRY_PASSWORD\" | buildah login -u \"$CI_REGISTRY_USER\" --password-stdin $CI_REGISTRY script : - buildah images - buildah build -t $FQ_IMAGE_NAME - buildah images - buildah push $FQ_IMAGE_NAME\n\nIf you are using GitLab Runner Operator deployed to an OpenShift cluster, try the tutorial for using Buildah to build images in rootless container .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1461
        }
      },
      {
        "header": "Use the GitLab container registry",
        "content": "After you’ve built a Docker image, you can push it to the GitLab container registry .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 85
        }
      },
      {
        "header": "Troubleshooting",
        "content": "open //./pipe/docker_engine: The system cannot find the file specified\n\nThe following error might appear when you run a docker command in the PowerShell script to access the mounted Docker pipe:\n\nPS C: \\> docker version Client : Version : 25.0 . 5 API version : 1.44 Go version : go1 . 21 . 8 Git commit : 5dc9bcc Built : Tue Mar 19 15 : 06 : 12 2024 OS / Arch : windows / amd64 Context : default error during connect : this error may indicate that the docker daemon is not running : Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.44/version\" : open //./ pipe / docker_engine : The system cannot find the file specified .\n\nThe error indicates that the Docker Engine is not running on the Windows EKS Node and the Docker pipe binding could not be used in the Windows-based Executor container.\n\nTo solve the problem, use the workaround described in Use the Kubernetes executor with Docker pipe binding .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 900
        }
      },
      {
        "header": "open //./pipe/docker_engine: The system cannot find the file specified",
        "content": "The following error might appear when you run a docker command in the PowerShell script to access the mounted Docker pipe:\n\nPS C: \\> docker version Client : Version : 25.0 . 5 API version : 1.44 Go version : go1 . 21 . 8 Git commit : 5dc9bcc Built : Tue Mar 19 15 : 06 : 12 2024 OS / Arch : windows / amd64 Context : default error during connect : this error may indicate that the docker daemon is not running : Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.44/version\" : open //./ pipe / docker_engine : The system cannot find the file specified .\n\nThe error indicates that the Docker Engine is not running on the Windows EKS Node and the Docker pipe binding could not be used in the Windows-based Executor container.\n\nTo solve the problem, use the workaround described in Use the Kubernetes executor with Docker pipe binding .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 828
        }
      }
    ],
    "url": "https://docs.gitlab.com/ci/docker/using_docker_build/",
    "doc_type": "docker",
    "total_sections": 30
  },
  {
    "title": "Docker executor",
    "summary": "GitLab Runner uses the Docker executor to run jobs on Docker images. You can use the Docker executor to:",
    "sections": [
      {
        "header": "Docker executor workflow",
        "content": "The Docker executor uses a Docker image based on Alpine Linux that contains the tools to run the prepare, pre-job, and post-job steps. To view the definition of the special Docker image, see the GitLab Runner repository .\n\nThe Docker executor divides the job into several steps:\n\nPrepare : Creates and starts the services . Pre-job : Clones, restores cache , and downloads artifacts from previous stages. Runs on a special Docker image. Job : Runs your build in the Docker image you configure for the runner. Post-job : Create cache, upload artifacts to GitLab. Runs on a special Docker Image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 593
        }
      },
      {
        "header": "Supported configurations",
        "content": "The Docker executor supports the following configurations.\n\nFor known issues and additional requirements of Windows configurations, see Use Windows containers .\n\nRunner is installed on: Executor is: Container is running: Windows docker-windows Windows Windows docker Linux Linux docker Linux macOS docker Linux\n\nThese configurations are not supported:\n\nRunner is installed on: Executor is: Container is running: Linux docker-windows Linux Linux docker Windows Linux docker-windows Windows Windows docker Windows Windows docker-windows Linux\n\nGitLab Runner uses Docker Engine API v1.25 to talk to the Docker Engine. This means the minimum supported version of Docker on a Linux server is 1.13.0 . On Windows Server, it needs to be more recent to identify the Windows Server version.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 3,
          "content_length": 781
        }
      },
      {
        "header": "Use the Docker executor",
        "content": "To use the Docker executor, manually define Docker as the executor in config.toml or use the gitlab-runner register --executor \"docker\" command to automatically define it.\n\nThe following sample configuration shows Docker defined as the executor. For more information about these values, see Advanced configuration\n\nconcurrent = 4 [[ runners ]] name = \"myRunner\" url = \"https://gitlab.com/ci\" token = \"......\" executor = \"docker\" [ runners . docker ] tls_verify = true image = \"my.registry.tld:5000/alpine:latest\" privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [ \"/cache\" , ] shm_size = 0 allowed_pull_policies = [ \"always\" , \"if-not-present\" ] allowed_images = [ \"my.registry.tld:5000/*:*\" ] allowed_services = [ \"my.registry.tld:5000/*:*\" ] [ runners . docker . volume_driver_ops ] \"size\" = \"50G\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 863
        }
      },
      {
        "header": "Configure images and services",
        "content": "Prerequisites:\n\nThe image where your job runs must have a working shell in its operating system PATH . Supported shells are: For Linux: sh bash PowerShell Core ( pwsh ). Introduced in 13.9 . For Windows: PowerShell ( powershell ) PowerShell Core ( pwsh ). Introduced in 13.6 . For Linux: sh bash PowerShell Core ( pwsh ). Introduced in 13.9 . sh bash PowerShell Core ( pwsh ). Introduced in 13.9 . For Windows: PowerShell ( powershell ) PowerShell Core ( pwsh ). Introduced in 13.6 . PowerShell ( powershell ) PowerShell Core ( pwsh ). Introduced in 13.6 .\n\nTo configure the Docker executor, you define the Docker images and services in .gitlab-ci.yml and config.toml .\n\nUse the following keywords:\n\nimage : The name of the Docker image that the runner uses to run jobs. Enter an image from the local Docker Engine, or any image in Docker Hub. For more information, see the Docker documentation . To define the image version, use a colon ( : ) to add a tag. If you don’t specify a tag, Docker uses latest as the version. Enter an image from the local Docker Engine, or any image in Docker Hub. For more information, see the Docker documentation . To define the image version, use a colon ( : ) to add a tag. If you don’t specify a tag, Docker uses latest as the version. services : The additional image that creates another container and links to the image . For more information about types of services, see Services .\n\nDefine images and services in .gitlab-ci.yml\n\nDefine an image that the runner uses for all jobs and a list of services to use during build time.\n\nExample:\n\nimage : ruby:3.3 services : - postgres:9.3 before_script : - bundle install test : script : - bundle exec rake spec\n\nTo define different images and services per job:\n\nbefore_script : - bundle install test:3.3 : image : ruby:3.3 services : - postgres:9.3 script : - bundle exec rake spec test:3.4 : image : ruby:3.4 services : - postgres:9.4 script : - bundle exec rake spec\n\nIf you don’t define an image in .gitlab-ci.yml , the runner uses the image defined in config.toml .\n\nDefine images and services in config.toml\n\nTo add images and services to all jobs run by a runner, update [runners.docker] in the config.toml .\n\nBy default, the Docker executer uses the image defined in .gitlab-ci.yml . If you don’t define one in .gitlab-ci.yml , the runner uses the image defined in config.toml .\n\nExample:\n\n[ runners . docker ] image = \"ruby:3.3\" [[ runners . docker . services ]] name = \"mysql:latest\" alias = \"db\" [[ runners . docker . services ]] name = \"redis:latest\" alias = \"cache\"\n\nThis example uses the array of tables syntax .\n\nDefine an image from a private registry\n\nPrerequisites:\n\nTo access images from a private registry, you must authenticate GitLab Runner .\n\nTo define an image from a private registry, provide the registry name and the image in .gitlab-ci.yml .\n\nExample:\n\nimage : my.registry.tld:5000/namespace/image:tag\n\nIn this example, GitLab Runner searches the registry my.registry.tld:5000 for the image namespace/image:tag .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 3021
        }
      },
      {
        "header": "Define images and services in .gitlab-ci.yml",
        "content": "Define an image that the runner uses for all jobs and a list of services to use during build time.\n\nExample:\n\nimage : ruby:3.3 services : - postgres:9.3 before_script : - bundle install test : script : - bundle exec rake spec\n\nTo define different images and services per job:\n\nbefore_script : - bundle install test:3.3 : image : ruby:3.3 services : - postgres:9.3 script : - bundle exec rake spec test:3.4 : image : ruby:3.4 services : - postgres:9.4 script : - bundle exec rake spec\n\nIf you don’t define an image in .gitlab-ci.yml , the runner uses the image defined in config.toml .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 584
        }
      },
      {
        "header": "Define images and services in config.toml",
        "content": "To add images and services to all jobs run by a runner, update [runners.docker] in the config.toml .\n\nBy default, the Docker executer uses the image defined in .gitlab-ci.yml . If you don’t define one in .gitlab-ci.yml , the runner uses the image defined in config.toml .\n\nExample:\n\n[ runners . docker ] image = \"ruby:3.3\" [[ runners . docker . services ]] name = \"mysql:latest\" alias = \"db\" [[ runners . docker . services ]] name = \"redis:latest\" alias = \"cache\"\n\nThis example uses the array of tables syntax .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 511
        }
      },
      {
        "header": "Define an image from a private registry",
        "content": "Prerequisites:\n\nTo access images from a private registry, you must authenticate GitLab Runner .\n\nTo define an image from a private registry, provide the registry name and the image in .gitlab-ci.yml .\n\nExample:\n\nimage : my.registry.tld:5000/namespace/image:tag\n\nIn this example, GitLab Runner searches the registry my.registry.tld:5000 for the image namespace/image:tag .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 371
        }
      },
      {
        "header": "Network configurations",
        "content": "You must configure a network to connect services to a CI/CD job.\n\nTo configure a network, you can either:\n\nRecommended. Configure the runner to create a network for each job. Define container links. Container links are a legacy feature of Docker.\n\nCreate a network for each job\n\nYou can configure the runner to create a network for each job.\n\nWhen you enable this networking mode, the runner creates and uses a user-defined Docker bridge network for each job. Docker environment variables are not shared across the containers. For more information about user-defined bridge networks, see the Docker documentation .\n\nTo use this networking mode, enable FF_NETWORK_PER_BUILD in either the feature flag or the environment variable in the config.toml .\n\nDo not set the network_mode .\n\nExample:\n\n[[ runners ]] ( ... ) executor = \"docker\" environment = [ \"FF_NETWORK_PER_BUILD=1\" ]\n\nOr:\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . feature_flags ] FF_NETWORK_PER_BUILD = true\n\nTo set the default Docker address pool, use default-address-pool in dockerd . If CIDR ranges are already used in the network, Docker networks may conflict with other networks on the host, including other Docker networks.\n\nThis feature works only when the Docker daemon is configured with IPv6 enabled. To enable IPv6 support, set enable_ipv6 to true in the Docker configuration. For more information, see the Docker documentation .\n\nThe runner uses the build alias to resolve the job container.\n\nDNS might not work correctly with a Docker-in-Docker ( dind ) service when you use this feature. This behavior is due to an issue with Docker/Moby , where dind containers don’t inherit custom DNS entries when you specify a network. As a workaround, manually provide the custom DNS settings to the dind service. For example, if your custom DNS server is 1.1.1.1 , you can use 127.0.0.11 , which is Docker’s internal DNS service: services : - name : docker:dind command : [ -- dns=127.0.0.11, --dns=1.1.1.1] This approach also allows containers to resolve services on the same network.\n\nHow the runner creates a network for each job\n\nWhen a job starts, the runner:\n\nCreates a bridge network, similar to the Docker command docker network create <network> . Connects the service and containers to the bridge network. Removes the network at the end of the job.\n\nThe container running the job and the containers running the service resolve each other’s hostnames and aliases. This functionality is provided by Docker .\n\nConfigure a network with container links\n\nYou can configure a network mode that uses Docker legacy container links and the default Docker bridge to link the job container with the services. This network mode is the default if FF_NETWORK_PER_BUILD is not enabled.\n\nTo configure the network, specify the networking mode in the config.toml file:\n\nbridge : Use the bridge network. Default. host : Use the host’s network stack inside the container. none : No networking. Not recommended.\n\nExample:\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] network_mode = \"bridge\"\n\nIf you use any other network_mode value, these are taken as the name of an already existing Docker network, which the build container connects to.\n\nDuring name resolution, Docker updates the /etc/hosts file in the container with the service container hostname and alias. However, the service container is not able to resolve the container name. To resolve the container name, you must create a network for each job.\n\nLinked containers share their environment variables.\n\nOverriding the MTU of the created network\n\nFor some environments, like virtual machines in OpenStack, a custom MTU is necessary. The Docker daemon does not respect the MTU in docker.json (see Moby issue 34981 ). You can set network_mtu in your config.toml to any valid value so the Docker daemon can use the correct MTU for the newly created network. You must also enable FF_NETWORK_PER_BUILD for the override to take effect.\n\nThe following configuration sets the MTU to 1402 for the network created for each job. Make sure to adjust the value to your specific environment requirements.\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] network_mtu = 1402 [ runners . feature_flags ] FF_NETWORK_PER_BUILD = true",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 21,
          "content_length": 4269
        }
      },
      {
        "header": "Create a network for each job",
        "content": "You can configure the runner to create a network for each job.\n\nWhen you enable this networking mode, the runner creates and uses a user-defined Docker bridge network for each job. Docker environment variables are not shared across the containers. For more information about user-defined bridge networks, see the Docker documentation .\n\nTo use this networking mode, enable FF_NETWORK_PER_BUILD in either the feature flag or the environment variable in the config.toml .\n\nDo not set the network_mode .\n\nExample:\n\n[[ runners ]] ( ... ) executor = \"docker\" environment = [ \"FF_NETWORK_PER_BUILD=1\" ]\n\nOr:\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . feature_flags ] FF_NETWORK_PER_BUILD = true\n\nTo set the default Docker address pool, use default-address-pool in dockerd . If CIDR ranges are already used in the network, Docker networks may conflict with other networks on the host, including other Docker networks.\n\nThis feature works only when the Docker daemon is configured with IPv6 enabled. To enable IPv6 support, set enable_ipv6 to true in the Docker configuration. For more information, see the Docker documentation .\n\nThe runner uses the build alias to resolve the job container.\n\nDNS might not work correctly with a Docker-in-Docker ( dind ) service when you use this feature. This behavior is due to an issue with Docker/Moby , where dind containers don’t inherit custom DNS entries when you specify a network. As a workaround, manually provide the custom DNS settings to the dind service. For example, if your custom DNS server is 1.1.1.1 , you can use 127.0.0.11 , which is Docker’s internal DNS service: services : - name : docker:dind command : [ -- dns=127.0.0.11, --dns=1.1.1.1] This approach also allows containers to resolve services on the same network.\n\nHow the runner creates a network for each job\n\nWhen a job starts, the runner:\n\nCreates a bridge network, similar to the Docker command docker network create <network> . Connects the service and containers to the bridge network. Removes the network at the end of the job.\n\nThe container running the job and the containers running the service resolve each other’s hostnames and aliases. This functionality is provided by Docker .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2210
        }
      },
      {
        "header": "How the runner creates a network for each job",
        "content": "When a job starts, the runner:\n\nCreates a bridge network, similar to the Docker command docker network create <network> . Connects the service and containers to the bridge network. Removes the network at the end of the job.\n\nThe container running the job and the containers running the service resolve each other’s hostnames and aliases. This functionality is provided by Docker .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 380
        }
      },
      {
        "header": "Configure a network with container links",
        "content": "You can configure a network mode that uses Docker legacy container links and the default Docker bridge to link the job container with the services. This network mode is the default if FF_NETWORK_PER_BUILD is not enabled.\n\nTo configure the network, specify the networking mode in the config.toml file:\n\nbridge : Use the bridge network. Default. host : Use the host’s network stack inside the container. none : No networking. Not recommended.\n\nExample:\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] network_mode = \"bridge\"\n\nIf you use any other network_mode value, these are taken as the name of an already existing Docker network, which the build container connects to.\n\nDuring name resolution, Docker updates the /etc/hosts file in the container with the service container hostname and alias. However, the service container is not able to resolve the container name. To resolve the container name, you must create a network for each job.\n\nLinked containers share their environment variables.\n\nOverriding the MTU of the created network\n\nFor some environments, like virtual machines in OpenStack, a custom MTU is necessary. The Docker daemon does not respect the MTU in docker.json (see Moby issue 34981 ). You can set network_mtu in your config.toml to any valid value so the Docker daemon can use the correct MTU for the newly created network. You must also enable FF_NETWORK_PER_BUILD for the override to take effect.\n\nThe following configuration sets the MTU to 1402 for the network created for each job. Make sure to adjust the value to your specific environment requirements.\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] network_mtu = 1402 [ runners . feature_flags ] FF_NETWORK_PER_BUILD = true",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1736
        }
      },
      {
        "header": "Overriding the MTU of the created network",
        "content": "For some environments, like virtual machines in OpenStack, a custom MTU is necessary. The Docker daemon does not respect the MTU in docker.json (see Moby issue 34981 ). You can set network_mtu in your config.toml to any valid value so the Docker daemon can use the correct MTU for the newly created network. You must also enable FF_NETWORK_PER_BUILD for the override to take effect.\n\nThe following configuration sets the MTU to 1402 for the network created for each job. Make sure to adjust the value to your specific environment requirements.\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] network_mtu = 1402 [ runners . feature_flags ] FF_NETWORK_PER_BUILD = true",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 682
        }
      },
      {
        "header": "Restrict Docker images and services",
        "content": "To restrict Docker images and services, specify a wildcard pattern in the allowed_images and allowed_services parameters. For more details on syntax, see doublestar documentation .\n\nFor example, to allow images from your private Docker registry only:\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) allowed_images = [ \"my.registry.tld:5000/*:*\" ] allowed_services = [ \"my.registry.tld:5000/*:*\" ]\n\nTo restrict to a list of images from your private Docker registry:\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) allowed_images = [ \"my.registry.tld:5000/ruby:*\" , \"my.registry.tld:5000/node:*\" ] allowed_services = [ \"postgres:9.4\" , \"postgres:latest\" ]\n\nTo exclude specific images like Kali:\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) allowed_images = [ \"**\" , \"!*/kali*\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 851
        }
      },
      {
        "header": "Access services hostnames",
        "content": "To access a service hostname, add the service to services in .gitlab-ci.yml .\n\nFor example, to use a Wordpress instance to test an API integration with your application, use tutum/wordpress as the service image:\n\nservices : - tutum/wordpress:latest\n\nWhen the job runs, the tutum/wordpress service starts. You can access it from your build container under the hostname tutum__wordpress and tutum-wordpress .\n\nIn addition to the specified service aliases, the runner assigns the name of the service image as an alias to the service container. You can use any of these aliases.\n\nThe runner uses the following rules to create the alias based on the image name:\n\nEverything after : is stripped. For the first alias, the slash ( / ) is replaced with double underscores ( __ ). For the second alias, the slash ( / ) is replaced with a single dash ( - ).\n\nIf you use a private service image, the runner strips any specified port and applies the rules. The service registry.gitlab-wp.com:4999/tutum/wordpress results in the hostname registry.gitlab-wp.com__tutum__wordpress and registry.gitlab-wp.com-tutum-wordpress .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1109
        }
      },
      {
        "header": "Configuring services",
        "content": "To change database names or set account names, you can define environment variables for the service.\n\nWhen the runner passes variables:\n\nVariables are passed to all containers. The runner cannot pass variables to specific containers. Secure variables are passed to the build container.\n\nFor more information about configuration variables, see the documentation of each image provided in their corresponding Docker Hub page.\n\nMount a directory in RAM\n\nYou can use the tmpfs option to mount a directory in RAM. This speeds up the time required to test if there is a lot of I/O related work, such as with databases.\n\nIf you use the tmpfs and services_tmpfs options in the runner configuration, you can specify multiple paths, each with its own options. For more information, see the Docker documentation .\n\nFor example, to mount the data directory for the official MySQL container in RAM, configure the config.toml :\n\n[ runners . docker ] # For the main container [ runners . docker . tmpfs ] \"/var/lib/mysql\" = \"rw,noexec\" # For services [ runners . docker . services_tmpfs ] \"/var/lib/mysql\" = \"rw,noexec\"\n\nBuilding a directory in a service\n\nGitLab Runner mounts a /builds directory to all shared services.\n\nFor more information about using different services see:\n\nUsing PostgreSQL Using MySQL\n\nHow GitLab Runner performs the services health check\n\nHistory Introduced multiple port checks in GitLab 16.0.\n\nAfter the service starts, GitLab Runner waits for the service to respond. The Docker executor tries to open a TCP connection to the exposed service port in the service container.\n\nIn GitLab 15.11 and earlier, only the first exposed port is checked. In GitLab 16.0 and later, the first 20 exposed ports are checked.\n\nThe HEALTHCHECK_TCP_PORT service variable can be used to perform the health check on a specific port:\n\njob : services : - name : mongo variables : HEALTHCHECK_TCP_PORT : \"27017\"\n\nTo see how this is implemented, use the health check Go command .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 1966
        }
      },
      {
        "header": "Mount a directory in RAM",
        "content": "You can use the tmpfs option to mount a directory in RAM. This speeds up the time required to test if there is a lot of I/O related work, such as with databases.\n\nIf you use the tmpfs and services_tmpfs options in the runner configuration, you can specify multiple paths, each with its own options. For more information, see the Docker documentation .\n\nFor example, to mount the data directory for the official MySQL container in RAM, configure the config.toml :\n\n[ runners . docker ] # For the main container [ runners . docker . tmpfs ] \"/var/lib/mysql\" = \"rw,noexec\" # For services [ runners . docker . services_tmpfs ] \"/var/lib/mysql\" = \"rw,noexec\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 653
        }
      },
      {
        "header": "Building a directory in a service",
        "content": "GitLab Runner mounts a /builds directory to all shared services.\n\nFor more information about using different services see:\n\nUsing PostgreSQL Using MySQL",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 152
        }
      },
      {
        "header": "How GitLab Runner performs the services health check",
        "content": "History Introduced multiple port checks in GitLab 16.0.\n\nAfter the service starts, GitLab Runner waits for the service to respond. The Docker executor tries to open a TCP connection to the exposed service port in the service container.\n\nIn GitLab 15.11 and earlier, only the first exposed port is checked. In GitLab 16.0 and later, the first 20 exposed ports are checked.\n\nThe HEALTHCHECK_TCP_PORT service variable can be used to perform the health check on a specific port:\n\njob : services : - name : mongo variables : HEALTHCHECK_TCP_PORT : \"27017\"\n\nTo see how this is implemented, use the health check Go command .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 617
        }
      },
      {
        "header": "Specify Docker driver operations",
        "content": "Specify arguments to supply to the Docker volume driver when you create volumes for builds. For example, you can use these arguments to limit the space for each build to run, in addition to all other driver specific options. The following example shows a config.toml where the limit that each build can consume is set to 50 GB.\n\n[ runners . docker ] [ runners . docker . volume_driver_ops ] \"size\" = \"50G\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 405
        }
      },
      {
        "header": "Using host devices",
        "content": "History Introduced in GitLab 17.10.\n\nYou can expose hardware devices on the GitLab Runner host to the container that runs the job. To do this, configure the runner’s devices and services_devices options.\n\nTo expose devices to build and helper containers, use the devices option. To expose devices to services containers, use the services_devices option. To restrict a service container’s device access to specific images, use exact image names or glob patterns. This action prevents direct access to host system devices.\n\nFor more information on device access, see Docker documentation .\n\nBuild container example\n\nIn this example, the config.toml section exposes /dev/bus/usb to build containers. This configuration allows pipelines to access USB devices attached to the host machine, such as Android smartphones controlled over the Android Debug Bridge ( adb ) .\n\nSince build job containers can directly access host USB devices, simultaneous pipeline executions may conflict with each other when accessing the same hardware. To prevent these conflicts, use resource_group .\n\n[[ runners ]] name = \"hardware-runner\" url = \"https://gitlab.com\" token = \"__REDACTED__\" executor = \"docker\" [ runners . docker ] # All job containers may access the host device devices = [ \"/dev/bus/usb\" ]\n\nPrivate registry example\n\nThis example shows how to expose /dev/kvm and /dev/dri devices to container images from a private Docker registry. These devices are commonly used for hardware-accelerated virtualization and rendering. To mitigate risks involved with providing users direct access to hardware resources, restrict device access to trusted images in the myregistry:5000/emulator/* namespace:\n\n[ runners . docker ] [ runners . docker . services_devices ] # Only images from an internal registry may access the host devices \"myregistry:5000/emulator/*\" = [ \"/dev/kvm\" , \"/dev/dri\" ]\n\nThe image name **/* might expose devices to any image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1927
        }
      },
      {
        "header": "Build container example",
        "content": "In this example, the config.toml section exposes /dev/bus/usb to build containers. This configuration allows pipelines to access USB devices attached to the host machine, such as Android smartphones controlled over the Android Debug Bridge ( adb ) .\n\nSince build job containers can directly access host USB devices, simultaneous pipeline executions may conflict with each other when accessing the same hardware. To prevent these conflicts, use resource_group .\n\n[[ runners ]] name = \"hardware-runner\" url = \"https://gitlab.com\" token = \"__REDACTED__\" executor = \"docker\" [ runners . docker ] # All job containers may access the host device devices = [ \"/dev/bus/usb\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 668
        }
      },
      {
        "header": "Private registry example",
        "content": "This example shows how to expose /dev/kvm and /dev/dri devices to container images from a private Docker registry. These devices are commonly used for hardware-accelerated virtualization and rendering. To mitigate risks involved with providing users direct access to hardware resources, restrict device access to trusted images in the myregistry:5000/emulator/* namespace:\n\n[ runners . docker ] [ runners . docker . services_devices ] # Only images from an internal registry may access the host devices \"myregistry:5000/emulator/*\" = [ \"/dev/kvm\" , \"/dev/dri\" ]\n\nThe image name **/* might expose devices to any image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 617
        }
      },
      {
        "header": "Configure directories for the container build and cache",
        "content": "To define where data is stored in the container, configure /builds and /cache directories in the [[runners]] section in config.toml .\n\nIf you modify the /cache storage path, to mark the path as persistent you must define it in volumes = [\"/my/cache/\"] , under the [runners.docker] section in config.toml .\n\nBy default, the Docker executor stores builds and caches in the following directories:\n\nBuilds in /builds/<namespace>/<project-name> Caches in /cache inside the container.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 478
        }
      },
      {
        "header": "Clear the Docker cache",
        "content": "Use clear-docker-cache to remove unused containers and volumes created by the runner.\n\nFor a list of options, run the script with the help option:\n\nclear-docker-cache help\n\nThe default option is prune-volumes , which removes all unused containers (dangling and unreferenced) and volumes.\n\nTo manage cache storage efficiently, you should:\n\nRun clear-docker-cache with cron regularly (for example, once a week). Maintain some recent containers in the cache for performance while you reclaim disk space.\n\nThe FILTER_FLAG environment variable controls which objects are pruned. For example usage, see the Docker image prune documentation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 634
        }
      },
      {
        "header": "Clear Docker build images",
        "content": "The clear-docker-cache script does not remove Docker images because they are not tagged by the GitLab Runner.\n\nTo clear Docker build images:\n\nConfirm what disk space can be reclaimed: clear-docker-cache space Show docker disk usage ---------------------- TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 14 9 1.306GB 545.8MB ( 41% ) Containers 19 18 115kB 0B ( 0% ) Local Volumes 0 0 0B 0B Build Cache 0 0 0B 0B To remove all unused containers, networks, images (dangling and unreferenced), and untagged volumes, run docker system prune .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 531
        }
      },
      {
        "header": "Persistent storage",
        "content": "The Docker executor provides persistent storage when it runs containers. All directories defined in volumes = are persistent between builds.\n\nThe volumes directive supports the following types of storage:\n\nFor dynamic storage, use <path> . The <path> is persistent between subsequent runs of the same concurrent job for that project. If you don’t set runners.docker.cache_dir , the data persists in Docker volumes. Otherwise, it persists in the configured directory on the host (mounted into the build container). Volume names for volume-based persistent storage: For GitLab Runner before 18.4.0: runner-<short-token>-project-<project-id>-concurrent-<concurrency-id>-cache-<md5-of-path> For GitLab Runner 18.4.0 and later: runner-<runner-id-hash>-cache-<md5-of-path><protection> Data that is no longer human readable in the volume name is moved to the volume’s labels. Host directories for host-based persistent storage: For GitLab Runner before 18.4.0: <cache-dir>/runner-<short-token>-project-<project-id>-concurrent-<concurrency-id>/<md5-of-path> For GitLab Runner 18.4.0 and later: <cache-dir>/runner-<runner-id-hash>/<md5-of-path><protection> Description of the variable parts: <short-token> : The shortened version of the runner’s token (first 8 letters) <project-id> : The ID of the GitLab project <concurrency-id> : The index of the runner (from the list of all runners running a build for the same project concurrently) <md5-of-path> : The MD5 sum of the path within the container <runner-id-hash> : The hash for the following data: Runner’s token Runner’s system ID <project-id> <concurrency-id> <protection> : The value is empty for builds on unprotected branches, and -protected for protected branch builds <cache-dir> : The configuration in runners.docker.cache_dir For GitLab Runner before 18.4.0: runner-<short-token>-project-<project-id>-concurrent-<concurrency-id>-cache-<md5-of-path> For GitLab Runner 18.4.0 and later: runner-<runner-id-hash>-cache-<md5-of-path><protection> Data that is no longer human readable in the volume name is moved to the volume’s labels. For GitLab Runner before 18.4.0: <cache-dir>/runner-<short-token>-project-<project-id>-concurrent-<concurrency-id>/<md5-of-path> For GitLab Runner 18.4.0 and later: <cache-dir>/runner-<runner-id-hash>/<md5-of-path><protection> <short-token> : The shortened version of the runner’s token (first 8 letters) <project-id> : The ID of the GitLab project <concurrency-id> : The index of the runner (from the list of all runners running a build for the same project concurrently) <md5-of-path> : The MD5 sum of the path within the container <runner-id-hash> : The hash for the following data: Runner’s token Runner’s system ID <project-id> <concurrency-id> Runner’s token Runner’s system ID <project-id> <concurrency-id> <protection> : The value is empty for builds on unprotected branches, and -protected for protected branch builds <cache-dir> : The configuration in runners.docker.cache_dir For host-bound storage, use <host-path>:<path>[:<mode>] . GitLab Runner binds the <path> to <host-path> on the host system. The optional <mode> specifies whether this storage is read-only or read-write (default).\n\nWith GitLab Runner 18.4.0, the naming of sources for dynamic storage (see above) changed for both Docker volume-based and host directory-based persistent storage. When you upgrade to 18.4.0, GitLab Runner ignores the cached data from previous runner versions and creates new dynamic storage on-demand, either through new Docker volumes or new host directories. Host-bound storage (with a <host-path> configuration), in contrast to dynamic storage, is not affected.\n\nPersistent storage for builds\n\nIf you make the /builds directory a host-bound storage, your builds are stored in: /builds/<short-token>/<concurrent-id>/<namespace>/<project-name> , where:\n\n<short-token> is a shortened version of the Runner’s token (first 8 letters). <concurrent-id> is a unique number that identifies the local job ID of the particular runner in context of the project.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 4037
        }
      },
      {
        "header": "Persistent storage for builds",
        "content": "If you make the /builds directory a host-bound storage, your builds are stored in: /builds/<short-token>/<concurrent-id>/<namespace>/<project-name> , where:\n\n<short-token> is a shortened version of the Runner’s token (first 8 letters). <concurrent-id> is a unique number that identifies the local job ID of the particular runner in context of the project.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 355
        }
      },
      {
        "header": "IPC mode",
        "content": "The Docker executor supports sharing the IPC namespace of containers with other locations. This maps to the docker run --ipc flag. More details on IPC settings in Docker documentation",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 183
        }
      },
      {
        "header": "Privileged mode",
        "content": "The Docker executor supports several options that allows fine-tuning of the build container. One of these options is the privileged mode .\n\nUse Docker-in-Docker with privileged mode\n\nThe configured privileged flag is passed to the build container and all services. With this flag, you can use the Docker-in-Docker approach.\n\nFirst, configure your runner ( config.toml ) to run in privileged mode:\n\n[[ runners ]] executor = \"docker\" [ runners . docker ] privileged = true\n\nThen, make your build script ( .gitlab-ci.yml ) to use Docker-in-Docker container:\n\nimage : docker:git services : - docker:dind build : script : - docker build -t my-image . - docker push my-image\n\nContainers that run in privileged mode have security risks. When your containers run in privileged mode, you disable the container security mechanisms and expose your host to privilege escalation. Running containers in privileged mode can lead to container breakout. For more information, see the Docker documentation about runtime privilege and Linux capabilities .\n\nYou might need to configure Docker in Docker with TLS, or disable TLS to avoid an error similar to the following:\n\nCannot connect to the Docker daemon at tcp://docker:2375. Is the docker daemon running?\n\nUse rootless Docker-in-Docker with restricted privileged mode\n\nIn this version, only Docker-in-Docker rootless images are allowed to run as services in privileged mode.\n\nThe services_privileged and allowed_privileged_services configuration parameters limit which containers are allowed to run in privileged mode.\n\nTo use rootless Docker-in-Docker with restricted privileged mode:\n\nIn the config.toml , configure the runner to use services_privileged and allowed_privileged_services : [[ runners ]] executor = \"docker\" [ runners . docker ] services_privileged = true allowed_privileged_services = [ \"docker.io/library/docker:*-dind-rootless\" , \"docker.io/library/docker:dind-rootless\" , \"docker:*-dind-rootless\" , \"docker:dind-rootless\" ] In .gitlab-ci.yml , edit your build script to use Docker-in-Docker rootless container: image : docker:git services : - docker:dind-rootless build : script : - docker build -t my-image . - docker push my-image\n\nOnly the Docker-in-Docker rootless images you list in allowed_privileged_services are allowed to run in privileged mode. All other containers for jobs and services run in unprivileged mode.\n\nBecause they run as non-root, it’s almost safe to use with privileged mode images like Docker-in-Docker rootless or BuildKit rootless.\n\nFor more information about security issues, see Security risks for Docker executors .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 2602
        }
      },
      {
        "header": "Use Docker-in-Docker with privileged mode",
        "content": "The configured privileged flag is passed to the build container and all services. With this flag, you can use the Docker-in-Docker approach.\n\nFirst, configure your runner ( config.toml ) to run in privileged mode:\n\n[[ runners ]] executor = \"docker\" [ runners . docker ] privileged = true\n\nThen, make your build script ( .gitlab-ci.yml ) to use Docker-in-Docker container:\n\nimage : docker:git services : - docker:dind build : script : - docker build -t my-image . - docker push my-image\n\nContainers that run in privileged mode have security risks. When your containers run in privileged mode, you disable the container security mechanisms and expose your host to privilege escalation. Running containers in privileged mode can lead to container breakout. For more information, see the Docker documentation about runtime privilege and Linux capabilities .\n\nYou might need to configure Docker in Docker with TLS, or disable TLS to avoid an error similar to the following:\n\nCannot connect to the Docker daemon at tcp://docker:2375. Is the docker daemon running?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1057
        }
      },
      {
        "header": "Use rootless Docker-in-Docker with restricted privileged mode",
        "content": "In this version, only Docker-in-Docker rootless images are allowed to run as services in privileged mode.\n\nThe services_privileged and allowed_privileged_services configuration parameters limit which containers are allowed to run in privileged mode.\n\nTo use rootless Docker-in-Docker with restricted privileged mode:\n\nIn the config.toml , configure the runner to use services_privileged and allowed_privileged_services : [[ runners ]] executor = \"docker\" [ runners . docker ] services_privileged = true allowed_privileged_services = [ \"docker.io/library/docker:*-dind-rootless\" , \"docker.io/library/docker:dind-rootless\" , \"docker:*-dind-rootless\" , \"docker:dind-rootless\" ] In .gitlab-ci.yml , edit your build script to use Docker-in-Docker rootless container: image : docker:git services : - docker:dind-rootless build : script : - docker build -t my-image . - docker push my-image\n\nOnly the Docker-in-Docker rootless images you list in allowed_privileged_services are allowed to run in privileged mode. All other containers for jobs and services run in unprivileged mode.\n\nBecause they run as non-root, it’s almost safe to use with privileged mode images like Docker-in-Docker rootless or BuildKit rootless.\n\nFor more information about security issues, see Security risks for Docker executors .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1297
        }
      },
      {
        "header": "Configure a Docker ENTRYPOINT",
        "content": "By default, the Docker executor doesn’t override the ENTRYPOINT of a Docker image . It passes sh or bash as COMMAND to start a container that runs the job script.\n\nTo ensure a job can run, its Docker image must:\n\nProvide sh or bash and grep Define an ENTRYPOINT that starts a shell when passed sh / bash as argument\n\nThe Docker Executor runs the job’s container with an equivalent of the following command:\n\ndocker run <image> sh -c \"echo 'It works!'\" # or bash\n\nIf your Docker image doesn’t support this mechanism, you can override the image’s ENTRYPOINT in the project configuration as follows:\n\n# Equivalent of # docker run --entrypoint \"\" <image> sh -c \"echo 'It works!'\" image : name : my-image entrypoint : [ \"\" ]\n\nFor more information, see Override the Entrypoint of an image and How CMD and ENTRYPOINT interact in Docker .\n\nJob script as ENTRYPOINT\n\nYou can use ENTRYPOINT to create a Docker image that runs the build script in a custom environment, or in secure mode.\n\nFor example, you can create a Docker image that uses an ENTRYPOINT that doesn’t execute the build script. Instead, the Docker image executes a predefined set of commands to build the Docker image from your directory. You run the build container in privileged mode , and secure the build environment of the runner.\n\nCreate a new Dockerfile: FROM docker:dind ADD / /entrypoint.sh ENTRYPOINT [ \"/bin/sh\" , \"/entrypoint.sh\" ] Create a bash script ( entrypoint.sh ) that is used as the ENTRYPOINT : #!/bin/sh dind docker daemon --host = unix:///var/run/docker.sock \\ --host = tcp://0.0.0.0:2375 \\ --storage-driver = vf & docker build -t \" $BUILD_IMAGE \" . docker push \" $BUILD_IMAGE \" Push the image to the Docker registry. Run Docker executor in privileged mode. In config.toml define: [[ runners ]] executor = \"docker\" [ runners . docker ] privileged = true In your project use the following .gitlab-ci.yml : variables : BUILD_IMAGE : my.image build : image : my/docker-build:image script : - Dummy Script",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1980
        }
      },
      {
        "header": "Job script as ENTRYPOINT",
        "content": "You can use ENTRYPOINT to create a Docker image that runs the build script in a custom environment, or in secure mode.\n\nFor example, you can create a Docker image that uses an ENTRYPOINT that doesn’t execute the build script. Instead, the Docker image executes a predefined set of commands to build the Docker image from your directory. You run the build container in privileged mode , and secure the build environment of the runner.\n\nCreate a new Dockerfile: FROM docker:dind ADD / /entrypoint.sh ENTRYPOINT [ \"/bin/sh\" , \"/entrypoint.sh\" ] Create a bash script ( entrypoint.sh ) that is used as the ENTRYPOINT : #!/bin/sh dind docker daemon --host = unix:///var/run/docker.sock \\ --host = tcp://0.0.0.0:2375 \\ --storage-driver = vf & docker build -t \" $BUILD_IMAGE \" . docker push \" $BUILD_IMAGE \" Push the image to the Docker registry. Run Docker executor in privileged mode. In config.toml define: [[ runners ]] executor = \"docker\" [ runners . docker ] privileged = true In your project use the following .gitlab-ci.yml : variables : BUILD_IMAGE : my.image build : image : my/docker-build:image script : - Dummy Script",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1122
        }
      },
      {
        "header": "Use Podman to run Docker commands",
        "content": "History Introduced in GitLab 15.3.\n\nIf you have GitLab Runner installed on Linux, your jobs can use Podman to replace Docker as the container runtime in the Docker executor.\n\nPrerequisites:\n\nPodman v4.2.0 or later. To run services with Podman as an executor, enable the FF_NETWORK_PER_BUILD feature flag . Docker container links are legacy and are not supported by Podman . For services that create a network alias, you must install the podman-plugins package.\n\nPodman uses aardvark-dns as the DNS server for containers. The aardvark-dns versions 1.10.0 and earlier cause sporadic DNS resolution failures in CI/CD jobs. Make sure that you have installed a newer version. For more information, see GitHub issue 389 .\n\nOn your Linux host, install GitLab Runner. If you installed GitLab Runner by using your system’s package manager, it automatically creates a gitlab-runner user. Sign in as the user who runs GitLab Runner. You must do so in a way that doesn’t go around pam_systemd . You can use SSH with the correct user. This ensures you can run systemctl as this user. Make sure that your system fulfills the prerequisites for a rootless Podman setup . Specifically, make sure your user has correct entries in /etc/subuid and /etc/subgid . On the Linux host, install Podman . Enable and start the Podman socket: systemctl --user --now enable podman.socket Verify the Podman socket is listening: systemctl status --user podman.socket Copy the socket string in the Listen key through which the Podman API is being accessed. Make sure the Podman socket remains available after the GitLab Runner user is logged out: sudo loginctl enable-linger gitlab-runner Edit the GitLab Runner config.toml file and add the socket value to the host entry in the [runners.docker] section. For example: [[ runners ]] name = \"podman-test-runner-2025-06-07\" url = \"https://gitlab.com\" token = \"TOKEN\" executor = \"docker\" [ runners . docker ] host = \"unix:///run/user/1012/podman/podman.sock\" tls_verify = false image = \"quay.io/podman/stable\" privileged = true\n\nUse Podman to build container images from a Dockerfile\n\nThe following example uses Podman to build a container image and push the image to the GitLab Container registry.\n\nThe default container image in the Runner config.toml is set to quay.io/podman/stable , so that the CI job uses that image to execute the included commands.\n\nvariables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG before_script : - podman login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY oci-container-build : stage : build script : - podman build -t $IMAGE_TAG . - podman push $IMAGE_TAG when : manual\n\nUse Buildah to build container images from a Dockerfile\n\nThe following example shows how to use Buildah to build a container image and push the image to the GitLab Container registry.\n\nimage : quay.io/buildah/stable variables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG before_script : - buildah login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY oci-container-build : stage : build script : - buildah bud -t $IMAGE_TAG . - buildah push $IMAGE_TAG when : manual\n\nKnown issues\n\nUnlike Docker, Podman enforces SELinux policies by default. While many pipelines run without issues, some may fail due to SELinux context inheritance when tools use temporary directories.\n\nFor example, the following pipeline fails under Podman:\n\ntesting : image : alpine:3.20 script : - apk add --no-cache python3 py3-pip - pip3 install --target $CI_PROJECT_DIR requests==2.28.2\n\nThe failure occurs because pip uses /tmp as a working directory. Files created in /tmp inherit its SELinux context, which prevents the container from modifying these files when they’re moved to $CI_PROJECT_DIR .\n\nSolution: Add /tmp to the volumes in the runner’s config.toml under the runners.docker section:\n\n[[ runners ]] [ runners . docker ] volumes = [ \"/cache\" , \"/tmp\" ]\n\nThis addition ensures consistent SELinux contexts across the mounted directories.\n\nTroubleshooting SELinux Issues\n\nOther Podman/SELinux issues may require additional troubleshooting to identify the necessary configuration changes.\n\nTo test whether a Podman runner issue is SELinux-related, temporarily add the following directive to the runner’s config.toml under the runners.docker section:\n\n[[ runners ]] [ runners . docker ] security_opt = [ \"label:disable\" ]\n\nThis addition turns off SELinux enforcement in the container (which is Docker’s default behavior). Use this configuration only for testing purposes and not as a permanent solution because it can have security implications.\n\nConfigure SELinux MCS\n\nIf SELinux blocks some write operations (such as reinitializing an existing Git repository), you can force a Multi-Category Security (MCS) on all containers launched by the runner:\n\n[[ runners ]] [ runners . docker ] security_opt = [ \"label=level:s0:c1000\" ]\n\nThis option does not disable SELinux, but sets the container’s MCS level. This approach is more secure than using label:disable .\n\nMultiple containers that use the same MCS category can access the same files tagged with that category.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 5111
        }
      },
      {
        "header": "Use Podman to build container images from a Dockerfile",
        "content": "The following example uses Podman to build a container image and push the image to the GitLab Container registry.\n\nThe default container image in the Runner config.toml is set to quay.io/podman/stable , so that the CI job uses that image to execute the included commands.\n\nvariables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG before_script : - podman login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY oci-container-build : stage : build script : - podman build -t $IMAGE_TAG . - podman push $IMAGE_TAG when : manual",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 544
        }
      },
      {
        "header": "Use Buildah to build container images from a Dockerfile",
        "content": "The following example shows how to use Buildah to build a container image and push the image to the GitLab Container registry.\n\nimage : quay.io/buildah/stable variables : IMAGE_TAG : $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG before_script : - buildah login -u \"$CI_REGISTRY_USER\" -p \"$CI_REGISTRY_PASSWORD\" $CI_REGISTRY oci-container-build : stage : build script : - buildah bud -t $IMAGE_TAG . - buildah push $IMAGE_TAG when : manual",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 431
        }
      },
      {
        "header": "Known issues",
        "content": "Unlike Docker, Podman enforces SELinux policies by default. While many pipelines run without issues, some may fail due to SELinux context inheritance when tools use temporary directories.\n\nFor example, the following pipeline fails under Podman:\n\ntesting : image : alpine:3.20 script : - apk add --no-cache python3 py3-pip - pip3 install --target $CI_PROJECT_DIR requests==2.28.2\n\nThe failure occurs because pip uses /tmp as a working directory. Files created in /tmp inherit its SELinux context, which prevents the container from modifying these files when they’re moved to $CI_PROJECT_DIR .\n\nSolution: Add /tmp to the volumes in the runner’s config.toml under the runners.docker section:\n\n[[ runners ]] [ runners . docker ] volumes = [ \"/cache\" , \"/tmp\" ]\n\nThis addition ensures consistent SELinux contexts across the mounted directories.\n\nTroubleshooting SELinux Issues\n\nOther Podman/SELinux issues may require additional troubleshooting to identify the necessary configuration changes.\n\nTo test whether a Podman runner issue is SELinux-related, temporarily add the following directive to the runner’s config.toml under the runners.docker section:\n\n[[ runners ]] [ runners . docker ] security_opt = [ \"label:disable\" ]\n\nThis addition turns off SELinux enforcement in the container (which is Docker’s default behavior). Use this configuration only for testing purposes and not as a permanent solution because it can have security implications.\n\nConfigure SELinux MCS\n\nIf SELinux blocks some write operations (such as reinitializing an existing Git repository), you can force a Multi-Category Security (MCS) on all containers launched by the runner:\n\n[[ runners ]] [ runners . docker ] security_opt = [ \"label=level:s0:c1000\" ]\n\nThis option does not disable SELinux, but sets the container’s MCS level. This approach is more secure than using label:disable .\n\nMultiple containers that use the same MCS category can access the same files tagged with that category.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1963
        }
      },
      {
        "header": "Troubleshooting SELinux Issues",
        "content": "Other Podman/SELinux issues may require additional troubleshooting to identify the necessary configuration changes.\n\nTo test whether a Podman runner issue is SELinux-related, temporarily add the following directive to the runner’s config.toml under the runners.docker section:\n\n[[ runners ]] [ runners . docker ] security_opt = [ \"label:disable\" ]\n\nThis addition turns off SELinux enforcement in the container (which is Docker’s default behavior). Use this configuration only for testing purposes and not as a permanent solution because it can have security implications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 571
        }
      },
      {
        "header": "Configure SELinux MCS",
        "content": "If SELinux blocks some write operations (such as reinitializing an existing Git repository), you can force a Multi-Category Security (MCS) on all containers launched by the runner:\n\n[[ runners ]] [ runners . docker ] security_opt = [ \"label=level:s0:c1000\" ]\n\nThis option does not disable SELinux, but sets the container’s MCS level. This approach is more secure than using label:disable .\n\nMultiple containers that use the same MCS category can access the same files tagged with that category.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 494
        }
      },
      {
        "header": "Specify which user runs the job",
        "content": "By default, the runner runs jobs as the root user in the container. To specify a different, non-root user to run the job, use the USER directive in the Dockerfile of the Docker image.\n\nFROM amazonlinux RUN [ \"yum\" , \"install\" , \"-y\" , \"nginx\" ] RUN [ \"useradd\" , \"www\" ] USER \"www \" CMD [ \"/bin/bash\" ]\n\nWhen you use that Docker image to execute your job, it runs as the specified user:\n\nbuild : image : my/docker-build:image script : - whoami # www",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 449
        }
      },
      {
        "header": "Configure how runners pull images",
        "content": "Configure the pull policy in the config.toml to define how runners pull Docker images from registries. You can set a single policy, a list of policies , or allow specific pull policies .\n\nUse the following values for the pull_policy :\n\nalways : Default. Pull an image even if a local image exists. This pull policy does not apply to images specified by their SHA256 that already exist on disk. if-not-present : Pull an image only when a local version does not exist. never : Never pull an image and use only local images.\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = \"always\" # available: always, if-not-present, never\n\nSet the always pull policy\n\nThe always option, which is on by default, always initiates a pull before creating the container. This option makes sure the image is up-to-date, and prevents you from using outdated images even if a local image exists.\n\nUse this pull policy if:\n\nRunners must always pull the most recent images. Runners are publicly available and configured for auto-scale or as an instance runner in your GitLab instance.\n\nDo not use this policy if runners must use locally stored images.\n\nSet always as the pull policy in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = \"always\"\n\nSet the if-not-present pull policy\n\nWhen you set the pull policy to if-not-present , the runner first checks if a local image exists. If there is no local image, the runner pulls an image from the registry.\n\nUse the if-not-present policy to:\n\nUse local images but also pull images if a local image does not exist. Reduce time that runners analyze the difference in image layers for heavy and rarely updated images. In this case, you must manually remove the image regularly from the local Docker Engine store to force the image update.\n\nDo not use this policy:\n\nFor instance runners where different users that use the runner may have access to private images. For more information about security issues, see Usage of private Docker images with if-not-present pull policy . If jobs are frequently updated and must be run in the most recent image version. This may result in a network load reduction that outweighs the value of frequent deletion of local images.\n\nSet the if-not-present policy in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = \"if-not-present\"\n\nSet the never pull policy\n\nPrerequisites:\n\nLocal images must contain an installed Docker Engine and a local copy of used images.\n\nWhen you set the pull policy to never , image pulling is disabled. Users can only use images that have been manually pulled on the Docker host where the runner runs.\n\nUse the never pull policy:\n\nTo control the images used by runner users. For private runners that are dedicated to a project that can only use specific images that are not publicly available on any registries.\n\nDo not use the never pull policy for auto-scaled Docker executors. The never pull policy is usable only when using a pre-defined cloud instance images for chosen cloud provider.\n\nSet the never policy in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = \"never\"\n\nSet multiple pull policies\n\nYou can list multiple pull policies to execute if a pull fails. The runner processes pull policies in the order listed until a pull attempt is successful or the list is exhausted. For example, if a runner uses the always pull policy and the registry is not available, you can add the if-not-present as a second pull policy. This configuration lets the runner use a locally cached Docker image.\n\nFor information about the security implications of this pull policy, see Usage of private Docker images with if-not-present pull policy .\n\nTo set multiple pull policies, add them as a list in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = [ \"always\" , \"if-not-present\" ]\n\nAllow Docker pull policies\n\nHistory Introduced in GitLab 15.1.\n\nIn the .gitlab-ci.yml file, you can specify a pull policy. This policy determines how a CI/CD job fetches images.\n\nTo restrict which pull policies can be used from those specified in the .gitlab-ci.yml file, use allowed_pull_policies .\n\nFor example, to allow only the always and if-not-present pull policies, add them to the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) allowed_pull_policies = [ \"always\" , \"if-not-present\" ]\n\nIf you don’t specify allowed_pull_policies , the list matches the values specified in the pull_policy keyword. If you don’t specify pull_policy , the default is always . The job uses only the pull policies that are listed in both pull_policy and allowed_pull_policies . The effective pull policy is determined by comparing the policies specified in pull_policy keyword and allowed_pull_policies . GitLab uses the intersection of these two policy lists. For example, if pull_policy is [\"always\", \"if-not-present\"] and allowed_pull_policies is [\"if-not-present\"] , then the job uses only if-not-present because it’s the only pull policy defined in both lists. The existing pull_policy keyword must include at least one pull policy specified in allowed_pull_policies . The job fails if none of the pull_policy values match allowed_pull_policies .\n\nImage pull error messages\n\nError message Description Pulling docker image registry.tld/my/image:latest ... ERROR: Build failed: Error: image registry.tld/my/image:latest not found The runner cannot find the image. Displays when the always pull policy is set Pulling docker image local_image:latest ... ERROR: Build failed: Error: image local_image:latest not found The image was built locally and doesn’t exist in any public or default Docker registry. Displays when the always pull policy is set. Pulling docker image registry.tld/my/image:latest ... WARNING: Cannot pull the latest version of image registry.tld/my/image:latest : Error: image registry.tld/my/image:latest not found WARNING: Locally found image will be used instead. The runner has used a local image instead of pulling an image. Pulling docker image local_image:latest ... ERROR: Build failed: Error: image local_image:latest not found The image cannot be found locally. Displays when the never pull policy is set. WARNING: Failed to pull image with policy \"always\": Error response from daemon: received unexpected HTTP status: 502 Bad Gateway (docker.go:143:0s) Attempt #2: Trying \"if-not-present\" pull policy Using locally found image version due to \"if-not-present\" pull policy The runner failed to pull an image and attempts to pull an image by using the next listed pull policy. Displays when multiple pull policies are set.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 21,
          "content_length": 6778
        }
      },
      {
        "header": "Set the always pull policy",
        "content": "The always option, which is on by default, always initiates a pull before creating the container. This option makes sure the image is up-to-date, and prevents you from using outdated images even if a local image exists.\n\nUse this pull policy if:\n\nRunners must always pull the most recent images. Runners are publicly available and configured for auto-scale or as an instance runner in your GitLab instance.\n\nDo not use this policy if runners must use locally stored images.\n\nSet always as the pull policy in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = \"always\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 620
        }
      },
      {
        "header": "Set the if-not-present pull policy",
        "content": "When you set the pull policy to if-not-present , the runner first checks if a local image exists. If there is no local image, the runner pulls an image from the registry.\n\nUse the if-not-present policy to:\n\nUse local images but also pull images if a local image does not exist. Reduce time that runners analyze the difference in image layers for heavy and rarely updated images. In this case, you must manually remove the image regularly from the local Docker Engine store to force the image update.\n\nDo not use this policy:\n\nFor instance runners where different users that use the runner may have access to private images. For more information about security issues, see Usage of private Docker images with if-not-present pull policy . If jobs are frequently updated and must be run in the most recent image version. This may result in a network load reduction that outweighs the value of frequent deletion of local images.\n\nSet the if-not-present policy in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = \"if-not-present\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1079
        }
      },
      {
        "header": "Set the never pull policy",
        "content": "Prerequisites:\n\nLocal images must contain an installed Docker Engine and a local copy of used images.\n\nWhen you set the pull policy to never , image pulling is disabled. Users can only use images that have been manually pulled on the Docker host where the runner runs.\n\nUse the never pull policy:\n\nTo control the images used by runner users. For private runners that are dedicated to a project that can only use specific images that are not publicly available on any registries.\n\nDo not use the never pull policy for auto-scaled Docker executors. The never pull policy is usable only when using a pre-defined cloud instance images for chosen cloud provider.\n\nSet the never policy in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = \"never\"",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 794
        }
      },
      {
        "header": "Set multiple pull policies",
        "content": "You can list multiple pull policies to execute if a pull fails. The runner processes pull policies in the order listed until a pull attempt is successful or the list is exhausted. For example, if a runner uses the always pull policy and the registry is not available, you can add the if-not-present as a second pull policy. This configuration lets the runner use a locally cached Docker image.\n\nFor information about the security implications of this pull policy, see Usage of private Docker images with if-not-present pull policy .\n\nTo set multiple pull policies, add them as a list in the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) pull_policy = [ \"always\" , \"if-not-present\" ]",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 722
        }
      },
      {
        "header": "Allow Docker pull policies",
        "content": "History Introduced in GitLab 15.1.\n\nIn the .gitlab-ci.yml file, you can specify a pull policy. This policy determines how a CI/CD job fetches images.\n\nTo restrict which pull policies can be used from those specified in the .gitlab-ci.yml file, use allowed_pull_policies .\n\nFor example, to allow only the always and if-not-present pull policies, add them to the config.toml :\n\n[[ runners ]] ( ... ) executor = \"docker\" [ runners . docker ] ( ... ) allowed_pull_policies = [ \"always\" , \"if-not-present\" ]\n\nIf you don’t specify allowed_pull_policies , the list matches the values specified in the pull_policy keyword. If you don’t specify pull_policy , the default is always . The job uses only the pull policies that are listed in both pull_policy and allowed_pull_policies . The effective pull policy is determined by comparing the policies specified in pull_policy keyword and allowed_pull_policies . GitLab uses the intersection of these two policy lists. For example, if pull_policy is [\"always\", \"if-not-present\"] and allowed_pull_policies is [\"if-not-present\"] , then the job uses only if-not-present because it’s the only pull policy defined in both lists. The existing pull_policy keyword must include at least one pull policy specified in allowed_pull_policies . The job fails if none of the pull_policy values match allowed_pull_policies .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1347
        }
      },
      {
        "header": "Image pull error messages",
        "content": "Error message Description Pulling docker image registry.tld/my/image:latest ... ERROR: Build failed: Error: image registry.tld/my/image:latest not found The runner cannot find the image. Displays when the always pull policy is set Pulling docker image local_image:latest ... ERROR: Build failed: Error: image local_image:latest not found The image was built locally and doesn’t exist in any public or default Docker registry. Displays when the always pull policy is set. Pulling docker image registry.tld/my/image:latest ... WARNING: Cannot pull the latest version of image registry.tld/my/image:latest : Error: image registry.tld/my/image:latest not found WARNING: Locally found image will be used instead. The runner has used a local image instead of pulling an image. Pulling docker image local_image:latest ... ERROR: Build failed: Error: image local_image:latest not found The image cannot be found locally. Displays when the never pull policy is set. WARNING: Failed to pull image with policy \"always\": Error response from daemon: received unexpected HTTP status: 502 Bad Gateway (docker.go:143:0s) Attempt #2: Trying \"if-not-present\" pull policy Using locally found image version due to \"if-not-present\" pull policy The runner failed to pull an image and attempts to pull an image by using the next listed pull policy. Displays when multiple pull policies are set.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 0,
          "content_length": 1371
        }
      },
      {
        "header": "Retry a failed pull",
        "content": "To configure a runner to retry a failed image pull, specify the same policy more than once in the config.toml .\n\nFor example, this configuration retries the pull one time:\n\n[ runners . docker ] pull_policy = [ \"always\" , \"always\" ]\n\nThis setting is similar to the retry directive in the .gitlab-ci.yml files of individual projects, but only takes effect if specifically the Docker pull fails initially.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 402
        }
      },
      {
        "header": "Use Windows containers",
        "content": "To use Windows containers with the Docker executor, note the following information about limitations, supported Windows versions, and configuring a Windows Docker executor.\n\nNanoserver support\n\nWith the support for PowerShell Core introduced in the Windows helper image, it is now possible to leverage the nanoserver variants for the helper image.\n\nKnown issues with Docker executor on Windows\n\nThe following are some limitations of using Windows containers with Docker executor:\n\nDocker-in-Docker is not supported, because it’s not supported by Docker itself. Interactive web terminals are not supported. Host device mounting not supported. When mounting a volume directory it has to exist, or Docker fails to start the container, see #3754 for additional detail. docker-windows executor can be run only using GitLab Runner running on Windows. Linux containers on Windows are not supported, because they are still experimental. Read the relevant issue for more details. Because of a limitation in Docker , if the destination path drive letter is not c: , paths are not supported for: builds_dir cache_dir volumes This means values such as f:\\\\cache_dir are not supported, but f: is supported. However, if the destination path is on the c: drive, paths are also supported (for example c:\\\\cache_dir ). To configure where the Docker daemon keeps images and containers, update the data-root parameter in the daemon.json file of the Docker daemon. For more information, see Configure Docker with a configuration file . builds_dir cache_dir volumes\n\nSupported Windows versions\n\nGitLab Runner only supports the following versions of Windows which follows our support lifecycle for Windows :\n\nWindows Server 2022 LTSC (21H2) Windows Server 2019 LTSC (1809)\n\nFor future Windows Server versions, we have a future version support policy .\n\nYou can only run containers based on the same OS version that the Docker daemon is running on. For example, the following Windows Server Core images can be used:\n\nmcr.microsoft.com/windows/servercore:ltsc2022 mcr.microsoft.com/windows/servercore:ltsc2022-amd64 mcr.microsoft.com/windows/servercore:1809 mcr.microsoft.com/windows/servercore:1809-amd64 mcr.microsoft.com/windows/servercore:ltsc2019\n\nSupported Docker versions\n\nGitLab Runner uses Docker to detect what version of Windows Server is running. Hence, a Windows Server running GitLab Runner must be running a recent version of Docker.\n\nA known version of Docker that doesn’t work with GitLab Runner is Docker 17.06 . Docker does not identify the version of Windows Server resulting in the following error:\n\nunsupported Windows Version: Windows Server Datacenter\n\nRead more about troubleshooting this .\n\nConfigure a Windows Docker executor\n\nWhen a runner is registered with c:\\\\cache as a source directory when passing the --docker-volumes or DOCKER_VOLUMES environment variable, there is a known issue .\n\nBelow is an example of the configuration for a Docker executor running Windows.\n\n[[ runners ]] name = \"windows-docker-2019\" url = \"https://gitlab.com/\" token = \"xxxxxxx\" executor = \"docker-windows\" [ runners . docker ] image = \"mcr.microsoft.com/windows/servercore:1809_amd64\" volumes = [ \"c:\\\\cache\" ]\n\nFor other configuration options for the Docker executor, see the advanced configuration section.\n\nServices\n\nYou can use services by enabling a network for each job .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 3364
        }
      },
      {
        "header": "Nanoserver support",
        "content": "With the support for PowerShell Core introduced in the Windows helper image, it is now possible to leverage the nanoserver variants for the helper image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 153
        }
      },
      {
        "header": "Known issues with Docker executor on Windows",
        "content": "The following are some limitations of using Windows containers with Docker executor:\n\nDocker-in-Docker is not supported, because it’s not supported by Docker itself. Interactive web terminals are not supported. Host device mounting not supported. When mounting a volume directory it has to exist, or Docker fails to start the container, see #3754 for additional detail. docker-windows executor can be run only using GitLab Runner running on Windows. Linux containers on Windows are not supported, because they are still experimental. Read the relevant issue for more details. Because of a limitation in Docker , if the destination path drive letter is not c: , paths are not supported for: builds_dir cache_dir volumes This means values such as f:\\\\cache_dir are not supported, but f: is supported. However, if the destination path is on the c: drive, paths are also supported (for example c:\\\\cache_dir ). To configure where the Docker daemon keeps images and containers, update the data-root parameter in the daemon.json file of the Docker daemon. For more information, see Configure Docker with a configuration file . builds_dir cache_dir volumes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1149
        }
      },
      {
        "header": "Supported Windows versions",
        "content": "GitLab Runner only supports the following versions of Windows which follows our support lifecycle for Windows :\n\nWindows Server 2022 LTSC (21H2) Windows Server 2019 LTSC (1809)\n\nFor future Windows Server versions, we have a future version support policy .\n\nYou can only run containers based on the same OS version that the Docker daemon is running on. For example, the following Windows Server Core images can be used:\n\nmcr.microsoft.com/windows/servercore:ltsc2022 mcr.microsoft.com/windows/servercore:ltsc2022-amd64 mcr.microsoft.com/windows/servercore:1809 mcr.microsoft.com/windows/servercore:1809-amd64 mcr.microsoft.com/windows/servercore:ltsc2019",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 653
        }
      },
      {
        "header": "Supported Docker versions",
        "content": "GitLab Runner uses Docker to detect what version of Windows Server is running. Hence, a Windows Server running GitLab Runner must be running a recent version of Docker.\n\nA known version of Docker that doesn’t work with GitLab Runner is Docker 17.06 . Docker does not identify the version of Windows Server resulting in the following error:\n\nunsupported Windows Version: Windows Server Datacenter\n\nRead more about troubleshooting this .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 435
        }
      },
      {
        "header": "Configure a Windows Docker executor",
        "content": "When a runner is registered with c:\\\\cache as a source directory when passing the --docker-volumes or DOCKER_VOLUMES environment variable, there is a known issue .\n\nBelow is an example of the configuration for a Docker executor running Windows.\n\n[[ runners ]] name = \"windows-docker-2019\" url = \"https://gitlab.com/\" token = \"xxxxxxx\" executor = \"docker-windows\" [ runners . docker ] image = \"mcr.microsoft.com/windows/servercore:1809_amd64\" volumes = [ \"c:\\\\cache\" ]\n\nFor other configuration options for the Docker executor, see the advanced configuration section.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 565
        }
      },
      {
        "header": "Services",
        "content": "You can use services by enabling a network for each job .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 57
        }
      },
      {
        "header": "Native Step Runner Integration",
        "content": "History Introduced in GitLab 17.6.0 behind the feature-flag FF_USE_NATIVE_STEPS , which is disabled by default. Updated in GitLab 17.9.0. GitLab Runner injects the step-runner binary into the build container and adjusts the $PATH environment variable accordingly. This enhancement makes it possible to use any image as the build image.\n\nThe Docker executor supports running the CI/CD steps natively by using the gRPC API provided by step-runner .\n\nTo enable this mode of execution, you must specify CI/CD jobs using the run keyword instead of the legacy script keyword. Additionally, you must enable the FF_USE_NATIVE_STEPS feature flag. You can enable this feature flag at either the job or pipeline level.\n\nstep job : stage : test variables : FF_USE_NATIVE_STEPS : true image : name : alpine:latest run : - name : step1 script : pwd - name : step2 script : env - name : step3 script : ls -Rlah ../\n\nKnown Issues\n\nIn GitLab 17.9 and later, the build image must have the ca-certificates package installed or the step-runner will fail to pull the steps defined in the job. Debian-based Linux distribution for example do not install ca-certificates by default. In GitLab versions before 17.9, the build image must include a step-runner binary in $PATH . To achieve this, you can either: Create your own custom build image and include the step-runner binary in it. Use the registry.gitlab.com/gitlab-org/step-runner:v0 image if it includes the dependencies you need to run your job. Create your own custom build image and include the step-runner binary in it. Use the registry.gitlab.com/gitlab-org/step-runner:v0 image if it includes the dependencies you need to run your job. Running a step that runs a Docker container must adhere to the same configuration parameters and constraints as traditional scripts . For example, you must use Docker-in-Docker . This mode of execution does not yet support running Github Actions .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1922
        }
      },
      {
        "header": "Known Issues",
        "content": "In GitLab 17.9 and later, the build image must have the ca-certificates package installed or the step-runner will fail to pull the steps defined in the job. Debian-based Linux distribution for example do not install ca-certificates by default. In GitLab versions before 17.9, the build image must include a step-runner binary in $PATH . To achieve this, you can either: Create your own custom build image and include the step-runner binary in it. Use the registry.gitlab.com/gitlab-org/step-runner:v0 image if it includes the dependencies you need to run your job. Create your own custom build image and include the step-runner binary in it. Use the registry.gitlab.com/gitlab-org/step-runner:v0 image if it includes the dependencies you need to run your job. Running a step that runs a Docker container must adhere to the same configuration parameters and constraints as traditional scripts . For example, you must use Docker-in-Docker . This mode of execution does not yet support running Github Actions .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1007
        }
      }
    ],
    "url": "https://docs.gitlab.com/runner/executors/docker/",
    "doc_type": "docker",
    "total_sections": 57
  },
  {
    "title": "GitLab container registry",
    "summary": "You can use the integrated container registry to store container images for each GitLab project. An administrator must enable the container registry for your GitLab instance. For more information, see GitLab container registry administration .",
    "sections": [
      {
        "header": "View the container registry",
        "content": "You can view the container registry for a project or group.\n\nOn the left sidebar, select Search or go to and find your project or group. If you’ve turned on the new navigation , this field is on the top bar. Select Deploy > Container Registry .\n\nYou can search, sort, filter, and delete your container images. You can share a filtered view by copying the URL from your browser.\n\nView the tags of a specific container image in the container registry\n\nYou can use the container registry Tag Details page to view a list of tags associated with a given container image:\n\nOn the left sidebar, select Search or go to and find your project or group. If you’ve turned on the new navigation , this field is on the top bar. Select Deploy > Container Registry . Select your container image.\n\nYou can view details about each tag, such as when it was published, how much storage it consumes, and the manifest and configuration digests.\n\nYou can search, sort (by tag name), and delete tags on this page. You can share a filtered view by copying the URL from your browser.\n\nStorage usage\n\nView container registry storage usage to track and manage the size of your container repositories across projects and groups.\n\nFor more information, see View container registry usage .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1258
        }
      },
      {
        "header": "View the tags of a specific container image in the container registry",
        "content": "You can use the container registry Tag Details page to view a list of tags associated with a given container image:\n\nOn the left sidebar, select Search or go to and find your project or group. If you’ve turned on the new navigation , this field is on the top bar. Select Deploy > Container Registry . Select your container image.\n\nYou can view details about each tag, such as when it was published, how much storage it consumes, and the manifest and configuration digests.\n\nYou can search, sort (by tag name), and delete tags on this page. You can share a filtered view by copying the URL from your browser.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 607
        }
      },
      {
        "header": "Storage usage",
        "content": "View container registry storage usage to track and manage the size of your container repositories across projects and groups.\n\nFor more information, see View container registry usage .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 184
        }
      },
      {
        "header": "Use container images from the container registry",
        "content": "To download and run a container image hosted in the container registry:\n\nOn the left sidebar, select Search or go to and find your project or group. If you’ve turned on the new navigation , this field is on the top bar. Select Deploy > Container Registry . Find the container image you want to work with and select Copy image path （ copy-to-clipboard ）. Use docker run with the copied link: docker run [ options ] registry.example.com/group/project/image [ arguments ]\n\nYou must authenticate with the container registry to download container images from a private repository. For more information, see authenticate with the container registry .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 644
        }
      },
      {
        "header": "Naming convention for your container images",
        "content": "Your container images must follow this naming convention:\n\n<registry server>/<namespace>/<project>[/<optional path>]\n\nFor example, if your project is gitlab.example.com/mynamespace/myproject , then your container image must be named gitlab.example.com/mynamespace/myproject .\n\nYou can append additional names to the end of a container image name, up to two levels deep.\n\nFor example, these are all valid names for container images in the project named myproject :\n\nregistry.example.com/mynamespace/myproject:some-tag\n\nregistry.example.com/mynamespace/myproject/image:latest\n\nregistry.example.com/mynamespace/myproject/my/image:rc1",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 630
        }
      },
      {
        "header": "Move or rename container registry repositories",
        "content": "History Enabled on GitLab.com in GitLab 16.7. The path of a container repository always matches the related project’s repository path, so renaming or moving only the container registry is not possible. Instead, you can either:\n\nRename the project’s repository . Transfer the project .\n\nFor performance reasons, renaming projects with populated container repositories is limited to projects with up to 1,000 container repositories.\n\nOn a GitLab Self-Managed instance, you can delete all container images before moving or renaming a group or project. Alternatively, issue 18383 contains community suggestions to work around this limitation. Epic 9459 proposes adding support for moving projects and groups with container repositories to GitLab Self-Managed.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 755
        }
      },
      {
        "header": "Disable the container registry for a project",
        "content": "The container registry is enabled by default.\n\nYou can, however, remove the container registry for a project:\n\nOn the left sidebar, select Search or go to and find your project. If you’ve turned on the new navigation , this field is on the top bar. Select Settings > General . Expand the Visibility, project features, permissions section and disable Container registry . Select Save changes .\n\nThe Deploy > Container Registry entry is removed from the project’s sidebar.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 470
        }
      },
      {
        "header": "Change visibility of the container registry",
        "content": "By default, the container registry is visible to everyone with access to the project. You can, however, change the visibility of the container registry for a project.\n\nFor more information about the permissions that this setting grants to users, see Container registry visibility permissions .\n\nOn the left sidebar, select Search or go to and find your project. If you’ve turned on the new navigation , this field is on the top bar. Select Settings > General . Expand the section Visibility, project features, permissions . Under Container registry , select an option from the dropdown list: Everyone With Access (Default): The container registry is visible to everyone with access to the project. If the project is public, the container registry is also public. If the project is internal or private, the container registry is also internal or private. Only Project Members : The container registry is visible only to project members with at least the Reporter role. This visibility is similar to the behavior of a private project with Container Registry visibility set to Everyone With Access . Everyone With Access (Default): The container registry is visible to everyone with access to the project. If the project is public, the container registry is also public. If the project is internal or private, the container registry is also internal or private. Only Project Members : The container registry is visible only to project members with at least the Reporter role. This visibility is similar to the behavior of a private project with Container Registry visibility set to Everyone With Access . Select Save changes .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1623
        }
      },
      {
        "header": "Container registry visibility permissions",
        "content": "The ability to view the container registry and pull container images is controlled by the container registry’s visibility permissions. You can change the visibility through the visibility setting on the UI or the API .\n\nOther permissions, such as updating the container registry and pushing or deleting container images, are not affected by this setting. However, disabling the container registry disables all container registry operations. For more information, see Roles and permissions .\n\nAnonymous (Everyone on internet) Guest Reporter, Developer, Maintainer, Owner Public project with container registry visibility set to Everyone With Access (UI) or enabled (API) View container registry and pull images Yes Yes Yes Public project with container registry visibility set to Only Project Members (UI) or private (API) View container registry and pull images No No Yes Internal project with container registry visibility set to Everyone With Access (UI) or enabled (API) View container registry and pull images No Yes Yes Internal project with container registry visibility set to Only Project Members (UI) or private (API) View container registry and pull images No No Yes Private project with container registry visibility set to Everyone With Access (UI) or enabled (API) View container registry and pull images No No Yes Private project with container registry visibility set to Only Project Members (UI) or private (API) View container registry and pull images No No Yes Any project with container registry disabled All operations on container registry No No No",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 1569
        }
      },
      {
        "header": "Supported image types",
        "content": "History OCI conformance introduced in GitLab 16.6.\n\nThe container registry supports the Docker V2 and Open Container Initiative (OCI) image formats. Additionally, the container registry conforms to the OCI distribution specification .\n\nOCI support means that you can host OCI-based image formats in the registry, such as Helm 3+ chart packages. There is no distinction between image formats in the GitLab API and the UI. Issue 38047 addresses this distinction, starting with Helm.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 480
        }
      },
      {
        "header": "Container image signatures",
        "content": "History Container image signature display introduced in GitLab 17.1.\n\nIn the GitLab container registry, you can use the OCI 1.1 manifest subject field to associate container images with Cosign signatures . You can then view signature information alongside its associated container image without having to search for that signature’s tag.\n\nWhen viewing a container image’s tags, you see an icon displayed next to each tag that has an associated signature. To see the details of the signature, select the icon.\n\nPrerequisites:\n\nTo sign container images, Cosign v2.0 or later. For GitLab Self-Managed, you need a GitLab container registry configured with a metadata database to display signatures. For more information, see container registry metadata database .\n\nSign container images with OCI referrer data\n\nTo add referrer data to signatures using Cosign, you must:\n\nSet the COSIGN_EXPERIMENTAL environment variable to 1 . Add --registry-referrers-mode oci-1-1 to the signature command.\n\nFor example:\n\nCOSIGN_EXPERIMENTAL = 1 cosign sign --registry-referrers-mode oci-1-1 <container image>\n\nWhile the GitLab container registry supports the OCI 1.1 manifest subject field, it does not fully implement the OCI 1.1 Referrers API .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1227
        }
      },
      {
        "header": "Sign container images with OCI referrer data",
        "content": "To add referrer data to signatures using Cosign, you must:\n\nSet the COSIGN_EXPERIMENTAL environment variable to 1 . Add --registry-referrers-mode oci-1-1 to the signature command.\n\nFor example:\n\nCOSIGN_EXPERIMENTAL = 1 cosign sign --registry-referrers-mode oci-1-1 <container image>\n\nWhile the GitLab container registry supports the OCI 1.1 manifest subject field, it does not fully implement the OCI 1.1 Referrers API .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 420
        }
      }
    ],
    "url": "https://docs.gitlab.com/user/packages/container_registry/",
    "doc_type": "docker",
    "total_sections": 12
  },
  {
    "title": "Containers",
    "summary": "This page will discuss containers and container images, as well as their use in operations and solution development. The word container is an overloaded term. Whenever you use the word, check whether your audience uses the same definition.",
    "sections": [
      {
        "header": "Container images",
        "content": "A container image is a ready-to-run software package containing everything needed to run an application: the code and any runtime it requires, application and system libraries, and default values for any essential settings.\n\nContainers are intended to be stateless and immutable : you should not change the code of a container that is already running. If you have a containerized application and want to make changes, the correct process is to build a new image that includes the change, then recreate the container to start from the updated image.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 548
        }
      },
      {
        "header": "Container runtimes",
        "content": "A fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.\n\nKubernetes supports container runtimes such as containerd , CRI-O , and any other implementation of the Kubernetes CRI (Container Runtime Interface) .\n\nUsually, you can allow your cluster to pick the default container runtime for a Pod. If you need to use more than one container runtime in your cluster, you can specify the RuntimeClass for a Pod to make sure that Kubernetes runs those containers using a particular container runtime.\n\nYou can also use RuntimeClass to run different Pods with the same container runtime but with different settings.\n\nContainer Environment Container Lifecycle Hooks Container Runtime Interface (CRI)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 822
        }
      },
      {
        "header": "Feedback",
        "content": "Was this page helpful?\n\nYes\n\nNo\n\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow . Open an issue in the GitHub Repository if you want to report a problem or suggest an improvement .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 256
        }
      }
    ],
    "url": "https://kubernetes.io/docs/concepts/containers/",
    "doc_type": "docker",
    "total_sections": 3
  },
  {
    "title": "Images",
    "summary": "A container image represents binary data that encapsulates an application and all its software dependencies. Container images are executable software bundles that can run standalone and that make very well-defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a Pod .",
    "sections": [
      {
        "header": "Note:",
        "content": "If you are looking for the container images for a Kubernetes release (such as v1.34, the latest minor release), visit\n\nDownload Kubernetes\n\n.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 141
        }
      },
      {
        "header": "Image names",
        "content": "Container images are usually given a name such as pause , example/mycontainer , or kube-apiserver . Images can also include a registry hostname; for example: fictional.registry.example/imagename , and possibly a port number as well; for example: fictional.registry.example:10443/imagename .\n\nIf you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry . You can change this behavior by setting a default image registry in the container runtime configuration.\n\nAfter the image name part you can add a tag or digest (in the same way you would when using with commands like docker or podman ). Tags let you identify different versions of the same series of images. Digests are a unique identifier for a specific version of an image. Digests are hashes of the image's content, and are immutable. Tags can be moved to point to different images, but digests are fixed.\n\nImage tags consist of lowercase and uppercase letters, digits, underscores ( _ ), periods ( . ), and dashes ( - ). A tag can be up to 128 characters long, and must conform to the following regex pattern: [a-zA-Z0-9_][a-zA-Z0-9._-]{0,127} . You can read more about it and find the validation regex in the OCI Distribution Specification . If you don't specify a tag, Kubernetes assumes you mean the tag latest .\n\nImage digests consists of a hash algorithm (such as sha256 ) and a hash value. For example: sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07 . You can find more information about the digest format in the OCI Image Specification .\n\nSome image name examples that Kubernetes can use are:\n\nbusybox — Image name only, no tag or digest. Kubernetes will use the Docker public registry and latest tag. Equivalent to docker.io/library/busybox:latest . busybox:1.32.0 — Image name with tag. Kubernetes will use the Docker public registry. Equivalent to docker.io/library/busybox:1.32.0 . registry.k8s.io/pause:latest — Image name with a custom registry and latest tag. registry.k8s.io/pause:3.5 — Image name with a custom registry and non-latest tag. registry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07 — Image name with digest. registry.k8s.io/pause:3.5@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07 — Image name with tag and digest. Only the digest will be used for pulling.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2374
        }
      },
      {
        "header": "Updating images",
        "content": "When you first create a Deployment , StatefulSet , Pod, or other object that includes a PodTemplate, and a pull policy was not explicitly specified, then by default the pull policy of all containers in that Pod will be set to IfNotPresent . This policy causes the kubelet to skip pulling an image if it already exists.\n\nImage pull policy\n\nThe imagePullPolicy for a container and the tag of the image both affect when the kubelet attempts to pull (download) the specified image.\n\nHere's a list of the values you can set for imagePullPolicy and the effects these values have:\n\nIfNotPresent the image is pulled only if it is not already present locally. Always every time the kubelet launches a container, the kubelet queries the container image registry to resolve the name to an image digest . If the kubelet has a container image with that exact digest cached locally, the kubelet uses its cached image; otherwise, the kubelet pulls the image with the resolved digest, and uses that image to launch the container. Never the kubelet does not try fetching the image. If the image is somehow already present locally, the kubelet attempts to start the container; otherwise, startup fails. See pre-pulled images for more details.\n\nThe caching semantics of the underlying image provider make even imagePullPolicy: Always efficient, as long as the registry is reliably accessible. Your container runtime can notice that the image layers already exist on the node so that they don't need to be downloaded again.\n\nNote: You should avoid using the :latest tag when deploying containers in production as it is harder to track which version of the image is running and more difficult to roll back properly. Instead, specify a meaningful tag such as v1.42.0 and/or a digest.\n\nTo make sure the Pod always uses the same version of a container image, you can specify the image's digest; replace <image-name>:<tag> with <image-name>@<digest> (for example, image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 ).\n\nWhen using image tags, if the image registry were to change the code that the tag on that image represents, you might end up with a mix of Pods running the old and new code. An image digest uniquely identifies a specific version of the image, so Kubernetes runs the same code every time it starts a container with that image name and digest specified. Specifying an image by digest pins the code that you run so that a change at the registry cannot lead to that mix of versions.\n\nThere are third-party admission controllers that mutate Pods (and PodTemplates) when they are created, so that the running workload is defined based on an image digest rather than a tag. That might be useful if you want to make sure that your entire workload is running the same code no matter what tag changes happen at the registry.\n\nDefault image pull policy\n\nWhen you (or a controller) submit a new Pod to the API server, your cluster sets the imagePullPolicy field when specific conditions are met:\n\nif you omit the imagePullPolicy field, and you specify the digest for the container image, the imagePullPolicy is automatically set to IfNotPresent . if you omit the imagePullPolicy field, and the tag for the container image is :latest , imagePullPolicy is automatically set to Always . if you omit the imagePullPolicy field, and you don't specify the tag for the container image, imagePullPolicy is automatically set to Always . if you omit the imagePullPolicy field, and you specify a tag for the container image that isn't :latest , the imagePullPolicy is automatically set to IfNotPresent .\n\nNote: The value of imagePullPolicy of the container is always set when the object is first created , and is not updated if the image's tag or digest later changes. For example, if you create a Deployment with an image whose tag is not :latest , and later update that Deployment's image to a :latest tag, the imagePullPolicy field will not change to Always . You must manually change the pull policy of any object after its initial creation.\n\nRequired image pull\n\nIf you would like to always force a pull, you can do one of the following:\n\nSet the imagePullPolicy of the container to Always . Omit the imagePullPolicy and use :latest as the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod. Omit the imagePullPolicy and the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod. Enable the AlwaysPullImages admission controller.\n\nImagePullBackOff\n\nWhen a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state because of ImagePullBackOff .\n\nThe status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image (for reasons such as invalid image name, or pulling from a private registry without imagePullSecret ). The BackOff part indicates that Kubernetes will keep trying to pull the image, with an increasing back-off delay.\n\nKubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is 300 seconds (5 minutes).\n\nImage pull per runtime class\n\nFEATURE STATE: Kubernetes v1.29 [alpha] (disabled by default) Kubernetes includes alpha support for performing image pulls based on the RuntimeClass of a Pod.\n\nIf you enable the RuntimeClassInImageCriApi feature gate , the kubelet references container images by a tuple of image name and runtime handler rather than just the image name or digest. Your container runtime may adapt its behavior based on the selected runtime handler. Pulling images based on runtime class is useful for VM-based containers, such as Windows Hyper-V containers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 5722
        }
      },
      {
        "header": "Image pull policy",
        "content": "The imagePullPolicy for a container and the tag of the image both affect when the kubelet attempts to pull (download) the specified image.\n\nHere's a list of the values you can set for imagePullPolicy and the effects these values have:\n\nIfNotPresent the image is pulled only if it is not already present locally. Always every time the kubelet launches a container, the kubelet queries the container image registry to resolve the name to an image digest . If the kubelet has a container image with that exact digest cached locally, the kubelet uses its cached image; otherwise, the kubelet pulls the image with the resolved digest, and uses that image to launch the container. Never the kubelet does not try fetching the image. If the image is somehow already present locally, the kubelet attempts to start the container; otherwise, startup fails. See pre-pulled images for more details.\n\nThe caching semantics of the underlying image provider make even imagePullPolicy: Always efficient, as long as the registry is reliably accessible. Your container runtime can notice that the image layers already exist on the node so that they don't need to be downloaded again.\n\nNote: You should avoid using the :latest tag when deploying containers in production as it is harder to track which version of the image is running and more difficult to roll back properly. Instead, specify a meaningful tag such as v1.42.0 and/or a digest.\n\nTo make sure the Pod always uses the same version of a container image, you can specify the image's digest; replace <image-name>:<tag> with <image-name>@<digest> (for example, image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 ).\n\nWhen using image tags, if the image registry were to change the code that the tag on that image represents, you might end up with a mix of Pods running the old and new code. An image digest uniquely identifies a specific version of the image, so Kubernetes runs the same code every time it starts a container with that image name and digest specified. Specifying an image by digest pins the code that you run so that a change at the registry cannot lead to that mix of versions.\n\nThere are third-party admission controllers that mutate Pods (and PodTemplates) when they are created, so that the running workload is defined based on an image digest rather than a tag. That might be useful if you want to make sure that your entire workload is running the same code no matter what tag changes happen at the registry.\n\nDefault image pull policy\n\nWhen you (or a controller) submit a new Pod to the API server, your cluster sets the imagePullPolicy field when specific conditions are met:\n\nif you omit the imagePullPolicy field, and you specify the digest for the container image, the imagePullPolicy is automatically set to IfNotPresent . if you omit the imagePullPolicy field, and the tag for the container image is :latest , imagePullPolicy is automatically set to Always . if you omit the imagePullPolicy field, and you don't specify the tag for the container image, imagePullPolicy is automatically set to Always . if you omit the imagePullPolicy field, and you specify a tag for the container image that isn't :latest , the imagePullPolicy is automatically set to IfNotPresent .\n\nNote: The value of imagePullPolicy of the container is always set when the object is first created , and is not updated if the image's tag or digest later changes. For example, if you create a Deployment with an image whose tag is not :latest , and later update that Deployment's image to a :latest tag, the imagePullPolicy field will not change to Always . You must manually change the pull policy of any object after its initial creation.\n\nRequired image pull\n\nIf you would like to always force a pull, you can do one of the following:\n\nSet the imagePullPolicy of the container to Always . Omit the imagePullPolicy and use :latest as the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod. Omit the imagePullPolicy and the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod. Enable the AlwaysPullImages admission controller.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 4169
        }
      },
      {
        "header": "Note:",
        "content": "You should avoid using the :latest tag when deploying containers in production as it is harder to track which version of the image is running and more difficult to roll back properly.\n\nInstead, specify a meaningful tag such as v1.42.0 and/or a digest.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 251
        }
      },
      {
        "header": "Default image pull policy",
        "content": "When you (or a controller) submit a new Pod to the API server, your cluster sets the imagePullPolicy field when specific conditions are met:\n\nif you omit the imagePullPolicy field, and you specify the digest for the container image, the imagePullPolicy is automatically set to IfNotPresent . if you omit the imagePullPolicy field, and the tag for the container image is :latest , imagePullPolicy is automatically set to Always . if you omit the imagePullPolicy field, and you don't specify the tag for the container image, imagePullPolicy is automatically set to Always . if you omit the imagePullPolicy field, and you specify a tag for the container image that isn't :latest , the imagePullPolicy is automatically set to IfNotPresent .\n\nNote: The value of imagePullPolicy of the container is always set when the object is first created , and is not updated if the image's tag or digest later changes. For example, if you create a Deployment with an image whose tag is not :latest , and later update that Deployment's image to a :latest tag, the imagePullPolicy field will not change to Always . You must manually change the pull policy of any object after its initial creation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1178
        }
      },
      {
        "header": "Note:",
        "content": "The value of imagePullPolicy of the container is always set when the object is first created , and is not updated if the image's tag or digest later changes.\n\nFor example, if you create a Deployment with an image whose tag is not :latest , and later update that Deployment's image to a :latest tag, the imagePullPolicy field will not change to Always . You must manually change the pull policy of any object after its initial creation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 435
        }
      },
      {
        "header": "Required image pull",
        "content": "If you would like to always force a pull, you can do one of the following:\n\nSet the imagePullPolicy of the container to Always . Omit the imagePullPolicy and use :latest as the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod. Omit the imagePullPolicy and the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod. Enable the AlwaysPullImages admission controller.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 443
        }
      },
      {
        "header": "ImagePullBackOff",
        "content": "When a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state because of ImagePullBackOff .\n\nThe status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image (for reasons such as invalid image name, or pulling from a private registry without imagePullSecret ). The BackOff part indicates that Kubernetes will keep trying to pull the image, with an increasing back-off delay.\n\nKubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is 300 seconds (5 minutes).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 622
        }
      },
      {
        "header": "Image pull per runtime class",
        "content": "FEATURE STATE: Kubernetes v1.29 [alpha] (disabled by default) Kubernetes includes alpha support for performing image pulls based on the RuntimeClass of a Pod.\n\nIf you enable the RuntimeClassInImageCriApi feature gate , the kubelet references container images by a tuple of image name and runtime handler rather than just the image name or digest. Your container runtime may adapt its behavior based on the selected runtime handler. Pulling images based on runtime class is useful for VM-based containers, such as Windows Hyper-V containers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 540
        }
      },
      {
        "header": "Serial and parallel image pulls",
        "content": "By default, the kubelet pulls images serially. In other words, the kubelet sends only one image pull request to the image service at a time. Other image pull requests have to wait until the one being processed is complete.\n\nNodes make image pull decisions in isolation. Even when you use serialized image pulls, two different nodes can pull the same image in parallel.\n\nIf you would like to enable parallel image pulls, you can set the field serializeImagePulls to false in the kubelet configuration . With serializeImagePulls set to false, image pull requests will be sent to the image service immediately, and multiple images will be pulled at the same time.\n\nWhen enabling parallel image pulls, ensure that the image service of your container runtime can handle parallel image pulls.\n\nThe kubelet never pulls multiple images in parallel on behalf of one Pod. For example, if you have a Pod that has an init container and an application container, the image pulls for the two containers will not be parallelized. However, if you have two Pods that use different images, and the parallel image pull feature is enabled, the kubelet will pull the images in parallel on behalf of the two different Pods.\n\nMaximum parallel image pulls\n\nFEATURE STATE: Kubernetes v1.32 [beta]\n\nWhen serializeImagePulls is set to false, the kubelet defaults to no limit on the maximum number of images being pulled at the same time. If you would like to limit the number of parallel image pulls, you can set the field maxParallelImagePulls in the kubelet configuration. With maxParallelImagePulls set to n , only n images can be pulled at the same time, and any image pull beyond n will have to wait until at least one ongoing image pull is complete.\n\nLimiting the number of parallel image pulls prevents image pulling from consuming too much network bandwidth or disk I/O, when parallel image pulling is enabled.\n\nYou can set maxParallelImagePulls to a positive number that is greater than or equal to 1. If you set maxParallelImagePulls to be greater than or equal to 2, you must set serializeImagePulls to false. The kubelet will fail to start with an invalid maxParallelImagePulls setting.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2171
        }
      },
      {
        "header": "Maximum parallel image pulls",
        "content": "FEATURE STATE: Kubernetes v1.32 [beta]\n\nWhen serializeImagePulls is set to false, the kubelet defaults to no limit on the maximum number of images being pulled at the same time. If you would like to limit the number of parallel image pulls, you can set the field maxParallelImagePulls in the kubelet configuration. With maxParallelImagePulls set to n , only n images can be pulled at the same time, and any image pull beyond n will have to wait until at least one ongoing image pull is complete.\n\nLimiting the number of parallel image pulls prevents image pulling from consuming too much network bandwidth or disk I/O, when parallel image pulling is enabled.\n\nYou can set maxParallelImagePulls to a positive number that is greater than or equal to 1. If you set maxParallelImagePulls to be greater than or equal to 2, you must set serializeImagePulls to false. The kubelet will fail to start with an invalid maxParallelImagePulls setting.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 938
        }
      },
      {
        "header": "Multi-architecture images with image indexes",
        "content": "As well as providing binary images, a container registry can also serve a container image index . An image index can point to multiple image manifests for architecture-specific versions of a container. The idea is that you can have a name for an image (for example: pause , example/mycontainer , kube-apiserver ) and allow different systems to fetch the right binary image for the machine architecture they are using.\n\nThe Kubernetes project typically creates container images for its releases with names that include the suffix -$(ARCH) . For backward compatibility, generate older images with suffixes. For instance, an image named as pause would be a multi-architecture image containing manifests for all supported architectures, while pause-amd64 would be a backward-compatible version for older configurations, or for YAML files with hardcoded image names containing suffixes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 881
        }
      },
      {
        "header": "Using a private registry",
        "content": "Private registries may require authentication to be able to discover and/or pull images from them. Credentials can be provided in several ways:\n\nSpecifying imagePullSecrets when you define a Pod Only Pods which provide their own keys can access the private registry. Configuring Nodes to Authenticate to a Private Registry All Pods can read any configured private registries. Requires node configuration by cluster administrator. All Pods can read any configured private registries. Requires node configuration by cluster administrator. Using a kubelet credential provider plugin to dynamically fetch credentials for private registries The kubelet can be configured to use credential provider exec plugin for the respective private registry. Pre-pulled Images All Pods can use any images cached on a node. Requires root access to all nodes to set up. All Pods can use any images cached on a node. Requires root access to all nodes to set up. Vendor-specific or local extensions If you're using a custom node configuration, you (or your cloud provider) can implement your mechanism for authenticating the node to the container registry.\n\nThese options are explained in more detail below.\n\nSpecifying imagePullSecrets on a Pod\n\nNote: This is the recommended approach to run containers based on images in private registries.\n\nKubernetes supports specifying container image registry keys on a Pod. All imagePullSecrets must be Secrets that exist in the same Namespace as the Pod. These Secrets must be of type kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson .\n\nConfiguring nodes to authenticate to a private registry\n\nSpecific instructions for setting credentials depends on the container runtime and registry you chose to use. You should refer to your solution's documentation for the most accurate information.\n\nFor an example of configuring a private container image registry, see the Pull an Image from a Private Registry task. That example uses a private registry in Docker Hub.\n\nKubelet credential provider for authenticated image pulls\n\nYou can configure the kubelet to invoke a plugin binary to dynamically fetch registry credentials for a container image. This is the most robust and versatile way to fetch credentials for private registries, but also requires kubelet-level configuration to enable.\n\nThis technique can be especially useful for running static Pods that require container images hosted in a private registry. Using a ServiceAccount or a Secret to provide private registry credentials is not possible in the specification of a static Pod, because it cannot have references to other API resources in its specification.\n\nSee Configure a kubelet image credential provider for more details.\n\nInterpretation of config.json\n\nThe interpretation of config.json varies between the original Docker implementation and the Kubernetes interpretation. In Docker, the auths keys can only specify root URLs, whereas Kubernetes allows glob URLs as well as prefix-matched paths. The only limitation is that glob patterns ( * ) have to include the dot ( . ) for each subdomain. The amount of matched subdomains has to be equal to the amount of glob patterns ( *. ), for example:\n\n*.kubernetes.io will not match kubernetes.io , but will match abc.kubernetes.io . *.*.kubernetes.io will not match abc.kubernetes.io , but will match abc.def.kubernetes.io . prefix.*.io will match prefix.kubernetes.io . *-good.kubernetes.io will match prefix-good.kubernetes.io .\n\nThis means that a config.json like this is valid:\n\n{ \"auths\" : { \"my-registry.example/images\" : { \"auth\" : \"…\" }, \"*.my-registry.example/images\" : { \"auth\" : \"…\" } } }\n\nImage pull operations pass the credentials to the CRI container runtime for every valid pattern. For example, the following container image names would match successfully:\n\nmy-registry.example/images my-registry.example/images/my-image my-registry.example/images/another-image sub.my-registry.example/images/my-image\n\nHowever, these container image names would not match:\n\na.sub.my-registry.example/images/my-image a.b.sub.my-registry.example/images/my-image\n\nThe kubelet performs image pulls sequentially for every found credential. This means that multiple entries in config.json for different paths are possible, too:\n\n{ \"auths\" : { \"my-registry.example/images\" : { \"auth\" : \"…\" }, \"my-registry.example/images/subpath\" : { \"auth\" : \"…\" } } }\n\nIf now a container specifies an image my-registry.example/images/subpath/my-image to be pulled, then the kubelet will try to download it using both authentication sources if one of them fails.\n\nPre-pulled images\n\nNote: This approach is suitable if you can control node configuration. It will not work reliably if your cloud provider manages nodes and replaces them automatically.\n\nBy default, the kubelet tries to pull each image from the specified registry. However, if the imagePullPolicy property of the container is set to IfNotPresent or Never , then a local image is used (preferentially or exclusively, respectively).\n\nIf you want to rely on pre-pulled images as a substitute for registry authentication, you must ensure all nodes in the cluster have the same pre-pulled images.\n\nThis can be used to preload certain images for speed or as an alternative to authenticating to a private registry.\n\nSimilar to the usage of the kubelet credential provider , pre-pulled images are also suitable for launching static Pods that depend on images hosted in a private registry.\n\nNote: FEATURE STATE: Kubernetes v1.33 [alpha] (disabled by default) Access to pre-pulled images may be authorized according to image pull credential verification .\n\nEnsure image pull credential verification\n\nFEATURE STATE: Kubernetes v1.33 [alpha] (disabled by default)\n\nIf the KubeletEnsureSecretPulledImages feature gate is enabled for your cluster, Kubernetes will validate image credentials for every image that requires credentials to be pulled, even if that image is already present on the node. This validation ensures that images in a Pod request which have not been successfully pulled with the provided credentials must re-pull the images from the registry. Additionally, image pulls that re-use the same credentials which previously resulted in a successful image pull will not need to re-pull from the registry and are instead validated locally without accessing the registry (provided the image is available locally). This is controlled by the imagePullCredentialsVerificationPolicy field in the Kubelet configuration .\n\nThis configuration controls when image pull credentials must be verified if the image is already present on the node:\n\nNeverVerify : Mimics the behavior of having this feature gate disabled. If the image is present locally, image pull credentials are not verified. NeverVerifyPreloadedImages : Images pulled outside the kubelet are not verified, but all other images will have their credentials verified. This is the default behavior. NeverVerifyAllowListedImages : Images pulled outside the kubelet and mentioned within the preloadedImagesVerificationAllowlist specified in the kubelet config are not verified. AlwaysVerify : All images will have their credentials verified before they can be used.\n\nThis verification applies to pre-pulled images , images pulled using node-wide secrets, and images pulled using Pod-level secrets.\n\nNote: In the case of credential rotation, the credentials previously used to pull the image will continue to verify without the need to access the registry. New or rotated credentials will require the image to be re-pulled from the registry.\n\nCreating a Secret with a Docker config\n\nYou need to know the username, registry password and client email address for authenticating to the registry, as well as its hostname. Run the following command, substituting placeholders with the appropriate values:\n\nkubectl create secret docker-registry <name> \\ --docker-server = <docker-registry-server> \\ --docker-username = <docker-user> \\ --docker-password = <docker-password> \\ --docker-email = <docker-email>\n\nIf you already have a Docker credentials file then, rather than using the above command, you can import the credentials file as a Kubernetes Secret . Create a Secret based on existing Docker credentials explains how to set this up.\n\nThis is particularly useful if you are using multiple private container registries, as kubectl create secret docker-registry creates a Secret that only works with a single private registry.\n\nNote: Pods can only reference image pull secrets in their own namespace, so this process needs to be done one time per namespace.\n\nReferring to imagePullSecrets on a Pod\n\nNow, you can create pods which reference that secret by adding the imagePullSecrets section to a Pod definition. Each item in the imagePullSecrets array can only reference one Secret in the same namespace.\n\nFor example:\n\ncat <<EOF > pod.yaml apiVersion: v1 kind: Pod metadata: name: foo namespace: awesomeapps spec: containers: - name: foo image: janedoe/awesomeapp:v1 imagePullSecrets: - name: myregistrykey EOF cat <<EOF >> ./kustomization.yaml resources: - pod.yaml EOF\n\nThis needs to be done for each Pod that is using a private registry.\n\nHowever, you can automate this process by specifying the imagePullSecrets section in a ServiceAccount resource. See Add ImagePullSecrets to a Service Account for detailed instructions.\n\nYou can use this in conjunction with a per-node .docker/config.json . The credentials will be merged.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 9458
        }
      },
      {
        "header": "Specifying imagePullSecrets on a Pod",
        "content": "Note: This is the recommended approach to run containers based on images in private registries.\n\nKubernetes supports specifying container image registry keys on a Pod. All imagePullSecrets must be Secrets that exist in the same Namespace as the Pod. These Secrets must be of type kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 339
        }
      },
      {
        "header": "Note:",
        "content": "This is the recommended approach to run containers based on images in private registries.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 89
        }
      },
      {
        "header": "Configuring nodes to authenticate to a private registry",
        "content": "Specific instructions for setting credentials depends on the container runtime and registry you chose to use. You should refer to your solution's documentation for the most accurate information.\n\nFor an example of configuring a private container image registry, see the Pull an Image from a Private Registry task. That example uses a private registry in Docker Hub.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 365
        }
      },
      {
        "header": "Kubelet credential provider for authenticated image pulls",
        "content": "You can configure the kubelet to invoke a plugin binary to dynamically fetch registry credentials for a container image. This is the most robust and versatile way to fetch credentials for private registries, but also requires kubelet-level configuration to enable.\n\nThis technique can be especially useful for running static Pods that require container images hosted in a private registry. Using a ServiceAccount or a Secret to provide private registry credentials is not possible in the specification of a static Pod, because it cannot have references to other API resources in its specification.\n\nSee Configure a kubelet image credential provider for more details.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 666
        }
      },
      {
        "header": "Interpretation of config.json",
        "content": "The interpretation of config.json varies between the original Docker implementation and the Kubernetes interpretation. In Docker, the auths keys can only specify root URLs, whereas Kubernetes allows glob URLs as well as prefix-matched paths. The only limitation is that glob patterns ( * ) have to include the dot ( . ) for each subdomain. The amount of matched subdomains has to be equal to the amount of glob patterns ( *. ), for example:\n\n*.kubernetes.io will not match kubernetes.io , but will match abc.kubernetes.io . *.*.kubernetes.io will not match abc.kubernetes.io , but will match abc.def.kubernetes.io . prefix.*.io will match prefix.kubernetes.io . *-good.kubernetes.io will match prefix-good.kubernetes.io .\n\nThis means that a config.json like this is valid:\n\n{ \"auths\" : { \"my-registry.example/images\" : { \"auth\" : \"…\" }, \"*.my-registry.example/images\" : { \"auth\" : \"…\" } } }\n\nImage pull operations pass the credentials to the CRI container runtime for every valid pattern. For example, the following container image names would match successfully:\n\nmy-registry.example/images my-registry.example/images/my-image my-registry.example/images/another-image sub.my-registry.example/images/my-image\n\nHowever, these container image names would not match:\n\na.sub.my-registry.example/images/my-image a.b.sub.my-registry.example/images/my-image\n\nThe kubelet performs image pulls sequentially for every found credential. This means that multiple entries in config.json for different paths are possible, too:\n\n{ \"auths\" : { \"my-registry.example/images\" : { \"auth\" : \"…\" }, \"my-registry.example/images/subpath\" : { \"auth\" : \"…\" } } }\n\nIf now a container specifies an image my-registry.example/images/subpath/my-image to be pulled, then the kubelet will try to download it using both authentication sources if one of them fails.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1830
        }
      },
      {
        "header": "Pre-pulled images",
        "content": "Note: This approach is suitable if you can control node configuration. It will not work reliably if your cloud provider manages nodes and replaces them automatically.\n\nBy default, the kubelet tries to pull each image from the specified registry. However, if the imagePullPolicy property of the container is set to IfNotPresent or Never , then a local image is used (preferentially or exclusively, respectively).\n\nIf you want to rely on pre-pulled images as a substitute for registry authentication, you must ensure all nodes in the cluster have the same pre-pulled images.\n\nThis can be used to preload certain images for speed or as an alternative to authenticating to a private registry.\n\nSimilar to the usage of the kubelet credential provider , pre-pulled images are also suitable for launching static Pods that depend on images hosted in a private registry.\n\nNote: FEATURE STATE: Kubernetes v1.33 [alpha] (disabled by default) Access to pre-pulled images may be authorized according to image pull credential verification .\n\nEnsure image pull credential verification\n\nFEATURE STATE: Kubernetes v1.33 [alpha] (disabled by default)\n\nIf the KubeletEnsureSecretPulledImages feature gate is enabled for your cluster, Kubernetes will validate image credentials for every image that requires credentials to be pulled, even if that image is already present on the node. This validation ensures that images in a Pod request which have not been successfully pulled with the provided credentials must re-pull the images from the registry. Additionally, image pulls that re-use the same credentials which previously resulted in a successful image pull will not need to re-pull from the registry and are instead validated locally without accessing the registry (provided the image is available locally). This is controlled by the imagePullCredentialsVerificationPolicy field in the Kubelet configuration .\n\nThis configuration controls when image pull credentials must be verified if the image is already present on the node:\n\nNeverVerify : Mimics the behavior of having this feature gate disabled. If the image is present locally, image pull credentials are not verified. NeverVerifyPreloadedImages : Images pulled outside the kubelet are not verified, but all other images will have their credentials verified. This is the default behavior. NeverVerifyAllowListedImages : Images pulled outside the kubelet and mentioned within the preloadedImagesVerificationAllowlist specified in the kubelet config are not verified. AlwaysVerify : All images will have their credentials verified before they can be used.\n\nThis verification applies to pre-pulled images , images pulled using node-wide secrets, and images pulled using Pod-level secrets.\n\nNote: In the case of credential rotation, the credentials previously used to pull the image will continue to verify without the need to access the registry. New or rotated credentials will require the image to be re-pulled from the registry.\n\nCreating a Secret with a Docker config\n\nYou need to know the username, registry password and client email address for authenticating to the registry, as well as its hostname. Run the following command, substituting placeholders with the appropriate values:\n\nkubectl create secret docker-registry <name> \\ --docker-server = <docker-registry-server> \\ --docker-username = <docker-user> \\ --docker-password = <docker-password> \\ --docker-email = <docker-email>\n\nIf you already have a Docker credentials file then, rather than using the above command, you can import the credentials file as a Kubernetes Secret . Create a Secret based on existing Docker credentials explains how to set this up.\n\nThis is particularly useful if you are using multiple private container registries, as kubectl create secret docker-registry creates a Secret that only works with a single private registry.\n\nNote: Pods can only reference image pull secrets in their own namespace, so this process needs to be done one time per namespace.\n\nReferring to imagePullSecrets on a Pod\n\nNow, you can create pods which reference that secret by adding the imagePullSecrets section to a Pod definition. Each item in the imagePullSecrets array can only reference one Secret in the same namespace.\n\nFor example:\n\ncat <<EOF > pod.yaml apiVersion: v1 kind: Pod metadata: name: foo namespace: awesomeapps spec: containers: - name: foo image: janedoe/awesomeapp:v1 imagePullSecrets: - name: myregistrykey EOF cat <<EOF >> ./kustomization.yaml resources: - pod.yaml EOF\n\nThis needs to be done for each Pod that is using a private registry.\n\nHowever, you can automate this process by specifying the imagePullSecrets section in a ServiceAccount resource. See Add ImagePullSecrets to a Service Account for detailed instructions.\n\nYou can use this in conjunction with a per-node .docker/config.json . The credentials will be merged.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 4858
        }
      },
      {
        "header": "Note:",
        "content": "This approach is suitable if you can control node configuration. It will not work reliably if your cloud provider manages nodes and replaces them automatically.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 160
        }
      },
      {
        "header": "Note:",
        "content": "FEATURE STATE: Kubernetes v1.33 [alpha] (disabled by default)\n\nAccess to pre-pulled images may be authorized according to image pull credential verification .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 158
        }
      },
      {
        "header": "Ensure image pull credential verification",
        "content": "FEATURE STATE: Kubernetes v1.33 [alpha] (disabled by default)\n\nIf the KubeletEnsureSecretPulledImages feature gate is enabled for your cluster, Kubernetes will validate image credentials for every image that requires credentials to be pulled, even if that image is already present on the node. This validation ensures that images in a Pod request which have not been successfully pulled with the provided credentials must re-pull the images from the registry. Additionally, image pulls that re-use the same credentials which previously resulted in a successful image pull will not need to re-pull from the registry and are instead validated locally without accessing the registry (provided the image is available locally). This is controlled by the imagePullCredentialsVerificationPolicy field in the Kubelet configuration .\n\nThis configuration controls when image pull credentials must be verified if the image is already present on the node:\n\nNeverVerify : Mimics the behavior of having this feature gate disabled. If the image is present locally, image pull credentials are not verified. NeverVerifyPreloadedImages : Images pulled outside the kubelet are not verified, but all other images will have their credentials verified. This is the default behavior. NeverVerifyAllowListedImages : Images pulled outside the kubelet and mentioned within the preloadedImagesVerificationAllowlist specified in the kubelet config are not verified. AlwaysVerify : All images will have their credentials verified before they can be used.\n\nThis verification applies to pre-pulled images , images pulled using node-wide secrets, and images pulled using Pod-level secrets.\n\nNote: In the case of credential rotation, the credentials previously used to pull the image will continue to verify without the need to access the registry. New or rotated credentials will require the image to be re-pulled from the registry.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1900
        }
      },
      {
        "header": "Note:",
        "content": "In the case of credential rotation, the credentials previously used to pull the image will continue to verify without the need to access the registry. New or rotated credentials will require the image to be re-pulled from the registry.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 235
        }
      },
      {
        "header": "Creating a Secret with a Docker config",
        "content": "You need to know the username, registry password and client email address for authenticating to the registry, as well as its hostname. Run the following command, substituting placeholders with the appropriate values:\n\nkubectl create secret docker-registry <name> \\ --docker-server = <docker-registry-server> \\ --docker-username = <docker-user> \\ --docker-password = <docker-password> \\ --docker-email = <docker-email>\n\nIf you already have a Docker credentials file then, rather than using the above command, you can import the credentials file as a Kubernetes Secret . Create a Secret based on existing Docker credentials explains how to set this up.\n\nThis is particularly useful if you are using multiple private container registries, as kubectl create secret docker-registry creates a Secret that only works with a single private registry.\n\nNote: Pods can only reference image pull secrets in their own namespace, so this process needs to be done one time per namespace.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 972
        }
      },
      {
        "header": "Note:",
        "content": "Pods can only reference image pull secrets in their own namespace, so this process needs to be done one time per namespace.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 123
        }
      },
      {
        "header": "Referring to imagePullSecrets on a Pod",
        "content": "Now, you can create pods which reference that secret by adding the imagePullSecrets section to a Pod definition. Each item in the imagePullSecrets array can only reference one Secret in the same namespace.\n\nFor example:\n\ncat <<EOF > pod.yaml apiVersion: v1 kind: Pod metadata: name: foo namespace: awesomeapps spec: containers: - name: foo image: janedoe/awesomeapp:v1 imagePullSecrets: - name: myregistrykey EOF cat <<EOF >> ./kustomization.yaml resources: - pod.yaml EOF\n\nThis needs to be done for each Pod that is using a private registry.\n\nHowever, you can automate this process by specifying the imagePullSecrets section in a ServiceAccount resource. See Add ImagePullSecrets to a Service Account for detailed instructions.\n\nYou can use this in conjunction with a per-node .docker/config.json . The credentials will be merged.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 831
        }
      },
      {
        "header": "Use cases",
        "content": "There are a number of solutions for configuring private registries. Here are some common use cases and suggested solutions.\n\nCluster running only non-proprietary (e.g. open-source) images. No need to hide images. Use public images from a public registry No configuration required. Some cloud providers automatically cache or mirror public images, which improves availability and reduces the time to pull images. Use public images from a public registry No configuration required. Some cloud providers automatically cache or mirror public images, which improves availability and reduces the time to pull images. No configuration required. Some cloud providers automatically cache or mirror public images, which improves availability and reduces the time to pull images. Cluster running some proprietary images which should be hidden to those outside the company, but visible to all cluster users. Use a hosted private registry Manual configuration may be required on the nodes that need to access to private registry. Or, run an internal private registry behind your firewall with open read access. No Kubernetes configuration is required. Use a hosted container image registry service that controls image access It will work better with Node autoscaling than manual node configuration. Or, on a cluster where changing the node configuration is inconvenient, use imagePullSecrets . Use a hosted private registry Manual configuration may be required on the nodes that need to access to private registry. Manual configuration may be required on the nodes that need to access to private registry. Or, run an internal private registry behind your firewall with open read access. No Kubernetes configuration is required. No Kubernetes configuration is required. Use a hosted container image registry service that controls image access It will work better with Node autoscaling than manual node configuration. It will work better with Node autoscaling than manual node configuration. Or, on a cluster where changing the node configuration is inconvenient, use imagePullSecrets . Cluster with proprietary images, a few of which require stricter access control. Ensure AlwaysPullImages admission controller is active. Otherwise, all Pods potentially have access to all images. Move sensitive data into a Secret resource, instead of packaging it in an image. Ensure AlwaysPullImages admission controller is active. Otherwise, all Pods potentially have access to all images. Move sensitive data into a Secret resource, instead of packaging it in an image. A multi-tenant cluster where each tenant needs own private registry. Ensure AlwaysPullImages admission controller is active. Otherwise, all Pods of all tenants potentially have access to all images. Run a private registry with authorization required. Generate registry credentials for each tenant, store into a Secret, and propagate the Secret to every tenant namespace. The tenant then adds that Secret to imagePullSecrets of each namespace. Ensure AlwaysPullImages admission controller is active. Otherwise, all Pods of all tenants potentially have access to all images. Run a private registry with authorization required. Generate registry credentials for each tenant, store into a Secret, and propagate the Secret to every tenant namespace. The tenant then adds that Secret to imagePullSecrets of each namespace.\n\nIf you need access to multiple registries, you can create one Secret per registry.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 3445
        }
      },
      {
        "header": "Legacy built-in kubelet credential provider",
        "content": "In older versions of Kubernetes, the kubelet had a direct integration with cloud provider credentials. This provided the ability to dynamically fetch credentials for image registries.\n\nThere were three built-in implementations of the kubelet credential provider integration: ACR (Azure Container Registry), ECR (Elastic Container Registry), and GCR (Google Container Registry).\n\nStarting with version 1.26 of Kubernetes, the legacy mechanism has been removed, so you would need to either:\n\nconfigure a kubelet image credential provider on each node; or specify image pull credentials using imagePullSecrets and at least one Secret.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 631
        }
      },
      {
        "header": "What's next",
        "content": "Read the OCI Image Manifest Specification . Learn about container image garbage collection . Learn more about pulling an Image from a Private Registry .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 152
        }
      },
      {
        "header": "Feedback",
        "content": "Was this page helpful?\n\nYes\n\nNo\n\nThanks for the feedback. If you have a specific, answerable question about how to use Kubernetes, ask it on Stack Overflow . Open an issue in the GitHub Repository if you want to report a problem or suggest an improvement .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 256
        }
      }
    ],
    "url": "https://kubernetes.io/docs/concepts/containers/images/",
    "doc_type": "docker",
    "total_sections": 31
  },
  {
    "title": "nginx - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/nginx",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "Deploying NGINX and NGINX Plus with Docker",
    "summary": "Editor – The NGINX Plus Dockerfiles for Alpine Linux and Debian were updated in November 2021 to reflect the latest software versions. They also (along with the revised instructions) use Docker secrets to pass license information when building an NGINX Plus image. Docker is an open platform for building, shipping, and running distributed applications as containers (lightweight, standalone, executable packages of software that include everything needed to run an application). Containers can in turn be deployed and orchestrated by container orchestration platforms such as Kubernetes . (In addition to the Docker container technology discussed in this blog, NGINX provides the F5 NGINX Ingress Controller in NGINX Open Source‑based and NGINX Plus‑based versions; for NGINX Plus subscribers, support is included at no extra cost.)",
    "sections": [
      {
        "header": "Introduction",
        "content": "The Docker open platform includes the Docker Engine – the open source runtime that builds, runs, and orchestrates containers – and Docker Hub, a hosted service where Dockerized applications are distributed, shared, and collaborated on by the entire development community or within the confines of a specific organization.\n\nDocker containers enable developers to focus their efforts on application “content” by separating applications from the constraints of infrastructure. Dockerized applications are instantly portable to any infrastructure – laptop, bare‑metal server, VM, or cloud – making them modular components that can be readily assembled and reassembled into fully featured distributed applications and continuously innovated on in real time.\n\nFor more information about Docker, see Why Docker? or the full Docker documentation .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 839
        }
      },
      {
        "header": "Using the NGINX Open Source Docker Image",
        "content": "You can create an NGINX instance in a Docker container using the NGINX Open Source image from Docker Hub.\n\nLet’s start with a very simple example. To launch an instance of NGINX running in a container and using the default NGINX configuration, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThis command creates a container named mynginx1 based on the NGINX image. The command returns the long form of the container ID, which is used in the name of log files; see Managing Logging .\n\nThe -p option tells Docker to map the port exposed in the container by the NGINX image – port 80 – to the specified port on the Docker host. The first parameter specifies the port in the Docker host, while the second parameter is mapped to the port exposed in the container.\n\nThe -d option specifies that the container runs in detached mode, which means that it continues to run until stopped but does not respond to commands run on the command line. In the next section we explain how to interact with the container.\n\nTo verify that the container was created and is running, and to see the port mappings, we run docker ps . (We’ve split the output across multiple lines here to make it easier to read.)\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThe PORTS field in the output reports that port 80 on the Docker host is mapped to port 80 in the container. Another way to verify that NGINX is running is to make an HTTP request to that port. The code for the default NGINX welcome page appears:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 1746
        }
      },
      {
        "header": "Working with the NGINX Docker Container",
        "content": "So now we have a working NGINX Docker container, but how do we manage the content and the NGINX configuration? And what about logging?\n\nA Note About SSH\n\nIt is common to enable SSH access to NGINX instances, but the NGINX image does not have OpenSSH installed, because Docker containers are generally intended to be for a single purpose (in this case running NGINX). Instead we’ll use other methods supported by Docker.\n\nAs an alternative to the following commands, you can run the following command to open an interactive shell to a running NGINX container (instead of starting an SSH session). However, we recommend this only for advanced users.\n\nOn Alpine Linux systems: [@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop On Debian systems: [@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nManaging Content and Configuration Files\n\nThere are several ways you can manage both the content served by NGINX and the NGINX configuration files. Here we cover a few of the options.\n\nOption 1 – Maintain the Content and Configuration on the Docker Host\n\nWhen the container is created we can tell Docker to mount a local directory on the Docker host to a directory in the container. The NGINX image uses the default NGINX configuration, which uses /usr/share/nginx/html as the container’s root directory and puts configuration files in /etc/nginx . For a Docker host with content in the local directory /var/www and configuration files in /var/nginx/conf , run this command (which appears on multiple lines here only for legibility):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow any change made to the files in the local directories /var/www and /var/nginx/conf on the Docker host are reflected in the directories /usr/share/nginx/html and /etc/nginx in the container. The readonly option means these directories can be changed only on the Docker host, not from within the container.\n\nOption 2 – Copy Files from the Docker Host\n\nAnother option is to have Docker copy the content and configuration files from a local directory on the Docker host during container creation. Once a container is created, the files are maintained by creating a new container when files change or by modifying the files in the container. A simple way to copy the files is to create a Dockerfile with commands that are run during generation of a new Docker image based on the NGINX image from Docker Hub. For the file‑copy ( COPY ) commands in the Dockerfile , the local directory path is relative to the build context where the Dockerfile is located.\n\nIn our example, the content is in the content directory and the configuration files are in the conf directory, both subdirectories of the directory where the Dockerfile is located. The NGINX image includes default NGINX configuration files as /etc/nginx/nginx.conf and /etc/nginx/conf.d/default.conf . Because we instead want to use the configuration files from the host, we include a RUN command that deletes the default files:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe create our own NGINX image by running the following command from the directory where the Dockerfile is located. Note the period (“.”) at the end of the command. It defines the current directory as the build context, which contains the Dockerfile and the directories to be copied.\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow we run this command to create a container called mynginx3 based on the mynginx_image1 image:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nIf we want to make changes to the files in the container, we use a helper container as described in Option 3.\n\nOption 3 – Maintain Files in the Container\n\nAs mentioned in A Note About SSH , we can’t use SSH to access the NGINX container, so if we want to edit the content or configuration files directly we have to create a helper container that has shell access. For the helper container to have access to the files, we must create a new image that has the proper Docker data volumes defined for the image. Assuming we want to copy files as in Option 2 while also defining volumes, we use the following Dockerfile :\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe then create the new NGINX image by running the following command (again note the final period):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow we run this command to create an NGINX container ( mynginx4 ) based on the mynginx_image2 image:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe then run the following command to start a helper container mynginx4_files that has a shell, enabling us to access the content and configuration directories of the mynginx4 container we just created:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThe new mynginx4_files helper container runs in the foreground with a persistent standard input (the -i option) and a tty (the -t option). All volumes defined in mynginx4 are mounted as local directories in the helper container.\n\nThe debian argument means that the helper container uses the Debian image from Docker Hub. Because the NGINX image also uses Debian (and all of our examples so far use the NGINX image), it is most efficient to use Debian for the helper container, rather than having Docker load another operating system. The /bin/bash argument means that the bash shell runs in the helper container, presenting a shell prompt that you can use to modify files as needed.\n\nTo start and stop the container, run the following commands:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo exit the shell but leave the container running, press Ctrl+p followed by Ctrl+q . To regain shell access to a running container, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo exit the shell and terminate the container, run the exit command.\n\nManaging Logging\n\nYou can configure either default or customized logging.\n\nUsing Default Logging\n\nThe NGINX image is configured to send the main NGINX access and error logs to the Docker log collector by default. This is done by linking them to stdout and stderr respectively; all messages from both logs are then written to the file /var/lib/docker/containers/ container-ID / container-ID -json.log on the Docker host, where container‑ID is the long‑form ID returned when you create a container. For the initial container we created in Using the NGINX Open Source Docker Image , for example, it is fcd1fb01b14557c7c9d991238f2558ae2704d129cf9fb97bb4fadf673a58580d .\n\nTo retrieve the container ID for an existing container, run this command, where container‑name is the value set by the --name parameter when the container is created (for the container ID above, for example, it is mynginx1 ):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nAlthough you can view the logs by opening the container-ID -json.log file directly, it is usually easier to run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nYou can use also the Docker Engine API to extract the log messages, by issuing a GET request against the Docker Unix socket. This command returns both the access log (represented by stdout=1 ) and the error log ( stderr=1 ), but you can request them singly as well:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo learn about other query parameters, see the Docker Engine API documentation (search for “Get container logs” on that page).\n\nUsing Customized Logging\n\nIf you want to implement another method of log collection, or if you want to configure logging differently in certain configuration blocks (such as server{} and location{} ), define a Docker volume for the directory or directories in which to store the log files in the container, create a helper container to access the log files, and use whatever logging tools you like. To implement this, create a new image that contains the volume or volumes for the logging files.\n\nFor example, to configure NGINX to store log files in /var/log/nginx/log , we can start with the Dockerfile from Option 3 and simply add a VOLUME definition for this directory:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe can then create an image as described above and use it to create an NGINX container and a helper container that have access to the logging directory. The helper container can have any desired logging tools installed.\n\nControlling NGINX\n\nSince we do not have direct access to the command line of the NGINX container, we cannot use the nginx command to control NGINX. Fortunately we can use signals to control NGINX, and Docker provides the kill command for sending signals to a container.\n\nTo reload the NGINX configuration, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo restart NGINX, run this command to restart the container:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 48,
          "content_length": 9915
        }
      },
      {
        "header": "A Note About SSH",
        "content": "It is common to enable SSH access to NGINX instances, but the NGINX image does not have OpenSSH installed, because Docker containers are generally intended to be for a single purpose (in this case running NGINX). Instead we’ll use other methods supported by Docker.\n\nAs an alternative to the following commands, you can run the following command to open an interactive shell to a running NGINX container (instead of starting an SSH session). However, we recommend this only for advanced users.\n\nOn Alpine Linux systems: [@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop On Debian systems: [@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 762
        }
      },
      {
        "header": "Managing Content and Configuration Files",
        "content": "There are several ways you can manage both the content served by NGINX and the NGINX configuration files. Here we cover a few of the options.\n\nOption 1 – Maintain the Content and Configuration on the Docker Host\n\nWhen the container is created we can tell Docker to mount a local directory on the Docker host to a directory in the container. The NGINX image uses the default NGINX configuration, which uses /usr/share/nginx/html as the container’s root directory and puts configuration files in /etc/nginx . For a Docker host with content in the local directory /var/www and configuration files in /var/nginx/conf , run this command (which appears on multiple lines here only for legibility):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow any change made to the files in the local directories /var/www and /var/nginx/conf on the Docker host are reflected in the directories /usr/share/nginx/html and /etc/nginx in the container. The readonly option means these directories can be changed only on the Docker host, not from within the container.\n\nOption 2 – Copy Files from the Docker Host\n\nAnother option is to have Docker copy the content and configuration files from a local directory on the Docker host during container creation. Once a container is created, the files are maintained by creating a new container when files change or by modifying the files in the container. A simple way to copy the files is to create a Dockerfile with commands that are run during generation of a new Docker image based on the NGINX image from Docker Hub. For the file‑copy ( COPY ) commands in the Dockerfile , the local directory path is relative to the build context where the Dockerfile is located.\n\nIn our example, the content is in the content directory and the configuration files are in the conf directory, both subdirectories of the directory where the Dockerfile is located. The NGINX image includes default NGINX configuration files as /etc/nginx/nginx.conf and /etc/nginx/conf.d/default.conf . Because we instead want to use the configuration files from the host, we include a RUN command that deletes the default files:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe create our own NGINX image by running the following command from the directory where the Dockerfile is located. Note the period (“.”) at the end of the command. It defines the current directory as the build context, which contains the Dockerfile and the directories to be copied.\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow we run this command to create a container called mynginx3 based on the mynginx_image1 image:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nIf we want to make changes to the files in the container, we use a helper container as described in Option 3.\n\nOption 3 – Maintain Files in the Container\n\nAs mentioned in A Note About SSH , we can’t use SSH to access the NGINX container, so if we want to edit the content or configuration files directly we have to create a helper container that has shell access. For the helper container to have access to the files, we must create a new image that has the proper Docker data volumes defined for the image. Assuming we want to copy files as in Option 2 while also defining volumes, we use the following Dockerfile :\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe then create the new NGINX image by running the following command (again note the final period):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow we run this command to create an NGINX container ( mynginx4 ) based on the mynginx_image2 image:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe then run the following command to start a helper container mynginx4_files that has a shell, enabling us to access the content and configuration directories of the mynginx4 container we just created:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThe new mynginx4_files helper container runs in the foreground with a persistent standard input (the -i option) and a tty (the -t option). All volumes defined in mynginx4 are mounted as local directories in the helper container.\n\nThe debian argument means that the helper container uses the Debian image from Docker Hub. Because the NGINX image also uses Debian (and all of our examples so far use the NGINX image), it is most efficient to use Debian for the helper container, rather than having Docker load another operating system. The /bin/bash argument means that the bash shell runs in the helper container, presenting a shell prompt that you can use to modify files as needed.\n\nTo start and stop the container, run the following commands:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo exit the shell but leave the container running, press Ctrl+p followed by Ctrl+q . To regain shell access to a running container, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo exit the shell and terminate the container, run the exit command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 27,
          "content_length": 5578
        }
      },
      {
        "header": "Option 1 – Maintain the Content and Configuration on the Docker Host",
        "content": "When the container is created we can tell Docker to mount a local directory on the Docker host to a directory in the container. The NGINX image uses the default NGINX configuration, which uses /usr/share/nginx/html as the container’s root directory and puts configuration files in /etc/nginx . For a Docker host with content in the local directory /var/www and configuration files in /var/nginx/conf , run this command (which appears on multiple lines here only for legibility):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow any change made to the files in the local directories /var/www and /var/nginx/conf on the Docker host are reflected in the directories /usr/share/nginx/html and /etc/nginx in the container. The readonly option means these directories can be changed only on the Docker host, not from within the container.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 901
        }
      },
      {
        "header": "Option 2 – Copy Files from the Docker Host",
        "content": "Another option is to have Docker copy the content and configuration files from a local directory on the Docker host during container creation. Once a container is created, the files are maintained by creating a new container when files change or by modifying the files in the container. A simple way to copy the files is to create a Dockerfile with commands that are run during generation of a new Docker image based on the NGINX image from Docker Hub. For the file‑copy ( COPY ) commands in the Dockerfile , the local directory path is relative to the build context where the Dockerfile is located.\n\nIn our example, the content is in the content directory and the configuration files are in the conf directory, both subdirectories of the directory where the Dockerfile is located. The NGINX image includes default NGINX configuration files as /etc/nginx/nginx.conf and /etc/nginx/conf.d/default.conf . Because we instead want to use the configuration files from the host, we include a RUN command that deletes the default files:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe create our own NGINX image by running the following command from the directory where the Dockerfile is located. Note the period (“.”) at the end of the command. It defines the current directory as the build context, which contains the Dockerfile and the directories to be copied.\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow we run this command to create a container called mynginx3 based on the mynginx_image1 image:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nIf we want to make changes to the files in the container, we use a helper container as described in Option 3.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1861
        }
      },
      {
        "header": "Option 3 – Maintain Files in the Container",
        "content": "As mentioned in A Note About SSH , we can’t use SSH to access the NGINX container, so if we want to edit the content or configuration files directly we have to create a helper container that has shell access. For the helper container to have access to the files, we must create a new image that has the proper Docker data volumes defined for the image. Assuming we want to copy files as in Option 2 while also defining volumes, we use the following Dockerfile :\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe then create the new NGINX image by running the following command (again note the final period):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nNow we run this command to create an NGINX container ( mynginx4 ) based on the mynginx_image2 image:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe then run the following command to start a helper container mynginx4_files that has a shell, enabling us to access the content and configuration directories of the mynginx4 container we just created:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThe new mynginx4_files helper container runs in the foreground with a persistent standard input (the -i option) and a tty (the -t option). All volumes defined in mynginx4 are mounted as local directories in the helper container.\n\nThe debian argument means that the helper container uses the Debian image from Docker Hub. Because the NGINX image also uses Debian (and all of our examples so far use the NGINX image), it is most efficient to use Debian for the helper container, rather than having Docker load another operating system. The /bin/bash argument means that the bash shell runs in the helper container, presenting a shell prompt that you can use to modify files as needed.\n\nTo start and stop the container, run the following commands:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo exit the shell but leave the container running, press Ctrl+p followed by Ctrl+q . To regain shell access to a running container, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo exit the shell and terminate the container, run the exit command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 2511
        }
      },
      {
        "header": "Managing Logging",
        "content": "You can configure either default or customized logging.\n\nUsing Default Logging\n\nThe NGINX image is configured to send the main NGINX access and error logs to the Docker log collector by default. This is done by linking them to stdout and stderr respectively; all messages from both logs are then written to the file /var/lib/docker/containers/ container-ID / container-ID -json.log on the Docker host, where container‑ID is the long‑form ID returned when you create a container. For the initial container we created in Using the NGINX Open Source Docker Image , for example, it is fcd1fb01b14557c7c9d991238f2558ae2704d129cf9fb97bb4fadf673a58580d .\n\nTo retrieve the container ID for an existing container, run this command, where container‑name is the value set by the --name parameter when the container is created (for the container ID above, for example, it is mynginx1 ):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nAlthough you can view the logs by opening the container-ID -json.log file directly, it is usually easier to run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nYou can use also the Docker Engine API to extract the log messages, by issuing a GET request against the Docker Unix socket. This command returns both the access log (represented by stdout=1 ) and the error log ( stderr=1 ), but you can request them singly as well:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo learn about other query parameters, see the Docker Engine API documentation (search for “Get container logs” on that page).\n\nUsing Customized Logging\n\nIf you want to implement another method of log collection, or if you want to configure logging differently in certain configuration blocks (such as server{} and location{} ), define a Docker volume for the directory or directories in which to store the log files in the container, create a helper container to access the log files, and use whatever logging tools you like. To implement this, create a new image that contains the volume or volumes for the logging files.\n\nFor example, to configure NGINX to store log files in /var/log/nginx/log , we can start with the Dockerfile from Option 3 and simply add a VOLUME definition for this directory:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe can then create an image as described above and use it to create an NGINX container and a helper container that have access to the logging directory. The helper container can have any desired logging tools installed.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 13,
          "content_length": 2744
        }
      },
      {
        "header": "Using Default Logging",
        "content": "The NGINX image is configured to send the main NGINX access and error logs to the Docker log collector by default. This is done by linking them to stdout and stderr respectively; all messages from both logs are then written to the file /var/lib/docker/containers/ container-ID / container-ID -json.log on the Docker host, where container‑ID is the long‑form ID returned when you create a container. For the initial container we created in Using the NGINX Open Source Docker Image , for example, it is fcd1fb01b14557c7c9d991238f2558ae2704d129cf9fb97bb4fadf673a58580d .\n\nTo retrieve the container ID for an existing container, run this command, where container‑name is the value set by the --name parameter when the container is created (for the container ID above, for example, it is mynginx1 ):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nAlthough you can view the logs by opening the container-ID -json.log file directly, it is usually easier to run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nYou can use also the Docker Engine API to extract the log messages, by issuing a GET request against the Docker Unix socket. This command returns both the access log (represented by stdout=1 ) and the error log ( stderr=1 ), but you can request them singly as well:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo learn about other query parameters, see the Docker Engine API documentation (search for “Get container logs” on that page).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1655
        }
      },
      {
        "header": "Using Customized Logging",
        "content": "If you want to implement another method of log collection, or if you want to configure logging differently in certain configuration blocks (such as server{} and location{} ), define a Docker volume for the directory or directories in which to store the log files in the container, create a helper container to access the log files, and use whatever logging tools you like. To implement this, create a new image that contains the volume or volumes for the logging files.\n\nFor example, to configure NGINX to store log files in /var/log/nginx/log , we can start with the Dockerfile from Option 3 and simply add a VOLUME definition for this directory:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nWe can then create an image as described above and use it to create an NGINX container and a helper container that have access to the logging directory. The helper container can have any desired logging tools installed.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 981
        }
      },
      {
        "header": "Controlling NGINX",
        "content": "Since we do not have direct access to the command line of the NGINX container, we cannot use the nginx command to control NGINX. Fortunately we can use signals to control NGINX, and Docker provides the kill command for sending signals to a container.\n\nTo reload the NGINX configuration, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo restart NGINX, run this command to restart the container:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 592
        }
      },
      {
        "header": "Deploying NGINX Plus with Docker",
        "content": "So far we have discussed Docker for NGINX Open Source, but you can also use it with the commercial product, NGINX Plus. The difference is you first need to create an NGINX Plus image, because as a commercial offering NGINX Plus is not available at Docker Hub. Fortunately, this is quite easy to do.\n\nNote: Never upload your NGINX Plus images to a public repository such as Docker Hub. Doing so violates your license agreement .\n\nCreating a Docker Image of NGINX Plus\n\nTo generate an NGINX Plus image, first create a Dockerfile . The examples we provide here use Alpine Linux 3.14 and Debian 11 (Bullseye) as the base Docker images. Before you can create the NGINX Plus Docker image, you have to download your version of the nginx-repo.crt and nginx-repo.key files. NGINX Plus customers can find them at the customer portal ; if you are doing a free trial of NGINX Plus, they were provided with your trial package. Copy the files to the directory where the Dockerfile is located (the Docker build context).\n\nAs with NGINX Open Source, by default the NGINX Plus access and error logs are linked to the Docker log collector. No volumes are specified, but you can add them if desired, or each Dockerfile can be used to create base images from which you can create new images with volumes specified, as described previously.\n\nWe purposely do not specify an NGINX Plus version in the sample Dockerfile s, so that you don’t have to edit the file when you update to a new release of NGINX Plus. We have, however, included commented versions of the relevant instructions for you to uncomment if you want to make the file version‑specific.\n\nSimilarly, we’ve included instructions (commented out) that install official dynamic modules for NGINX Plus .\n\nBy default, no files are copied from the Docker host as a container is created. You can add COPY definitions to each Dockerfile , or the image you create can be used as the basis for another image as described above.\n\nNGINX Plus Dockerfile (Debian 11)\n\nLoading gist…\n\nNGINX Plus Dockerfile (Alpine Linux 3.14)\n\nLoading gist…\n\nCreating the NGINX Plus Image\n\nWith the Dockerfile , nginx-repo.crt, and nginx-repo.key files in the same directory, run the following command there to create a Docker image called nginxplus (as before, note the final period):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThe DOCKER_BUILDKIT=1 flag indicates that we are using Docker BuildKit to build the image, as required when including the --secret option which is discussed below.\n\nThe --no-cache option tells Docker to build the image from scratch and ensures the installation of the latest version of NGINX Plus. If the Dockerfile was previously used to build an image and you do not include the --no-cache option, the new image uses the version of NGINX Plus from the Docker cache. (As noted, we purposely do not specify an NGINX Plus version in the Dockerfile so that the file does not need to change at every new release of NGINX Plus.) Omit the --no-cache option if it’s acceptable to use the NGINX Plus version from the previously built image.\n\nThe --secret option passes the certificate and key for your NGINX Plus license to the Docker build context without risking exposing the data or having the data persist between Docker build layers. The values of the id arguments cannot be changed without altering the base Dockerfile , but you need to set the src arguments to the path to your NGINX Plus certificate and key files (the same directory where you are building the Docker image if you followed the previous instructions).\n\nOutput like the following from the docker images nginxplus command indicates that the image was created successfully:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo create a container named mynginxplus based on this image, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nYou can control and manage NGINX Plus containers in the same way as NGINX Open Source containers .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 4152
        }
      },
      {
        "header": "Creating a Docker Image of NGINX Plus",
        "content": "To generate an NGINX Plus image, first create a Dockerfile . The examples we provide here use Alpine Linux 3.14 and Debian 11 (Bullseye) as the base Docker images. Before you can create the NGINX Plus Docker image, you have to download your version of the nginx-repo.crt and nginx-repo.key files. NGINX Plus customers can find them at the customer portal ; if you are doing a free trial of NGINX Plus, they were provided with your trial package. Copy the files to the directory where the Dockerfile is located (the Docker build context).\n\nAs with NGINX Open Source, by default the NGINX Plus access and error logs are linked to the Docker log collector. No volumes are specified, but you can add them if desired, or each Dockerfile can be used to create base images from which you can create new images with volumes specified, as described previously.\n\nWe purposely do not specify an NGINX Plus version in the sample Dockerfile s, so that you don’t have to edit the file when you update to a new release of NGINX Plus. We have, however, included commented versions of the relevant instructions for you to uncomment if you want to make the file version‑specific.\n\nSimilarly, we’ve included instructions (commented out) that install official dynamic modules for NGINX Plus .\n\nBy default, no files are copied from the Docker host as a container is created. You can add COPY definitions to each Dockerfile , or the image you create can be used as the basis for another image as described above.\n\nNGINX Plus Dockerfile (Debian 11)\n\nLoading gist…\n\nNGINX Plus Dockerfile (Alpine Linux 3.14)\n\nLoading gist…\n\nCreating the NGINX Plus Image\n\nWith the Dockerfile , nginx-repo.crt, and nginx-repo.key files in the same directory, run the following command there to create a Docker image called nginxplus (as before, note the final period):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThe DOCKER_BUILDKIT=1 flag indicates that we are using Docker BuildKit to build the image, as required when including the --secret option which is discussed below.\n\nThe --no-cache option tells Docker to build the image from scratch and ensures the installation of the latest version of NGINX Plus. If the Dockerfile was previously used to build an image and you do not include the --no-cache option, the new image uses the version of NGINX Plus from the Docker cache. (As noted, we purposely do not specify an NGINX Plus version in the Dockerfile so that the file does not need to change at every new release of NGINX Plus.) Omit the --no-cache option if it’s acceptable to use the NGINX Plus version from the previously built image.\n\nThe --secret option passes the certificate and key for your NGINX Plus license to the Docker build context without risking exposing the data or having the data persist between Docker build layers. The values of the id arguments cannot be changed without altering the base Dockerfile , but you need to set the src arguments to the path to your NGINX Plus certificate and key files (the same directory where you are building the Docker image if you followed the previous instructions).\n\nOutput like the following from the docker images nginxplus command indicates that the image was created successfully:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo create a container named mynginxplus based on this image, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nYou can control and manage NGINX Plus containers in the same way as NGINX Open Source containers .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 17,
          "content_length": 3684
        }
      },
      {
        "header": "NGINX Plus Dockerfile (Debian 11)",
        "content": "Loading gist…",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 13
        }
      },
      {
        "header": "NGINX Plus Dockerfile (Alpine Linux 3.14)",
        "content": "Loading gist…",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 13
        }
      },
      {
        "header": "Creating the NGINX Plus Image",
        "content": "With the Dockerfile , nginx-repo.crt, and nginx-repo.key files in the same directory, run the following command there to create a Docker image called nginxplus (as before, note the final period):\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nThe DOCKER_BUILDKIT=1 flag indicates that we are using Docker BuildKit to build the image, as required when including the --secret option which is discussed below.\n\nThe --no-cache option tells Docker to build the image from scratch and ensures the installation of the latest version of NGINX Plus. If the Dockerfile was previously used to build an image and you do not include the --no-cache option, the new image uses the version of NGINX Plus from the Docker cache. (As noted, we purposely do not specify an NGINX Plus version in the Dockerfile so that the file does not need to change at every new release of NGINX Plus.) Omit the --no-cache option if it’s acceptable to use the NGINX Plus version from the previously built image.\n\nThe --secret option passes the certificate and key for your NGINX Plus license to the Docker build context without risking exposing the data or having the data persist between Docker build layers. The values of the id arguments cannot be changed without altering the base Dockerfile , but you need to set the src arguments to the path to your NGINX Plus certificate and key files (the same directory where you are building the Docker image if you followed the previous instructions).\n\nOutput like the following from the docker images nginxplus command indicates that the image was created successfully:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nTo create a container named mynginxplus based on this image, run this command:\n\n[@portabletext/react] Unknown block type \"codeBlock\", specify a component for it in the `components.types` prop\n\nYou can control and manage NGINX Plus containers in the same way as NGINX Open Source containers .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2053
        }
      },
      {
        "header": "Summary",
        "content": "NGINX, NGINX Plus, and Docker work extremely well together. Whether you use the NGINX Open Source image from Docker Hub or create your own NGINX Plus image, you can easily spin up new instances of NGINX and NGINX Plus in Docker containers and deploy them in your Kubernetes environment. You can also easily create new Docker images from the base images, making your containers even easier to control and manage. Make sure that all NGINX Plus instances running in your Docker containers are covered by your subscription. For details, please contact the NGINX sales team .\n\nThere is much more to Docker than we have been able to cover in this article. For more information, download our free O’Reilly eBook – Container Networking: From Docker to Kubernetes – or check out www.docker.com .\n\nRelated Documentation\n\nDeploying NGINX and NGINX Plus on Docker\n\nShare\n\nFeatured Blog Posts Current trends in cloud-native technologies, platform engineering, and AI Full lifecycle management of app and API delivery with F5 NGINX One Simplifying Modern Application Delivery: Nine Months of F5 NGINX One\n\nTags: F5 NGINX , 2021 , Tech\n\nAbout the Author Rick Nelson RVP, Solution Engineering More blogs by Rick Nelson",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1202
        }
      },
      {
        "header": "Related Documentation",
        "content": "Deploying NGINX and NGINX Plus on Docker\n\nShare\n\nFeatured Blog Posts Current trends in cloud-native technologies, platform engineering, and AI Full lifecycle management of app and API delivery with F5 NGINX One Simplifying Modern Application Delivery: Nine Months of F5 NGINX One\n\nTags: F5 NGINX , 2021 , Tech\n\nAbout the Author Rick Nelson RVP, Solution Engineering More blogs by Rick Nelson",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 391
        }
      },
      {
        "header": "About the Author",
        "content": "Rick Nelson RVP, Solution Engineering More blogs by Rick Nelson",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 63
        }
      },
      {
        "header": "Related Blog Posts",
        "content": "NGINX | 10/05/2022 Automating Certificate Management in a Kubernetes Environment Simplify cert management by providing unique, automatically renewed and updated certificates to your endpoints. DNS , SSL/TLS , F5 NGINX , 2022 , Tech NGINX | 05/26/2022 Secure Your API Gateway with NGINX App Protect WAF As monoliths move to microservices, applications are developed faster than ever. Speed is necessary to stay competitive and APIs sit at the front of these rapid modernization efforts. But the popularity of APIs for application modernization has significant implications for app security. F5 NGINX , Tech , 2022 NGINX | 12/09/2021 How Do I Choose? API Gateway vs. Ingress Controller vs. Service Mesh When you need an API gateway in Kubernetes, how do you choose among API gateway vs. Ingress controller vs. service mesh? We guide you through the decision, with sample scenarios for north-south and east-west API traffic, plus use cases where an API gateway is the right tool. F5 NGINX , Tech , 2021 NGINX | 01/20/2021 Deploying NGINX as an API Gateway, Part 2: Protecting Backend Services In the second post in our API gateway series, Liam shows you how to batten down the hatches on your API services. You can use rate limiting, access restrictions, request size limits, and request body validation to frustrate illegitimate or overly burdensome requests. Security , F5 NGINX , 2021 , Tech NGINX | 12/15/2015 New Joomla Exploit CVE-2015-8562 Read about the new zero day exploit in Joomla and see the NGINX configuration for how to apply a fix in NGINX or NGINX Plus. CVE-2015-8562 , F5 NGINX , News , Tech , Application Delivery NGINX | 01/01/2014 Why Do I See “Welcome to nginx!” on My Favorite Website? The ‘Welcome to NGINX!’ page is presented when NGINX web server software is installed on a computer but has not finished configuring F5 NGINX , Tech Previous slide Next slide",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1881
        }
      },
      {
        "header": "Automating Certificate Management in a Kubernetes Environment",
        "content": "Simplify cert management by providing unique, automatically renewed and updated certificates to your endpoints.\n\nDNS , SSL/TLS , F5 NGINX , 2022 , Tech",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 151
        }
      },
      {
        "header": "Secure Your API Gateway with NGINX App Protect WAF",
        "content": "As monoliths move to microservices, applications are developed faster than ever. Speed is necessary to stay competitive and APIs sit at the front of these rapid modernization efforts. But the popularity of APIs for application modernization has significant implications for app security.\n\nF5 NGINX , Tech , 2022",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 311
        }
      },
      {
        "header": "How Do I Choose? API Gateway vs. Ingress Controller vs. Service Mesh",
        "content": "When you need an API gateway in Kubernetes, how do you choose among API gateway vs. Ingress controller vs. service mesh? We guide you through the decision, with sample scenarios for north-south and east-west API traffic, plus use cases where an API gateway is the right tool.\n\nF5 NGINX , Tech , 2021",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 299
        }
      },
      {
        "header": "Deploying NGINX as an API Gateway, Part 2: Protecting Backend Services",
        "content": "In the second post in our API gateway series, Liam shows you how to batten down the hatches on your API services. You can use rate limiting, access restrictions, request size limits, and request body validation to frustrate illegitimate or overly burdensome requests.\n\nSecurity , F5 NGINX , 2021 , Tech",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 302
        }
      },
      {
        "header": "New Joomla Exploit CVE-2015-8562",
        "content": "Read about the new zero day exploit in Joomla and see the NGINX configuration for how to apply a fix in NGINX or NGINX Plus.\n\nCVE-2015-8562 , F5 NGINX , News , Tech , Application Delivery",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 187
        }
      },
      {
        "header": "Why Do I See “Welcome to nginx!” on My Favorite Website?",
        "content": "The ‘Welcome to NGINX!’ page is presented when NGINX web server software is installed on a computer but has not finished configuring\n\nF5 NGINX , Tech",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h4",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 149
        }
      }
    ],
    "url": "https://www.f5.com/company/blog/nginx/deploying-nginx-nginx-plus-docker",
    "doc_type": "docker",
    "total_sections": 27
  },
  {
    "title": "Using the Docker execution environment",
    "summary": "You can use the Docker execution environment to run your jobs in Docker containers. The Docker execution environment is accessed using the Docker executor . Using Docker increases performance by building only what is required for your application. Specify a Docker image in your .circleci/config.yml file to spin up a container. All steps in your job will be run in this container.",
    "sections": [
      {
        "header": "Specifying Docker images",
        "content": "Docker images may be specified in a few ways: By the image name and version tag on Docker Hub, or By using the URL to an image in a registry. Nearly all of the public images on Docker Hub and other Docker registries are supported by default when you specify the docker: key in your config.yml file. If you want to work with private images/registries, refer to Using Docker Authenticated Pulls . The following examples show how you can use public images from various sources: CircleCI’s public convenience images on Docker Hub name:tag cimg/node:14.17-browsers name@digest cimg/node@sha256:aa6d08a04d13dd8a... Public images on Docker Hub name:tag alpine:3.13 name@digest alpine@sha256:e15947432b813e8f... Public images on Docker registries image_full_url:tag gcr.io/google-containers/busybox:1.24 image_full_url@digest gcr.io/google-containers/busybox@sha256:4bdd623e848417d9612...",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 880
        }
      },
      {
        "header": "CircleCI’s public convenience images on Docker Hub",
        "content": "name:tag cimg/node:14.17-browsers name@digest cimg/node@sha256:aa6d08a04d13dd8a...",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 82
        }
      },
      {
        "header": "Public images on Docker Hub",
        "content": "name:tag alpine:3.13 name@digest alpine@sha256:e15947432b813e8f...",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 66
        }
      },
      {
        "header": "Public images on Docker registries",
        "content": "image_full_url:tag gcr.io/google-containers/busybox:1.24 image_full_url@digest gcr.io/google-containers/busybox@sha256:4bdd623e848417d9612...",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 141
        }
      },
      {
        "header": "Available Docker resource classes",
        "content": "The resource_class key allows you to configure CPU and RAM resources for each job. Specify a resource class using the resource_class key, as follows: jobs: build: docker: - image: cimg/base:current resource_class: xlarge steps: # ... other config x86 For the Docker execution environment, the following resources classes are available for the x86 architecture: For credit and access information, see the Resource classes page . Resource class access is dependent on your Plan . Class vCPUs RAM Cloud Server small 1 2GB medium 2 4GB medium+ 3 6GB large 4 8GB xlarge 8 16GB 2xlarge 16 32GB 2xlarge+ 20 40GB Arm The following resource classes are available for Arm with Docker: Arm on Docker For credit and access information see the Resource classes page . Resource class access is dependent on your Plan To find out which CircleCI Docker convenience images support Arm resource classes, you can refer to Docker Hub : Select the image (for example, cimg/python ). Select the tags tab. View what is supported under OS/ARCH for the latest tags. For example, cimg/python has linux/amd64 and linux/arm64 , which means Arm is supported. Class vCPUs RAM Cloud Server arm.medium 2 8 GB arm.large 4 16 GB arm.xlarge 8 32 GB arm.2xlarge 16 64 GB View resource usage To view the compute resource usage for the duration of a job in the CircleCI web app: Select Dashboard from the sidebar menu. Use the dropdown menus to select a project and a branch. Expand your workflow. Select a job by clicking on the job name. Select the Resources tab to view CPU and RAM usage for the duration of the job. You can use these insights to decide whether to make changes to the job’s configured resource class.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1682
        }
      },
      {
        "header": "x86",
        "content": "For the Docker execution environment, the following resources classes are available for the x86 architecture:\n\nFor credit and access information, see the Resource classes page . Resource class access is dependent on your Plan .\n\nClass vCPUs RAM Cloud Server small 1 2GB medium 2 4GB medium+ 3 6GB large 4 8GB xlarge 8 16GB 2xlarge 16 32GB 2xlarge+ 20 40GB",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 355
        }
      },
      {
        "header": "Arm",
        "content": "The following resource classes are available for Arm with Docker:\n\nArm on Docker For credit and access information see the Resource classes page . Resource class access is dependent on your Plan To find out which CircleCI Docker convenience images support Arm resource classes, you can refer to Docker Hub : Select the image (for example, cimg/python ). Select the tags tab. View what is supported under OS/ARCH for the latest tags. For example, cimg/python has linux/amd64 and linux/arm64 , which means Arm is supported.\n\nClass vCPUs RAM Cloud Server arm.medium 2 8 GB arm.large 4 16 GB arm.xlarge 8 32 GB arm.2xlarge 16 64 GB",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 627
        }
      },
      {
        "header": "View resource usage",
        "content": "To view the compute resource usage for the duration of a job in the CircleCI web app:\n\nSelect Dashboard from the sidebar menu. Use the dropdown menus to select a project and a branch. Expand your workflow. Select a job by clicking on the job name. Select the Resources tab to view CPU and RAM usage for the duration of the job.\n\nYou can use these insights to decide whether to make changes to the job’s configured resource class.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 429
        }
      },
      {
        "header": "Docker benefits and limitations",
        "content": "Docker also has built-in image caching and enables you to build, run, and publish Docker images via Remote Docker . Consider the requirements of your application as well. If the following are true for your application, Docker may be the right choice: Your application is self-sufficient. Your application requires additional services to be tested. Your application is distributed as a Docker image (requires using Remote Docker ). You want to use docker compose (requires using Remote Docker ). Choosing Docker limits your runs to what is possible from within a Docker container (including our Remote Docker feature). For instance, if you require low-level access to the network or need to mount external volumes, consider using machine . The table below shows tradeoffs between using a docker image versus an Ubuntu-based machine image as the environment for the container: Capability docker machine Start time Instant Instant for most (1) Clean environment Yes Yes Custom images Yes (2) No Build Docker images Yes (3) Yes Full control over job environment No Yes Full root access No Yes Run multiple databases Yes (4) Yes Run multiple versions of the same software No Yes Docker layer caching Yes Yes Run privileged containers No Yes Use Docker compose with volumes No Yes Configurable resources (CPU/RAM) Yes Yes (1) Some less commonly used execution environments may see up to 90 seconds of start time. (2) See Using Custom Docker Images . (3) Requires using Remote Docker . (4) While you can run multiple databases with Docker, all images (primary and secondary) share the underlying resource limits. Performance in this regard will be dictated by the compute capacities of your plan. For more information on machine , see the next section below.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1751
        }
      },
      {
        "header": "Docker image best practices",
        "content": "If you encounter problems with rate limits imposed by your registry provider, using authenticated Docker pulls may grant higher limits. CircleCI has partnered with Docker to ensure that our users can continue to access Docker Hub without rate limits. As of November 1st 2020, with few exceptions, you should not be impacted by any rate limits when pulling images from Docker Hub through CircleCI. However, these rate limits may go into effect for CircleCI users in the future. We encourage you to add Docker Hub authentication to your CircleCI configuration and consider upgrading your Docker Hub plan, as appropriate, to prevent any impact from rate limits in the future. Avoid using mutable tags like latest or 1 as the image version in your config.yml file . It is best practice to use precise image versions or digests, like redis:3.2.7 or redis@sha256:95f0c9434f37db0a4f... as shown in the examples. Mutable tags often lead to unexpected changes in your job environment. CircleCI cannot guarantee that mutable tags will return an up-to-date version of an image. You could specify alpine:latest and actually get a stale cache from a month ago. If you experience increases in your run times due to installing additional tools during execution, consider creating and using a custom-built image that comes with those tools pre-installed. See the Using Custom-Built Docker Images page for more information. When you use AWS ECR images, it is best practice to use us-east-1 region. Our job execution infrastructure is in us-east-1 region, so having your image on the same region reduces the image download time. If your pipelines are failing despite there being little to no changes in your project, you may need to investigate upstream issues with the Docker images being used. More details on the Docker executor are available on the Configuration reference page.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1864
        }
      },
      {
        "header": "Using multiple Docker images",
        "content": "It is possible to specify multiple images for your job. Each image will be used to spin up a separate container. Using multiple containers for a job will be useful if you need to use a database for your tests, or for some other required service. When using a multi-container job setup, all containers run in a common network and every exposed port will be available on localhost . All containers can communicate with one another. It is also possible to change this hostname using the name key. For a full list of options, see the Configuration reference . In a multi-image configuration job, all steps are executed in the container created by the first image listed . jobs: build: docker: # Primary container image where all steps run. - image: cimg/base:current # Secondary container image on common network. - image: cimg/mariadb:10.6 steps: # command will execute in an Ubuntu-based container # and can access MariaDB on localhost - run: sleep 5 && nc -vz localhost 3306",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 973
        }
      },
      {
        "header": "RAM disks",
        "content": "A RAM disk is available at /mnt/ramdisk that offers a temporary file storage paradigm , similar to using /dev/shm . Using the RAM disk can help speed up your build, provided that the resource_class you are using has enough memory to fit the entire contents of your project (all files checked out from git, dependencies, assets generated etc). The simplest way to use this RAM disk is to configure the working_directory of a job to be /mnt/ramdisk : jobs: build: docker: - image: alpine working_directory: /mnt/ramdisk steps: - run: | echo '#!/bin/sh' > run.sh echo 'echo Hello world!' >> run.sh chmod +x run.sh - run: ./run.sh",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 626
        }
      },
      {
        "header": "Caching Docker images",
        "content": "This section discusses caching the Docker images used to spin up a Docker execution environment. It does not apply to Docker layer caching , which is a feature used to speed up building new Docker images in your projects. The time it takes to spin up a Docker container to run a job can vary based on several different factors, such as the size of the image and if some, or all, of the layers are already cached on the underlying Docker host machine. If you are using a more popular image, such as CircleCI convenience images, then cache hits are more likely for a larger number of layers. Most of the popular CircleCI images use the same base image. The majority of the base layers are the same between images, so you have a greater chance of having a cache hit. The environment has to spin up for every new job, regardless of whether it is in the same workflow or if it is a re-run/subsequent run. (CircleCI never reuses containers, for security reasons.) Once the job is finished, the container is destroyed. There is no guarantee that jobs, even in the same workflow, will run on the same Docker host machine. This implies that the cache status may differ. In all cases, cache hits are not guaranteed, but are a bonus convenience when available. With this in mind, a worst-case scenario of a full image pull should be accounted for in all jobs. In summary, the availability of caching is not something that can be controlled via settings or configuration, but by choosing a popular image, such as CircleCI convenience images , you will have more chances of hitting cached layers in the \"Spin Up Environment\" step.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1617
        }
      },
      {
        "header": "Next steps",
        "content": "Find out more about using Convenience Images with the Docker executor.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 70
        }
      }
    ],
    "url": "https://circleci.com/docs/guides/execution-managed/using-docker/",
    "doc_type": "docker",
    "total_sections": 14
  },
  {
    "title": "Docker layer caching overview",
    "summary": "Use Docker layer caching (DLC) to reduce Docker image build times on CircleCI. DLC is available on all CircleCI plans. Docker layer caching (DLC) is beneficial if building Docker images is a regular part of your CI/CD process. DLC saves Docker image layers created within your jobs, and caches them to be reused during future builds.",
    "sections": [
      {
        "header": "Introduction",
        "content": "Docker layer caching (DLC) is beneficial if building Docker images is a regular part of your CI/CD process. DLC saves Docker image layers created within your jobs, and caches them to be reused during future builds. DLC caches the individual layers of any Docker images built during your CircleCI jobs, and then reuses unchanged image layers on subsequent job runs, rather than rebuilding the entire image every time. In short, the less your Dockerfiles change from commit to commit, the less time your image-building jobs will take to run. DLC can be used with both the machine executor and in a remote Docker environment ( setup_remote_docker ). DLC is charged at 200 credits per job run. For more information about DLC charges, see the FAQ .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 743
        }
      },
      {
        "header": "Quickstart",
        "content": "Remote Docker environment To use DLC with the Docker execution environment, you will need to configure your job to run in a Remote Docker Environment . To do this, add docker_layer_caching: true under the setup_remote_docker key in your .circleci/config.yml file: version: 2.1 jobs: build: docker: - image: cimg/base:2023.04 steps: - checkout - setup_remote_docker: docker_layer_caching: true Machine executor DLC can be used when building Docker images using the machine executor . Use DLC with the machine executor by adding docker_layer_caching: true below your machine key: version: 2.1 jobs: build: machine: image: ubuntu-2004:202104-01 # any available image docker_layer_caching: true # default - false steps: - checkout",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 726
        }
      },
      {
        "header": "Remote Docker environment",
        "content": "To use DLC with the Docker execution environment, you will need to configure your job to run in a Remote Docker Environment . To do this, add docker_layer_caching: true under the setup_remote_docker key in your .circleci/config.yml file:\n\nversion: 2.1 jobs: build: docker: - image: cimg/base:2023.04 steps: - checkout - setup_remote_docker: docker_layer_caching: true",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 367
        }
      },
      {
        "header": "Machine executor",
        "content": "DLC can be used when building Docker images using the machine executor . Use DLC with the machine executor by adding docker_layer_caching: true below your machine key:\n\nversion: 2.1 jobs: build: machine: image: ubuntu-2004:202104-01 # any available image docker_layer_caching: true # default - false steps: - checkout",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 317
        }
      },
      {
        "header": "Limitations",
        "content": "Building Docker images DLC is only useful when creating your own Docker image with docker build , docker compose , or similar Docker commands. It does not decrease the wall clock time that all builds take to spin up the initial environment. version: 2.1 orbs: browser-tools: circleci/browser-tools@1.2.3 jobs: build: docker: - image: cimg/node:17.2-browsers steps: - checkout - setup_remote_docker: docker_layer_caching: true # DLC will explicitly cache layers here and try to avoid rebuilding. - run: docker build . DLC has no effect on Docker images used as build containers. That is, containers that are used to run your jobs are specified with the image key when using the docker executor and appear in the Spin up Environment step on your jobs pages. Buildx builder instances When using Buildx builder instances with DLC, it is important to name your builders . Doing so ensures CircleCI can detect and preserve the Docker volumes for subsequent job runs. If you do not name your builders, each time the job is run they will have different, randomly generated names, and the resulting volumes will be automatically cleaned up. If you store Docker build artifacts in a Docker volume, managed by the BuildKit inside Buildx builder instances, the DLC feature cannot maintain these artifacts, but they can still be supported. DLC is not able to prune these images/build cache, but Buildx builders do have some in-built pruning. For more information, see the Docker docs .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1472
        }
      },
      {
        "header": "Building Docker images",
        "content": "DLC is only useful when creating your own Docker image with docker build , docker compose , or similar Docker commands. It does not decrease the wall clock time that all builds take to spin up the initial environment.\n\nversion: 2.1 orbs: browser-tools: circleci/browser-tools@1.2.3 jobs: build: docker: - image: cimg/node:17.2-browsers steps: - checkout - setup_remote_docker: docker_layer_caching: true # DLC will explicitly cache layers here and try to avoid rebuilding. - run: docker build .\n\nDLC has no effect on Docker images used as build containers. That is, containers that are used to run your jobs are specified with the image key when using the docker executor and appear in the Spin up Environment step on your jobs pages.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 734
        }
      },
      {
        "header": "Buildx builder instances",
        "content": "When using Buildx builder instances with DLC, it is important to name your builders . Doing so ensures CircleCI can detect and preserve the Docker volumes for subsequent job runs. If you do not name your builders, each time the job is run they will have different, randomly generated names, and the resulting volumes will be automatically cleaned up.\n\nIf you store Docker build artifacts in a Docker volume, managed by the BuildKit inside Buildx builder instances, the DLC feature cannot maintain these artifacts, but they can still be supported. DLC is not able to prune these images/build cache, but Buildx builders do have some in-built pruning. For more information, see the Docker docs .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 692
        }
      },
      {
        "header": "How DLC works",
        "content": "DLC caches your Docker image layers within the container/virtual machine used to run your job. If, for example, the first run of your job takes over two minutes to build a Docker image, and nothing changes in the Dockerfile before the second run, the Dockerfile build steps happen instantly, in zero seconds. When none of the layers in the image change between job runs, DLC pulls the layers from the cache and reuses those instead of rebuilding the entire image. If part of the Dockerfile changes (which changes part of the image), a subsequent run of the exact same job with the modified Dockerfile may still finish faster than rebuilding the entire image. This speed optimization happens because the cache can still be used for the first few steps that did not change in the Dockerfile. The steps that follow the change must be rerun because the Dockerfile change invalidates the cache for those layers. If you change something in your Dockerfile, all of the later steps (from the point that the change was made) are invalidated and the layers have to be rebuilt. When some of the steps remain the same (the steps before the one you removed), those steps can be reused, therefore, it is still faster than rebuilding the entire image. At the beginning of each job that uses DLC, there is a DLC set-up step. You are not charged for this step. At the end of each job that uses DLC, the cache upload is done asynchronously, and does not prevent the workflow from continuing to progress. This means that jobs within the same workflow are unlikely to access a cache uploaded from an upstream job. You are not charged for this DLC teardown step. Parallelism and DLC DLC operates in the same way for jobs that use parallelism. If a machine job using DLC is configured with parallelism: 2 , two jobs run in parallel. Each virtual machine in this case will have a separate DLC cache, and whichever is saved last will be used for the next build.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1937
        }
      },
      {
        "header": "Parallelism and DLC",
        "content": "DLC operates in the same way for jobs that use parallelism. If a machine job using DLC is configured with parallelism: 2 , two jobs run in parallel. Each virtual machine in this case will have a separate DLC cache, and whichever is saved last will be used for the next build.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 275
        }
      },
      {
        "header": "Deprecated keys",
        "content": "DLC was previously enabled via the reusable: true key. The reusable key has been deprecated in favor of the docker_layer_caching key. In addition, the exclusive: true option is deprecated and all remote Docker VMs are now treated as exclusive. This means that when using DLC, jobs are guaranteed to have an exclusive remote Docker environment that other jobs cannot access.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 373
        }
      }
    ],
    "url": "https://circleci.com/docs/guides/optimize/docker-layer-caching/",
    "doc_type": "docker",
    "total_sections": 10
  },
  {
    "title": "Docker",
    "summary": "> User Documentation Home Docker is a platform for running applications in an isolated environment called a \"container\" (or Docker container). Applications like Jenkins can be downloaded as read-only \"images\" (or Docker images), each of which is run in Docker as a container. A Docker container is a \"running instance\" of a Docker image. A Docker image is stored permanently, based on when image updates are published, whereas containers are stored temporarily. Learn more about these concepts in Getting Started, Part 1: Orientation and setup in the Docker documentation.",
    "sections": [
      {
        "header": "Installing Docker",
        "content": "To install Docker on your operating system, follow the instructions in the Guided Tour prerequisites .\n\nAlternatively, visit Docker Hub , and select the Docker Community Edition suitable for your operating system or cloud service. Follow the installation instructions on their website.\n\nIf you are installing Docker on a Linux-based operating system, ensure you configure Docker so it can be managed as a non-root user. Read more about this in Docker’s Post-installation steps for Linux page of their documentation. This page also contains information about how to configure Docker to start on boot.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 599
        }
      },
      {
        "header": "Prerequisites",
        "content": "Minimum hardware requirements: 256 MB of RAM 1 GB of drive space (although 10 GB is a recommended minimum if running Jenkins as a Docker container) Recommended hardware configuration for a small team: 4 GB+ of RAM 50 GB+ of drive space Comprehensive hardware recommendations: Hardware: see the Hardware Recommendations page Software requirements: Java: see the Java Requirements page Web browser: see the Web Browser Compatibility page For Windows operating system: Windows Support Policy For Linux operating system: Linux Support Policy For servlet containers: Servlet Container Support Policy Downloading and running Jenkins in Docker There are several Docker images of Jenkins available. Use the recommended official jenkins/jenkins image from the Docker Hub repository . This image contains the current Long-Term Support (LTS) release of Jenkins , which is production-ready. However, this image doesn’t contain Docker CLI, and is not bundled with the frequently used Blue Ocean plugins and its features. To use the full power of Jenkins and Docker, you may want to go through the installation process described below. A new jenkins/jenkins image is published each time a new release of Jenkins Docker is published. You can see a list of previously published versions of the jenkins/jenkins image on the tags page. On macOS and Linux Open up a terminal window. Create a bridge network in Docker using the following docker network create command: docker network create jenkins In order to execute Docker commands inside Jenkins nodes, download and run the docker:dind Docker image using the following docker run command: docker run \\ --name jenkins-docker \\ (1) --rm \\ (2) --detach \\ (3) --privileged \\ (4) --network jenkins \\ (5) --network-alias docker \\ (6) --env DOCKER_TLS_CERTDIR = /certs \\ (7) --volume jenkins-docker-certs:/certs/client \\ (8) --volume jenkins-data:/var/jenkins_home \\ (9) --publish 2376:2376 \\ (10) docker:dind \\ (11) --storage-driver overlay2 (12) 1 ( Optional ) Specifies the Docker container name to use for running the image. By default, Docker generates a unique name for the container. 2 ( Optional ) Automatically removes the Docker container (the replica of the Docker image) when it is shut down. 3 ( Optional ) Runs the Docker container in the background. You can stop this process by running docker stop jenkins-docker . 4 Running Docker in Docker currently requires privileged access to function properly. This requirement may be relaxed with newer Linux kernel versions. 5 This corresponds with the network created in the earlier step. 6 Makes the Docker in Docker container available as the hostname docker within the jenkins network. 7 Enables the use of TLS in the Docker server. Due to the use of a privileged container, this is recommended, though it requires the use of the shared volume described below. This environment variable controls the root directory where Docker TLS certificates are managed. 8 Maps the /certs/client directory inside the container to a Docker volume named jenkins-docker-certs as created above. 9 Maps the /var/jenkins_home directory inside the container to the Docker volume named jenkins-data . This allows for other Docker containers controlled by this Docker container’s Docker daemon to mount data from Jenkins. 10 ( Optional ) Exposes the Docker daemon port on the host machine. This is useful for executing docker commands on the host machine to control this inner Docker daemon. 11 The docker:dind image itself. Download this image before running, by using the command: docker image pull docker:dind . 12 The storage driver for the Docker volume. Refer to the Docker storage drivers documentation for supported options. If you have problems copying and pasting the above command snippet, use the annotation-free version below: docker run --name jenkins-docker --rm --detach \\ --privileged --network jenkins --network-alias docker \\ --env DOCKER_TLS_CERTDIR = /certs \\ --volume jenkins-docker-certs:/certs/client \\ --volume jenkins-data:/var/jenkins_home \\ --publish 2376:2376 \\ docker:dind --storage-driver overlay2 Customize the official Jenkins Docker image, by executing the following two steps: Create a Dockerfile with the following content: FROM jenkins/jenkins:2.528.2-jdk21 USER root RUN apt-get update && apt-get install -y lsb-release ca-certificates curl && \\ install -m 0755 -d /etc/apt/keyrings && \\ curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc && \\ chmod a+r /etc/apt/keyrings/docker.asc && \\ echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] \\ https://download.docker.com/linux/debian $(. /etc/os-release && echo \\\"$VERSION_CODENAME\\\") stable\" \\ | tee /etc/apt/sources.list.d/docker.list > /dev/null && \\ apt-get update && apt-get install -y docker-ce-cli && \\ apt-get clean && rm -rf /var/lib/apt/lists/* USER jenkins RUN jenkins-plugin-cli --plugins \"blueocean docker-workflow json-path-api\" Build a new docker image from this Dockerfile, and assign the image a meaningful name, such as \"myjenkins-blueocean:2.528.2-1\": docker build -t myjenkins-blueocean:2.528.2-1 . If you have not yet downloaded the official Jenkins Docker image, the above process automatically downloads it for you. Run your own myjenkins-blueocean:2.528.2-1 image as a container in Docker using the following docker run command: docker run \\ --name jenkins-blueocean \\ (1) --restart = on-failure \\ (2) --detach \\ (3) --network jenkins \\ (4) --env DOCKER_HOST = tcp://docker:2376 \\ (5) --env DOCKER_CERT_PATH = /certs/client \\ --env DOCKER_TLS_VERIFY = 1 \\ --publish 8080:8080 \\ (6) --publish 50000:50000 \\ (7) --volume jenkins-data:/var/jenkins_home \\ (8) --volume jenkins-docker-certs:/certs/client:ro \\ (9) myjenkins-blueocean:2.528.2-1 (10) 1 ( Optional ) Specifies the Docker container name for this instance of the Docker image. 2 Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. 3 ( Optional ) Runs the current container in the background, known as \"detached\" mode, and outputs the container ID. If you do not specify this option, then the running Docker log for this container is displayed in the terminal window. 4 Connects this container to the jenkins network previously defined. The Docker daemon is now available to this Jenkins container through the hostname docker . 5 Specifies the environment variables used by docker , docker-compose , and other Docker tools to connect to the Docker daemon from the previous step. 6 Maps, or publishes, port 8080 of the current container to port 8080 on the host machine. The first number represents the port on the host, while the last represents the container’s port. For example, to access Jenkins on your host machine through port 49000, enter -p 49000:8080 for this option. 7 ( Optional ) Maps port 50000 of the current container to port 50000 on the host machine. This is only necessary if you have set up one or more inbound Jenkins agents on other machines, which in turn interact with your jenkins-blueocean container, known as the Jenkins \"controller\". Inbound Jenkins agents communicate with the Jenkins controller through TCP port 50000 by default. You can change this port number on your Jenkins controller through the Security page. For example, if you update the TCP port for inbound Jenkins agents of your Jenkins controller to 51000, you need to re-run Jenkins via the docker run …​ command. Specify the \"publish\" option as follows: the first value is the port number on the machine hosting the Jenkins controller, and the last value matches the changed value on the Jenkins controller, for example, --publish 52000:51000 . Inbound Jenkins agents communicate with the Jenkins controller on that port (52000 in this example). Note that WebSocket agents do not need this configuration. 8 Maps the /var/jenkins_home directory in the container to the Docker volume with the name jenkins-data . Instead of mapping the /var/jenkins_home directory to a Docker volume, you can also map this directory to one on your machine’s local file system. For example, specify the option --volume $HOME/jenkins:/var/jenkins_home to map the container’s /var/jenkins_home directory to the jenkins subdirectory within the $HOME directory on your local machine — typically /Users/<your-username>/jenkins or /home/<your-username>/jenkins . NOTE: If you change the source volume or directory for this, the volume from the docker:dind container above needs to be updated to match this. 9 Maps the /certs/client directory to the previously created jenkins-docker-certs volume. The client TLS certificates required to connect to the Docker daemon are now available in the path specified by the DOCKER_CERT_PATH environment variable. 10 The name of the Docker image, which you built in the previous step. If you have problems copying and pasting the command snippet, use the annotation-free version below: docker run --name jenkins-blueocean --restart = on-failure --detach \\ --network jenkins --env DOCKER_HOST = tcp://docker:2376 \\ --env DOCKER_CERT_PATH = /certs/client --env DOCKER_TLS_VERIFY = 1 \\ --publish 8080:8080 --publish 50000:50000 \\ --volume jenkins-data:/var/jenkins_home \\ --volume jenkins-docker-certs:/certs/client:ro \\ myjenkins-blueocean:2.528.2-1 Proceed to the Post-installation setup wizard . On Windows The Jenkins project provides a Linux container image, not a Windows container image. Be sure that your Docker for Windows installation is configured to run Linux Containers rather than Windows Containers . Refer to the Docker documentation for instructions to switch to Linux containers . Once configured to run Linux Containers , the steps are: Open up a command prompt window and similar to the macOS and Linux instructions above do the following: Create a bridge network in Docker docker network create jenkins Run a docker:dind Docker image docker run --name jenkins-docker --rm --detach ^ --privileged --network jenkins --network-alias docker ^ --env DOCKER_TLS_CERTDIR = /certs ^ --volume jenkins-docker-certs:/certs/client ^ --volume jenkins-data:/var/jenkins_home ^ --publish 2376:2376 ^ docker:dind Customize the official Jenkins Docker image, by executing the following two steps: Create a Dockerfile with the following content: FROM jenkins/jenkins:2.528.2-jdk21 USER root RUN apt-get update && apt-get install -y lsb-release RUN curl -fsSLo /usr/share/keyrings/docker-archive-keyring.asc \\ https://download.docker.com/linux/debian/gpg RUN echo \"deb [arch= $( dpkg --print-architecture ) \\ signed-by=/usr/share/keyrings/docker-archive-keyring.asc] \\ https://download.docker.com/linux/debian \\ $( lsb_release -cs ) stable\" > /etc/apt/sources.list.d/docker.list RUN apt-get update && apt-get install -y docker-ce-cli USER jenkins RUN jenkins-plugin-cli --plugins \"blueocean docker-workflow json-path-api\" Build a new docker image from this Dockerfile and assign the image a meaningful name, e.g. \"myjenkins-blueocean:2.528.2-1\": docker build -t myjenkins-blueocean:2.528.2-1 . If you have not yet downloaded the official Jenkins Docker image, the above process automatically downloads it for you. Run your own myjenkins-blueocean:2.528.2-1 image as a container in Docker using the following docker run command: docker run --name jenkins-blueocean --restart = on-failure --detach ^ --network jenkins --env DOCKER_HOST = tcp://docker:2376 ^ --env DOCKER_CERT_PATH = /certs/client --env DOCKER_TLS_VERIFY = 1 ^ --volume jenkins-data:/var/jenkins_home ^ --volume jenkins-docker-certs:/certs/client:ro ^ --publish 8080:8080 --publish 50000:50000 myjenkins-blueocean:2.528.2-1 Proceed to the Setup wizard .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 11739
        }
      },
      {
        "header": "Downloading and running Jenkins in Docker",
        "content": "There are several Docker images of Jenkins available.\n\nUse the recommended official jenkins/jenkins image from the Docker Hub repository . This image contains the current Long-Term Support (LTS) release of Jenkins , which is production-ready. However, this image doesn’t contain Docker CLI, and is not bundled with the frequently used Blue Ocean plugins and its features. To use the full power of Jenkins and Docker, you may want to go through the installation process described below.\n\nA new jenkins/jenkins image is published each time a new release of Jenkins Docker is published. You can see a list of previously published versions of the jenkins/jenkins image on the tags page.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 682
        }
      },
      {
        "header": "On macOS and Linux",
        "content": "Open up a terminal window. Create a bridge network in Docker using the following docker network create command: docker network create jenkins In order to execute Docker commands inside Jenkins nodes, download and run the docker:dind Docker image using the following docker run command: docker run \\ --name jenkins-docker \\ (1) --rm \\ (2) --detach \\ (3) --privileged \\ (4) --network jenkins \\ (5) --network-alias docker \\ (6) --env DOCKER_TLS_CERTDIR = /certs \\ (7) --volume jenkins-docker-certs:/certs/client \\ (8) --volume jenkins-data:/var/jenkins_home \\ (9) --publish 2376:2376 \\ (10) docker:dind \\ (11) --storage-driver overlay2 (12) 1 ( Optional ) Specifies the Docker container name to use for running the image. By default, Docker generates a unique name for the container. 2 ( Optional ) Automatically removes the Docker container (the replica of the Docker image) when it is shut down. 3 ( Optional ) Runs the Docker container in the background. You can stop this process by running docker stop jenkins-docker . 4 Running Docker in Docker currently requires privileged access to function properly. This requirement may be relaxed with newer Linux kernel versions. 5 This corresponds with the network created in the earlier step. 6 Makes the Docker in Docker container available as the hostname docker within the jenkins network. 7 Enables the use of TLS in the Docker server. Due to the use of a privileged container, this is recommended, though it requires the use of the shared volume described below. This environment variable controls the root directory where Docker TLS certificates are managed. 8 Maps the /certs/client directory inside the container to a Docker volume named jenkins-docker-certs as created above. 9 Maps the /var/jenkins_home directory inside the container to the Docker volume named jenkins-data . This allows for other Docker containers controlled by this Docker container’s Docker daemon to mount data from Jenkins. 10 ( Optional ) Exposes the Docker daemon port on the host machine. This is useful for executing docker commands on the host machine to control this inner Docker daemon. 11 The docker:dind image itself. Download this image before running, by using the command: docker image pull docker:dind . 12 The storage driver for the Docker volume. Refer to the Docker storage drivers documentation for supported options. If you have problems copying and pasting the above command snippet, use the annotation-free version below: docker run --name jenkins-docker --rm --detach \\ --privileged --network jenkins --network-alias docker \\ --env DOCKER_TLS_CERTDIR = /certs \\ --volume jenkins-docker-certs:/certs/client \\ --volume jenkins-data:/var/jenkins_home \\ --publish 2376:2376 \\ docker:dind --storage-driver overlay2 Customize the official Jenkins Docker image, by executing the following two steps: Create a Dockerfile with the following content: FROM jenkins/jenkins:2.528.2-jdk21 USER root RUN apt-get update && apt-get install -y lsb-release ca-certificates curl && \\ install -m 0755 -d /etc/apt/keyrings && \\ curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc && \\ chmod a+r /etc/apt/keyrings/docker.asc && \\ echo \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] \\ https://download.docker.com/linux/debian $(. /etc/os-release && echo \\\"$VERSION_CODENAME\\\") stable\" \\ | tee /etc/apt/sources.list.d/docker.list > /dev/null && \\ apt-get update && apt-get install -y docker-ce-cli && \\ apt-get clean && rm -rf /var/lib/apt/lists/* USER jenkins RUN jenkins-plugin-cli --plugins \"blueocean docker-workflow json-path-api\" Build a new docker image from this Dockerfile, and assign the image a meaningful name, such as \"myjenkins-blueocean:2.528.2-1\": docker build -t myjenkins-blueocean:2.528.2-1 . If you have not yet downloaded the official Jenkins Docker image, the above process automatically downloads it for you. Run your own myjenkins-blueocean:2.528.2-1 image as a container in Docker using the following docker run command: docker run \\ --name jenkins-blueocean \\ (1) --restart = on-failure \\ (2) --detach \\ (3) --network jenkins \\ (4) --env DOCKER_HOST = tcp://docker:2376 \\ (5) --env DOCKER_CERT_PATH = /certs/client \\ --env DOCKER_TLS_VERIFY = 1 \\ --publish 8080:8080 \\ (6) --publish 50000:50000 \\ (7) --volume jenkins-data:/var/jenkins_home \\ (8) --volume jenkins-docker-certs:/certs/client:ro \\ (9) myjenkins-blueocean:2.528.2-1 (10) 1 ( Optional ) Specifies the Docker container name for this instance of the Docker image. 2 Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. 3 ( Optional ) Runs the current container in the background, known as \"detached\" mode, and outputs the container ID. If you do not specify this option, then the running Docker log for this container is displayed in the terminal window. 4 Connects this container to the jenkins network previously defined. The Docker daemon is now available to this Jenkins container through the hostname docker . 5 Specifies the environment variables used by docker , docker-compose , and other Docker tools to connect to the Docker daemon from the previous step. 6 Maps, or publishes, port 8080 of the current container to port 8080 on the host machine. The first number represents the port on the host, while the last represents the container’s port. For example, to access Jenkins on your host machine through port 49000, enter -p 49000:8080 for this option. 7 ( Optional ) Maps port 50000 of the current container to port 50000 on the host machine. This is only necessary if you have set up one or more inbound Jenkins agents on other machines, which in turn interact with your jenkins-blueocean container, known as the Jenkins \"controller\". Inbound Jenkins agents communicate with the Jenkins controller through TCP port 50000 by default. You can change this port number on your Jenkins controller through the Security page. For example, if you update the TCP port for inbound Jenkins agents of your Jenkins controller to 51000, you need to re-run Jenkins via the docker run …​ command. Specify the \"publish\" option as follows: the first value is the port number on the machine hosting the Jenkins controller, and the last value matches the changed value on the Jenkins controller, for example, --publish 52000:51000 . Inbound Jenkins agents communicate with the Jenkins controller on that port (52000 in this example). Note that WebSocket agents do not need this configuration. 8 Maps the /var/jenkins_home directory in the container to the Docker volume with the name jenkins-data . Instead of mapping the /var/jenkins_home directory to a Docker volume, you can also map this directory to one on your machine’s local file system. For example, specify the option --volume $HOME/jenkins:/var/jenkins_home to map the container’s /var/jenkins_home directory to the jenkins subdirectory within the $HOME directory on your local machine — typically /Users/<your-username>/jenkins or /home/<your-username>/jenkins . NOTE: If you change the source volume or directory for this, the volume from the docker:dind container above needs to be updated to match this. 9 Maps the /certs/client directory to the previously created jenkins-docker-certs volume. The client TLS certificates required to connect to the Docker daemon are now available in the path specified by the DOCKER_CERT_PATH environment variable. 10 The name of the Docker image, which you built in the previous step. If you have problems copying and pasting the command snippet, use the annotation-free version below: docker run --name jenkins-blueocean --restart = on-failure --detach \\ --network jenkins --env DOCKER_HOST = tcp://docker:2376 \\ --env DOCKER_CERT_PATH = /certs/client --env DOCKER_TLS_VERIFY = 1 \\ --publish 8080:8080 --publish 50000:50000 \\ --volume jenkins-data:/var/jenkins_home \\ --volume jenkins-docker-certs:/certs/client:ro \\ myjenkins-blueocean:2.528.2-1 Proceed to the Post-installation setup wizard .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 8081
        }
      },
      {
        "header": "On Windows",
        "content": "The Jenkins project provides a Linux container image, not a Windows container image. Be sure that your Docker for Windows installation is configured to run Linux Containers rather than Windows Containers . Refer to the Docker documentation for instructions to switch to Linux containers . Once configured to run Linux Containers , the steps are:\n\nOpen up a command prompt window and similar to the macOS and Linux instructions above do the following: Create a bridge network in Docker docker network create jenkins Run a docker:dind Docker image docker run --name jenkins-docker --rm --detach ^ --privileged --network jenkins --network-alias docker ^ --env DOCKER_TLS_CERTDIR = /certs ^ --volume jenkins-docker-certs:/certs/client ^ --volume jenkins-data:/var/jenkins_home ^ --publish 2376:2376 ^ docker:dind Customize the official Jenkins Docker image, by executing the following two steps: Create a Dockerfile with the following content: FROM jenkins/jenkins:2.528.2-jdk21 USER root RUN apt-get update && apt-get install -y lsb-release RUN curl -fsSLo /usr/share/keyrings/docker-archive-keyring.asc \\ https://download.docker.com/linux/debian/gpg RUN echo \"deb [arch= $( dpkg --print-architecture ) \\ signed-by=/usr/share/keyrings/docker-archive-keyring.asc] \\ https://download.docker.com/linux/debian \\ $( lsb_release -cs ) stable\" > /etc/apt/sources.list.d/docker.list RUN apt-get update && apt-get install -y docker-ce-cli USER jenkins RUN jenkins-plugin-cli --plugins \"blueocean docker-workflow json-path-api\" Build a new docker image from this Dockerfile and assign the image a meaningful name, e.g. \"myjenkins-blueocean:2.528.2-1\": docker build -t myjenkins-blueocean:2.528.2-1 . If you have not yet downloaded the official Jenkins Docker image, the above process automatically downloads it for you. Run your own myjenkins-blueocean:2.528.2-1 image as a container in Docker using the following docker run command: docker run --name jenkins-blueocean --restart = on-failure --detach ^ --network jenkins --env DOCKER_HOST = tcp://docker:2376 ^ --env DOCKER_CERT_PATH = /certs/client --env DOCKER_TLS_VERIFY = 1 ^ --volume jenkins-data:/var/jenkins_home ^ --volume jenkins-docker-certs:/certs/client:ro ^ --publish 8080:8080 --publish 50000:50000 myjenkins-blueocean:2.528.2-1 Proceed to the Setup wizard .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 2310
        }
      },
      {
        "header": "Accessing the Docker container",
        "content": "If you want to access your Docker container through a terminal/command prompt using the docker exec command, add an option like --name jenkins-tutorial to the docker exec command. That will access the Jenkins Docker container named \"jenkins-tutorial\". You can access your docker container (through a separate terminal/command prompt window) with a docker exec command such as: docker exec -it jenkins-blueocean bash",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 415
        }
      },
      {
        "header": "Accessing the Docker logs",
        "content": "You may want to access the Jenkins console log, for instance, when Unlocking Jenkins as part of the Post-installation setup wizard . Access the Jenkins console log through the terminal/command prompt window from which you executed the docker run …​ command. Alternatively, you can also access the Jenkins console log through the Docker logs of your container using the following command: docker logs <docker-container-name> Your <docker-container-name> can be obtained using the docker ps command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 497
        }
      },
      {
        "header": "Accessing the Jenkins home directory",
        "content": "You can access the Jenkins home directory, to check the details of a Jenkins build in the workspace subdirectory, for example. If you mapped the Jenkins home directory ( /var/jenkins_home ) to one on your machine’s local file system, for example, in the docker run …​ command above , access the directory contents through your machine’s usual terminal/command prompt. If you specified the --volume jenkins-data:/var/jenkins_home option in the docker run …​ command, access the contents of the Jenkins home directory through your container’s terminal/command prompt using the docker container exec command: docker container exec -it <docker-container-name> bash As per the previous section , get your <docker-container-name> using the docker container ls command. If you specified the --name jenkins-blueocean option in the docker container run …​ command above (refer to Accessing the Jenkins/Blue Ocean Docker container if needed), use the docker container exec command: docker container exec -it jenkins-blueocean bash",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1020
        }
      },
      {
        "header": "Post-installation setup wizard",
        "content": "After downloading, installing and running Jenkins using one of the procedures above (except for installation with Jenkins Operator), the post-installation setup wizard begins. This setup wizard takes you through a few quick \"one-off\" steps to unlock Jenkins, customize it with plugins and create the first administrator user through which you can continue accessing Jenkins. Unlocking Jenkins When you first access a new Jenkins controller, you are asked to unlock it using an automatically-generated password. Browse to http://localhost:8080 (or whichever port you configured for Jenkins when installing it) and wait until the Unlock Jenkins page appears. From the Jenkins console log output, copy the automatically-generated alphanumeric password (between the 2 sets of asterisks). Note: The command: sudo cat /var/lib/jenkins/secrets/initialAdminPassword will print the password at console. If you are running Jenkins in Docker using the official jenkins/jenkins image you can use sudo docker exec ${CONTAINER_ID or CONTAINER_NAME} cat /var/jenkins_home/secrets/initialAdminPassword to print the password in the console without having to exec into the container. On the Unlock Jenkins page, paste this password into the Administrator password field and click Continue . Note: The Jenkins console log indicates the location (in the Jenkins home directory) where this password can also be obtained. This password must be entered in the setup wizard on new Jenkins installations before you can access Jenkins’s main UI. This password also serves as the default administrator account’s password (with username \"admin\") if you happen to skip the subsequent user-creation step in the setup wizard. Customizing Jenkins with plugins After unlocking Jenkins , the Customize Jenkins page appears. Here you can install any number of useful plugins as part of your initial setup. Click one of the two options shown: Install suggested plugins - to install the recommended set of plugins, which are based on most common use cases. Select plugins to install - to choose which set of plugins to initially install. When you first access the plugin selection page, the suggested plugins are selected by default. If you are not sure what plugins you need, choose Install suggested plugins . You can install (or remove) additional Jenkins plugins at a later point in time via the Manage Jenkins > Plugins page in Jenkins. The setup wizard shows the progression of Jenkins being configured and your chosen set of Jenkins plugins being installed. This process may take a few minutes. Creating the first administrator user Finally, after customizing Jenkins with plugins , Jenkins asks you to create your first administrator user. When the Create First Admin User page appears, specify the details for your administrator user in the respective fields and click Save and Finish . When the Jenkins is ready page appears, click Start using Jenkins . Notes: This page may indicate Jenkins is almost ready! instead and if so, click Restart . If the page does not automatically refresh after a minute, use your web browser to refresh the page manually. If required, log in to Jenkins with the credentials of the user you just created and you are ready to start using Jenkins!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 3249
        }
      },
      {
        "header": "Unlocking Jenkins",
        "content": "When you first access a new Jenkins controller, you are asked to unlock it using an automatically-generated password.\n\nBrowse to http://localhost:8080 (or whichever port you configured for Jenkins when installing it) and wait until the Unlock Jenkins page appears. From the Jenkins console log output, copy the automatically-generated alphanumeric password (between the 2 sets of asterisks). Note: The command: sudo cat /var/lib/jenkins/secrets/initialAdminPassword will print the password at console. If you are running Jenkins in Docker using the official jenkins/jenkins image you can use sudo docker exec ${CONTAINER_ID or CONTAINER_NAME} cat /var/jenkins_home/secrets/initialAdminPassword to print the password in the console without having to exec into the container. On the Unlock Jenkins page, paste this password into the Administrator password field and click Continue . Note: The Jenkins console log indicates the location (in the Jenkins home directory) where this password can also be obtained. This password must be entered in the setup wizard on new Jenkins installations before you can access Jenkins’s main UI. This password also serves as the default administrator account’s password (with username \"admin\") if you happen to skip the subsequent user-creation step in the setup wizard.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1302
        }
      },
      {
        "header": "Customizing Jenkins with plugins",
        "content": "After unlocking Jenkins , the Customize Jenkins page appears. Here you can install any number of useful plugins as part of your initial setup.\n\nClick one of the two options shown:\n\nInstall suggested plugins - to install the recommended set of plugins, which are based on most common use cases. Select plugins to install - to choose which set of plugins to initially install. When you first access the plugin selection page, the suggested plugins are selected by default.\n\nIf you are not sure what plugins you need, choose Install suggested plugins . You can install (or remove) additional Jenkins plugins at a later point in time via the Manage Jenkins > Plugins page in Jenkins.\n\nThe setup wizard shows the progression of Jenkins being configured and your chosen set of Jenkins plugins being installed. This process may take a few minutes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 840
        }
      },
      {
        "header": "Creating the first administrator user",
        "content": "Finally, after customizing Jenkins with plugins , Jenkins asks you to create your first administrator user.\n\nWhen the Create First Admin User page appears, specify the details for your administrator user in the respective fields and click Save and Finish . When the Jenkins is ready page appears, click Start using Jenkins . Notes: This page may indicate Jenkins is almost ready! instead and if so, click Restart . If the page does not automatically refresh after a minute, use your web browser to refresh the page manually. If required, log in to Jenkins with the credentials of the user you just created and you are ready to start using Jenkins!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 647
        }
      }
    ],
    "url": "https://www.jenkins.io/doc/book/installing/docker/",
    "doc_type": "docker",
    "total_sections": 12
  },
  {
    "title": "Install Elasticsearch with Docker",
    "summary": "Self-Managed Docker images for Elasticsearch are available from the Elastic Docker registry. A list of all published Docker images and tags is available at www.docker.elastic.co . The source code is in GitHub .",
    "sections": [
      {
        "header": "",
        "content": "Install Elasticsearch with Docker Self-Managed Docker images for Elasticsearch are available from the Elastic Docker registry. A list of all published Docker images and tags is available at www.docker.elastic.co . The source code is in GitHub . This package contains both free and subscription features. Start a 30-day trial to try out all of the features. Tip If you just want to test Elasticsearch in local development, refer to Run Elasticsearch locally . Note that this setup is not suitable for production environments. Review the following guides to install Elasticsearch with Docker: Start a single-node cluster in Docker Start a multi-node cluster with Docker Compose Using the Docker images in production Configure Elasticsearch with Docker",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 749
        }
      }
    ],
    "url": "https://www.elastic.co/docs/deploy-manage/deploy/self-managed/install-elasticsearch-with-docker",
    "doc_type": "docker",
    "total_sections": 1
  },
  {
    "title": "Get Started - Docker",
    "summary": "Build, change, and destroy Docker infrastructure using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time.",
    "sections": [
      {
        "header": "",
        "content": "Get Started - Docker Build, change, and destroy Docker infrastructure using Terraform. Step-by-step, command-line tutorials will walk you through the Terraform basics for the first time. Start 7 tutorials Interactive 3min What is Infrastructure as Code with Terraform? Learn how infrastructure as code lets you safely build, change, and manage infrastructure. Try Terraform. Terraform Video 7min Install Terraform Install Terraform on Mac, Linux, or Windows by downloading the binary or using a package manager (Homebrew or Chocolatey). Then create a Docker container locally by following a quick-start tutorial to check that Terraform installed correctly. Terraform Video 10min Build infrastructure Use Terraform to deploy a Docker container, format your Terraform configuration, and review your infrastructure state. Terraform 4min Change infrastructure Modify Docker container configuration to use a different external port. Plan and apply the changes to re-provision a new container that reflects the new configuration. Learn how Terraform handles infrastructure change management. Terraform 2min Destroy infrastructure Destroy the Docker container you created in the previous tutorials. Evaluate the plan and confirm the destruction. Terraform 4min Define input variables Declare your Docker container name as a variable. Reference the variable in Terraform configuration. Define variables using command line flags, environment variables, .tfvars files or default values. Terraform 5min Query data with outputs Declare output values to display a Docker containers name and ID. Display all outputs and query specific outputs. Define what data stored in Terraform state is relevant to the operator or end user. Terraform",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 1723
        }
      }
    ],
    "url": "https://developer.hashicorp.com/terraform/tutorials/docker-get-started",
    "doc_type": "docker",
    "total_sections": 1
  },
  {
    "title": "Docker",
    "summary": "Docker is a tool for deploying and running executables in isolated and reproducible environments. This may be useful, for example, to test code in an environment identical to production. IntelliJ IDEA integrates the Docker functionality and provides assistance for creating Docker images, running Docker containers, managing Docker Compose applications, using public and private Docker registries, and much more directly from the IDE.",
    "sections": [
      {
        "header": "Enable the Docker plugin",
        "content": "This functionality relies on the Docker plugin, which is bundled and enabled in IntelliJ IDEA by default. If the relevant features are not available, make sure that you did not disable the plugin.\n\nThe Docker plugin is available by default only in IntelliJ IDEA Ultimate. For IntelliJ IDEA Community Edition, you need to install the Docker plugin as described in Install plugins .\n\nPress Ctrl+Alt+S to open settings and then select Plugins . Open the Installed tab, find the Docker plugin, and select the checkbox next to the plugin name.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 538
        }
      },
      {
        "header": "Install and run Docker",
        "content": "Install and run Docker as described in Docker documentation .\n\nIntelliJ IDEA supports alternative Docker daemons: Colima and Rancher Desktop (with the dockerd engine).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 167
        }
      },
      {
        "header": "Connect to the Docker daemon",
        "content": "Press Ctrl+Alt+S to open settings and then select Build, Execution, Deployment | Docker . Click to add a Docker configuration and specify how to connect to the Docker daemon. The connection settings depend on your Docker version and operating system. For more information, refer to Docker connection settings . The Connection successful message should appear at the bottom of the dialog. For more information about mapping local paths to the virtual machine running the Docker daemon when using Docker on Windows or macOS, refer to Virtual machine path mappings for Windows and macOS hosts . You will not be able to use volumes and bind mounts for directories outside of the mapped local path. This table is not available on a Linux host, where Docker runs natively and you can mount any directory to the container. Open the Services tool window ( View | Tool Windows | Services or Alt+8 ), select the configured Docker connection node and click , or select Connect from the context menu. To edit the Docker connection settings, select the Docker node and click on the toolbar, or select Edit Configuration from the context menu. You can also click and select Docker Connection to add a Docker connection directly from the Services tool window. If you have Docker contexts configured, you can select Docker Connections from Docker Contexts to add the corresponding connections.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1377
        }
      },
      {
        "header": "Prerequisites for working with Docker on a remote server",
        "content": "Before you start working with Docker on a remote machine, make sure the following prerequisites are met:\n\nA local Docker CLI , which is necessary for connecting to the remote Docker instance. You can either install Docker Desktop or just Docker CLI. Docker Buildx plugin, which is necessary for building images from Dockerfile. Note that using Buildx with Docker requires Docker Engine version 19.03 or later.\n\nFor Dev Containers on a remote server, refer to Limitations .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 472
        }
      }
    ],
    "url": "https://www.jetbrains.com/help/idea/docker.html",
    "doc_type": "docker",
    "total_sections": 4
  },
  {
    "title": "postgres - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/postgres",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "mysql - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/mysql",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "redis - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/redis",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "node - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/node",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "python - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/python",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "ubuntu - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/ubuntu",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "alpine - Official Image | Docker Hub",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/_/alpine",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "Docker Hub Explore",
    "summary": "",
    "sections": [],
    "url": "https://hub.docker.com/search?q=&type=image",
    "doc_type": "docker",
    "total_sections": 0
  },
  {
    "title": "Get Docker",
    "summary": "Docker is an open platform for developing, shipping, and running applications. Docker allows you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications.",
    "sections": [
      {
        "header": "",
        "content": "Home / Get started / Get Docker Get Docker Page options Copy page as Markdown for LLMs View page as plain text Ask questions with Docs AI Claude Open in Claude Docker is an open platform for developing, shipping, and running applications. Docker allows you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Dockerâs methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. You can download and install Docker on multiple platforms. Refer to the following section and choose the best installation path for you. Docker Desktop terms Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription . Docker Desktop for Mac A native application using the macOS sandbox security model that delivers all Docker tools to your Mac. Docker Desktop for Windows A native Windows application that delivers all Docker tools to your Windows computer. Docker Desktop for Linux A native Linux application that delivers all Docker tools to your Linux computer. Note If you're looking for information on how to install Docker Engine, see Docker Engine installation overview .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 1410
        }
      }
    ],
    "url": "https://docs.docker.com/get-started/get-docker/",
    "doc_type": "docker",
    "total_sections": 1
  },
  {
    "title": "Install Docker Engine",
    "summary": "This section describes how to install Docker Engine on Linux, also known as Docker CE. Docker Engine is also available for Windows, macOS, and Linux, through Docker Desktop. For instructions on how to install Docker Desktop, see: Overview of Docker Desktop . Click on a platform's link to view the relevant installation procedure.",
    "sections": [
      {
        "header": "Installation procedures for supported platforms",
        "content": "Click on a platform's link to view the relevant installation procedure.\n\nPlatform x86_64 / amd64 arm64 / aarch64 arm (32-bit) ppc64le s390x CentOS â â â Debian â â â â Fedora â â â Raspberry Pi OS (32-bit) â ï¸ RHEL â â â SLES â Ubuntu â â â â â Binaries â â â\n\nOther Linux distributions\n\nNote While the following instructions may work, Docker doesn't test or verify installation on distribution derivatives.\n\nIf you use Debian derivatives such as \"BunsenLabs Linux\", \"Kali Linux\" or \"LMDE\" (Debian-based Mint) should follow the installation instructions for Debian , substitute the version of your distribution for the corresponding Debian release. Refer to the documentation of your distribution to find which Debian release corresponds with your derivative version. Likewise, if you use Ubuntu derivatives such as \"Kubuntu\", \"Lubuntu\" or \"Xubuntu\" you should follow the installation instructions for Ubuntu , substituting the version of your distribution for the corresponding Ubuntu release. Refer to the documentation of your distribution to find which Ubuntu release corresponds with your derivative version. Some Linux distributions provide a package of Docker Engine through their package repositories. These packages are built and maintained by the Linux distribution's package maintainers and may have differences in configuration or are built from modified source code. Docker isn't involved in releasing these packages and you should report any bugs or issues involving these packages to your Linux distribution's issue tracker.\n\nDocker provides binaries for manual installation of Docker Engine. These binaries are statically linked and you can use them on any Linux distribution.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1718
        }
      },
      {
        "header": "Other Linux distributions",
        "content": "Note While the following instructions may work, Docker doesn't test or verify installation on distribution derivatives.\n\nIf you use Debian derivatives such as \"BunsenLabs Linux\", \"Kali Linux\" or \"LMDE\" (Debian-based Mint) should follow the installation instructions for Debian , substitute the version of your distribution for the corresponding Debian release. Refer to the documentation of your distribution to find which Debian release corresponds with your derivative version. Likewise, if you use Ubuntu derivatives such as \"Kubuntu\", \"Lubuntu\" or \"Xubuntu\" you should follow the installation instructions for Ubuntu , substituting the version of your distribution for the corresponding Ubuntu release. Refer to the documentation of your distribution to find which Ubuntu release corresponds with your derivative version. Some Linux distributions provide a package of Docker Engine through their package repositories. These packages are built and maintained by the Linux distribution's package maintainers and may have differences in configuration or are built from modified source code. Docker isn't involved in releasing these packages and you should report any bugs or issues involving these packages to your Linux distribution's issue tracker.\n\nDocker provides binaries for manual installation of Docker Engine. These binaries are statically linked and you can use them on any Linux distribution.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1404
        }
      },
      {
        "header": "Release channels",
        "content": "Docker Engine has two types of update channels, stable and test :\n\nThe stable channel gives you the latest versions released for general availability. The test channel gives you pre-release versions that are ready for testing before general availability.\n\nUse the test channel with caution. Pre-release versions include experimental and early-access features that are subject to breaking changes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 396
        }
      },
      {
        "header": "Support",
        "content": "Docker Engine is an open source project, supported by the Moby project maintainers and community members. Docker doesn't provide support for Docker Engine. Docker provides support for Docker products, including Docker Desktop, which uses Docker Engine as one of its components.\n\nFor information about the open source project, refer to the Moby project website .\n\nUpgrade path\n\nPatch releases are always backward compatible with its major and minor version.\n\nLicensing\n\nCommercial use of Docker Engine obtained via Docker Desktop within larger enterprises (exceeding 250 employees OR with annual revenue surpassing $10 million USD), requires a paid subscription . Apache License, Version 2.0. See LICENSE for the full license.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 725
        }
      },
      {
        "header": "Upgrade path",
        "content": "Patch releases are always backward compatible with its major and minor version.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 79
        }
      },
      {
        "header": "Licensing",
        "content": "Commercial use of Docker Engine obtained via Docker Desktop within larger enterprises (exceeding 250 employees OR with annual revenue surpassing $10 million USD), requires a paid subscription . Apache License, Version 2.0. See LICENSE for the full license.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 256
        }
      },
      {
        "header": "Reporting security issues",
        "content": "If you discover a security issue, we request that you bring it to our attention immediately.\n\nDO NOT file a public issue. Instead, submit your report privately to security@docker.com .\n\nSecurity reports are greatly appreciated, and Docker will publicly thank you for it.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 270
        }
      },
      {
        "header": "Get started",
        "content": "After setting up Docker, you can learn the basics with Getting started with Docker .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 84
        }
      }
    ],
    "url": "https://docs.docker.com/engine/install/",
    "doc_type": "docker",
    "total_sections": 8
  },
  {
    "title": "Docker Engine API",
    "summary": "Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python. The SDKs allow you to efficiently build and scale Docker apps and solutions. If Go or Python don't work for you, you can use the Docker Engine API directly. For information about Docker Engine SDKs, see Develop with Docker Engine SDKs .",
    "sections": [
      {
        "header": "View the API reference",
        "content": "You can view the reference for the latest version of the API or choose a specific version .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 91
        }
      },
      {
        "header": "Versioned API and SDK",
        "content": "The version of the Docker Engine API you should use depends upon the version of your Docker daemon and Docker client.\n\nA given version of the Docker Engine SDK supports a specific version of the Docker Engine API, as well as all earlier versions. If breaking changes occur, they are documented prominently.\n\nNote The Docker daemon and client don't necessarily need to be the same version at all times. However, keep the following in mind. If the daemon is newer than the client, the client doesn't know about new features or deprecated API endpoints in the daemon. If the client is newer than the daemon, the client can request API endpoints that the daemon doesn't know about.\n\nA new version of the API is released when new features are added. The Docker API is backward-compatible, so you don't need to update code that uses the API unless you need to take advantage of new features.\n\nTo see the highest version of the API your Docker daemon and client support, use docker version :\n\n$ docker version Client: Docker Engine - Community Version: 29.0.0 API version: 1.52 Go version: go1.25.4 Git commit: 3d4129b Built: Mon Nov 10 21:47:17 2025 OS/Arch: linux/arm64 Context: default Server: Docker Engine - Community Engine: Version: 29.0.0 API version: 1.52 (minimum version 1.44) Go version: go1.25.4 Git commit: d105562 Built: Mon Nov 10 21:47:17 2025 OS/Arch: linux/arm64 ...\n\nYou can specify the API version to use in any of the following ways:\n\nWhen using the SDK, use the latest version. At a minimum, use the version that incorporates the API version with the features you need. When using curl directly, specify the version as the first part of the URL. For instance, if the endpoint is /containers/ you can use /v1.52/containers/ . To force the Docker CLI or the Docker Engine SDKs to use an older version of the API than the version reported by docker version , set the environment variable DOCKER_API_VERSION to the correct version. This works on Linux, Windows, or macOS clients. $ DOCKER_API_VERSION = 1.51 While the environment variable is set, that version of the API is used, even if the Docker daemon supports a newer version. This environment variable disables API version negotiation, so you should only use it if you must use a specific version of the API, or for debugging purposes. The Docker Go SDK allows you to enable API version negotiation, automatically selects an API version that's supported by both the client and the Docker Engine that's in use. For the SDKs, you can also specify the API version programmatically as a parameter to the client object. See the Go constructor or the Python SDK documentation for client .\n\nAPI version matrix\n\nDocker version Maximum API version Change log 29.0 1.52 changes 28.3 1.51 changes 28.2 1.50 changes 28.1 1.49 changes 28.0 1.48 changes 27.5 1.47 changes 27.4 1.47 changes 27.3 1.47 changes 27.2 1.47 changes 27.1 1.46 changes 27.0 1.46 changes 26.1 1.45 changes 26.0 1.45 changes 25.0 1.44 changes 24.0 1.43 changes 23.0 1.42 changes 20.10 1.41 changes 19.03 1.40 changes 18.09 1.39 changes 18.06 1.38 changes 18.05 1.37 changes 18.04 1.37 changes 18.03 1.37 changes 18.02 1.36 changes 17.12 1.35 changes 17.11 1.34 changes 17.10 1.33 changes 17.09 1.32 changes 17.07 1.31 changes 17.06 1.30 changes 17.05 1.29 changes 17.04 1.28 changes 17.03.1 1.27 changes 17.03 1.26 changes 1.13.1 1.26 changes 1.13 1.25 changes 1.12 1.24 changes\n\nDeprecated API versions\n\nAPI versions before v1.44 are deprecated. You can find archived documentation for deprecated versions of the API in the code repository on GitHub:\n\nDocumentation for API versions 1.24â1.43 . Documentation for API versions 1.18â1.23 . Documentation for API versions 1.17 and before .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 3718
        }
      },
      {
        "header": "API version matrix",
        "content": "Docker version Maximum API version Change log 29.0 1.52 changes 28.3 1.51 changes 28.2 1.50 changes 28.1 1.49 changes 28.0 1.48 changes 27.5 1.47 changes 27.4 1.47 changes 27.3 1.47 changes 27.2 1.47 changes 27.1 1.46 changes 27.0 1.46 changes 26.1 1.45 changes 26.0 1.45 changes 25.0 1.44 changes 24.0 1.43 changes 23.0 1.42 changes 20.10 1.41 changes 19.03 1.40 changes 18.09 1.39 changes 18.06 1.38 changes 18.05 1.37 changes 18.04 1.37 changes 18.03 1.37 changes 18.02 1.36 changes 17.12 1.35 changes 17.11 1.34 changes 17.10 1.33 changes 17.09 1.32 changes 17.07 1.31 changes 17.06 1.30 changes 17.05 1.29 changes 17.04 1.28 changes 17.03.1 1.27 changes 17.03 1.26 changes 1.13.1 1.26 changes 1.13 1.25 changes 1.12 1.24 changes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 733
        }
      },
      {
        "header": "Deprecated API versions",
        "content": "API versions before v1.44 are deprecated. You can find archived documentation for deprecated versions of the API in the code repository on GitHub:\n\nDocumentation for API versions 1.24â1.43 . Documentation for API versions 1.18â1.23 . Documentation for API versions 1.17 and before .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 286
        }
      }
    ],
    "url": "https://docs.docker.com/reference/api/engine/",
    "doc_type": "docker",
    "total_sections": 4
  }
]