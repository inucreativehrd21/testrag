[
  {
    "title": "What is Amazon EC2?",
    "summary": "What is Amazon EC2?Amazon Elastic Compute Cloud (Amazon EC2) provides on-demand, scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 reduces hardware costs so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. You can add capacity (scale up) to handle compute-heavy tasks, such as monthly or yearly processes, or spikes in website traf",
    "sections": [
      {
        "header": "",
        "content": "Amazon Elastic Compute Cloud (Amazon EC2) provides on-demand, scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 reduces hardware costs so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. You can add capacity (scale up) to handle compute-heavy tasks, such as monthly or yearly processes, or spikes in website traffic. When usage decreases, you can reduce capacity (scale down) again.\n\nAn EC2 instance is a virtual server in the AWS Cloud. When you launch an EC2 instance, the instance type that you specify determines the hardware available to your instance. Each instance type offers a different balance of compute, memory, network, and storage resources. For more information, see the Amazon EC2 Instance Types Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 887
        }
      },
      {
        "header": "Features of Amazon EC2",
        "content": "Amazon EC2 provides the following high-level features:\n\nPreconfigured templates for your instances that package the components you need for your server (including the operating system and additional software).\n\nVarious configurations of CPU, memory, storage, networking capacity, and graphics hardware for your instances.\n\nPersistent storage volumes for your data using Amazon Elastic Block Store (Amazon EBS).\n\nStorage volumes for temporary data that is deleted when you stop, hibernate, or terminate your instance.\n\nSecure login information for your instances. AWS stores the public key and you store the private key in a secure place.\n\nA virtual firewall that allows you to specify the protocols, ports, and source IP ranges that can reach your instances, and the destination IP ranges to which your instances can connect.\n\nAmazon EC2 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS). For more information about PCI DSS, including how to request a copy of the AWS PCI Compliance Package, see PCI DSS Level 1.\n\n**Instances**: Virtual servers.\n**Amazon Machine Images (AMIs)**: Preconfigured templates for your instances that package the components you need for your server (including the operating system and additional software).\n**Instance types**: Various configurations of CPU, memory, storage, networking capacity, and graphics hardware for your instances.\n**Amazon EBS volumes**: Persistent storage volumes for your data using Amazon Elastic Block Store (Amazon EBS).\n**Instance store volumes**: Storage volumes for temporary data that is deleted when you stop, hibernate, or terminate your instance.\n**Key pairs**: Secure login information for your instances. AWS stores the public key and you store the private key in a secure place.\n**Security groups**: A virtual firewall that allows you to specify the protocols, ports, and source IP ranges that can reach your instances, and the destination IP ranges to which your instances can connect.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2112
        }
      },
      {
        "header": "Related services",
        "content": "You can use other AWS services with the instances that you deploy using Amazon EC2.\n\nHelps ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.\n\nAutomate backing up your Amazon EC2 instances and the Amazon EBS volumes attached to them.\n\nMonitor your instances and Amazon EBS volumes.\n\nAutomatically distribute incoming application traffic across multiple instances.\n\nDetect potentially unauthorized or malicious use of your EC2 instances.\n\nAutomate the creation, management, and deployment of customized, secure, and up-to-date server images.\n\nSize, configure, and deploy AWS resources for third-party applications without having to manually identify and provision individual AWS resources.\n\nPerform operations at scale on EC2 instances with this secure end-to-end management solution.\n\nYou can launch instances using another AWS compute service instead of using Amazon EC2.\n\nBuild websites or web applications using Amazon Lightsail, a cloud platform that provides the resources that you need to deploy your project quickly, for a low, predictable monthly price. To compare Amazon EC2 and Lightsail, see Amazon Lightsail or Amazon EC2.\n\nDeploy, manage, and scale containerized applications on a cluster of EC2 instances. For more information, see Choosing an AWS container service.\n\nRun your Kubernetes applications on AWS. For more information, see Choosing an AWS container service.\n\n**Amazon EC2 Auto Scaling**: Helps ensure you have the correct number of Amazon EC2 instances available to handle the load for your application.\n**AWS Backup**: Automate backing up your Amazon EC2 instances and the Amazon EBS volumes attached to them.\n**Amazon CloudWatch**: Monitor your instances and Amazon EBS volumes.\n**Elastic Load Balancing**: Automatically distribute incoming application traffic across multiple instances.\n**Amazon GuardDuty**: Detect potentially unauthorized or malicious use of your EC2 instances.\n**EC2 Image Builder**: Automate the creation, management, and deployment of customized, secure, and up-to-date server images.\n**AWS Launch Wizard**: Size, configure, and deploy AWS resources for third-party applications without having to manually identify and provision individual AWS resources.\n**AWS Systems Manager**: Perform operations at scale on EC2 instances with this secure end-to-end management solution.\n**Amazon Lightsail**: Build websites or web applications using Amazon Lightsail, a cloud platform that provides the resources that you need to deploy your project quickly, for a low, predictable monthly price. To compare Amazon EC2 and Lightsail, see Amazon Lightsail or Amazon EC2.\n**Amazon Elastic Container Service (Amazon ECS)**: Deploy, manage, and scale containerized applications on a cluster of EC2 instances. For more information, see Choosing an AWS container service.\n**Amazon Elastic Kubernetes Service (Amazon EKS)**: Run your Kubernetes applications on AWS. For more information, see Choosing an AWS container service.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 13,
          "content_length": 3016
        }
      },
      {
        "header": "Access Amazon EC2",
        "content": "You can create and manage your Amazon EC2 instances using the following interfaces:\n\nA simple web interface to create and manage Amazon EC2 instances and resources. If you've signed up for an AWS account, you can access the Amazon EC2 console by signing into the AWS Management Console and selecting EC2 from the console home page.\n\nEnables you to interact with AWS services using commands in your command-line shell. It is supported on Windows, Mac, and Linux. For more information about the AWS CLI , see AWS Command Line Interface User Guide. You can find the Amazon EC2 commands in the AWS CLI Command Reference.\n\nAmazon EC2 supports creating resources using AWS CloudFormation. You create a template, in JSON or YAML format, that describes your AWS resources, and AWS CloudFormation provisions and configures those resources for you. You can reuse your CloudFormation templates to provision the same resources multiple times, whether in the same Region and account or in multiple Regions and accounts. For more information about supported resource types and properties for Amazon EC2, see EC2 resource type reference in the AWS CloudFormation User Guide.\n\nIf you prefer to build applications using language-specific APIs instead of submitting a request over HTTP or HTTPS, AWS provides libraries, sample code, tutorials, and other resources for software developers. These libraries provide basic functions that automate tasks such as cryptographically signing your requests, retrying requests, and handling error responses, making it easier for you to get started. For more information, see Tools to Build on AWS.\n\nA set of PowerShell modules that are built on the functionality exposed by the SDK for .NET. The Tools for PowerShell enable you to script operations on your AWS resources from the PowerShell command line. To get started, see the AWS Tools for PowerShell User Guide. You can find the cmdlets for Amazon EC2, in the AWS Tools for PowerShell Cmdlet Reference.\n\nAmazon EC2 provides a Query API. These requests are HTTP or HTTPS requests that use the HTTP verbs GET or POST and a Query parameter named Action. For more information about the API actions for Amazon EC2, see Actions in the Amazon EC2 API Reference.\n\n**Amazon EC2 console**: A simple web interface to create and manage Amazon EC2 instances and resources. If you've signed up for an AWS account, you can access the Amazon EC2 console by signing into the AWS Management Console and selecting EC2 from the console home page.\n**AWS Command Line Interface**: Enables you to interact with AWS services using commands in your command-line shell. It is supported on Windows, Mac, and Linux. For more information about the AWS CLI , see AWS Command Line Interface User Guide. You can find the Amazon EC2 commands in the AWS CLI Command Reference.\n**AWS CloudFormation**: Amazon EC2 supports creating resources using AWS CloudFormation. You create a template, in JSON or YAML format, that describes your AWS resources, and AWS CloudFormation provisions and configures those resources for you. You can reuse your CloudFormation templates to provision the same resources multiple times, whether in the same Region and account or in multiple Regions and accounts. For more information about supported resource types and properties for Amazon EC2, see EC2 resource type reference in the AWS CloudFormation User Guide.\n**AWS SDKs**: If you prefer to build applications using language-specific APIs instead of submitting a request over HTTP or HTTPS, AWS provides libraries, sample code, tutorials, and other resources for software developers. These libraries provide basic functions that automate tasks such as cryptographically signing your requests, retrying requests, and handling error responses, making it easier for you to get started. For more information, see Tools to Build on AWS.\n**AWS Tools for PowerShell**: A set of PowerShell modules that are built on the functionality exposed by the SDK for .NET. The Tools for PowerShell enable you to script operations on your AWS resources from the PowerShell command line. To get started, see the AWS Tools for PowerShell User Guide. You can find the cmdlets for Amazon EC2, in the AWS Tools for PowerShell Cmdlet Reference.\n**Query API**: Amazon EC2 provides a Query API. These requests are HTTP or HTTPS requests that use the HTTP verbs GET or POST and a Query parameter named Action. For more information about the API actions for Amazon EC2, see Actions in the Amazon EC2 API Reference.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 4509
        }
      },
      {
        "header": "Pricing for Amazon EC2",
        "content": "Amazon EC2 provides the following pricing options:\n\nYou can get started with Amazon EC2 for free. To explore the Free Tier options, see AWS Free Tier.\n\nPay for the instances that you use by the second, with a minimum of 60 seconds, with no long-term commitments or upfront payments.\n\nYou can reduce your Amazon EC2 costs by making a commitment to a consistent amount of usage, in USD per hour, for a term of 1 or 3 years.\n\nYou can reduce your Amazon EC2 costs by making a commitment to a specific instance configuration, including instance type and Region, for a term of 1 or 3 years.\n\nRequest unused EC2 instances, which can reduce your Amazon EC2 costs significantly.\n\nReduce costs by using a physical EC2 server that is fully dedicated for your use, either On-Demand or as part of a Savings Plan. You can use your existing server-bound software licenses and get help meeting compliance requirements.\n\nReserve compute capacity for your EC2 instances in a specific Availability Zone for any duration of time.\n\nRemoves the cost of unused minutes and seconds from your bill.\n\nFor a complete list of charges and prices for Amazon EC2 and more information about the purchase models, see Amazon EC2 pricing.\n\n**Free Tier**: You can get started with Amazon EC2 for free. To explore the Free Tier options, see AWS Free Tier.\n**On-Demand Instances**: Pay for the instances that you use by the second, with a minimum of 60 seconds, with no long-term commitments or upfront payments.\n**Savings Plans**: You can reduce your Amazon EC2 costs by making a commitment to a consistent amount of usage, in USD per hour, for a term of 1 or 3 years.\n**Reserved Instances**: You can reduce your Amazon EC2 costs by making a commitment to a specific instance configuration, including instance type and Region, for a term of 1 or 3 years.\n**Spot Instances**: Request unused EC2 instances, which can reduce your Amazon EC2 costs significantly.\n**Dedicated Hosts**: Reduce costs by using a physical EC2 server that is fully dedicated for your use, either On-Demand or as part of a Savings Plan. You can use your existing server-bound software licenses and get help meeting compliance requirements.\n**On-Demand Capacity Reservations**: Reserve compute capacity for your EC2 instances in a specific Availability Zone for any duration of time.\n**Per-second billing**: Removes the cost of unused minutes and seconds from your bill.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2404
        }
      },
      {
        "header": "Estimates, billing, and cost optimization",
        "content": "To create estimates for your AWS use cases, use the AWS Pricing Calculator.\n\nTo estimate the cost of transforming Microsoft workloads to a modern architecture that uses open source and cloud-native services deployed on AWS, use the AWS Modernization Calculator for Microsoft Workloads.\n\nTo see your bill, go to the Billing and Cost Management Dashboard in the AWS Billing and Cost Management console. Your bill contains links to usage reports that provide details about your bill. To learn more about AWS account billing, see AWS Billing and Cost Management User Guide.\n\nIf you have questions concerning AWS billing, accounts, and events, contact AWS Support.\n\nTo calculate the cost of a sample provisioned environment, see Cloud Economics Center. When calculating the cost of a provisioned environment, remember to include incidental costs such as snapshot storage for EBS volumes.\n\nYou can optimize the cost, security, and performance of your AWS environment using AWS Trusted Advisor.\n\nYou can use AWS Cost Explorer to analyze the cost and usage of your EC2 instances. You can view data up to the last 13 months, and forecast how much you are likely to spend for the next 12 months. For more information, see Analyzing your costs and usage with AWS Cost Explorer in the AWS Cost Management User Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1304
        }
      },
      {
        "header": "Resources",
        "content": "Thanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• Amazon EC2 features\n• AWS re:Post\n• AWS Skill Builder\n• AWS Support\n• Hands-on Tutorials\n• Web Hosting\n• Windows on AWS",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 417
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html",
    "doc_type": "aws",
    "total_sections": 7
  },
  {
    "title": "Get started with Amazon EC2",
    "summary": "Step 1: Launch an instanceStep 2: Connect to your instanceStep 3: Clean up your instanceNext steps\n\nGet started with Amazon EC2Use this tutorial to get started with Amazon Elastic Compute Cloud (Amazon EC2). You'll learn how to launch and connect to an EC2 instance. An instance is a virtual server in the AWS Cloud. With Amazon EC2, you can set up and configure the operating system and applications that run on your instance. Overview The following diagram shows the key components that you'll use ",
    "sections": [
      {
        "header": "",
        "content": "Use this tutorial to get started with Amazon Elastic Compute Cloud (Amazon EC2). You'll learn how to launch and connect to an EC2 instance. An instance is a virtual server in the AWS Cloud. With Amazon EC2, you can set up and configure the operating system and applications that run on your instance.\n\nThe following diagram shows the key components that you'll use in this tutorial:\n\nAn image â A template that contains the software to run on your instance, such as the operating system.\n\nA key pair â A set of security credentials that you use to prove your identity when connecting to your instance. The public key is on your instance and the private key is on your computer.\n\nA network â A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. To help you get started quickly, your account comes with a default VPC in each AWS Region, and each default VPC has a default subnet in each Availability Zone.\n\nA security group â Acts as a virtual firewall to control inbound and outbound traffic.\n\nAn EBS volume â We require a root volume for the image. You can optionally add data volumes.\n\nWhen you create your AWS account, you can get started with Amazon EC2 for free using the AWS Free Tier.\n\nIf you created your AWS account before July 15, 2025, it's less than 12 months old, and you haven't already exceeded the Free Tier benefits for Amazon EC2, it won't cost you anything to complete this tutorial, because we help you select options that are within the Free Tier benefits. Otherwise, you'll incur the standard Amazon EC2 usage fees from the time that you launch the instance (even if it remains idle) until you terminate it.\n\nIf you created your AWS account on or after July 15, 2025, it's less than 6 months old, and you haven't used up all your credits, it won't cost you anything to complete this tutorial, because we help you select options that are within the Free Tier benefits.\n\nFor information on how to determine whether you are eligible for the Free Tier, see Track your Free Tier usage for Amazon EC2.\n\nStep 1: Launch an instance\n\nStep 2: Connect to your instance\n\nStep 3: Clean up your instance\n\n• An image â A template that contains the software to run on your instance, such as the operating system.\n• A key pair â A set of security credentials that you use to prove your identity when connecting to your instance. The public key is on your instance and the private key is on your computer.\n• A network â A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. To help you get started quickly, your account comes with a default VPC in each AWS Region, and each default VPC has a default subnet in each Availability Zone.\n• A security group â Acts as a virtual firewall to control inbound and outbound traffic.\n• An EBS volume â We require a root volume for the image. You can optionally add data volumes.\n\n• Step 1: Launch an instance\n• Step 2: Connect to your instance\n• Step 3: Clean up your instance",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2995
        }
      },
      {
        "header": "Step 1: Launch an instance",
        "content": "You can launch an EC2 instance using the AWS Management Console as described in the following procedure. This tutorial is intended to help you quickly launch your first instance, so it doesn't cover all possible options.\n\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n\nIn the navigation bar at the top of the screen, we display the current AWS Region â for example, Ohio. You can use the selected Region, or optionally select a Region that is closer to you.\n\nFrom the EC2 console dashboard, in the Launch instance pane, choose Launch instance.\n\nUnder Name and tags, for Name, enter a descriptive name for your instance.\n\nUnder Application and OS Images (Amazon Machine Image), do the following:\n\nChoose Quick Start, and then choose the operating system (OS) for your instance. For your first Linux instance, we recommend that you choose Amazon Linux.\n\nFrom Amazon Machine Image (AMI), select an AMI that is marked Free Tier eligible.\n\nUnder Instance type, for Instance type, select an instance type that is marked Free Tier eligible.\n\nUnder Key pair (login), for Key pair name, choose an existing key pair or choose Create new key pair to create your first key pair.\n\nIf you choose Proceed without a key pair (Not recommended), you won't be able to connect to your instance using the methods described in this tutorial.\n\nUnder Network settings, notice that we selected your default VPC, selected the option to use the default subnet in an Availability Zone that we choose for you, and configured a security group with a rule that allows connections to your instance from anywhere (0.0.0.0.0/0).\n\nIf you specify 0.0.0.0/0, you are enabling traffic from any IP addresses in the world. For the SSH and RDP protocols, you might consider this acceptable for a short time in a test environment, but it's unsafe for production environments. In production, be sure to authorize access only from the appropriate individual IP address or range of addresses.\n\nFor your first instance, we recommend that you use the default settings. Otherwise, you can update your network settings as follows:\n\n(Optional) To use a specific default subnet, choose Edit and then choose a subnet.\n\n(Optional) To use a different VPC, choose Edit and then choose an existing VPC. If the VPC isn't configured for public internet access, you won't be able to connect to your instance.\n\n(Optional) To restrict inbound connection traffic to a specific network, choose Custom instead of Anywhere, and enter the CIDR block for your network.\n\n(Optional) To use a different security group, choose Select existing security group and choose an existing security group. If the security group does not have a rule that allows connection traffic from your network, you won't be able to connect to your instance. For a Linux instance, you must allow SSH traffic. For a Windows instance, you must allow RDP traffic.\n\nUnder Configure storage, notice that we configured a root volume but no data volumes. This is sufficient for test purposes.\n\nReview a summary of your instance configuration in the Summary panel, and when you're ready, choose Launch instance.\n\nIf the launch is successful, choose the ID of the instance from the Success notification to open the Instances page and monitor the status of the launch.\n\nSelect the checkbox for the instance. The initial instance state is pending. After the instance starts, its state changes to running. Choose the Status and alarms tab. After your instance passes its status checks, it is ready to receive connection requests.\n\n• Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n• In the navigation bar at the top of the screen, we display the current AWS Region â for example, Ohio. You can use the selected Region, or optionally select a Region that is closer to you.\n• From the EC2 console dashboard, in the Launch instance pane, choose Launch instance.\n• Under Name and tags, for Name, enter a descriptive name for your instance.\n• Under Application and OS Images (Amazon Machine Image), do the following: Choose Quick Start, and then choose the operating system (OS) for your instance. For your first Linux instance, we recommend that you choose Amazon Linux. From Amazon Machine Image (AMI), select an AMI that is marked Free Tier eligible.\n• Under Instance type, for Instance type, select an instance type that is marked Free Tier eligible.\n• Under Key pair (login), for Key pair name, choose an existing key pair or choose Create new key pair to create your first key pair. WarningIf you choose Proceed without a key pair (Not recommended), you won't be able to connect to your instance using the methods described in this tutorial.\n• Under Network settings, notice that we selected your default VPC, selected the option to use the default subnet in an Availability Zone that we choose for you, and configured a security group with a rule that allows connections to your instance from anywhere (0.0.0.0.0/0). WarningIf you specify 0.0.0.0/0, you are enabling traffic from any IP addresses in the world. For the SSH and RDP protocols, you might consider this acceptable for a short time in a test environment, but it's unsafe for production environments. In production, be sure to authorize access only from the appropriate individual IP address or range of addresses. For your first instance, we recommend that you use the default settings. Otherwise, you can update your network settings as follows: (Optional) To use a specific default subnet, choose Edit and then choose a subnet. (Optional) To use a different VPC, choose Edit and then choose an existing VPC. If the VPC isn't configured for public internet access, you won't be able to connect to your instance. (Optional) To restrict inbound connection traffic to a specific network, choose Custom instead of Anywhere, and enter the CIDR block for your network. (Optional) To use a different security group, choose Select existing security group and choose an existing security group. If the security group does not have a rule that allows connection traffic from your network, you won't be able to connect to your instance. For a Linux instance, you must allow SSH traffic. For a Windows instance, you must allow RDP traffic.\n• Under Configure storage, notice that we configured a root volume but no data volumes. This is sufficient for test purposes.\n• Review a summary of your instance configuration in the Summary panel, and when you're ready, choose Launch instance.\n• If the launch is successful, choose the ID of the instance from the Success notification to open the Instances page and monitor the status of the launch.\n• Select the checkbox for the instance. The initial instance state is pending. After the instance starts, its state changes to running. Choose the Status and alarms tab. After your instance passes its status checks, it is ready to receive connection requests.\n\n• Choose Quick Start, and then choose the operating system (OS) for your instance. For your first Linux instance, we recommend that you choose Amazon Linux.\n• From Amazon Machine Image (AMI), select an AMI that is marked Free Tier eligible.\n\n• (Optional) To use a specific default subnet, choose Edit and then choose a subnet.\n• (Optional) To use a different VPC, choose Edit and then choose an existing VPC. If the VPC isn't configured for public internet access, you won't be able to connect to your instance.\n• (Optional) To restrict inbound connection traffic to a specific network, choose Custom instead of Anywhere, and enter the CIDR block for your network.\n• (Optional) To use a different security group, choose Select existing security group and choose an existing security group. If the security group does not have a rule that allows connection traffic from your network, you won't be able to connect to your instance. For a Linux instance, you must allow SSH traffic. For a Windows instance, you must allow RDP traffic.\n\n[Note] WarningIf you choose Proceed without a key pair (Not recommended), you won't be able to connect to your instance using the methods described in this tutorial.\n\n[Note] If you choose Proceed without a key pair (Not recommended), you won't be able to connect to your instance using the methods described in this tutorial.\n\n[Note] WarningIf you specify 0.0.0.0/0, you are enabling traffic from any IP addresses in the world. For the SSH and RDP protocols, you might consider this acceptable for a short time in a test environment, but it's unsafe for production environments. In production, be sure to authorize access only from the appropriate individual IP address or range of addresses.\n\n[Note] If you specify 0.0.0.0/0, you are enabling traffic from any IP addresses in the world. For the SSH and RDP protocols, you might consider this acceptable for a short time in a test environment, but it's unsafe for production environments. In production, be sure to authorize access only from the appropriate individual IP address or range of addresses.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 22,
          "content_length": 8981
        }
      },
      {
        "header": "Step 2: Connect to your instance",
        "content": "The procedure that you use depends on the operating system of the instance. If you can't connect to your instance, see Troubleshoot issues connecting to your Amazon EC2 Linux instance for assistance.\n\nYou can connect to your Linux instance using any SSH client. If you are running Windows on your computer, open a terminal and run the ssh command to verify that you have an SSH client installed. If the command is not found, install OpenSSH for Windows.\n\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n\nIn the navigation pane, choose Instances.\n\nSelect the instance and then choose Connect.\n\nOn the Connect to instance page, choose the SSH client tab.\n\n(Optional) If you created a key pair when you launched the instance and downloaded the private key (.pem file) to a computer running Linux or macOS, run the example chmod command to set the permissions for your private key.\n\nCopy the example SSH command. The following is an example, where key-pair-name.pem is the name of your private key file, ec2-user is the username associated with the image, and the string after the @ symbol is the public DNS name of the instance.\n\nIn a terminal window on your computer, run the ssh command that you saved in the previous step. If the private key file is not in the current directory, you must specify the fully-qualified path to the key file in this command.\n\nThe following is an example response:\n\n(Optional) Verify that the fingerprint in the security alert matches the instance fingerprint contained in the console output when you first start an instance. To get the console output, choose Actions, Monitor and troubleshoot, Get system log. If the fingerprints don't match, someone might be attempting a man-in-the-middle attack. If they match, continue to the next step.\n\nThe following is an example response:\n\nTo connect to a Windows instance using RDP, you must retrieve the initial administrator password and then enter this password when you connect to your instance. It takes a few minutes after instance launch before this password is available. Your account must have permission to call the GetPasswordData action. For more information, see Example policies to control access the Amazon EC2 API.\n\nThe default username for the Administrator account depends on the language of the operating system (OS) contained in the AMI. To determine the correct username, identify the language of the OS, and then choose the corresponding username. For example, for an English OS, the username is Administrator, for a French OS it's Administrateur, and for a Portuguese OS it's Administrador. If a language version of the OS does not have a username in the same language, choose the username Administrator (Other). For more information, see Localized Names for Administrator Account in Windows in the Microsoft website.\n\nOpen the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n\nIn the navigation pane, choose Instances.\n\nSelect the instance and then choose Connect.\n\nOn the Connect to instance page, choose the RDP client tab.\n\nFor Username, choose the default username for the Administrator account. The username you choose must match the language of the operating system (OS) contained in the AMI that you used to launch your instance. If there is no username in the same language as your OS, choose Administrator (Other).\n\nChoose Get password.\n\nOn the Get Windows password page, do the following:\n\nChoose Upload private key file and navigate to the private key (.pem) file that you specified when you launched the instance. Select the file and choose Open to copy the entire contents of the file to this window.\n\nChoose Decrypt password. The Get Windows password page closes, and the default administrator password for the instance appears under Password, replacing the Get password link shown previously.\n\nCopy the password and save it in a safe place. This password is required to connect to the instance.\n\nThe following procedure uses the Remote Desktop Connection client for Windows (MSTSC). If you're using a different RDP client, download the RDP file and then see the documentation for the RDP client for the steps to establish the RDP connection.\n\nOn the Connect to instance page, choose Download remote desktop file. When the file download is finished, choose Cancel to return to the Instances page. The RDP file is downloaded to your Downloads folder.\n\nRun mstsc.exe to open the RDP client.\n\nExpand Show options, choose Open, and select the .rdp file from your Downloads folder.\n\nBy default, Computer is the public IPv4 DNS name of the instance and User name is the administrator account. To connect to the instance using IPv6 instead, replace the public IPv4 DNS name of the instance with its IPv6 address. Review the default settings and change them as needed.\n\nChoose Connect. If you receive a warning that the publisher of the remote connection is unknown, choose Connect to continue.\n\nEnter the password that you saved previously, and then choose OK.\n\nDue to the nature of self-signed certificates, you might get a warning that the security certificate could not be authenticated. Do one of the following:\n\nIf you trust the certificate, choose Yes to connect to your instance.\n\n[Windows] Before you proceed, compare the thumbprint of the certificate with the value in the system log to confirm the identity of the remote computer. Choose View certificate and then choose Thumbprint from the Details tab. Compare this value to the value of RDPCERTIFICATE-THUMBPRINT in Actions, Monitor and troubleshoot, Get system log.\n\n[Mac OS X] Before you proceed, compare the fingerprint of the certificate with the value in the system log to confirm the identity of the remote computer. Choose Show Certificate, expand Details, and choose SHA1 Fingerprints. Compare this value to the value of RDPCERTIFICATE-THUMBPRINT in Actions, Monitor and troubleshoot, Get system log.\n\nIf the RDP connection is successful, the RDP client displays the Windows login screen and then the Windows desktop. If you receive an error message instead, see Remote Desktop can't connect to the remote computer. When you are finished with the RDP connection, you can close the RDP client.\n\n• Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n• In the navigation pane, choose Instances.\n• Select the instance and then choose Connect.\n• On the Connect to instance page, choose the SSH client tab.\n• (Optional) If you created a key pair when you launched the instance and downloaded the private key (.pem file) to a computer running Linux or macOS, run the example chmod command to set the permissions for your private key.\n• Copy the example SSH command. The following is an example, where key-pair-name.pem is the name of your private key file, ec2-user is the username associated with the image, and the string after the @ symbol is the public DNS name of the instance. ssh -i key-pair-name.pem ec2-user@ec2-198-51-100-1.us-east-2.compute.amazonaws.com\n• In a terminal window on your computer, run the ssh command that you saved in the previous step. If the private key file is not in the current directory, you must specify the fully-qualified path to the key file in this command. The following is an example response: The authenticity of host 'ec2-198-51-100-1.us-east-2.compute.amazonaws.com (198-51-100-1)' can't be established. ECDSA key fingerprint is l4UB/neBad9tvkgJf1QZWxheQmR59WgrgzEimCG6kZY. Are you sure you want to continue connecting (yes/no)?\n• (Optional) Verify that the fingerprint in the security alert matches the instance fingerprint contained in the console output when you first start an instance. To get the console output, choose Actions, Monitor and troubleshoot, Get system log. If the fingerprints don't match, someone might be attempting a man-in-the-middle attack. If they match, continue to the next step.\n• Enter yes. The following is an example response: Warning: Permanently added 'ec2-198-51-100-1.us-east-2.compute.amazonaws.com' (ECDSA) to the list of known hosts.\n\n• Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/.\n• In the navigation pane, choose Instances.\n• Select the instance and then choose Connect.\n• On the Connect to instance page, choose the RDP client tab.\n• For Username, choose the default username for the Administrator account. The username you choose must match the language of the operating system (OS) contained in the AMI that you used to launch your instance. If there is no username in the same language as your OS, choose Administrator (Other).\n• Choose Get password.\n• On the Get Windows password page, do the following: Choose Upload private key file and navigate to the private key (.pem) file that you specified when you launched the instance. Select the file and choose Open to copy the entire contents of the file to this window. Choose Decrypt password. The Get Windows password page closes, and the default administrator password for the instance appears under Password, replacing the Get password link shown previously. Copy the password and save it in a safe place. This password is required to connect to the instance.\n\n• Choose Upload private key file and navigate to the private key (.pem) file that you specified when you launched the instance. Select the file and choose Open to copy the entire contents of the file to this window.\n• Choose Decrypt password. The Get Windows password page closes, and the default administrator password for the instance appears under Password, replacing the Get password link shown previously.\n• Copy the password and save it in a safe place. This password is required to connect to the instance.\n\n• On the Connect to instance page, choose Download remote desktop file. When the file download is finished, choose Cancel to return to the Instances page. The RDP file is downloaded to your Downloads folder.\n• Run mstsc.exe to open the RDP client.\n• Expand Show options, choose Open, and select the .rdp file from your Downloads folder.\n• By default, Computer is the public IPv4 DNS name of the instance and User name is the administrator account. To connect to the instance using IPv6 instead, replace the public IPv4 DNS name of the instance with its IPv6 address. Review the default settings and change them as needed.\n• Choose Connect. If you receive a warning that the publisher of the remote connection is unknown, choose Connect to continue.\n• Enter the password that you saved previously, and then choose OK.\n• Due to the nature of self-signed certificates, you might get a warning that the security certificate could not be authenticated. Do one of the following: If you trust the certificate, choose Yes to connect to your instance. [Windows] Before you proceed, compare the thumbprint of the certificate with the value in the system log to confirm the identity of the remote computer. Choose View certificate and then choose Thumbprint from the Details tab. Compare this value to the value of RDPCERTIFICATE-THUMBPRINT in Actions, Monitor and troubleshoot, Get system log. [Mac OS X] Before you proceed, compare the fingerprint of the certificate with the value in the system log to confirm the identity of the remote computer. Choose Show Certificate, expand Details, and choose SHA1 Fingerprints. Compare this value to the value of RDPCERTIFICATE-THUMBPRINT in Actions, Monitor and troubleshoot, Get system log.\n• If the RDP connection is successful, the RDP client displays the Windows login screen and then the Windows desktop. If you receive an error message instead, see Remote Desktop can't connect to the remote computer. When you are finished with the RDP connection, you can close the RDP client.\n\n• If you trust the certificate, choose Yes to connect to your instance.\n• [Windows] Before you proceed, compare the thumbprint of the certificate with the value in the system log to confirm the identity of the remote computer. Choose View certificate and then choose Thumbprint from the Details tab. Compare this value to the value of RDPCERTIFICATE-THUMBPRINT in Actions, Monitor and troubleshoot, Get system log.\n• [Mac OS X] Before you proceed, compare the fingerprint of the certificate with the value in the system log to confirm the identity of the remote computer. Choose Show Certificate, expand Details, and choose SHA1 Fingerprints. Compare this value to the value of RDPCERTIFICATE-THUMBPRINT in Actions, Monitor and troubleshoot, Get system log.",
        "code_examples": [
          "```\nssh -ikey-pair-name.pemec2-user@ec2-198-51-100-1.us-east-2.compute.amazonaws.com\n```",
          "```\nThe authenticity of host 'ec2-198-51-100-1.us-east-2.compute.amazonaws.com (198-51-100-1)' can't be established.\nECDSA key fingerprint is l4UB/neBad9tvkgJf1QZWxheQmR59WgrgzEimCG6kZY.\nAre you sure you want to continue connecting (yes/no)?\n```",
          "```\nWarning: Permanently added 'ec2-198-51-100-1.us-east-2.compute.amazonaws.com' (ECDSA) to the list of known hosts.\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 36,
          "content_length": 12427
        }
      },
      {
        "header": "Step 3: Clean up your instance",
        "content": "Terminating an instance is permanent and irreversible.\n\nAfter you terminate an instance, you can no longer connect to it, and it can't be recovered. All attached Amazon EBS volumes that are configured to be deleted on termination are also permanently deleted and can't be recovered. All data stored on instance store volumes is permanently lost. For more information, see How instance termination works.\n\nBefore you terminate an instance, ensure that you have backed up all data that you need to retain after the termination to persistent storage.\n\nAfter you've finished with the instance that you created for this tutorial, you should clean up by terminating the instance. If you want to do more with this instance before you clean up, see Next steps.\n\nYou'll stop incurring charges for that instance or usage that counts against your Free Tier limits as soon as the instance status changes to shutting down or terminated. To keep your instance for later, but not incur charges or usage that counts against your Free Tier limits, you can stop the instance now and then start it again later. For more information, see Stop and start Amazon EC2 instances.\n\nIn the navigation pane, choose Instances. In the list of instances, select the instance.\n\nChoose Instance state, Terminate (delete) instance.\n\nChoose Terminate (delete) when prompted for confirmation.\n\nAmazon EC2 shuts down and terminates your instance. After your instance is terminated, it remains visible on the console for a short while, and then the entry is automatically deleted. You cannot remove the terminated instance from the console display yourself.\n\n• In the navigation pane, choose Instances. In the list of instances, select the instance.\n• Choose Instance state, Terminate (delete) instance.\n• Choose Terminate (delete) when prompted for confirmation. Amazon EC2 shuts down and terminates your instance. After your instance is terminated, it remains visible on the console for a short while, and then the entry is automatically deleted. You cannot remove the terminated instance from the console display yourself.\n\n[Note] WarningTerminating an instance is permanent and irreversible.After you terminate an instance, you can no longer connect to it, and it can't be recovered. All attached Amazon EBS volumes that are configured to be deleted on termination are also permanently deleted and can't be recovered. All data stored on instance store volumes is permanently lost. For more information, see How instance termination works.Before you terminate an instance, ensure that you have backed up all data that you need to retain after the termination to persistent storage.\n\n[Note] Terminating an instance is permanent and irreversible.After you terminate an instance, you can no longer connect to it, and it can't be recovered. All attached Amazon EBS volumes that are configured to be deleted on termination are also permanently deleted and can't be recovered. All data stored on instance store volumes is permanently lost. For more information, see How instance termination works.Before you terminate an instance, ensure that you have backed up all data that you need to retain after the termination to persistent storage.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 3198
        }
      },
      {
        "header": "Next steps",
        "content": "After you start your instance, you might want to explore the following next steps:\n\nExplore the Amazon EC2 core concepts with the introductory tutorials. For more information, see Tutorials for launching EC2 instances.\n\nLearn how to track your Amazon EC2 Free Tier usage using the console. For more information, see Track your Free Tier usage for Amazon EC2.\n\nConfigure a CloudWatch alarm to notify you if your usage exceeds the Free Tier (for accounts created before July 15, 2025). For more information, see Tracking your AWS Free Tier usage in the AWS Billing User Guide.\n\nAdd an EBS volume. For more information, see Create an Amazon EBS volume in the Amazon EBS User Guide.\n\nLearn how to remotely manage your EC2 instance using the Run command. For more information, see AWS Systems Manager Run Command in the AWS Systems Manager User Guide.\n\nLearn about instance purchasing options. For more information, see Amazon EC2 billing and purchasing options.\n\nGet advice about instance types. For more information, see Get recommendations from EC2 instance type finder.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• Explore the Amazon EC2 core concepts with the introductory tutorials. For more information, see Tutorials for launching EC2 instances.\n• Learn how to track your Amazon EC2 Free Tier usage using the console. For more information, see Track your Free Tier usage for Amazon EC2.\n• Configure a CloudWatch alarm to notify you if your usage exceeds the Free Tier (for accounts created before July 15, 2025). For more information, see Tracking your AWS Free Tier usage in the AWS Billing User Guide.\n• Add an EBS volume. For more information, see Create an Amazon EBS volume in the Amazon EBS User Guide.\n• Learn how to remotely manage your EC2 instance using the Run command. For more information, see AWS Systems Manager Run Command in the AWS Systems Manager User Guide.\n• Learn about instance purchasing options. For more information, see Amazon EC2 billing and purchasing options.\n• Get advice about instance types. For more information, see Get recommendations from EC2 instance type finder.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 2358
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "What is Amazon S3?",
    "summary": "Features of Amazon S3How Amazon S3 worksAmazon S3 data consistency modelRelated services Accessing Amazon S3Paying for Amazon S3PCI DSS compliance\n\nWhat is Amazon S3?Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, ba",
    "sections": [
      {
        "header": "",
        "content": "Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nFeatures of Amazon S3\n\nAmazon S3 data consistency model\n\nPaying for Amazon S3\n\n• Features of Amazon S3\n• How Amazon S3 works\n• Amazon S3 data consistency model\n• Related services\n• Accessing Amazon S3\n• Paying for Amazon S3\n• PCI DSS compliance\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1361
        }
      },
      {
        "header": "Storage classes",
        "content": "Amazon S3 offers a range of storage classes designed for different use cases. For example, you can store mission-critical production data in S3 Standard or S3 Express One Zone for frequent access, save costs by storing infrequently accessed data in S3 Standard-IA or S3 One Zone-IA, and archive data at the lowest costs in S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive.\n\nAmazon S3 Express One Zone is a high-performance, single-zone Amazon S3 storage class that is purpose-built to deliver consistent, single-digit millisecond data access for your most latency-sensitive applications. S3 Express One Zone is the lowest latency cloud object storage class available today, with data access speeds up to 10x faster and with request costs 50 percent lower than S3 Standard. S3 Express One Zone is the first S3 storage class where you can select a single Availability Zone with the option to co-locate your object storage with your compute resources, which provides the highest possible access speed. Additionally, to further increase access speed and support hundreds of thousands of requests per second, data is stored in a new bucket type: an Amazon S3 directory bucket. For more information, see S3 Express One Zone and Working with directory buckets.\n\nYou can store data with changing or unknown access patterns in S3 Intelligent-Tiering, which optimizes storage costs by automatically moving your data between four access tiers when your access patterns change. These four access tiers include two low-latency access tiers optimized for frequent and infrequent access, and two opt-in archive access tiers designed for asynchronous access for rarely accessed data.\n\nFor more information, see Understanding and managing Amazon S3 storage classes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1788
        }
      },
      {
        "header": "Storage management",
        "content": "Amazon S3 has storage management features that you can use to manage costs, meet regulatory requirements, reduce latency, and save multiple distinct copies of your data for compliance requirements.\n\nS3 Lifecycle â Configure a lifecycle configuration to manage your objects and store them cost effectively throughout their lifecycle. You can transition objects to other S3 storage classes or expire objects that reach the end of their lifetimes.\n\nS3 Object Lock â Prevent Amazon S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require write-once-read-many (WORM) storage or to simply add another layer of protection against object changes and deletions.\n\nS3 Replication â Replicate objects and their respective metadata and object tags to one or more destination buckets in the same or different AWS Regions for reduced latency, compliance, security, and other use cases.\n\nS3 Batch Operations â Manage billions of objects at scale with a single S3 API request or a few clicks in the Amazon S3 console. You can use Batch Operations to perform operations such as Copy, Invoke AWS Lambda function, and Restore on millions or billions of objects.\n\n• S3 Lifecycle â Configure a lifecycle configuration to manage your objects and store them cost effectively throughout their lifecycle. You can transition objects to other S3 storage classes or expire objects that reach the end of their lifetimes.\n• S3 Object Lock â Prevent Amazon S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely. You can use Object Lock to help meet regulatory requirements that require write-once-read-many (WORM) storage or to simply add another layer of protection against object changes and deletions.\n• S3 Replication â Replicate objects and their respective metadata and object tags to one or more destination buckets in the same or different AWS Regions for reduced latency, compliance, security, and other use cases.\n• S3 Batch Operations â Manage billions of objects at scale with a single S3 API request or a few clicks in the Amazon S3 console. You can use Batch Operations to perform operations such as Copy, Invoke AWS Lambda function, and Restore on millions or billions of objects.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2320
        }
      },
      {
        "header": "Access management and security",
        "content": "Amazon S3 provides features for auditing and managing access to your buckets and objects. By default, S3 buckets and the objects in them are private. You have access only to the S3 resources that you create. To grant granular resource permissions that support your specific use case or to audit the permissions of your Amazon S3 resources, you can use the following features.\n\nS3 Block Public Access â Block public access to S3 buckets and objects. By default, Block Public Access settings are turned on at the bucket level. We recommend that you keep all Block Public Access settings enabled unless you know that you need to turn off one or more of them for your specific use case. For more information, see Configuring block public access settings for your S3 buckets.\n\nAWS Identity and Access Management (IAM) â IAM is a web service that helps you securely control access to AWS resources, including your Amazon S3 resources. With IAM, you can centrally manage permissions that control which AWS resources users can access. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.\n\nBucket policies â Use IAM-based policy language to configure resource-based permissions for your S3 buckets and the objects in them.\n\nAmazon S3 access points â Configure named network endpoints with dedicated access policies to manage data access at scale for shared datasets in Amazon S3.\n\nAccess control lists (ACLs) â Grant read and write permissions for individual buckets and objects to authorized users. As a general rule, we recommend using S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access control option. With bucket policies and access point policies, you can define rules that apply broadly across all requests to your Amazon S3 resources. For more information about the specific cases when you'd use ACLs instead of resource-based policies or IAM user policies, see Managing access with ACLs.\n\nS3 Object Ownership â Take ownership of every object in your bucket, simplifying access management for data stored in Amazon S3. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to disable or enable ACLs. By default, ACLs are disabled. With ACLs disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies.\n\nIAM Access Analyzer for S3 â Evaluate and monitor your S3 bucket access policies, ensuring that the policies provide only the intended access to your S3 resources.\n\n• S3 Block Public Access â Block public access to S3 buckets and objects. By default, Block Public Access settings are turned on at the bucket level. We recommend that you keep all Block Public Access settings enabled unless you know that you need to turn off one or more of them for your specific use case. For more information, see Configuring block public access settings for your S3 buckets.\n• AWS Identity and Access Management (IAM) â IAM is a web service that helps you securely control access to AWS resources, including your Amazon S3 resources. With IAM, you can centrally manage permissions that control which AWS resources users can access. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.\n• Bucket policies â Use IAM-based policy language to configure resource-based permissions for your S3 buckets and the objects in them.\n• Amazon S3 access points â Configure named network endpoints with dedicated access policies to manage data access at scale for shared datasets in Amazon S3.\n• Access control lists (ACLs) â Grant read and write permissions for individual buckets and objects to authorized users. As a general rule, we recommend using S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access control option. With bucket policies and access point policies, you can define rules that apply broadly across all requests to your Amazon S3 resources. For more information about the specific cases when you'd use ACLs instead of resource-based policies or IAM user policies, see Managing access with ACLs.\n• S3 Object Ownership â Take ownership of every object in your bucket, simplifying access management for data stored in Amazon S3. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to disable or enable ACLs. By default, ACLs are disabled. With ACLs disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies.\n• IAM Access Analyzer for S3 â Evaluate and monitor your S3 bucket access policies, ensuring that the policies provide only the intended access to your S3 resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 4947
        }
      },
      {
        "header": "Data processing",
        "content": "To transform data and trigger workflows to automate a variety of other processing activities at scale, you can use the following features.\n\nS3 Object Lambda â Add your own code to S3 GET, HEAD, and LIST requests to modify and process data as it is returned to an application. Filter rows, dynamically resize images, redact confidential data, and much more.\n\nEvent notifications â Trigger workflows that use Amazon Simple Notification Service (Amazon SNS), Amazon Simple Queue Service (Amazon SQS), and AWS Lambda when a change is made to your S3 resources.\n\n• S3 Object Lambda â Add your own code to S3 GET, HEAD, and LIST requests to modify and process data as it is returned to an application. Filter rows, dynamically resize images, redact confidential data, and much more.\n• Event notifications â Trigger workflows that use Amazon Simple Notification Service (Amazon SNS), Amazon Simple Queue Service (Amazon SQS), and AWS Lambda when a change is made to your S3 resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 985
        }
      },
      {
        "header": "Storage logging and monitoring",
        "content": "Amazon S3 provides logging and monitoring tools that you can use to monitor and control how your Amazon S3 resources are being used. For more information, see Monitoring tools.\n\nAmazon CloudWatch metrics for Amazon S3 â Track the operational health of your S3 resources and configure billing alerts when estimated charges reach a user-defined threshold.\n\nAWS CloudTrail â Record actions taken by a user, a role, or an AWS service in Amazon S3. CloudTrail logs provide you with detailed API tracking for S3 bucket-level and object-level operations.\n\nServer access logging â Get detailed records for the requests that are made to a bucket. You can use server access logs for many use cases, such as conducting security and access audits, learning about your customer base, and understanding your Amazon S3 bill.\n\nAWS Trusted Advisor â Evaluate your account by using AWS best practice checks to identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. You can then follow the recommendations to optimize your services and resources.\n\n• Amazon CloudWatch metrics for Amazon S3 â Track the operational health of your S3 resources and configure billing alerts when estimated charges reach a user-defined threshold.\n• AWS CloudTrail â Record actions taken by a user, a role, or an AWS service in Amazon S3. CloudTrail logs provide you with detailed API tracking for S3 bucket-level and object-level operations.\n\n• Server access logging â Get detailed records for the requests that are made to a bucket. You can use server access logs for many use cases, such as conducting security and access audits, learning about your customer base, and understanding your Amazon S3 bill.\n• AWS Trusted Advisor â Evaluate your account by using AWS best practice checks to identify ways to optimize your AWS infrastructure, improve security and performance, reduce costs, and monitor service quotas. You can then follow the recommendations to optimize your services and resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2044
        }
      },
      {
        "header": "Analytics and insights",
        "content": "Amazon S3 offers features to help you gain visibility into your storage usage, which empowers you to better understand, analyze, and optimize your storage at scale.\n\nAmazon S3 Storage Lens â Understand, analyze, and optimize your storage. S3 Storage Lens provides 60+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, AWS Regions, buckets, or prefixes.\n\nStorage Class Analysis â Analyze storage access patterns to decide when it's time to move data to a more cost-effective storage class.\n\nS3 Inventory with Inventory reports â Audit and report on objects and their corresponding metadata and configure other Amazon S3 features to take action in Inventory reports. For example, you can report on the replication and encryption status of your objects. For a list of all the metadata available for each object in Inventory reports, see Amazon S3 Inventory list.\n\n• Amazon S3 Storage Lens â Understand, analyze, and optimize your storage. S3 Storage Lens provides 60+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, AWS Regions, buckets, or prefixes.\n• Storage Class Analysis â Analyze storage access patterns to decide when it's time to move data to a more cost-effective storage class.\n• S3 Inventory with Inventory reports â Audit and report on objects and their corresponding metadata and configure other Amazon S3 features to take action in Inventory reports. For example, you can report on the replication and encryption status of your objects. For a list of all the metadata available for each object in Inventory reports, see Amazon S3 Inventory list.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1712
        }
      },
      {
        "header": "Strong consistency",
        "content": "Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of objects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both writes of new objects as well as PUT requests that overwrite existing objects and DELETE requests. In addition, read operations on Amazon S3 Select, Amazon S3 access control lists (ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object) are strongly consistent. For more information, see Amazon S3 data consistency model.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 515
        }
      },
      {
        "header": "How Amazon S3 works",
        "content": "Amazon S3 is an object storage service that stores data as objects, hierarchical data, or tabular data within buckets. An object is a file and any metadata that describes the file. A bucket is a container for objects.\n\nTo store your data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object has a key (or key name), which is the unique identifier for the object within the bucket.\n\nS3 provides features that you can configure to support your specific use case. For example, you can use S3 Versioning to keep multiple versions of an object in the same bucket, which allows you to restore objects that are accidentally deleted or overwritten.\n\nBuckets and the objects in them are private and can be accessed only if you explicitly grant access permissions. You can use bucket policies, AWS Identity and Access Management (IAM) policies, access control lists (ACLs), and S3 Access Points to manage access.\n\nAccess control lists (ACLs)\n\n• S3 Versioning\n• Bucket policy\n• S3 access points\n• Access control lists (ACLs)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1121
        }
      },
      {
        "header": "Buckets",
        "content": "Amazon S3 supports four types of bucketsâgeneral purpose buckets, directory buckets, table buckets, and vector buckets. Each type of bucket provides a unique set of features for different use cases.\n\nGeneral purpose buckets â General purpose buckets are recommended for most use cases and access patterns and are the original S3 bucket type. A general purpose bucket is a container for objects stored in Amazon S3, and you can store any number of objects in a bucket and across all storage classes (except for S3 Express One Zone), so you can redundantly store objects across multiple Availability Zones. For more information, see Creating, configuring, and working with Amazon S3 general purpose buckets.\n\nBy default, all general purpose buckets are private. However, you can grant public access to general purpose buckets. You can control access to general purpose buckets at the bucket, prefix (folder), or object tag level. For more information, see Access control in Amazon S3.\n\nDirectory buckets â Recommended for low-latency use cases and data-residency use cases. By default, you can create up to 100 directory buckets in your AWS account, with no limit on the number of objects that you can store in a directory bucket. Directory buckets organize objects into hierarchical directories (prefixes) instead of the flat storage structure of general purpose buckets. This bucket type has no prefix limits and individual directories can scale horizontally. For more information, see Working with directory buckets.\n\nFor low-latency use cases, you can create a directory bucket in a single AWS Availability Zone to store data. Directory buckets in Availability Zones support the S3 Express One Zone storage class. With S3 Express One Zone, your data is redundantly stored on multiple devices within a single Availability Zone. The S3 Express One Zone storage class is recommended if your application is performance sensitive and benefits from single-digit millisecond PUT and GET latencies. To learn more about creating directory buckets in Availability Zones, see High performance workloads.\n\nFor data-residency use cases, you can create a directory bucket in a single AWS Dedicated Local Zone (DLZ) to store data. In Dedicated Local Zones, you can create S3 directory buckets to store data in a specific data perimeter, which helps support your data residency and isolation use cases. Directory buckets in Local Zones support the S3 One Zone-Infrequent Access (S3 One Zone-IA; Z-IA) storage class. To learn more about creating directory buckets in Local Zones, see Data residency workloads.\n\nDirectory buckets have all public access disabled by default. This behavior can't be changed. You can't grant access to objects stored in directory buckets. You can grant access only to your directory buckets. For more information, see Authenticating and authorizing requests.\n\nTable buckets â Recommended for storing tabular data, such as daily purchase transactions, streaming sensor data, or ad impressions. Tabular data represents data in columns and rows, like in a database table. Table buckets provide S3 storage that's optimized for analytics and machine learning workloads, with features designed to continuously improve query performance and reduce storage costs for tables. S3 Tables are purpose-built for storing tabular data in the Apache Iceberg format. You can query tabular data in S3 Tables with popular query engines, including Amazon Athena, Amazon Redshift, and Apache Spark. By default, you can create up to 10 table buckets per AWS account per AWS Region and up to 10,000 tables per table bucket. For more information, see Working with S3 Tables and table buckets.\n\nAll table buckets and tables are private and can't be made public. These resources can only be accessed by users who are explicitly granted access. To grant access, you can use IAM resource-based policies for table buckets and tables, and IAM identity-based policies for users and roles. For more information, see Security for S3 Tables.\n\nVector buckets â S3 vector buckets are a type of Amazon S3 bucket that are purpose-built to store and query vectors. Vector buckets use dedicated API operations to write and query vector data efficiently. With S3 vector buckets, you can store vector embeddings for machine learning models, perform similarity searches, and integrate with services like Amazon Bedrock and Amazon OpenSearch.\n\nS3 vector buckets organize data using vector indexes, which are resources within a bucket that store and organize vector data for efficient similarity search. Each vector index can be configured with specific dimensions, distance metrics (like cosine similarity), and metadata configurations to optimize for your specific use case. For more information, see Working with S3 Vectors and vector buckets.\n\nWhen you create a bucket, you enter a bucket name and choose the AWS Region where the bucket will reside. After you create a bucket, you cannot change the name of the bucket or its Region. Bucket names must follow the following bucket naming rules:\n\nGeneral purpose bucket naming rules\n\nDirectory bucket naming rules\n\nTable bucket naming rules\n\nOrganize the Amazon S3 namespace at the highest level. For general purpose buckets, this namespace is S3. For directory buckets, this namespace is s3express. For table buckets, this namespace is s3tables.\n\nIdentify the account responsible for storage and data transfer charges.\n\nServe as the unit of aggregation for usage reporting.\n\n• For low-latency use cases, you can create a directory bucket in a single AWS Availability Zone to store data. Directory buckets in Availability Zones support the S3 Express One Zone storage class. With S3 Express One Zone, your data is redundantly stored on multiple devices within a single Availability Zone. The S3 Express One Zone storage class is recommended if your application is performance sensitive and benefits from single-digit millisecond PUT and GET latencies. To learn more about creating directory buckets in Availability Zones, see High performance workloads.\n• For data-residency use cases, you can create a directory bucket in a single AWS Dedicated Local Zone (DLZ) to store data. In Dedicated Local Zones, you can create S3 directory buckets to store data in a specific data perimeter, which helps support your data residency and isolation use cases. Directory buckets in Local Zones support the S3 One Zone-Infrequent Access (S3 One Zone-IA; Z-IA) storage class. To learn more about creating directory buckets in Local Zones, see Data residency workloads.\n\n• General purpose bucket naming rules\n• Directory bucket naming rules\n• Table bucket naming rules\n\n• Organize the Amazon S3 namespace at the highest level. For general purpose buckets, this namespace is S3. For directory buckets, this namespace is s3express. For table buckets, this namespace is s3tables.\n• Identify the account responsible for storage and data transfer charges.\n• Serve as the unit of aggregation for usage reporting.\n\n[Note] NoteBy default, all general purpose buckets are private. However, you can grant public access to general purpose buckets. You can control access to general purpose buckets at the bucket, prefix (folder), or object tag level. For more information, see Access control in Amazon S3.\n\n[Note] By default, all general purpose buckets are private. However, you can grant public access to general purpose buckets. You can control access to general purpose buckets at the bucket, prefix (folder), or object tag level. For more information, see Access control in Amazon S3.\n\n[Note] NoteDirectory buckets have all public access disabled by default. This behavior can't be changed. You can't grant access to objects stored in directory buckets. You can grant access only to your directory buckets. For more information, see Authenticating and authorizing requests.\n\n[Note] Directory buckets have all public access disabled by default. This behavior can't be changed. You can't grant access to objects stored in directory buckets. You can grant access only to your directory buckets. For more information, see Authenticating and authorizing requests.\n\n[Note] NoteAll table buckets and tables are private and can't be made public. These resources can only be accessed by users who are explicitly granted access. To grant access, you can use IAM resource-based policies for table buckets and tables, and IAM identity-based policies for users and roles. For more information, see Security for S3 Tables.\n\n[Note] All table buckets and tables are private and can't be made public. These resources can only be accessed by users who are explicitly granted access. To grant access, you can use IAM resource-based policies for table buckets and tables, and IAM identity-based policies for users and roles. For more information, see Security for S3 Tables.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 8863
        }
      },
      {
        "header": "Objects",
        "content": "Objects are the fundamental entities stored in Amazon S3. Objects consist of object data and metadata. The metadata is a set of name-value pairs that describe the object. These pairs include some default metadata, such as the date last modified, and standard HTTP metadata, such as Content-Type. You can also specify custom metadata at the time that the object is stored.\n\nEvery object is contained in a bucket. For example, if the object named photos/puppy.jpg is stored in the amzn-s3-demo-bucket general purpose bucket in the US West (Oregon) Region, then it is addressable by using the URL https://amzn-s3-demo-bucket.s3.us-west-2.amazonaws.com/photos/puppy.jpg. For more information, see Accessing a Bucket.\n\nAn object is uniquely identified within a bucket by a key (name) and a version ID (if S3 Versioning is enabled on the bucket). For more information about objects, see Amazon S3 objects overview.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 908
        }
      },
      {
        "header": "Keys",
        "content": "An object key (or key name) is the unique identifier for an object within a bucket. Every object in a bucket has exactly one key. The combination of a bucket, object key, and optionally, version ID (if S3 Versioning is enabled for the bucket) uniquely identify each object. So you can think of Amazon S3 as a basic data map between \"bucket + key + version\" and the object itself.\n\nEvery object in Amazon S3 can be uniquely addressed through the combination of the web service endpoint, bucket name, key, and optionally, a version. For example, in the URL https://amzn-s3-demo-bucket.s3.us-west-2.amazonaws.com/photos/puppy.jpg, amzn-s3-demo-bucket is the name of the bucket and photos/puppy.jpg is the key.\n\nFor more information about object keys, see Naming Amazon S3 objects.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 777
        }
      },
      {
        "header": "S3 Versioning",
        "content": "You can use S3 Versioning to keep multiple variants of an object in the same bucket. With S3 Versioning, you can preserve, retrieve, and restore every version of every object stored in your buckets. You can easily recover from both unintended user actions and application failures.\n\nFor more information, see Retaining multiple versions of objects with S3 Versioning.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 367
        }
      },
      {
        "header": "Version ID",
        "content": "When you enable S3 Versioning in a bucket, Amazon S3 generates a unique version ID for each object added to the bucket. Objects that already existed in the bucket at the time that you enable versioning have a version ID of null. If you modify these (or any other) objects with other operations, such as CopyObject and PutObject, the new objects get a unique version ID.\n\nFor more information, see Retaining multiple versions of objects with S3 Versioning.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 455
        }
      },
      {
        "header": "Bucket policy",
        "content": "A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that you can use to grant access permissions to your bucket and the objects in it. Only the bucket owner can associate a policy with a bucket. The permissions attached to the bucket apply to all of the objects in the bucket that are owned by the bucket owner. Bucket policies are limited to 20 KB in size.\n\nBucket policies use JSON-based access policy language that is standard across AWS. You can use bucket policies to add or deny permissions for the objects in a bucket. Bucket policies allow or deny requests based on the elements in the policy, including the requester, S3 actions, resources, and aspects or conditions of the request (for example, the IP address used to make the request). For example, you can create a bucket policy that grants cross-account permissions to upload objects to an S3 bucket while ensuring that the bucket owner has full control of the uploaded objects. For more information, see Examples of Amazon S3 bucket policies.\n\nIn your bucket policy, you can use wildcard characters on Amazon Resource Names (ARNs) and other values to grant permissions to a subset of objects. For example, you can control access to groups of objects that begin with a common prefix or end with a given extension, such as .html.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1323
        }
      },
      {
        "header": "S3 access points",
        "content": "Amazon S3 access points are named network endpoints with dedicated access policies that describe how data can be accessed using that endpoint. Access points are attached to an underlying data source, such as a general purpose bucket, directory bucket, or a FSx for OpenZFS volume, that you can use to perform S3 object operations, such as GetObject and PutObject. Access points simplify managing data access at scale for shared datasets in Amazon S3.\n\nEach access point has its own access point policy. You can configure Block Public Access settings for each access point attached to a bucket. To restrict Amazon S3 data access to a private network, you can also configure any access point to accept requests only from a virtual private cloud (VPC).\n\nFor more information about access points for general purpose buckets, see Managing access to shared datasets with access points. For more information about access points for directory buckets, see Managing access to shared datasets in directory buckets with access points.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1023
        }
      },
      {
        "header": "Access control lists (ACLs)",
        "content": "You can use ACLs to grant read and write permissions to authorized users for individual general purpose buckets and objects. Each general purpose bucket and object has an ACL attached to it as a subresource. The ACL defines which AWS accounts or groups are granted access and the type of access. ACLs are an access control mechanism that predates IAM. For more information about ACLs, see Access control list (ACL) overview.\n\nS3 Object Ownership is an Amazon S3 bucket-level setting that you can use to both control ownership of the objects that are uploaded to your bucket and to disable or enable ACLs. By default, Object Ownership is set to the Bucket owner enforced setting, and all ACLs are disabled. When ACLs are disabled, the bucket owner owns all the objects in the bucket and manages access to them exclusively by using access-management policies.\n\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you need to control access for each object individually. With ACLs disabled, you can use policies to control access to all objects in your bucket, regardless of who uploaded the objects to your bucket. For more information, see Controlling ownership of objects and disabling ACLs for your bucket.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1306
        }
      },
      {
        "header": "Regions",
        "content": "You can choose the geographical AWS Region where Amazon S3 stores the buckets that you create. You might choose a Region to optimize latency, minimize costs, or address regulatory requirements. Objects stored in an AWS Region never leave the Region unless you explicitly transfer or replicate them to another Region. For example, objects stored in the Europe (Ireland) Region never leave it.\n\nYou can access Amazon S3 and its features only in the AWS Regions that are enabled for your account. For more information about enabling a Region to create and manage AWS resources, see Managing AWS Regions in the AWS General Reference.\n\nFor a list of Amazon S3 Regions and endpoints, see Regions and endpoints in the AWS General Reference.\n\n[Note] NoteYou can access Amazon S3 and its features only in the AWS Regions that are enabled for your account. For more information about enabling a Region to create and manage AWS resources, see Managing AWS Regions in the AWS General Reference.\n\n[Note] You can access Amazon S3 and its features only in the AWS Regions that are enabled for your account. For more information about enabling a Region to create and manage AWS resources, see Managing AWS Regions in the AWS General Reference.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1227
        }
      },
      {
        "header": "Amazon S3 data consistency model",
        "content": "Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of objects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both writes to new objects as well as PUT requests that overwrite existing objects and DELETE requests. In addition, read operations on Amazon S3 Select, Amazon S3 access controls lists (ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object) are strongly consistent.\n\nUpdates to a single key are atomic. For example, if you make a PUT request to an existing key from one thread and perform a GET request on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data.\n\nAmazon S3 achieves high availability by replicating data across multiple servers within AWS data centers. If a PUT request is successful, your data is safely stored. Any read (GET or LIST request) that is initiated following the receipt of a successful PUT response will return the data written by the PUT request. Here are examples of this behavior:\n\nA process writes a new object to Amazon S3 and immediately lists keys within its bucket. The new object appears in the list.\n\nA process replaces an existing object and immediately tries to read it. Amazon S3 returns the new data.\n\nA process deletes an existing object and immediately tries to read it. Amazon S3 does not return any data because the object has been deleted.\n\nA process deletes an existing object and immediately lists keys within its bucket. The object does not appear in the listing.\n\nAmazon S3 does not support object locking for concurrent writers. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you must build an object-locking mechanism into your application.\n\nUpdates are key-based. There is no way to make atomic updates across keys. For example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application.\n\nBucket configurations have an eventual consistency model. Specifically, this means that:\n\nIf you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list.\n\nIf you enable versioning on a bucket for the first time, it might take a short amount of time for the change to be fully propagated. We recommend that you wait for 15 minutes after enabling versioning before issuing write operations (PUT or DELETE requests) on objects in the bucket.\n\n• A process writes a new object to Amazon S3 and immediately lists keys within its bucket. The new object appears in the list.\n• A process replaces an existing object and immediately tries to read it. Amazon S3 returns the new data.\n• A process deletes an existing object and immediately tries to read it. Amazon S3 does not return any data because the object has been deleted.\n• A process deletes an existing object and immediately lists keys within its bucket. The object does not appear in the listing.\n\n• Amazon S3 does not support object locking for concurrent writers. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you must build an object-locking mechanism into your application.\n• Updates are key-based. There is no way to make atomic updates across keys. For example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application.\n\n• If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list.\n• If you enable versioning on a bucket for the first time, it might take a short amount of time for the change to be fully propagated. We recommend that you wait for 15 minutes after enabling versioning before issuing write operations (PUT or DELETE requests) on objects in the bucket.\n\n[Note] Note Amazon S3 does not support object locking for concurrent writers. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you must build an object-locking mechanism into your application. Updates are key-based. There is no way to make atomic updates across keys. For example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application.\n\n[Note] Amazon S3 does not support object locking for concurrent writers. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you must build an object-locking mechanism into your application. Updates are key-based. There is no way to make atomic updates across keys. For example, you cannot make the update of one key dependent on the update of another key unless you design this functionality into your application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 4931
        }
      },
      {
        "header": "Concurrent applications",
        "content": "This section provides examples of behavior to be expected from Amazon S3 when multiple clients are writing to the same items.\n\nIn this example, both W1 (write 1) and W2 (write 2) finish before the start of R1 (read 1) and R2 (read 2). Because S3 is strongly consistent, R1 and R2 both return color = ruby.\n\nIn the next example, W2 does not finish before the start of R1. Therefore, R1 might return color = ruby or color = garnet. However, because W1 and W2 finish before the start of R2, R2 returns color = garnet.\n\nIn the last example, W2 begins before W1 has received an acknowledgment. Therefore, these writes are considered concurrent. Amazon S3 internally uses last-writer-wins semantics to determine which write takes precedence. However, the order in which Amazon S3 receives the requests and the order in which applications receive acknowledgments cannot be predicted because of various factors, such as network latency. For example, W2 might be initiated by an Amazon EC2 instance in the same Region, while W1 might be initiated by a host that is farther away. The best way to determine the final value is to perform a read after both writes have been acknowledged.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1174
        }
      },
      {
        "header": "Related services",
        "content": "After you load your data into Amazon S3, you can use it with other AWS services. The following are the services that you might use most frequently:\n\nAmazon Elastic Compute Cloud (Amazon EC2) â Provides secure and scalable computing capacity in the AWS Cloud. Using Amazon EC2 eliminates your need to invest in hardware upfront, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage.\n\nAmazon EMR â Helps businesses, researchers, data analysts, and developers easily and cost-effectively process vast amounts of data. Amazon EMR uses a hosted Hadoop framework running on the web-scale infrastructure of Amazon EC2 and Amazon S3.\n\nAWS Snow Family â Helps customers that need to run operations in austere, non-data center environments, and in locations where there's a lack of consistent network connectivity. You can use AWS Snow Family devices to locally and cost-effectively access the storage and compute power of the AWS Cloud in places where an internet connection might not be an option.\n\nAWS Transfer Family â Provides fully managed support for file transfers directly into and out of Amazon S3 or Amazon Elastic File System (Amazon EFS) using Secure Shell (SSH) File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP).\n\n• Amazon Elastic Compute Cloud (Amazon EC2) â Provides secure and scalable computing capacity in the AWS Cloud. Using Amazon EC2 eliminates your need to invest in hardware upfront, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage.\n• Amazon EMR â Helps businesses, researchers, data analysts, and developers easily and cost-effectively process vast amounts of data. Amazon EMR uses a hosted Hadoop framework running on the web-scale infrastructure of Amazon EC2 and Amazon S3.\n• AWS Snow Family â Helps customers that need to run operations in austere, non-data center environments, and in locations where there's a lack of consistent network connectivity. You can use AWS Snow Family devices to locally and cost-effectively access the storage and compute power of the AWS Cloud in places where an internet connection might not be an option.\n• AWS Transfer Family â Provides fully managed support for file transfers directly into and out of Amazon S3 or Amazon Elastic File System (Amazon EFS) using Secure Shell (SSH) File Transfer Protocol (SFTP), File Transfer Protocol over SSL (FTPS), and File Transfer Protocol (FTP).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2678
        }
      },
      {
        "header": "Accessing Amazon S3",
        "content": "You can work with Amazon S3 in any of the following ways:",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 57
        }
      },
      {
        "header": "AWS Management Console",
        "content": "The console is a web-based user interface for managing Amazon S3 and AWS resources. If you've signed up for an AWS account, you can access the Amazon S3 console by signing into the AWS Management Console and choosing S3 from the AWS Management Console home page.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 262
        }
      },
      {
        "header": "AWS Command Line Interface",
        "content": "You can use the AWS command line tools to issue commands or build scripts at your system's command line to perform AWS (including S3) tasks.\n\nThe AWS Command Line Interface (AWS CLI) provides commands for a broad set of AWS services. The AWS CLI is supported on Windows, macOS, and Linux. To get started, see the AWS Command Line Interface User Guide. For more information about the commands for Amazon S3, see s3api and s3control in the AWS CLI Command Reference.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 464
        }
      },
      {
        "header": "AWS SDKs",
        "content": "AWS provides SDKs (software development kits) that consist of libraries and sample code for various programming languages and platforms (Java, Python, Ruby, .NET, iOS, Android, and so on). The AWS SDKs provide a convenient way to create programmatic access to S3 and AWS. Amazon S3 is a REST service. You can send requests to Amazon S3 using the AWS SDK libraries, which wrap the underlying Amazon S3 REST API and simplify your programming tasks. For example, the SDKs take care of tasks such as calculating signatures, cryptographically signing requests, managing errors, and retrying requests automatically. For information about the AWS SDKs, including how to download and install them, see Tools for AWS.\n\nEvery interaction with Amazon S3 is either authenticated or anonymous. If you are using the AWS SDKs, the libraries compute the signature for authentication from the keys that you provide. For more information about how to make requests to Amazon S3, see Making requests .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 982
        }
      },
      {
        "header": "Amazon S3 REST API",
        "content": "The architecture of Amazon S3 is designed to be programming language-neutral, using AWS-supported interfaces to store and retrieve objects. You can access S3 and AWS programmatically by using the Amazon S3 REST API. The REST API is an HTTP interface to Amazon S3. With the REST API, you use standard HTTP requests to create, fetch, and delete buckets and objects.\n\nTo use the REST API, you can use any toolkit that supports HTTP. You can even use a browser to fetch objects, as long as they are anonymously readable.\n\nThe REST API uses standard HTTP headers and status codes, so that standard browsers and toolkits work as expected. In some areas, we have added functionality to HTTP (for example, we added headers to support access control). In these cases, we have done our best to add the new functionality in a way that matches the style of standard HTTP usage.\n\nIf you make direct REST API calls in your application, you must write the code to compute the signature and add it to the request. For more information about how to make requests to Amazon S3, see Making requests in the Amazon S3 API Reference.\n\nSOAP API support over HTTP is deprecated, but it is still available over HTTPS. Newer Amazon S3 features are not supported for SOAP. We recommend that you use either the REST API or the AWS SDKs.\n\n[Note] NoteSOAP API support over HTTP is deprecated, but it is still available over HTTPS. Newer Amazon S3 features are not supported for SOAP. We recommend that you use either the REST API or the AWS SDKs.\n\n[Note] SOAP API support over HTTP is deprecated, but it is still available over HTTPS. Newer Amazon S3 features are not supported for SOAP. We recommend that you use either the REST API or the AWS SDKs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1720
        }
      },
      {
        "header": "Paying for Amazon S3",
        "content": "Pricing for Amazon S3 is designed so that you don't have to plan for the storage requirements of your application. Most storage providers require you to purchase a predetermined amount of storage and network transfer capacity. In this scenario, if you exceed that capacity, your service is shut off or you are charged high overage fees. If you do not exceed that capacity, you pay as though you used it all.\n\nAmazon S3 charges you only for what you actually use, with no hidden fees and no overage charges. This model gives you a variable-cost service that can grow with your business while giving you the cost advantages of the AWS infrastructure. For more information, see Amazon S3 Pricing.\n\nWhen you sign up for AWS, your AWS account is automatically signed up for all services in AWS, including Amazon S3. However, you are charged only for the services that you use. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For more information, see AWS free tier.\n\nTo see your bill, go to the Billing and Cost Management Dashboard in the AWS Billing and Cost Management console. To learn more about AWS account billing, see the AWS Billing User Guide. If you have questions concerning AWS billing and AWS accounts, contact AWS Support.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1266
        }
      },
      {
        "header": "PCI DSS compliance",
        "content": "Amazon S3 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS). For more information about PCI DSS, including how to request a copy of the AWS PCI Compliance Package, see PCI DSS Level 1.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 640
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
    "doc_type": "aws",
    "total_sections": 28
  },
  {
    "title": "Getting started with Amazon S3",
    "summary": "Setting upStep 1: Create a bucketStep 2: Upload an objectStep 3: Download an objectStep 4: Copy an objectStep 5: Delete the objects and bucketNext steps\n\nGetting started with Amazon S3You can get started with Amazon S3 by working with buckets and objects. A bucket is a container for objects. An object is a file and any metadata that describes that file.To store an object in Amazon S3, you create a bucket and then upload the object to the bucket. When the object is in the bucket, you can open it,",
    "sections": [
      {
        "header": "",
        "content": "You can get started with Amazon S3 by working with buckets and objects. A bucket is a container for objects. An object is a file and any metadata that describes that file.\n\nTo store an object in Amazon S3, you create a bucket and then upload the object to the bucket. When the object is in the bucket, you can open it, download it, and move it. When you no longer need an object or a bucket, you can clean up your resources.\n\nWith Amazon S3, you pay only for what you use. For more information about Amazon S3 features and pricing, see Amazon S3. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For more information, see AWS Free Tier.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nThe following video shows you how to get started with Amazon S3.\n\nBefore you begin, confirm that you've completed the steps in Setting up Amazon S3.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1326
        }
      },
      {
        "header": "Setting up Amazon S3",
        "content": "When you sign up for AWS, your AWS account is automatically signed up for all services in AWS, including Amazon S3. You are charged only for the services that you use.\n\nWith Amazon S3, you pay only for what you use. For more information about Amazon S3 features and pricing, see Amazon S3. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For more information, see AWS Free Tier.\n\nTo set up Amazon S3, use the steps in the following sections.\n\nWhen you sign up for AWS and set up Amazon S3, you can optionally change the display language in the AWS Management Console. For more information, see Changing the language of the AWS Management Console in the AWS Management Console Getting Started Guide.\n\nSign up for an AWS account\n\nCreate a user with administrative access\n\n• Sign up for an AWS account\n• Create a user with administrative access",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 875
        }
      },
      {
        "header": "Sign up for an AWS account",
        "content": "If you do not have an AWS account, complete the following steps to create one.\n\nOpen https://portal.aws.amazon.com/billing/signup.\n\nFollow the online instructions.\n\nPart of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad.\n\nWhen you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access.\n\nAWS sends you a confirmation email after the sign-up process is complete. At any time, you can view your current account activity and manage your account by going to https://aws.amazon.com/ and choosing My Account.\n\n• Open https://portal.aws.amazon.com/billing/signup.\n• Follow the online instructions. Part of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad. When you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1319
        }
      },
      {
        "header": "Create a user with administrative access",
        "content": "After you sign up for an AWS account, secure your AWS account root user, enable AWS IAM Identity Center, and create an administrative user so that you don't use the root user for everyday tasks.\n\nSign in to the AWS Management Console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password.\n\nFor help signing in by using root user, see Signing in as the root user in the AWS Sign-In User Guide.\n\nTurn on multi-factor authentication (MFA) for your root user.\n\nFor instructions, see Enable a virtual MFA device for your AWS account root user (console) in the IAM User Guide.\n\nEnable IAM Identity Center.\n\nFor instructions, see Enabling AWS IAM Identity Center in the AWS IAM Identity Center User Guide.\n\nIn IAM Identity Center, grant administrative access to a user.\n\nFor a tutorial about using the IAM Identity Center directory as your identity source, see Configure user access with the default IAM Identity Center directory in the AWS IAM Identity Center User Guide.\n\nTo sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user.\n\nFor help signing in using an IAM Identity Center user, see Signing in to the AWS access portal in the AWS Sign-In User Guide.\n\nIn IAM Identity Center, create a permission set that follows the best practice of applying least-privilege permissions.\n\nFor instructions, see Create a permission set in the AWS IAM Identity Center User Guide.\n\nAssign users to a group, and then assign single sign-on access to the group.\n\nFor instructions, see Add groups in the AWS IAM Identity Center User Guide.\n\n• Sign in to the AWS Management Console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password. For help signing in by using root user, see Signing in as the root user in the AWS Sign-In User Guide.\n• Turn on multi-factor authentication (MFA) for your root user. For instructions, see Enable a virtual MFA device for your AWS account root user (console) in the IAM User Guide.\n\n• Enable IAM Identity Center. For instructions, see Enabling AWS IAM Identity Center in the AWS IAM Identity Center User Guide.\n• In IAM Identity Center, grant administrative access to a user. For a tutorial about using the IAM Identity Center directory as your identity source, see Configure user access with the default IAM Identity Center directory in the AWS IAM Identity Center User Guide.\n\n• To sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user. For help signing in using an IAM Identity Center user, see Signing in to the AWS access portal in the AWS Sign-In User Guide.\n\n• In IAM Identity Center, create a permission set that follows the best practice of applying least-privilege permissions. For instructions, see Create a permission set in the AWS IAM Identity Center User Guide.\n• Assign users to a group, and then assign single sign-on access to the group. For instructions, see Add groups in the AWS IAM Identity Center User Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 3172
        }
      },
      {
        "header": "Step 1: Create your first S3 bucket",
        "content": "After you sign up for AWS, you're ready to create a bucket in Amazon S3 using the AWS Management Console. Every object in Amazon S3 is stored in a bucket. Before you can store data in Amazon S3, you must create a bucket.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nYou are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket. The charges that you incur through following the examples in this guide are minimal (less than $1). For more information about storage charges, see Amazon S3 pricing.\n\nSign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n\nIn the navigation bar on the top of the page, choose the name of the currently displayed AWS Region. Next, choose the Region in which you want to create a bucket.\n\nAfter you create a bucket, you can't change its Region.\n\nTo minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\nIn the left navigation pane, choose General purpose buckets.\n\nChoose Create bucket. The Create bucket page opens.\n\nFor Bucket name, enter a name for your bucket.\n\nThe bucket name must:\n\nBe unique within a partition. A partition is a grouping of Regions. AWS currently has three partitions: aws (commercial Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud (US) Regions).\n\nBe between 3 and 63 characters long.\n\nConsist only of lowercase letters, numbers, periods (.), and hyphens (-). For best compatibility, we recommend that you avoid using periods (.) in bucket names, except for buckets that are used only for static website hosting.\n\nBegin and end with a letter or number.\n\nFor a complete list of bucket-naming rules, see General purpose bucket naming rules.\n\nAfter you create the bucket, you can't change its name.\n\nDon't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n(Optional) Under General configuration, you can choose to copy an existing bucket's settings to your new bucket. If you don't want to copy the settings of an existing bucket, skip to the next step.\n\nIsn't available in the AWS CLI and is only available in the Amazon S3 console\n\nDoesn't copy the bucket policy from the existing bucket to the new bucket\n\nTo copy an existing bucket's settings, under Copy settings from existing bucket, select Choose bucket. The Choose bucket window opens. Find the bucket with the settings that you want to copy, and select Choose bucket. The Choose bucket window closes, and the Create bucket window reopens.\n\nUnder Copy settings from existing bucket, you now see the name of the bucket that you selected. The settings of your new bucket now match the settings of the bucket that you selected. If you want to remove the copied settings, choose Restore defaults. Review the remaining bucket settings on the Create bucket page. If you don't want to make any changes, you can skip to the final step.\n\nUnder Object Ownership, to disable or enable ACLs and control ownership of objects uploaded in your bucket, choose one of the following settings:\n\nBucket owner enforced (default) â ACLs are disabled, and the bucket owner automatically owns and has full control over every object in the general purpose bucket. ACLs no longer affect access permissions to data in the S3 general purpose bucket. The bucket uses policies exclusively to define access control.\n\nBy default, ACLs are disabled. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you must control access for each object individually. For more information, see Controlling ownership of objects and disabling ACLs for your bucket.\n\nBucket owner preferred â The bucket owner owns and has full control over new objects that other accounts write to the bucket with the bucket-owner-full-control canned ACL.\n\nIf you apply the Bucket owner preferred setting, to require all Amazon S3 uploads to include the bucket-owner-full-control canned ACL, you can add a bucket policy that allows only object uploads that use this ACL.\n\nObject writer â The AWS account that uploads an object owns the object, has full control over it, and can grant other users access to it through ACLs.\n\nThe default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n\nUnder Block Public Access settings for this bucket, choose the Block Public Access settings that you want to apply to the bucket.\n\nBy default, all four Block Public Access settings are enabled. We recommend that you keep all settings enabled, unless you know that you need to turn off one or more of them for your specific use case. For more information about blocking public access, see Blocking public access to your Amazon S3 storage.\n\nTo enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n\n(Optional) By default, Bucket Versioning is disabled. Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your bucket. With versioning, you can recover more easily from both unintended user actions and application failures. For more information about versioning, see Retaining multiple versions of objects with S3 Versioning.\n\nTo enable versioning on your bucket, choose Enable.\n\n(Optional) Under Tags, you can choose to add tags to your bucket. With AWS cost allocation, you can use bucket tags to annotate billing for your use of a bucket. A tag is a key-value pair that represents a label that you assign to a bucket. For more information, see Using cost allocation S3 bucket tags.\n\nTo add a bucket tag, enter a Key and optionally a Value and choose Add Tag.\n\nTo configure Default encryption, under Encryption type, choose one of the following:\n\nServer-side encryption with Amazon S3 managed keys (SSE-S3)\n\nServer-side encryption with AWS Key Management Service keys (SSE-KMS)\n\nDual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS)\n\nIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\nBuckets and new objects are encrypted by using server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption configuration. For more information about default encryption, see Setting default server-side encryption behavior for Amazon S3 buckets. For more information about SSE-S3, see Using server-side encryption with Amazon S3 managed keys (SSE-S3).\n\nFor more information about using server-side encryption to encrypt your data, see Protecting data with encryption.\n\nIf you chose Server-side encryption with AWS Key Management Service keys (SSE-KMS) or Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS), do the following:\n\nUnder AWS KMS key, specify your KMS key in one of the following ways:\n\nTo choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys.\n\nBoth the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide.\n\nTo enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears.\n\nTo create a new customer managed key in the AWS KMS console, choose Create a KMS key.\n\nFor more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide.\n\nYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).\n\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n\nWhen you configure your bucket to use default encryption with SSE-KMS, you can also use S3 Bucket Keys. S3 Bucket Keys lower the cost of encryption by decreasing request traffic from Amazon S3 to AWS KMS. For more information, see Reducing the cost of SSE-KMS with Amazon S3 Bucket Keys. S3 Bucket Keys aren't supported for DSSE-KMS.\n\nBy default, S3 Bucket Keys are enabled in the Amazon S3 console. We recommend leaving S3 Bucket Keys enabled to lower your costs. To disable S3 Bucket Keys for your bucket, under Bucket Key, choose Disable.\n\n(Optional) S3 Object Lock helps protect new objects from being deleted or overwritten. For more information, see Locking objects with Object Lock. If you want to enable S3 Object Lock, do the following:\n\nChoose Advanced settings.\n\nEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n\nIf you want to enable Object Lock, choose Enable, read the warning that appears, and acknowledge it.\n\nTo create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.\n\nChoose Create bucket.\n\nYou've created a bucket in Amazon S3.\n\nTo add an object to your bucket, see Step 2: Upload an object to your bucket.\n\n• Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n• In the navigation bar on the top of the page, choose the name of the currently displayed AWS Region. Next, choose the Region in which you want to create a bucket. Note After you create a bucket, you can't change its Region. To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n• In the left navigation pane, choose General purpose buckets.\n• Choose Create bucket. The Create bucket page opens.\n• For Bucket name, enter a name for your bucket. The bucket name must: Be unique within a partition. A partition is a grouping of Regions. AWS currently has three partitions: aws (commercial Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud (US) Regions). Be between 3 and 63 characters long. Consist only of lowercase letters, numbers, periods (.), and hyphens (-). For best compatibility, we recommend that you avoid using periods (.) in bucket names, except for buckets that are used only for static website hosting. Begin and end with a letter or number. For a complete list of bucket-naming rules, see General purpose bucket naming rules. Important After you create the bucket, you can't change its name. Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n• (Optional) Under General configuration, you can choose to copy an existing bucket's settings to your new bucket. If you don't want to copy the settings of an existing bucket, skip to the next step. NoteThis option: Isn't available in the AWS CLI and is only available in the Amazon S3 consoleDoesn't copy the bucket policy from the existing bucket to the new bucket To copy an existing bucket's settings, under Copy settings from existing bucket, select Choose bucket. The Choose bucket window opens. Find the bucket with the settings that you want to copy, and select Choose bucket. The Choose bucket window closes, and the Create bucket window reopens. Under Copy settings from existing bucket, you now see the name of the bucket that you selected. The settings of your new bucket now match the settings of the bucket that you selected. If you want to remove the copied settings, choose Restore defaults. Review the remaining bucket settings on the Create bucket page. If you don't want to make any changes, you can skip to the final step.\n• Under Object Ownership, to disable or enable ACLs and control ownership of objects uploaded in your bucket, choose one of the following settings: ACLs disabled Bucket owner enforced (default) â ACLs are disabled, and the bucket owner automatically owns and has full control over every object in the general purpose bucket. ACLs no longer affect access permissions to data in the S3 general purpose bucket. The bucket uses policies exclusively to define access control. By default, ACLs are disabled. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you must control access for each object individually. For more information, see Controlling ownership of objects and disabling ACLs for your bucket. ACLs enabled Bucket owner preferred â The bucket owner owns and has full control over new objects that other accounts write to the bucket with the bucket-owner-full-control canned ACL. If you apply the Bucket owner preferred setting, to require all Amazon S3 uploads to include the bucket-owner-full-control canned ACL, you can add a bucket policy that allows only object uploads that use this ACL. Object writer â The AWS account that uploads an object owns the object, has full control over it, and can grant other users access to it through ACLs. NoteThe default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n• Under Block Public Access settings for this bucket, choose the Block Public Access settings that you want to apply to the bucket. By default, all four Block Public Access settings are enabled. We recommend that you keep all settings enabled, unless you know that you need to turn off one or more of them for your specific use case. For more information about blocking public access, see Blocking public access to your Amazon S3 storage. NoteTo enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n• (Optional) By default, Bucket Versioning is disabled. Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your bucket. With versioning, you can recover more easily from both unintended user actions and application failures. For more information about versioning, see Retaining multiple versions of objects with S3 Versioning. To enable versioning on your bucket, choose Enable.\n• (Optional) Under Tags, you can choose to add tags to your bucket. With AWS cost allocation, you can use bucket tags to annotate billing for your use of a bucket. A tag is a key-value pair that represents a label that you assign to a bucket. For more information, see Using cost allocation S3 bucket tags. To add a bucket tag, enter a Key and optionally a Value and choose Add Tag.\n• To configure Default encryption, under Encryption type, choose one of the following: Server-side encryption with Amazon S3 managed keys (SSE-S3) Server-side encryption with AWS Key Management Service keys (SSE-KMS) Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS) ImportantIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide. Buckets and new objects are encrypted by using server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption configuration. For more information about default encryption, see Setting default server-side encryption behavior for Amazon S3 buckets. For more information about SSE-S3, see Using server-side encryption with Amazon S3 managed keys (SSE-S3). For more information about using server-side encryption to encrypt your data, see Protecting data with encryption.\n• If you chose Server-side encryption with AWS Key Management Service keys (SSE-KMS) or Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS), do the following: Under AWS KMS key, specify your KMS key in one of the following ways: To choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys. Both the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide. To enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears. To create a new customer managed key in the AWS KMS console, choose Create a KMS key. For more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide. ImportantYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide. When you configure your bucket to use default encryption with SSE-KMS, you can also use S3 Bucket Keys. S3 Bucket Keys lower the cost of encryption by decreasing request traffic from Amazon S3 to AWS KMS. For more information, see Reducing the cost of SSE-KMS with Amazon S3 Bucket Keys. S3 Bucket Keys aren't supported for DSSE-KMS. By default, S3 Bucket Keys are enabled in the Amazon S3 console. We recommend leaving S3 Bucket Keys enabled to lower your costs. To disable S3 Bucket Keys for your bucket, under Bucket Key, choose Disable.\n• (Optional) S3 Object Lock helps protect new objects from being deleted or overwritten. For more information, see Locking objects with Object Lock. If you want to enable S3 Object Lock, do the following: Choose Advanced settings. ImportantEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab. If you want to enable Object Lock, choose Enable, read the warning that appears, and acknowledge it. NoteTo create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.\n• Choose Create bucket.\n\n• After you create a bucket, you can't change its Region.\n• To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\n• Be unique within a partition. A partition is a grouping of Regions. AWS currently has three partitions: aws (commercial Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud (US) Regions).\n• Be between 3 and 63 characters long.\n• Consist only of lowercase letters, numbers, periods (.), and hyphens (-). For best compatibility, we recommend that you avoid using periods (.) in bucket names, except for buckets that are used only for static website hosting.\n• Begin and end with a letter or number.\n• For a complete list of bucket-naming rules, see General purpose bucket naming rules.\n\n• After you create the bucket, you can't change its name.\n• Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n• Isn't available in the AWS CLI and is only available in the Amazon S3 console\n• Doesn't copy the bucket policy from the existing bucket to the new bucket\n\n• Bucket owner enforced (default) â ACLs are disabled, and the bucket owner automatically owns and has full control over every object in the general purpose bucket. ACLs no longer affect access permissions to data in the S3 general purpose bucket. The bucket uses policies exclusively to define access control. By default, ACLs are disabled. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you must control access for each object individually. For more information, see Controlling ownership of objects and disabling ACLs for your bucket.\n\n• Bucket owner preferred â The bucket owner owns and has full control over new objects that other accounts write to the bucket with the bucket-owner-full-control canned ACL. If you apply the Bucket owner preferred setting, to require all Amazon S3 uploads to include the bucket-owner-full-control canned ACL, you can add a bucket policy that allows only object uploads that use this ACL.\n• Object writer â The AWS account that uploads an object owns the object, has full control over it, and can grant other users access to it through ACLs.\n\n• Server-side encryption with Amazon S3 managed keys (SSE-S3)\n• Server-side encryption with AWS Key Management Service keys (SSE-KMS)\n• Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS) ImportantIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\n• Under AWS KMS key, specify your KMS key in one of the following ways: To choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys. Both the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide. To enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears. To create a new customer managed key in the AWS KMS console, choose Create a KMS key. For more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide. ImportantYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n• When you configure your bucket to use default encryption with SSE-KMS, you can also use S3 Bucket Keys. S3 Bucket Keys lower the cost of encryption by decreasing request traffic from Amazon S3 to AWS KMS. For more information, see Reducing the cost of SSE-KMS with Amazon S3 Bucket Keys. S3 Bucket Keys aren't supported for DSSE-KMS. By default, S3 Bucket Keys are enabled in the Amazon S3 console. We recommend leaving S3 Bucket Keys enabled to lower your costs. To disable S3 Bucket Keys for your bucket, under Bucket Key, choose Disable.\n\n• To choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys. Both the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide.\n• To enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears.\n• To create a new customer managed key in the AWS KMS console, choose Create a KMS key. For more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide.\n\n• Choose Advanced settings. ImportantEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n• If you want to enable Object Lock, choose Enable, read the warning that appears, and acknowledge it.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] NoteYou are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket. The charges that you incur through following the examples in this guide are minimal (less than $1). For more information about storage charges, see Amazon S3 pricing.\n\n[Note] You are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket. The charges that you incur through following the examples in this guide are minimal (less than $1). For more information about storage charges, see Amazon S3 pricing.\n\n[Note] Note After you create a bucket, you can't change its Region. To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\n[Note] After you create a bucket, you can't change its Region. To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\n[Note] Important After you create the bucket, you can't change its name. Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n[Note] After you create the bucket, you can't change its name. Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n[Note] NoteThis option: Isn't available in the AWS CLI and is only available in the Amazon S3 consoleDoesn't copy the bucket policy from the existing bucket to the new bucket\n\n[Note] This option: Isn't available in the AWS CLI and is only available in the Amazon S3 consoleDoesn't copy the bucket policy from the existing bucket to the new bucket\n\n[Note] NoteThe default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n\n[Note] The default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n\n[Note] NoteTo enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n\n[Note] To enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n\n[Note] ImportantIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\n[Note] If you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\n[Note] ImportantYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n\n[Note] You can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n\n[Note] ImportantEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n\n[Note] Enabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n\n[Note] NoteTo create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.\n\n[Note] To create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 63,
          "content_length": 34865
        }
      },
      {
        "header": "Step 2: Upload an object to your bucket",
        "content": "After creating a bucket in Amazon S3, you're ready to upload an object to the bucket. An object can be any kind of file: a text file, a photo, a video, and so on.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nOpen the Amazon S3 console at https://console.aws.amazon.com/s3/.\n\nIn the Buckets list, choose the name of the bucket that you want to upload your object to.\n\nOn the Objects tab for your bucket, choose Upload.\n\nUnder Files and folders, choose Add files.\n\nChoose a file to upload, and then choose Open.\n\nYou've successfully uploaded an object to your bucket.\n\nTo view your object, see Step 3: Download an object.\n\n• Open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n• In the Buckets list, choose the name of the bucket that you want to upload your object to.\n• On the Objects tab for your bucket, choose Upload.\n• Under Files and folders, choose Add files.\n• Choose a file to upload, and then choose Open.\n• Choose Upload.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1408
        }
      },
      {
        "header": "Step 3: Download an object",
        "content": "After you upload an object to a bucket, you can view information about your object and download the object to your local computer.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 637
        }
      },
      {
        "header": "Using the S3 console",
        "content": "This section explains how to use the Amazon S3 console to download an object from an S3 bucket.\n\nYou can download only one object at a time.\n\nIf you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.\n\nSign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n\nIn the left navigation pane, choose General purpose buckets or Directory buckets.\n\nIn the buckets list, choose the name of the bucket that you want to download an object from.\n\nYou can download an object from an S3 bucket in any of the following ways:\n\nSelect the check box next to the object, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\nIf you want to download a specific version of the object, turn on Show versions (located next to the search box). Select the check box next to the version of the object that you want, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\nYou've successfully downloaded your object.\n\nTo copy and paste your object within Amazon S3, see Step 4: Copy your object to a folder.\n\n• You can download only one object at a time.\n• If you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.\n\n• Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n• In the left navigation pane, choose General purpose buckets or Directory buckets.\n• In the buckets list, choose the name of the bucket that you want to download an object from.\n• You can download an object from an S3 bucket in any of the following ways: Select the check box next to the object, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as. If you want to download a specific version of the object, turn on Show versions (located next to the search box). Select the check box next to the version of the object that you want, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\n• Select the check box next to the object, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n• If you want to download a specific version of the object, turn on Show versions (located next to the search box). Select the check box next to the version of the object that you want, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\n[Note] Note You can download only one object at a time. If you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.\n\n[Note] You can download only one object at a time. If you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 3832
        }
      },
      {
        "header": "Step 4: Copy your object to a folder",
        "content": "You've already added an object to a bucket and downloaded the object. Now, you create a folder and copy the object and paste it into the folder.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nIn the Buckets list, choose your bucket name.\n\nChoose Create folder and configure a new folder:\n\nEnter a folder name (for example, favorite-pics).\n\nFor the folder encryption setting, choose Disable.\n\nNavigate to the Amazon S3 bucket or folder that contains the objects that you want to copy.\n\nSelect the check box to the left of the names of the objects that you want to copy.\n\nChoose Actions and choose Copy from the list of options that appears.\n\nAlternatively, choose Copy from the options in the upper right.\n\nChoose the destination folder:\n\nChoose the option button to the left of the folder name.\n\nTo navigate into a folder and choose a subfolder as your destination, choose the folder name.\n\nChoose Choose destination.\n\nThe path to your destination folder appears in the Destination box. In Destination, you can alternately enter your destination path, for example, s3://bucket-name/folder-name/.\n\nIn the bottom right, choose Copy.\n\nAmazon S3 copies your objects to the destination folder.\n\nTo delete an object and a bucket in Amazon S3, see Step 5: Delete your objects and bucket.\n\n• In the Buckets list, choose your bucket name.\n• Choose Create folder and configure a new folder: Enter a folder name (for example, favorite-pics). For the folder encryption setting, choose Disable. Choose Save.\n• Navigate to the Amazon S3 bucket or folder that contains the objects that you want to copy.\n• Select the check box to the left of the names of the objects that you want to copy.\n• Choose Actions and choose Copy from the list of options that appears. Alternatively, choose Copy from the options in the upper right.\n• Choose the destination folder: Choose Browse S3. Choose the option button to the left of the folder name. To navigate into a folder and choose a subfolder as your destination, choose the folder name. Choose Choose destination. The path to your destination folder appears in the Destination box. In Destination, you can alternately enter your destination path, for example, s3://bucket-name/folder-name/.\n• In the bottom right, choose Copy. Amazon S3 copies your objects to the destination folder.\n\n• Enter a folder name (for example, favorite-pics).\n• For the folder encryption setting, choose Disable.\n• Choose Save.\n\n• Choose Browse S3.\n• Choose the option button to the left of the folder name. To navigate into a folder and choose a subfolder as your destination, choose the folder name.\n• Choose Choose destination.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 3094
        }
      },
      {
        "header": "Step 5: Delete your objects and bucket",
        "content": "When you no longer need an object or a bucket, we recommend that you delete them to prevent further charges. If you completed this getting started walkthrough as a learning exercise, and you don't plan to use your bucket or objects, we recommend that you delete your bucket and objects so that charges no longer accrue.\n\nBefore you delete your bucket, empty the bucket or delete the objects in the bucket. After you delete your objects and bucket, they are no longer available.\n\nIf you want to continue to use the same bucket name, we recommend that you delete the objects or empty the bucket, but don't delete the bucket. After you delete a bucket, the name becomes available to reuse. However, another AWS account might create a bucket with the same name before you have a chance to reuse it.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nEmptying your bucket\n\nDeleting your bucket\n\n• Deleting an object\n• Emptying your bucket\n• Deleting your bucket\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1413
        }
      },
      {
        "header": "Deleting an object",
        "content": "If you want to choose which objects you delete without emptying all the objects from your bucket, you can delete an object.\n\nIn the Buckets list, choose the name of the bucket that you want to delete an object from.\n\nSelect the object that you want to delete.\n\nChoose Delete from the options in the upper right.\n\nOn the Delete objects page, type delete to confirm deletion of your objects.\n\nChoose Delete objects.\n\n• In the Buckets list, choose the name of the bucket that you want to delete an object from.\n• Select the object that you want to delete.\n• Choose Delete from the options in the upper right.\n• On the Delete objects page, type delete to confirm deletion of your objects.\n• Choose Delete objects.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 709
        }
      },
      {
        "header": "Emptying your bucket",
        "content": "If you plan to delete your bucket, you must first empty your bucket, which deletes all the objects in the bucket.\n\nIn the Buckets list, select the bucket that you want to empty, and then choose Empty.\n\nTo confirm that you want to empty the bucket and delete all the objects in it, in Empty bucket, type permanently delete.\n\nEmptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.\n\nTo empty the bucket and delete all the objects in it, and choose Empty.\n\nAn Empty bucket: Status page opens that you can use to review a summary of failed and successful object deletions.\n\nTo return to your bucket list, choose Exit.\n\n• In the Buckets list, select the bucket that you want to empty, and then choose Empty.\n• To confirm that you want to empty the bucket and delete all the objects in it, in Empty bucket, type permanently delete. ImportantEmptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.\n• To empty the bucket and delete all the objects in it, and choose Empty. An Empty bucket: Status page opens that you can use to review a summary of failed and successful object deletions.\n• To return to your bucket list, choose Exit.\n\n[Note] ImportantEmptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.\n\n[Note] Emptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1550
        }
      },
      {
        "header": "Deleting your bucket",
        "content": "After you empty your bucket or delete all the objects from your bucket, you can delete your bucket.\n\nTo delete a bucket, in the Buckets list, select the bucket.\n\nTo confirm deletion, in Delete bucket, type the name of the bucket.\n\nDeleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.\n\nTo delete your bucket, choose Delete bucket.\n\n• To delete a bucket, in the Buckets list, select the bucket.\n• Choose Delete.\n• To confirm deletion, in Delete bucket, type the name of the bucket. ImportantDeleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.\n• To delete your bucket, choose Delete bucket.\n\n[Note] ImportantDeleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.\n\n[Note] Deleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1459
        }
      },
      {
        "header": "Next steps",
        "content": "In the preceding examples, you learned how to perform some basic Amazon S3 tasks.\n\nThe following topics explain the learning paths that you can use to gain a deeper understanding of Amazon S3 so that you can implement it in your applications.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nUnderstand common use cases\n\nControl access to your buckets and objects\n\nProtect and monitor your storage\n\nDevelop with Amazon S3\n\nLearn from tutorials\n\nExplore training and support\n\n• Understand common use cases\n• Control access to your buckets and objects\n• Protect and monitor your storage\n• Develop with Amazon S3\n• Learn from tutorials\n• Explore training and support\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1122
        }
      },
      {
        "header": "Understand common use cases",
        "content": "You can use Amazon S3 to support your specific use case. The AWS Solutions Library and AWS Blog provide use-case specific information and tutorials. The following are some common use cases for Amazon S3:\n\nBackup and storage â Use Amazon S3 storage management features to manage costs, meet regulatory requirements, reduce latency, and save multiple distinct copies of your data for compliance requirements.\n\nApplication hosting â Deploy, install, and manage web applications that are reliable, highly scalable, and low-cost. For example, you can configure your Amazon S3 bucket to host a static website. For more information, see Hosting a static website using Amazon S3.\n\nMedia hosting â Build a highly available infrastructure that hosts video, photo, or music uploads and downloads.\n\nSoftware delivery â Host your software applications for customers to download.\n\n• Backup and storage â Use Amazon S3 storage management features to manage costs, meet regulatory requirements, reduce latency, and save multiple distinct copies of your data for compliance requirements.\n• Application hosting â Deploy, install, and manage web applications that are reliable, highly scalable, and low-cost. For example, you can configure your Amazon S3 bucket to host a static website. For more information, see Hosting a static website using Amazon S3.\n• Media hosting â Build a highly available infrastructure that hosts video, photo, or music uploads and downloads.\n• Software delivery â Host your software applications for customers to download.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1548
        }
      },
      {
        "header": "Control access to your buckets and objects",
        "content": "Amazon S3 provides a variety of security features and tools. For an overview, see Access control in Amazon S3.\n\nBy default, S3 buckets and the objects in them are private. You have access only to the S3 resources that you create. You can use the following features to grant granular resource permissions that support your specific use case or to audit the permissions of your Amazon S3 resources.\n\nS3 Block Public Access â Block public access to S3 buckets and objects. By default, Block Public Access settings are turned on at the bucket level.\n\nAWS Identity and Access Management (IAM) identities â Use IAM or AWS IAM Identity Center to create IAM identities in your AWS account to manage access to your Amazon S3 resources. For example, you can use IAM with Amazon S3 to control the type of access that a user or group of users has to an Amazon S3 bucket that your AWS account owns. For more information about IAM identities and best practices, see IAM identities (users, user groups, and roles) in the IAM User Guide.\n\nBucket policies â Use IAM-based policy language to configure resource-based permissions for your S3 buckets and the objects in them.\n\nAccess control lists (ACLs) â Grant read and write permissions for individual buckets and objects to authorized users. As a general rule, we recommend using S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access-control option. With bucket policies and access point policies, you can define rules that apply broadly across all requests to your Amazon S3 resources. For more information about the specific cases when you'd use ACLs instead of resource-based policies or IAM user policies, see Identity and Access Management for Amazon S3.\n\nS3 Object Ownership â Take ownership of every object in your bucket, simplifying access management for data stored in Amazon S3. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to disable or enable ACLs. By default, ACLs are disabled. With ACLs disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies.\n\nIAM Access Analyzer for S3 â Evaluate and monitor your S3 bucket access policies, ensuring that the policies provide only the intended access to your S3 resources.\n\n• S3 Block Public Access â Block public access to S3 buckets and objects. By default, Block Public Access settings are turned on at the bucket level.\n• AWS Identity and Access Management (IAM) identities â Use IAM or AWS IAM Identity Center to create IAM identities in your AWS account to manage access to your Amazon S3 resources. For example, you can use IAM with Amazon S3 to control the type of access that a user or group of users has to an Amazon S3 bucket that your AWS account owns. For more information about IAM identities and best practices, see IAM identities (users, user groups, and roles) in the IAM User Guide.\n• Bucket policies â Use IAM-based policy language to configure resource-based permissions for your S3 buckets and the objects in them.\n• Access control lists (ACLs) â Grant read and write permissions for individual buckets and objects to authorized users. As a general rule, we recommend using S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access-control option. With bucket policies and access point policies, you can define rules that apply broadly across all requests to your Amazon S3 resources. For more information about the specific cases when you'd use ACLs instead of resource-based policies or IAM user policies, see Identity and Access Management for Amazon S3.\n• S3 Object Ownership â Take ownership of every object in your bucket, simplifying access management for data stored in Amazon S3. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to disable or enable ACLs. By default, ACLs are disabled. With ACLs disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies.\n• IAM Access Analyzer for S3 â Evaluate and monitor your S3 bucket access policies, ensuring that the policies provide only the intended access to your S3 resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 4421
        }
      },
      {
        "header": "Protect and monitor your storage",
        "content": "Protecting your storage â After you create buckets and upload objects in Amazon S3, you can protect your object storage. For example, you can use S3 Versioning, S3 Replication, and Multi-Region Access Point failover controls for disaster recovery, AWS Backup to back up your data, and S3 Object Lock to set retention periods, prevent deletions and overwrites, and meet compliance requirements.\n\nMonitoring your storage â Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions. You can monitor storage activity and costs. Also, we recommend that you collect monitoring data from all the parts of your AWS solution so that you can more easily debug a multipoint failure if one occurs.\n\nYou can also use analytics and insights in Amazon S3 to understand, analyze, and optimize your storage usage. For example, use Amazon S3 Storage Lens to understand, analyze, and optimize your storage. S3 Storage Lens provides 29+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, Regions, buckets, or prefixes. Use Storage Class Analysis to analyze storage access patterns to decide when it's time to move your data to a more cost-effective storage class. To manage your costs, you can use S3 Lifecycle.\n\n• Protecting your storage â After you create buckets and upload objects in Amazon S3, you can protect your object storage. For example, you can use S3 Versioning, S3 Replication, and Multi-Region Access Point failover controls for disaster recovery, AWS Backup to back up your data, and S3 Object Lock to set retention periods, prevent deletions and overwrites, and meet compliance requirements.\n• Monitoring your storage â Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions. You can monitor storage activity and costs. Also, we recommend that you collect monitoring data from all the parts of your AWS solution so that you can more easily debug a multipoint failure if one occurs. You can also use analytics and insights in Amazon S3 to understand, analyze, and optimize your storage usage. For example, use Amazon S3 Storage Lens to understand, analyze, and optimize your storage. S3 Storage Lens provides 29+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, Regions, buckets, or prefixes. Use Storage Class Analysis to analyze storage access patterns to decide when it's time to move your data to a more cost-effective storage class. To manage your costs, you can use S3 Lifecycle.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 2678
        }
      },
      {
        "header": "Develop with Amazon S3",
        "content": "Amazon S3 is a REST service. You can send requests to Amazon S3 using the REST API or the AWS SDK libraries, which wrap the underlying Amazon S3 REST API, simplifying your programming tasks. You can also use the AWS Command Line Interface (AWS CLI) to make Amazon S3 API calls. For more information, see Making requests in the Amazon S3 API Reference.\n\nThe Amazon S3 REST API is an HTTP interface to Amazon S3. With the REST API, you use standard HTTP requests to create, fetch, and delete buckets and objects. To use the REST API, you can use any toolkit that supports HTTP. You can even use a browser to fetch objects, as long as they are anonymously readable. For more information, see Developing with Amazon S3 in the Amazon S3 API Reference.\n\nTo help you build applications using the language of your choice, we provide the following resources.\n\nYou can access the features of Amazon S3 using the AWS CLI. To download and configure the AWS CLI, see Developing with Amazon S3 using the AWS CLI in the Amazon S3 API Reference.\n\nThe AWS CLI provides two tiers of commands for accessing Amazon S3: High-level (s3) commands and API-level (s3api and s3control commands. The high-level S3 commands simplify performing common tasks, such as creating, manipulating, and deleting objects and buckets. The s3api and s3control commands expose direct access to all Amazon S3 API operations, which you can use to carry out advanced operations that might not be possible with the high-level commands alone.\n\nFor a list of Amazon S3 AWS CLI commands, see s3, s3api, and s3control.\n\nYou can use the AWS SDKs when developing applications with Amazon S3. The AWS SDKs simplify your programming tasks by wrapping the underlying REST API. The AWS Mobile SDKs and the Amplify JavaScript library are also available for building connected mobile and web applications using AWS.\n\nIn addition to the AWS SDKs, AWS Explorers are available for Visual Studio and Eclipse for Java IDE. In this case, the SDKs and the explorers are bundled together as AWS Toolkits.\n\nFor more information, see Developing with Amazon S3 using the AWS SDKs in the Amazon S3 API Reference.\n\nThe AWS Developer Center and AWS Code Sample Catalog have sample code and libraries written especially for Amazon S3. You can use these code samples to understand how to implement the Amazon S3 API. You can also view the Amazon Simple Storage Service API Reference to understand the Amazon S3 API operations in detail.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2463
        }
      },
      {
        "header": "Learn from tutorials",
        "content": "You can get started with step-by-step tutorials to learn more about Amazon S3. These tutorials are intended for a lab-type environment, and they use fictitious company names, user names, and so on. Their purpose is to provide general guidance. They are not intended for direct use in a production environment without careful review and adaptation to meet the unique needs of your organization's environment.\n\nTutorial: Storing and retrieving a file with Amazon S3\n\nTutorial: Getting started using S3 Intelligent-Tiering\n\nTutorial: Getting started using the S3 Glacier storage classes\n\nTutorial: Getting started using S3 Intelligent-Tiering\n\nTutorial: Getting started using the S3 Glacier; storage classes\n\nTutorial: Optimizing costs and gaining visibility into usage with S3 Storage Lens\n\nTutorial: Getting started with Amazon S3 Multi-Region Access Points\n\nTutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication\n\nTutorial: Hosting on-demand streaming video with Amazon S3, Amazon CloudFront, and Amazon RouteÂ 53\n\nTutorial: Configuring a static website on Amazon S3\n\nTutorial: Configuring a static website using a custom domain registered with RouteÂ 53\n\nTutorial: Transforming data for your application with S3 Object Lambda\n\nTutorial: Detecting and redacting PII data with S3 Object Lambda and Amazon Comprehend\n\nTutorial: Using S3 Object Lambda to dynamically watermark images as they are retrieved\n\nTutorial: Batch-transcoding videos with S3 Batch Operations\n\nTutorial: Checking the integrity of data in Amazon S3 with additional checksums\n\nTutorial: Replicating data within and between AWS Regions using S3 Replication\n\nTutorial: Protecting data on Amazon S3 against accidental deletion or application bugs using S3 Versioning, S3 Object Lock, and S3 Replication\n\nTutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication\n\n• Tutorial: Storing and retrieving a file with Amazon S3\n• Tutorial: Getting started using S3 Intelligent-Tiering\n• Tutorial: Getting started using the S3 Glacier storage classes\n\n• Tutorial: Getting started using S3 Intelligent-Tiering\n• Tutorial: Getting started using the S3 Glacier; storage classes\n• Tutorial: Optimizing costs and gaining visibility into usage with S3 Storage Lens\n\n• Tutorial: Getting started with Amazon S3 Multi-Region Access Points\n• Tutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication\n\n• Tutorial: Hosting on-demand streaming video with Amazon S3, Amazon CloudFront, and Amazon RouteÂ 53\n• Tutorial: Configuring a static website on Amazon S3\n• Tutorial: Configuring a static website using a custom domain registered with RouteÂ 53\n\n• Tutorial: Transforming data for your application with S3 Object Lambda\n• Tutorial: Detecting and redacting PII data with S3 Object Lambda and Amazon Comprehend\n• Tutorial: Using S3 Object Lambda to dynamically watermark images as they are retrieved\n• Tutorial: Batch-transcoding videos with S3 Batch Operations\n\n• Tutorial: Checking the integrity of data in Amazon S3 with additional checksums\n• Tutorial: Replicating data within and between AWS Regions using S3 Replication\n• Tutorial: Protecting data on Amazon S3 against accidental deletion or application bugs using S3 Versioning, S3 Object Lock, and S3 Replication\n• Tutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 3410
        }
      },
      {
        "header": "Explore training and support",
        "content": "You can learn from AWS experts to advance your skills and get expert assistance achieving your objectives.\n\nTraining â Training resources provide a hands-on approach to learning Amazon S3. For more information, see AWS training and certification and AWS online tech talks.\n\nDiscussion Forums â On the forum, you can review posts to understand what you can and can't do with Amazon S3. You can also post your questions. For more information, see Discussion Forums.\n\nTechnical Support â If you have further questions, you can contact Technical Support.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• Training â Training resources provide a hands-on approach to learning Amazon S3. For more information, see AWS training and certification and AWS online tech talks.\n• Discussion Forums â On the forum, you can review posts to understand what you can and can't do with Amazon S3. You can also post your questions. For more information, see Discussion Forums.\n• Technical Support â If you have further questions, you can contact Technical Support.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1306
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/GetStartedWithS3.html",
    "doc_type": "aws",
    "total_sections": 20
  },
  {
    "title": "Getting started with Amazon S3",
    "summary": "Setting upStep 1: Create a bucketStep 2: Upload an objectStep 3: Download an objectStep 4: Copy an objectStep 5: Delete the objects and bucketNext steps\n\nGetting started with Amazon S3You can get started with Amazon S3 by working with buckets and objects. A bucket is a container for objects. An object is a file and any metadata that describes that file.To store an object in Amazon S3, you create a bucket and then upload the object to the bucket. When the object is in the bucket, you can open it,",
    "sections": [
      {
        "header": "",
        "content": "You can get started with Amazon S3 by working with buckets and objects. A bucket is a container for objects. An object is a file and any metadata that describes that file.\n\nTo store an object in Amazon S3, you create a bucket and then upload the object to the bucket. When the object is in the bucket, you can open it, download it, and move it. When you no longer need an object or a bucket, you can clean up your resources.\n\nWith Amazon S3, you pay only for what you use. For more information about Amazon S3 features and pricing, see Amazon S3. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For more information, see AWS Free Tier.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nThe following video shows you how to get started with Amazon S3.\n\nBefore you begin, confirm that you've completed the steps in Setting up Amazon S3.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1326
        }
      },
      {
        "header": "Setting up Amazon S3",
        "content": "When you sign up for AWS, your AWS account is automatically signed up for all services in AWS, including Amazon S3. You are charged only for the services that you use.\n\nWith Amazon S3, you pay only for what you use. For more information about Amazon S3 features and pricing, see Amazon S3. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For more information, see AWS Free Tier.\n\nTo set up Amazon S3, use the steps in the following sections.\n\nWhen you sign up for AWS and set up Amazon S3, you can optionally change the display language in the AWS Management Console. For more information, see Changing the language of the AWS Management Console in the AWS Management Console Getting Started Guide.\n\nSign up for an AWS account\n\nCreate a user with administrative access\n\n• Sign up for an AWS account\n• Create a user with administrative access",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 875
        }
      },
      {
        "header": "Sign up for an AWS account",
        "content": "If you do not have an AWS account, complete the following steps to create one.\n\nOpen https://portal.aws.amazon.com/billing/signup.\n\nFollow the online instructions.\n\nPart of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad.\n\nWhen you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access.\n\nAWS sends you a confirmation email after the sign-up process is complete. At any time, you can view your current account activity and manage your account by going to https://aws.amazon.com/ and choosing My Account.\n\n• Open https://portal.aws.amazon.com/billing/signup.\n• Follow the online instructions. Part of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad. When you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1319
        }
      },
      {
        "header": "Create a user with administrative access",
        "content": "After you sign up for an AWS account, secure your AWS account root user, enable AWS IAM Identity Center, and create an administrative user so that you don't use the root user for everyday tasks.\n\nSign in to the AWS Management Console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password.\n\nFor help signing in by using root user, see Signing in as the root user in the AWS Sign-In User Guide.\n\nTurn on multi-factor authentication (MFA) for your root user.\n\nFor instructions, see Enable a virtual MFA device for your AWS account root user (console) in the IAM User Guide.\n\nEnable IAM Identity Center.\n\nFor instructions, see Enabling AWS IAM Identity Center in the AWS IAM Identity Center User Guide.\n\nIn IAM Identity Center, grant administrative access to a user.\n\nFor a tutorial about using the IAM Identity Center directory as your identity source, see Configure user access with the default IAM Identity Center directory in the AWS IAM Identity Center User Guide.\n\nTo sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user.\n\nFor help signing in using an IAM Identity Center user, see Signing in to the AWS access portal in the AWS Sign-In User Guide.\n\nIn IAM Identity Center, create a permission set that follows the best practice of applying least-privilege permissions.\n\nFor instructions, see Create a permission set in the AWS IAM Identity Center User Guide.\n\nAssign users to a group, and then assign single sign-on access to the group.\n\nFor instructions, see Add groups in the AWS IAM Identity Center User Guide.\n\n• Sign in to the AWS Management Console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password. For help signing in by using root user, see Signing in as the root user in the AWS Sign-In User Guide.\n• Turn on multi-factor authentication (MFA) for your root user. For instructions, see Enable a virtual MFA device for your AWS account root user (console) in the IAM User Guide.\n\n• Enable IAM Identity Center. For instructions, see Enabling AWS IAM Identity Center in the AWS IAM Identity Center User Guide.\n• In IAM Identity Center, grant administrative access to a user. For a tutorial about using the IAM Identity Center directory as your identity source, see Configure user access with the default IAM Identity Center directory in the AWS IAM Identity Center User Guide.\n\n• To sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user. For help signing in using an IAM Identity Center user, see Signing in to the AWS access portal in the AWS Sign-In User Guide.\n\n• In IAM Identity Center, create a permission set that follows the best practice of applying least-privilege permissions. For instructions, see Create a permission set in the AWS IAM Identity Center User Guide.\n• Assign users to a group, and then assign single sign-on access to the group. For instructions, see Add groups in the AWS IAM Identity Center User Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 3172
        }
      },
      {
        "header": "Step 1: Create your first S3 bucket",
        "content": "After you sign up for AWS, you're ready to create a bucket in Amazon S3 using the AWS Management Console. Every object in Amazon S3 is stored in a bucket. Before you can store data in Amazon S3, you must create a bucket.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nYou are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket. The charges that you incur through following the examples in this guide are minimal (less than $1). For more information about storage charges, see Amazon S3 pricing.\n\nSign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n\nIn the navigation bar on the top of the page, choose the name of the currently displayed AWS Region. Next, choose the Region in which you want to create a bucket.\n\nAfter you create a bucket, you can't change its Region.\n\nTo minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\nIn the left navigation pane, choose General purpose buckets.\n\nChoose Create bucket. The Create bucket page opens.\n\nFor Bucket name, enter a name for your bucket.\n\nThe bucket name must:\n\nBe unique within a partition. A partition is a grouping of Regions. AWS currently has three partitions: aws (commercial Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud (US) Regions).\n\nBe between 3 and 63 characters long.\n\nConsist only of lowercase letters, numbers, periods (.), and hyphens (-). For best compatibility, we recommend that you avoid using periods (.) in bucket names, except for buckets that are used only for static website hosting.\n\nBegin and end with a letter or number.\n\nFor a complete list of bucket-naming rules, see General purpose bucket naming rules.\n\nAfter you create the bucket, you can't change its name.\n\nDon't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n(Optional) Under General configuration, you can choose to copy an existing bucket's settings to your new bucket. If you don't want to copy the settings of an existing bucket, skip to the next step.\n\nIsn't available in the AWS CLI and is only available in the Amazon S3 console\n\nDoesn't copy the bucket policy from the existing bucket to the new bucket\n\nTo copy an existing bucket's settings, under Copy settings from existing bucket, select Choose bucket. The Choose bucket window opens. Find the bucket with the settings that you want to copy, and select Choose bucket. The Choose bucket window closes, and the Create bucket window reopens.\n\nUnder Copy settings from existing bucket, you now see the name of the bucket that you selected. The settings of your new bucket now match the settings of the bucket that you selected. If you want to remove the copied settings, choose Restore defaults. Review the remaining bucket settings on the Create bucket page. If you don't want to make any changes, you can skip to the final step.\n\nUnder Object Ownership, to disable or enable ACLs and control ownership of objects uploaded in your bucket, choose one of the following settings:\n\nBucket owner enforced (default) â ACLs are disabled, and the bucket owner automatically owns and has full control over every object in the general purpose bucket. ACLs no longer affect access permissions to data in the S3 general purpose bucket. The bucket uses policies exclusively to define access control.\n\nBy default, ACLs are disabled. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you must control access for each object individually. For more information, see Controlling ownership of objects and disabling ACLs for your bucket.\n\nBucket owner preferred â The bucket owner owns and has full control over new objects that other accounts write to the bucket with the bucket-owner-full-control canned ACL.\n\nIf you apply the Bucket owner preferred setting, to require all Amazon S3 uploads to include the bucket-owner-full-control canned ACL, you can add a bucket policy that allows only object uploads that use this ACL.\n\nObject writer â The AWS account that uploads an object owns the object, has full control over it, and can grant other users access to it through ACLs.\n\nThe default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n\nUnder Block Public Access settings for this bucket, choose the Block Public Access settings that you want to apply to the bucket.\n\nBy default, all four Block Public Access settings are enabled. We recommend that you keep all settings enabled, unless you know that you need to turn off one or more of them for your specific use case. For more information about blocking public access, see Blocking public access to your Amazon S3 storage.\n\nTo enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n\n(Optional) By default, Bucket Versioning is disabled. Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your bucket. With versioning, you can recover more easily from both unintended user actions and application failures. For more information about versioning, see Retaining multiple versions of objects with S3 Versioning.\n\nTo enable versioning on your bucket, choose Enable.\n\n(Optional) Under Tags, you can choose to add tags to your bucket. With AWS cost allocation, you can use bucket tags to annotate billing for your use of a bucket. A tag is a key-value pair that represents a label that you assign to a bucket. For more information, see Using cost allocation S3 bucket tags.\n\nTo add a bucket tag, enter a Key and optionally a Value and choose Add Tag.\n\nTo configure Default encryption, under Encryption type, choose one of the following:\n\nServer-side encryption with Amazon S3 managed keys (SSE-S3)\n\nServer-side encryption with AWS Key Management Service keys (SSE-KMS)\n\nDual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS)\n\nIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\nBuckets and new objects are encrypted by using server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption configuration. For more information about default encryption, see Setting default server-side encryption behavior for Amazon S3 buckets. For more information about SSE-S3, see Using server-side encryption with Amazon S3 managed keys (SSE-S3).\n\nFor more information about using server-side encryption to encrypt your data, see Protecting data with encryption.\n\nIf you chose Server-side encryption with AWS Key Management Service keys (SSE-KMS) or Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS), do the following:\n\nUnder AWS KMS key, specify your KMS key in one of the following ways:\n\nTo choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys.\n\nBoth the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide.\n\nTo enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears.\n\nTo create a new customer managed key in the AWS KMS console, choose Create a KMS key.\n\nFor more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide.\n\nYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).\n\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n\nWhen you configure your bucket to use default encryption with SSE-KMS, you can also use S3 Bucket Keys. S3 Bucket Keys lower the cost of encryption by decreasing request traffic from Amazon S3 to AWS KMS. For more information, see Reducing the cost of SSE-KMS with Amazon S3 Bucket Keys. S3 Bucket Keys aren't supported for DSSE-KMS.\n\nBy default, S3 Bucket Keys are enabled in the Amazon S3 console. We recommend leaving S3 Bucket Keys enabled to lower your costs. To disable S3 Bucket Keys for your bucket, under Bucket Key, choose Disable.\n\n(Optional) S3 Object Lock helps protect new objects from being deleted or overwritten. For more information, see Locking objects with Object Lock. If you want to enable S3 Object Lock, do the following:\n\nChoose Advanced settings.\n\nEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n\nIf you want to enable Object Lock, choose Enable, read the warning that appears, and acknowledge it.\n\nTo create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.\n\nChoose Create bucket.\n\nYou've created a bucket in Amazon S3.\n\nTo add an object to your bucket, see Step 2: Upload an object to your bucket.\n\n• Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n• In the navigation bar on the top of the page, choose the name of the currently displayed AWS Region. Next, choose the Region in which you want to create a bucket. Note After you create a bucket, you can't change its Region. To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n• In the left navigation pane, choose General purpose buckets.\n• Choose Create bucket. The Create bucket page opens.\n• For Bucket name, enter a name for your bucket. The bucket name must: Be unique within a partition. A partition is a grouping of Regions. AWS currently has three partitions: aws (commercial Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud (US) Regions). Be between 3 and 63 characters long. Consist only of lowercase letters, numbers, periods (.), and hyphens (-). For best compatibility, we recommend that you avoid using periods (.) in bucket names, except for buckets that are used only for static website hosting. Begin and end with a letter or number. For a complete list of bucket-naming rules, see General purpose bucket naming rules. Important After you create the bucket, you can't change its name. Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n• (Optional) Under General configuration, you can choose to copy an existing bucket's settings to your new bucket. If you don't want to copy the settings of an existing bucket, skip to the next step. NoteThis option: Isn't available in the AWS CLI and is only available in the Amazon S3 consoleDoesn't copy the bucket policy from the existing bucket to the new bucket To copy an existing bucket's settings, under Copy settings from existing bucket, select Choose bucket. The Choose bucket window opens. Find the bucket with the settings that you want to copy, and select Choose bucket. The Choose bucket window closes, and the Create bucket window reopens. Under Copy settings from existing bucket, you now see the name of the bucket that you selected. The settings of your new bucket now match the settings of the bucket that you selected. If you want to remove the copied settings, choose Restore defaults. Review the remaining bucket settings on the Create bucket page. If you don't want to make any changes, you can skip to the final step.\n• Under Object Ownership, to disable or enable ACLs and control ownership of objects uploaded in your bucket, choose one of the following settings: ACLs disabled Bucket owner enforced (default) â ACLs are disabled, and the bucket owner automatically owns and has full control over every object in the general purpose bucket. ACLs no longer affect access permissions to data in the S3 general purpose bucket. The bucket uses policies exclusively to define access control. By default, ACLs are disabled. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you must control access for each object individually. For more information, see Controlling ownership of objects and disabling ACLs for your bucket. ACLs enabled Bucket owner preferred â The bucket owner owns and has full control over new objects that other accounts write to the bucket with the bucket-owner-full-control canned ACL. If you apply the Bucket owner preferred setting, to require all Amazon S3 uploads to include the bucket-owner-full-control canned ACL, you can add a bucket policy that allows only object uploads that use this ACL. Object writer â The AWS account that uploads an object owns the object, has full control over it, and can grant other users access to it through ACLs. NoteThe default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n• Under Block Public Access settings for this bucket, choose the Block Public Access settings that you want to apply to the bucket. By default, all four Block Public Access settings are enabled. We recommend that you keep all settings enabled, unless you know that you need to turn off one or more of them for your specific use case. For more information about blocking public access, see Blocking public access to your Amazon S3 storage. NoteTo enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n• (Optional) By default, Bucket Versioning is disabled. Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your bucket. With versioning, you can recover more easily from both unintended user actions and application failures. For more information about versioning, see Retaining multiple versions of objects with S3 Versioning. To enable versioning on your bucket, choose Enable.\n• (Optional) Under Tags, you can choose to add tags to your bucket. With AWS cost allocation, you can use bucket tags to annotate billing for your use of a bucket. A tag is a key-value pair that represents a label that you assign to a bucket. For more information, see Using cost allocation S3 bucket tags. To add a bucket tag, enter a Key and optionally a Value and choose Add Tag.\n• To configure Default encryption, under Encryption type, choose one of the following: Server-side encryption with Amazon S3 managed keys (SSE-S3) Server-side encryption with AWS Key Management Service keys (SSE-KMS) Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS) ImportantIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide. Buckets and new objects are encrypted by using server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption configuration. For more information about default encryption, see Setting default server-side encryption behavior for Amazon S3 buckets. For more information about SSE-S3, see Using server-side encryption with Amazon S3 managed keys (SSE-S3). For more information about using server-side encryption to encrypt your data, see Protecting data with encryption.\n• If you chose Server-side encryption with AWS Key Management Service keys (SSE-KMS) or Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS), do the following: Under AWS KMS key, specify your KMS key in one of the following ways: To choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys. Both the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide. To enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears. To create a new customer managed key in the AWS KMS console, choose Create a KMS key. For more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide. ImportantYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide. When you configure your bucket to use default encryption with SSE-KMS, you can also use S3 Bucket Keys. S3 Bucket Keys lower the cost of encryption by decreasing request traffic from Amazon S3 to AWS KMS. For more information, see Reducing the cost of SSE-KMS with Amazon S3 Bucket Keys. S3 Bucket Keys aren't supported for DSSE-KMS. By default, S3 Bucket Keys are enabled in the Amazon S3 console. We recommend leaving S3 Bucket Keys enabled to lower your costs. To disable S3 Bucket Keys for your bucket, under Bucket Key, choose Disable.\n• (Optional) S3 Object Lock helps protect new objects from being deleted or overwritten. For more information, see Locking objects with Object Lock. If you want to enable S3 Object Lock, do the following: Choose Advanced settings. ImportantEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab. If you want to enable Object Lock, choose Enable, read the warning that appears, and acknowledge it. NoteTo create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.\n• Choose Create bucket.\n\n• After you create a bucket, you can't change its Region.\n• To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\n• Be unique within a partition. A partition is a grouping of Regions. AWS currently has three partitions: aws (commercial Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud (US) Regions).\n• Be between 3 and 63 characters long.\n• Consist only of lowercase letters, numbers, periods (.), and hyphens (-). For best compatibility, we recommend that you avoid using periods (.) in bucket names, except for buckets that are used only for static website hosting.\n• Begin and end with a letter or number.\n• For a complete list of bucket-naming rules, see General purpose bucket naming rules.\n\n• After you create the bucket, you can't change its name.\n• Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n• Isn't available in the AWS CLI and is only available in the Amazon S3 console\n• Doesn't copy the bucket policy from the existing bucket to the new bucket\n\n• Bucket owner enforced (default) â ACLs are disabled, and the bucket owner automatically owns and has full control over every object in the general purpose bucket. ACLs no longer affect access permissions to data in the S3 general purpose bucket. The bucket uses policies exclusively to define access control. By default, ACLs are disabled. A majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in circumstances where you must control access for each object individually. For more information, see Controlling ownership of objects and disabling ACLs for your bucket.\n\n• Bucket owner preferred â The bucket owner owns and has full control over new objects that other accounts write to the bucket with the bucket-owner-full-control canned ACL. If you apply the Bucket owner preferred setting, to require all Amazon S3 uploads to include the bucket-owner-full-control canned ACL, you can add a bucket policy that allows only object uploads that use this ACL.\n• Object writer â The AWS account that uploads an object owns the object, has full control over it, and can grant other users access to it through ACLs.\n\n• Server-side encryption with Amazon S3 managed keys (SSE-S3)\n• Server-side encryption with AWS Key Management Service keys (SSE-KMS)\n• Dual-layer server-side encryption with AWS Key Management Service (AWS KMS) keys (DSSE-KMS) ImportantIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\n• Under AWS KMS key, specify your KMS key in one of the following ways: To choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys. Both the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide. To enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears. To create a new customer managed key in the AWS KMS console, choose Create a KMS key. For more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide. ImportantYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n• When you configure your bucket to use default encryption with SSE-KMS, you can also use S3 Bucket Keys. S3 Bucket Keys lower the cost of encryption by decreasing request traffic from Amazon S3 to AWS KMS. For more information, see Reducing the cost of SSE-KMS with Amazon S3 Bucket Keys. S3 Bucket Keys aren't supported for DSSE-KMS. By default, S3 Bucket Keys are enabled in the Amazon S3 console. We recommend leaving S3 Bucket Keys enabled to lower your costs. To disable S3 Bucket Keys for your bucket, under Bucket Key, choose Disable.\n\n• To choose from a list of available KMS keys, choose Choose from your AWS KMS keys, and choose your KMS key from the list of available keys. Both the AWS managed key (aws/s3) and your customer managed keys appear in this list. For more information about customer managed keys, see Customer keys and AWS keys in the AWS Key Management Service Developer Guide.\n• To enter the KMS key ARN, choose Enter AWS KMS key ARN, and enter your KMS key ARN in the field that appears.\n• To create a new customer managed key in the AWS KMS console, choose Create a KMS key. For more information about creating an AWS KMS key, see Creating keys in the AWS Key Management Service Developer Guide.\n\n• Choose Advanced settings. ImportantEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n• If you want to enable Object Lock, choose Enable, read the warning that appears, and acknowledge it.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] NoteYou are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket. The charges that you incur through following the examples in this guide are minimal (less than $1). For more information about storage charges, see Amazon S3 pricing.\n\n[Note] You are not charged for creating a bucket. You are charged only for storing objects in the bucket and for transferring objects in and out of the bucket. The charges that you incur through following the examples in this guide are minimal (less than $1). For more information about storage charges, see Amazon S3 pricing.\n\n[Note] Note After you create a bucket, you can't change its Region. To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\n[Note] After you create a bucket, you can't change its Region. To minimize latency and costs and address regulatory requirements, choose a Region close to you. Objects stored in a Region never leave that Region unless you explicitly transfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS service endpoints in the Amazon Web Services General Reference.\n\n[Note] Important After you create the bucket, you can't change its name. Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n[Note] After you create the bucket, you can't change its name. Don't include sensitive information in the bucket name. The bucket name is visible in the URLs that point to the objects in the bucket.\n\n[Note] NoteThis option: Isn't available in the AWS CLI and is only available in the Amazon S3 consoleDoesn't copy the bucket policy from the existing bucket to the new bucket\n\n[Note] This option: Isn't available in the AWS CLI and is only available in the Amazon S3 consoleDoesn't copy the bucket policy from the existing bucket to the new bucket\n\n[Note] NoteThe default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n\n[Note] The default setting is Bucket owner enforced. To apply the default setting and keep ACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you must have the s3:PutBucketOwnershipControls permission.\n\n[Note] NoteTo enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n\n[Note] To enable all Block Public Access settings, only the s3:CreateBucket permission is required. To turn off any Block Public Access settings, you must have the s3:PutBucketPublicAccessBlock permission.\n\n[Note] ImportantIf you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\n[Note] If you use the SSE-KMS or DSSE-KMS option for your default encryption configuration, you are subject to the requests per second (RPS) quota of AWS KMS. For more information about AWS KMS quotas and how to request a quota increase, see Quotas in the AWS Key Management Service Developer Guide.\n\n[Note] ImportantYou can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n\n[Note] You can use only KMS keys that are available in the same AWS Region as the bucket. The Amazon S3 console lists only the first 100 KMS keys in the same Region as the bucket. To use a KMS key that isn't listed, you must enter your KMS key ARN. If you want to use a KMS key that's owned by a different account, you must first have permission to use the key, and then you must enter the KMS key ARN. For more information about cross account permissions for KMS keys, see Creating KMS keys that other accounts can use in the AWS Key Management Service Developer Guide. For more information about SSE-KMS, see Specifying server-side encryption with AWS KMS (SSE-KMS). For more information about DSSE-KMS, see Using dual-layer server-side encryption with AWS KMS keys (DSSE-KMS).When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys. For more information, see Identifying symmetric and asymmetric KMS keys in the AWS Key Management Service Developer Guide.\n\n[Note] ImportantEnabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n\n[Note] Enabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab.\n\n[Note] NoteTo create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.\n\n[Note] To create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 63,
          "content_length": 34865
        }
      },
      {
        "header": "Step 2: Upload an object to your bucket",
        "content": "After creating a bucket in Amazon S3, you're ready to upload an object to the bucket. An object can be any kind of file: a text file, a photo, a video, and so on.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nOpen the Amazon S3 console at https://console.aws.amazon.com/s3/.\n\nIn the Buckets list, choose the name of the bucket that you want to upload your object to.\n\nOn the Objects tab for your bucket, choose Upload.\n\nUnder Files and folders, choose Add files.\n\nChoose a file to upload, and then choose Open.\n\nYou've successfully uploaded an object to your bucket.\n\nTo view your object, see Step 3: Download an object.\n\n• Open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n• In the Buckets list, choose the name of the bucket that you want to upload your object to.\n• On the Objects tab for your bucket, choose Upload.\n• Under Files and folders, choose Add files.\n• Choose a file to upload, and then choose Open.\n• Choose Upload.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1408
        }
      },
      {
        "header": "Step 3: Download an object",
        "content": "After you upload an object to a bucket, you can view information about your object and download the object to your local computer.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 637
        }
      },
      {
        "header": "Using the S3 console",
        "content": "This section explains how to use the Amazon S3 console to download an object from an S3 bucket.\n\nYou can download only one object at a time.\n\nIf you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.\n\nSign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n\nIn the left navigation pane, choose General purpose buckets or Directory buckets.\n\nIn the buckets list, choose the name of the bucket that you want to download an object from.\n\nYou can download an object from an S3 bucket in any of the following ways:\n\nSelect the check box next to the object, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\nIf you want to download a specific version of the object, turn on Show versions (located next to the search box). Select the check box next to the version of the object that you want, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\nYou've successfully downloaded your object.\n\nTo copy and paste your object within Amazon S3, see Step 4: Copy your object to a folder.\n\n• You can download only one object at a time.\n• If you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.\n\n• Sign in to the AWS Management Console and open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n• In the left navigation pane, choose General purpose buckets or Directory buckets.\n• In the buckets list, choose the name of the bucket that you want to download an object from.\n• You can download an object from an S3 bucket in any of the following ways: Select the check box next to the object, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as. If you want to download a specific version of the object, turn on Show versions (located next to the search box). Select the check box next to the version of the object that you want, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\n• Select the check box next to the object, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n• If you want to download a specific version of the object, turn on Show versions (located next to the search box). Select the check box next to the version of the object that you want, and choose Download. If you want to download the object to a specific folder, on the Actions menu, choose Download as.\n\n[Note] Note You can download only one object at a time. If you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.\n\n[Note] You can download only one object at a time. If you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name of the downloaded object, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 3832
        }
      },
      {
        "header": "Step 4: Copy your object to a folder",
        "content": "You've already added an object to a bucket and downloaded the object. Now, you create a folder and copy the object and paste it into the folder.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nIn the Buckets list, choose your bucket name.\n\nChoose Create folder and configure a new folder:\n\nEnter a folder name (for example, favorite-pics).\n\nFor the folder encryption setting, choose Disable.\n\nNavigate to the Amazon S3 bucket or folder that contains the objects that you want to copy.\n\nSelect the check box to the left of the names of the objects that you want to copy.\n\nChoose Actions and choose Copy from the list of options that appears.\n\nAlternatively, choose Copy from the options in the upper right.\n\nChoose the destination folder:\n\nChoose the option button to the left of the folder name.\n\nTo navigate into a folder and choose a subfolder as your destination, choose the folder name.\n\nChoose Choose destination.\n\nThe path to your destination folder appears in the Destination box. In Destination, you can alternately enter your destination path, for example, s3://bucket-name/folder-name/.\n\nIn the bottom right, choose Copy.\n\nAmazon S3 copies your objects to the destination folder.\n\nTo delete an object and a bucket in Amazon S3, see Step 5: Delete your objects and bucket.\n\n• In the Buckets list, choose your bucket name.\n• Choose Create folder and configure a new folder: Enter a folder name (for example, favorite-pics). For the folder encryption setting, choose Disable. Choose Save.\n• Navigate to the Amazon S3 bucket or folder that contains the objects that you want to copy.\n• Select the check box to the left of the names of the objects that you want to copy.\n• Choose Actions and choose Copy from the list of options that appears. Alternatively, choose Copy from the options in the upper right.\n• Choose the destination folder: Choose Browse S3. Choose the option button to the left of the folder name. To navigate into a folder and choose a subfolder as your destination, choose the folder name. Choose Choose destination. The path to your destination folder appears in the Destination box. In Destination, you can alternately enter your destination path, for example, s3://bucket-name/folder-name/.\n• In the bottom right, choose Copy. Amazon S3 copies your objects to the destination folder.\n\n• Enter a folder name (for example, favorite-pics).\n• For the folder encryption setting, choose Disable.\n• Choose Save.\n\n• Choose Browse S3.\n• Choose the option button to the left of the folder name. To navigate into a folder and choose a subfolder as your destination, choose the folder name.\n• Choose Choose destination.\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 3094
        }
      },
      {
        "header": "Step 5: Delete your objects and bucket",
        "content": "When you no longer need an object or a bucket, we recommend that you delete them to prevent further charges. If you completed this getting started walkthrough as a learning exercise, and you don't plan to use your bucket or objects, we recommend that you delete your bucket and objects so that charges no longer accrue.\n\nBefore you delete your bucket, empty the bucket or delete the objects in the bucket. After you delete your objects and bucket, they are no longer available.\n\nIf you want to continue to use the same bucket name, we recommend that you delete the objects or empty the bucket, but don't delete the bucket. After you delete a bucket, the name becomes available to reuse. However, another AWS account might create a bucket with the same name before you have a chance to reuse it.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nEmptying your bucket\n\nDeleting your bucket\n\n• Deleting an object\n• Emptying your bucket\n• Deleting your bucket\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1413
        }
      },
      {
        "header": "Deleting an object",
        "content": "If you want to choose which objects you delete without emptying all the objects from your bucket, you can delete an object.\n\nIn the Buckets list, choose the name of the bucket that you want to delete an object from.\n\nSelect the object that you want to delete.\n\nChoose Delete from the options in the upper right.\n\nOn the Delete objects page, type delete to confirm deletion of your objects.\n\nChoose Delete objects.\n\n• In the Buckets list, choose the name of the bucket that you want to delete an object from.\n• Select the object that you want to delete.\n• Choose Delete from the options in the upper right.\n• On the Delete objects page, type delete to confirm deletion of your objects.\n• Choose Delete objects.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 709
        }
      },
      {
        "header": "Emptying your bucket",
        "content": "If you plan to delete your bucket, you must first empty your bucket, which deletes all the objects in the bucket.\n\nIn the Buckets list, select the bucket that you want to empty, and then choose Empty.\n\nTo confirm that you want to empty the bucket and delete all the objects in it, in Empty bucket, type permanently delete.\n\nEmptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.\n\nTo empty the bucket and delete all the objects in it, and choose Empty.\n\nAn Empty bucket: Status page opens that you can use to review a summary of failed and successful object deletions.\n\nTo return to your bucket list, choose Exit.\n\n• In the Buckets list, select the bucket that you want to empty, and then choose Empty.\n• To confirm that you want to empty the bucket and delete all the objects in it, in Empty bucket, type permanently delete. ImportantEmptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.\n• To empty the bucket and delete all the objects in it, and choose Empty. An Empty bucket: Status page opens that you can use to review a summary of failed and successful object deletions.\n• To return to your bucket list, choose Exit.\n\n[Note] ImportantEmptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.\n\n[Note] Emptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1550
        }
      },
      {
        "header": "Deleting your bucket",
        "content": "After you empty your bucket or delete all the objects from your bucket, you can delete your bucket.\n\nTo delete a bucket, in the Buckets list, select the bucket.\n\nTo confirm deletion, in Delete bucket, type the name of the bucket.\n\nDeleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.\n\nTo delete your bucket, choose Delete bucket.\n\n• To delete a bucket, in the Buckets list, select the bucket.\n• Choose Delete.\n• To confirm deletion, in Delete bucket, type the name of the bucket. ImportantDeleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.\n• To delete your bucket, choose Delete bucket.\n\n[Note] ImportantDeleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.\n\n[Note] Deleting a bucket cannot be undone. Bucket names are unique. If you delete your bucket, another AWS user can use the name. If you want to continue to use the same bucket name, don't delete your bucket. Instead, empty and keep the bucket.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1459
        }
      },
      {
        "header": "Next steps",
        "content": "In the preceding examples, you learned how to perform some basic Amazon S3 tasks.\n\nThe following topics explain the learning paths that you can use to gain a deeper understanding of Amazon S3 so that you can implement it in your applications.\n\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\nUnderstand common use cases\n\nControl access to your buckets and objects\n\nProtect and monitor your storage\n\nDevelop with Amazon S3\n\nLearn from tutorials\n\nExplore training and support\n\n• Understand common use cases\n• Control access to your buckets and objects\n• Protect and monitor your storage\n• Develop with Amazon S3\n• Learn from tutorials\n• Explore training and support\n\n[Note] NoteFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.\n\n[Note] For more information about using the Amazon S3 Express One Zone storage class with directory buckets, see S3 Express One Zone and Working with directory buckets.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1122
        }
      },
      {
        "header": "Understand common use cases",
        "content": "You can use Amazon S3 to support your specific use case. The AWS Solutions Library and AWS Blog provide use-case specific information and tutorials. The following are some common use cases for Amazon S3:\n\nBackup and storage â Use Amazon S3 storage management features to manage costs, meet regulatory requirements, reduce latency, and save multiple distinct copies of your data for compliance requirements.\n\nApplication hosting â Deploy, install, and manage web applications that are reliable, highly scalable, and low-cost. For example, you can configure your Amazon S3 bucket to host a static website. For more information, see Hosting a static website using Amazon S3.\n\nMedia hosting â Build a highly available infrastructure that hosts video, photo, or music uploads and downloads.\n\nSoftware delivery â Host your software applications for customers to download.\n\n• Backup and storage â Use Amazon S3 storage management features to manage costs, meet regulatory requirements, reduce latency, and save multiple distinct copies of your data for compliance requirements.\n• Application hosting â Deploy, install, and manage web applications that are reliable, highly scalable, and low-cost. For example, you can configure your Amazon S3 bucket to host a static website. For more information, see Hosting a static website using Amazon S3.\n• Media hosting â Build a highly available infrastructure that hosts video, photo, or music uploads and downloads.\n• Software delivery â Host your software applications for customers to download.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1548
        }
      },
      {
        "header": "Control access to your buckets and objects",
        "content": "Amazon S3 provides a variety of security features and tools. For an overview, see Access control in Amazon S3.\n\nBy default, S3 buckets and the objects in them are private. You have access only to the S3 resources that you create. You can use the following features to grant granular resource permissions that support your specific use case or to audit the permissions of your Amazon S3 resources.\n\nS3 Block Public Access â Block public access to S3 buckets and objects. By default, Block Public Access settings are turned on at the bucket level.\n\nAWS Identity and Access Management (IAM) identities â Use IAM or AWS IAM Identity Center to create IAM identities in your AWS account to manage access to your Amazon S3 resources. For example, you can use IAM with Amazon S3 to control the type of access that a user or group of users has to an Amazon S3 bucket that your AWS account owns. For more information about IAM identities and best practices, see IAM identities (users, user groups, and roles) in the IAM User Guide.\n\nBucket policies â Use IAM-based policy language to configure resource-based permissions for your S3 buckets and the objects in them.\n\nAccess control lists (ACLs) â Grant read and write permissions for individual buckets and objects to authorized users. As a general rule, we recommend using S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access-control option. With bucket policies and access point policies, you can define rules that apply broadly across all requests to your Amazon S3 resources. For more information about the specific cases when you'd use ACLs instead of resource-based policies or IAM user policies, see Identity and Access Management for Amazon S3.\n\nS3 Object Ownership â Take ownership of every object in your bucket, simplifying access management for data stored in Amazon S3. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to disable or enable ACLs. By default, ACLs are disabled. With ACLs disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies.\n\nIAM Access Analyzer for S3 â Evaluate and monitor your S3 bucket access policies, ensuring that the policies provide only the intended access to your S3 resources.\n\n• S3 Block Public Access â Block public access to S3 buckets and objects. By default, Block Public Access settings are turned on at the bucket level.\n• AWS Identity and Access Management (IAM) identities â Use IAM or AWS IAM Identity Center to create IAM identities in your AWS account to manage access to your Amazon S3 resources. For example, you can use IAM with Amazon S3 to control the type of access that a user or group of users has to an Amazon S3 bucket that your AWS account owns. For more information about IAM identities and best practices, see IAM identities (users, user groups, and roles) in the IAM User Guide.\n• Bucket policies â Use IAM-based policy language to configure resource-based permissions for your S3 buckets and the objects in them.\n• Access control lists (ACLs) â Grant read and write permissions for individual buckets and objects to authorized users. As a general rule, we recommend using S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access-control option. With bucket policies and access point policies, you can define rules that apply broadly across all requests to your Amazon S3 resources. For more information about the specific cases when you'd use ACLs instead of resource-based policies or IAM user policies, see Identity and Access Management for Amazon S3.\n• S3 Object Ownership â Take ownership of every object in your bucket, simplifying access management for data stored in Amazon S3. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to disable or enable ACLs. By default, ACLs are disabled. With ACLs disabled, the bucket owner owns all the objects in the bucket and manages access to data exclusively by using access-management policies.\n• IAM Access Analyzer for S3 â Evaluate and monitor your S3 bucket access policies, ensuring that the policies provide only the intended access to your S3 resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 4421
        }
      },
      {
        "header": "Protect and monitor your storage",
        "content": "Protecting your storage â After you create buckets and upload objects in Amazon S3, you can protect your object storage. For example, you can use S3 Versioning, S3 Replication, and Multi-Region Access Point failover controls for disaster recovery, AWS Backup to back up your data, and S3 Object Lock to set retention periods, prevent deletions and overwrites, and meet compliance requirements.\n\nMonitoring your storage â Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions. You can monitor storage activity and costs. Also, we recommend that you collect monitoring data from all the parts of your AWS solution so that you can more easily debug a multipoint failure if one occurs.\n\nYou can also use analytics and insights in Amazon S3 to understand, analyze, and optimize your storage usage. For example, use Amazon S3 Storage Lens to understand, analyze, and optimize your storage. S3 Storage Lens provides 29+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, Regions, buckets, or prefixes. Use Storage Class Analysis to analyze storage access patterns to decide when it's time to move your data to a more cost-effective storage class. To manage your costs, you can use S3 Lifecycle.\n\n• Protecting your storage â After you create buckets and upload objects in Amazon S3, you can protect your object storage. For example, you can use S3 Versioning, S3 Replication, and Multi-Region Access Point failover controls for disaster recovery, AWS Backup to back up your data, and S3 Object Lock to set retention periods, prevent deletions and overwrites, and meet compliance requirements.\n• Monitoring your storage â Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon S3 and your AWS solutions. You can monitor storage activity and costs. Also, we recommend that you collect monitoring data from all the parts of your AWS solution so that you can more easily debug a multipoint failure if one occurs. You can also use analytics and insights in Amazon S3 to understand, analyze, and optimize your storage usage. For example, use Amazon S3 Storage Lens to understand, analyze, and optimize your storage. S3 Storage Lens provides 29+ usage and activity metrics and interactive dashboards to aggregate data for your entire organization, specific accounts, Regions, buckets, or prefixes. Use Storage Class Analysis to analyze storage access patterns to decide when it's time to move your data to a more cost-effective storage class. To manage your costs, you can use S3 Lifecycle.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 2678
        }
      },
      {
        "header": "Develop with Amazon S3",
        "content": "Amazon S3 is a REST service. You can send requests to Amazon S3 using the REST API or the AWS SDK libraries, which wrap the underlying Amazon S3 REST API, simplifying your programming tasks. You can also use the AWS Command Line Interface (AWS CLI) to make Amazon S3 API calls. For more information, see Making requests in the Amazon S3 API Reference.\n\nThe Amazon S3 REST API is an HTTP interface to Amazon S3. With the REST API, you use standard HTTP requests to create, fetch, and delete buckets and objects. To use the REST API, you can use any toolkit that supports HTTP. You can even use a browser to fetch objects, as long as they are anonymously readable. For more information, see Developing with Amazon S3 in the Amazon S3 API Reference.\n\nTo help you build applications using the language of your choice, we provide the following resources.\n\nYou can access the features of Amazon S3 using the AWS CLI. To download and configure the AWS CLI, see Developing with Amazon S3 using the AWS CLI in the Amazon S3 API Reference.\n\nThe AWS CLI provides two tiers of commands for accessing Amazon S3: High-level (s3) commands and API-level (s3api and s3control commands. The high-level S3 commands simplify performing common tasks, such as creating, manipulating, and deleting objects and buckets. The s3api and s3control commands expose direct access to all Amazon S3 API operations, which you can use to carry out advanced operations that might not be possible with the high-level commands alone.\n\nFor a list of Amazon S3 AWS CLI commands, see s3, s3api, and s3control.\n\nYou can use the AWS SDKs when developing applications with Amazon S3. The AWS SDKs simplify your programming tasks by wrapping the underlying REST API. The AWS Mobile SDKs and the Amplify JavaScript library are also available for building connected mobile and web applications using AWS.\n\nIn addition to the AWS SDKs, AWS Explorers are available for Visual Studio and Eclipse for Java IDE. In this case, the SDKs and the explorers are bundled together as AWS Toolkits.\n\nFor more information, see Developing with Amazon S3 using the AWS SDKs in the Amazon S3 API Reference.\n\nThe AWS Developer Center and AWS Code Sample Catalog have sample code and libraries written especially for Amazon S3. You can use these code samples to understand how to implement the Amazon S3 API. You can also view the Amazon Simple Storage Service API Reference to understand the Amazon S3 API operations in detail.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2463
        }
      },
      {
        "header": "Learn from tutorials",
        "content": "You can get started with step-by-step tutorials to learn more about Amazon S3. These tutorials are intended for a lab-type environment, and they use fictitious company names, user names, and so on. Their purpose is to provide general guidance. They are not intended for direct use in a production environment without careful review and adaptation to meet the unique needs of your organization's environment.\n\nTutorial: Storing and retrieving a file with Amazon S3\n\nTutorial: Getting started using S3 Intelligent-Tiering\n\nTutorial: Getting started using the S3 Glacier storage classes\n\nTutorial: Getting started using S3 Intelligent-Tiering\n\nTutorial: Getting started using the S3 Glacier; storage classes\n\nTutorial: Optimizing costs and gaining visibility into usage with S3 Storage Lens\n\nTutorial: Getting started with Amazon S3 Multi-Region Access Points\n\nTutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication\n\nTutorial: Hosting on-demand streaming video with Amazon S3, Amazon CloudFront, and Amazon RouteÂ 53\n\nTutorial: Configuring a static website on Amazon S3\n\nTutorial: Configuring a static website using a custom domain registered with RouteÂ 53\n\nTutorial: Transforming data for your application with S3 Object Lambda\n\nTutorial: Detecting and redacting PII data with S3 Object Lambda and Amazon Comprehend\n\nTutorial: Using S3 Object Lambda to dynamically watermark images as they are retrieved\n\nTutorial: Batch-transcoding videos with S3 Batch Operations\n\nTutorial: Checking the integrity of data in Amazon S3 with additional checksums\n\nTutorial: Replicating data within and between AWS Regions using S3 Replication\n\nTutorial: Protecting data on Amazon S3 against accidental deletion or application bugs using S3 Versioning, S3 Object Lock, and S3 Replication\n\nTutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication\n\n• Tutorial: Storing and retrieving a file with Amazon S3\n• Tutorial: Getting started using S3 Intelligent-Tiering\n• Tutorial: Getting started using the S3 Glacier storage classes\n\n• Tutorial: Getting started using S3 Intelligent-Tiering\n• Tutorial: Getting started using the S3 Glacier; storage classes\n• Tutorial: Optimizing costs and gaining visibility into usage with S3 Storage Lens\n\n• Tutorial: Getting started with Amazon S3 Multi-Region Access Points\n• Tutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication\n\n• Tutorial: Hosting on-demand streaming video with Amazon S3, Amazon CloudFront, and Amazon RouteÂ 53\n• Tutorial: Configuring a static website on Amazon S3\n• Tutorial: Configuring a static website using a custom domain registered with RouteÂ 53\n\n• Tutorial: Transforming data for your application with S3 Object Lambda\n• Tutorial: Detecting and redacting PII data with S3 Object Lambda and Amazon Comprehend\n• Tutorial: Using S3 Object Lambda to dynamically watermark images as they are retrieved\n• Tutorial: Batch-transcoding videos with S3 Batch Operations\n\n• Tutorial: Checking the integrity of data in Amazon S3 with additional checksums\n• Tutorial: Replicating data within and between AWS Regions using S3 Replication\n• Tutorial: Protecting data on Amazon S3 against accidental deletion or application bugs using S3 Versioning, S3 Object Lock, and S3 Replication\n• Tutorial: Replicating existing objects in your Amazon S3 buckets with S3 Batch Replication",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 3410
        }
      },
      {
        "header": "Explore training and support",
        "content": "You can learn from AWS experts to advance your skills and get expert assistance achieving your objectives.\n\nTraining â Training resources provide a hands-on approach to learning Amazon S3. For more information, see AWS training and certification and AWS online tech talks.\n\nDiscussion Forums â On the forum, you can review posts to understand what you can and can't do with Amazon S3. You can also post your questions. For more information, see Discussion Forums.\n\nTechnical Support â If you have further questions, you can contact Technical Support.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• Training â Training resources provide a hands-on approach to learning Amazon S3. For more information, see AWS training and certification and AWS online tech talks.\n• Discussion Forums â On the forum, you can review posts to understand what you can and can't do with Amazon S3. You can also post your questions. For more information, see Discussion Forums.\n• Technical Support â If you have further questions, you can contact Technical Support.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1306
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html",
    "doc_type": "aws",
    "total_sections": 20
  },
  {
    "title": "What is AWS Lambda?",
    "summary": "What is AWS Lambda?AWS Lambda is a compute service that runs code without the need to manage servers. Your code runs, scaling up and down automatically, with pay-per-use pricing. To get started, see Create your first function.You can use Lambda for: Stream processing: Process real-time data streams for analytics and monitoring. See Kinesis Data Streams for details. Web applications: Build scalable web apps that automatically adjust to demand. Mobile backends: Create secure API backends for mobil",
    "sections": [
      {
        "header": "",
        "content": "AWS Lambda is a compute service that runs code without the need to manage servers. Your code runs, scaling up and down automatically, with pay-per-use pricing. To get started, see Create your first function.\n\nYou can use Lambda for:\n\nStream processing: Process real-time data streams for analytics and monitoring. See Kinesis Data Streams for details.\n\nWeb applications: Build scalable web apps that automatically adjust to demand.\n\nMobile backends: Create secure API backends for mobile and web applications.\n\nIoT backends: Handle web, mobile, IoT, and third-party API requests. See IoT for details.\n\nFile processing: Process files automatically when uploaded to Amazon Simple Storage Service. See file processing examples for details.\n\nDatabase operations and integration examples: Respond to database changes and automate data workflows. See database examples for details.\n\nScheduled and periodic tasks: Run automated operations on a regular schedule using EventBridge. See scheduled task examples for details.\n\nFor pricing information, see AWS Lambda Pricing.\n\n• Stream processing: Process real-time data streams for analytics and monitoring. See Kinesis Data Streams for details.\n• Web applications: Build scalable web apps that automatically adjust to demand.\n• Mobile backends: Create secure API backends for mobile and web applications.\n• IoT backends: Handle web, mobile, IoT, and third-party API requests. See IoT for details.\n• File processing: Process files automatically when uploaded to Amazon Simple Storage Service. See file processing examples for details.\n• Database operations and integration examples: Respond to database changes and automate data workflows. See database examples for details.\n• Scheduled and periodic tasks: Run automated operations on a regular schedule using EventBridge. See scheduled task examples for details.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 1852
        }
      },
      {
        "header": "How Lambda works",
        "content": "When using Lambda, you are responsible only for your code. Lambda runs your code on a high-availability compute infrastructure and manages all the computing resources, including server and operating system maintenance, capacity provisioning, automatic scaling, and logging.\n\nBecause Lambda is a serverless, event-driven compute service, it uses a different programming paradigm than traditional web applications. The following model illustrates how Lambda works:\n\nYou write and organize your code in Lambda functions, which are the basic building blocks you use to create a Lambda application.\n\nYou control security and access through Lambda permissions, using execution roles to manage what AWS services your functions can interact with and what resource policies can interact with your code.\n\nEvent sources and AWS services trigger your Lambda functions, passing event data in JSON format, which your functions process (this includes event source mappings).\n\nLambda runs your code with language-specific runtimes (like Node.js and Python) in execution environments that package your runtime, layers, and extensions.\n\nTo learn how to build serverless solutions, check out the Serverless Developer Guide.\n\n• You write and organize your code in Lambda functions, which are the basic building blocks you use to create a Lambda application.\n• You control security and access through Lambda permissions, using execution roles to manage what AWS services your functions can interact with and what resource policies can interact with your code.\n• Event sources and AWS services trigger your Lambda functions, passing event data in JSON format, which your functions process (this includes event source mappings).\n• Lambda runs your code with language-specific runtimes (like Node.js and Python) in execution environments that package your runtime, layers, and extensions.\n\n[Note] TipTo learn how to build serverless solutions, check out the Serverless Developer Guide.\n\n[Note] To learn how to build serverless solutions, check out the Serverless Developer Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 2055
        }
      },
      {
        "header": "Key features",
        "content": "Configure, control, and deploy secure applications:\n\nEnvironment variables modify application behavior without new code deployments.\n\nVersions safely test new features while maintaining stable production environments.\n\nLambda layers optimize code reuse and maintenance by sharing common components across multiple functions.\n\nCode signing enforce security compliance by ensuring only approved code reaches production systems.\n\nScale and perform reliably:\n\nConcurrency and scaling controls precisely manage application responsiveness and resource utilization during traffic spikes.\n\nLambda SnapStart significantly reduce cold start times. Lambda SnapStart can provide as low as sub-second startup performance, typically with no changes to your function code.\n\nResponse streaming optimize function performance by delivering large payloads incrementally for real-time processing.\n\nContainer images package functions with complex dependencies using container workflows.\n\nConnect and integrate seamlessly:\n\nVPC networks secure sensitive resources and internal services.\n\nFile system integration that shares persistent data and manage stateful operations across function invocations.\n\nFunction URLs create public-facing APIs and endpoints without additional services.\n\nLambda extensions augment functions with monitoring, security, and operational tools.\n\n• Environment variables modify application behavior without new code deployments.\n• Versions safely test new features while maintaining stable production environments.\n• Lambda layers optimize code reuse and maintenance by sharing common components across multiple functions.\n• Code signing enforce security compliance by ensuring only approved code reaches production systems.\n\n• Concurrency and scaling controls precisely manage application responsiveness and resource utilization during traffic spikes.\n• Lambda SnapStart significantly reduce cold start times. Lambda SnapStart can provide as low as sub-second startup performance, typically with no changes to your function code.\n• Response streaming optimize function performance by delivering large payloads incrementally for real-time processing.\n• Container images package functions with complex dependencies using container workflows.\n\n• VPC networks secure sensitive resources and internal services.\n• File system integration that shares persistent data and manage stateful operations across function invocations.\n• Function URLs create public-facing APIs and endpoints without additional services.\n• Lambda extensions augment functions with monitoring, security, and operational tools.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 2596
        }
      },
      {
        "header": "Related information",
        "content": "For information on how Lambda works, see How Lambda works.\n\nTo start using Lambda, see Create your first Lambda function.\n\nFor a list of example applications, see Getting started with example applications and patterns.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• For information on how Lambda works, see How Lambda works.\n• To start using Lambda, see Create your first Lambda function.\n• For a list of example applications, see Getting started with example applications and patterns.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 738
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/lambda/latest/dg/welcome.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Create your first Lambda function",
    "summary": "PrerequisitesCreate the functionInvoke the functionClean upNext steps\n\nCreate your first Lambda functionTo get started with Lambda, use the Lambda console to create a function. In a few minutes, you can create and deploy a function and test it in the console.As you carry out the tutorial, you'll learn some fundamental Lambda concepts, like how to pass arguments to your function using the Lambda event object. You'll also learn how to return log outputs from your function, and how to view your fun",
    "sections": [
      {
        "header": "",
        "content": "To get started with Lambda, use the Lambda console to create a function. In a few minutes, you can create and deploy a function and test it in the console.\n\nAs you carry out the tutorial, you'll learn some fundamental Lambda concepts, like how to pass arguments to your function using the Lambda event object. You'll also learn how to return log outputs from your function, and how to view your function's invocation logs in Amazon CloudWatch Logs.\n\nTo keep things simple, you create your function using either the Python or Node.js runtime. With these interpreted languages, you can edit function code directly in the console's built-in code editor. With compiled languages like Java and C#, you must create a deployment package on your local build machine and upload it to Lambda. To learn about deploying functions to Lambda using other runtimes, see the links in the Additional resources and next steps section.\n\nTo learn how to build serverless solutions, check out the Serverless Developer Guide.\n\n[Note] TipTo learn how to build serverless solutions, check out the Serverless Developer Guide.\n\n[Note] To learn how to build serverless solutions, check out the Serverless Developer Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1193
        }
      },
      {
        "header": "Prerequisites",
        "content": "If you do not have an AWS account, complete the following steps to create one.\n\nOpen https://portal.aws.amazon.com/billing/signup.\n\nFollow the online instructions.\n\nPart of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad.\n\nWhen you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access.\n\nAWS sends you a confirmation email after the sign-up process is complete. At any time, you can view your current account activity and manage your account by going to https://aws.amazon.com/ and choosing My Account.\n\nAfter you sign up for an AWS account, secure your AWS account root user, enable AWS IAM Identity Center, and create an administrative user so that you don't use the root user for everyday tasks.\n\nSign in to the AWS Management Console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password.\n\nFor help signing in by using root user, see Signing in as the root user in the AWS Sign-In User Guide.\n\nTurn on multi-factor authentication (MFA) for your root user.\n\nFor instructions, see Enable a virtual MFA device for your AWS account root user (console) in the IAM User Guide.\n\nEnable IAM Identity Center.\n\nFor instructions, see Enabling AWS IAM Identity Center in the AWS IAM Identity Center User Guide.\n\nIn IAM Identity Center, grant administrative access to a user.\n\nFor a tutorial about using the IAM Identity Center directory as your identity source, see Configure user access with the default IAM Identity Center directory in the AWS IAM Identity Center User Guide.\n\nTo sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user.\n\nFor help signing in using an IAM Identity Center user, see Signing in to the AWS access portal in the AWS Sign-In User Guide.\n\nIn IAM Identity Center, create a permission set that follows the best practice of applying least-privilege permissions.\n\nFor instructions, see Create a permission set in the AWS IAM Identity Center User Guide.\n\nAssign users to a group, and then assign single sign-on access to the group.\n\nFor instructions, see Add groups in the AWS IAM Identity Center User Guide.\n\n• Open https://portal.aws.amazon.com/billing/signup.\n• Follow the online instructions. Part of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad. When you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access.\n\n• Sign in to the AWS Management Console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password. For help signing in by using root user, see Signing in as the root user in the AWS Sign-In User Guide.\n• Turn on multi-factor authentication (MFA) for your root user. For instructions, see Enable a virtual MFA device for your AWS account root user (console) in the IAM User Guide.\n\n• Enable IAM Identity Center. For instructions, see Enabling AWS IAM Identity Center in the AWS IAM Identity Center User Guide.\n• In IAM Identity Center, grant administrative access to a user. For a tutorial about using the IAM Identity Center directory as your identity source, see Configure user access with the default IAM Identity Center directory in the AWS IAM Identity Center User Guide.\n\n• To sign in with your IAM Identity Center user, use the sign-in URL that was sent to your email address when you created the IAM Identity Center user. For help signing in using an IAM Identity Center user, see Signing in to the AWS access portal in the AWS Sign-In User Guide.\n\n• In IAM Identity Center, create a permission set that follows the best practice of applying least-privilege permissions. For instructions, see Create a permission set in the AWS IAM Identity Center User Guide.\n• Assign users to a group, and then assign single sign-on access to the group. For instructions, see Add groups in the AWS IAM Identity Center User Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 21,
          "content_length": 4493
        }
      },
      {
        "header": "Create a Lambda function with the console",
        "content": "In this example, your function takes a JSON object containing two integer values labeled \"length\" and \"width\". The function multiplies these values to calculate an area and returns this as a JSON string.\n\nYour function also prints the calculated area, along with the name of its CloudWatch log group. Later in the tutorial, youâll learn to use CloudWatch Logs to view records of your functionsâ invocation.\n\nOpen the Functions page of the Lambda console.\n\nChoose Create function.\n\nSelect Author from scratch.\n\nIn the Basic information pane, for Function name, enter myLambdaFunction.\n\nFor Runtime, choose either Node.js 22 or Python 3.13.\n\nLeave architecture set to x86_64, and then choose Create function.\n\nIn addition to a simple function that returns the message Hello from Lambda!, Lambda also creates an execution role for your function. An execution role is an AWS Identity and Access Management (IAM) role that grants a Lambda function permission to access AWS services and resources. For your function, the role that Lambda creates grants basic permissions to write to CloudWatch Logs.\n\nUse the console's built-in code editor to replace the Hello world code that Lambda created with your own function code.\n\nChoose the Code tab.\n\nIn the console's built-in code editor, you should see the function code that Lambda created. If you don't see the index.mjs tab in the code editor, select index.mjs in the file explorer as shown on the following diagram.\n\nPaste the following code into the index.mjs tab, replacing the code that Lambda created.\n\nIn the DEPLOY section, choose Deploy to update your function's code:\n\nBefore you move to the next step, let's take a moment to look at the function code and understand some key Lambda concepts.\n\nYour Lambda function contains a Node.js function named handler. A Lambda function in Node.js can contain more than one Node.js function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method.\n\nWhen you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to handler. Be sure not to edit the name of this Node.js function. If you do, Lambda wonât be able to run your code when you invoke your function.\n\nTo learn more about the Lambda handler in Node.js, see Define Lambda function handler in Node.js.\n\nThe Lambda event object:\n\nThe function handler takes two arguments, event and context. An event in Lambda is a JSON formatted document that contains data for your function to process.\n\nIf your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key.\n\nIn this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs.\n\nThe Lambda context object:\n\nThe second argument that your function takes is context. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment.\n\nYou can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the logGroupName parameter to output the name of its CloudWatch log group.\n\nTo learn more about the Lambda context object in Node.js, see Using the Lambda context object to retrieve Node.js function information.\n\nWith Node.js, you can use console methods like console.log and console.error to send information to your function's log. The example code uses console.log statements to output the calculated area and the name of the function's CloudWatch Logs group. You can also use any logging library that writes to stdout or stderr.\n\nTo learn more, see Log and monitor Node.js Lambda functions. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in.\n\nChoose the Code tab.\n\nIn the console's built-in code editor, you should see the function code that Lambda created. If you don't see the lambda_function.py tab in the code editor, select lambda_function.py in the file explorer as shown on the following diagram.\n\nPaste the following code into the lambda_function.py tab, replacing the code that Lambda created.\n\nIn the DEPLOY section, choose Deploy to update your function's code:\n\nBefore you move to the next step, let's take a moment to look at the function code and understand some key Lambda concepts.\n\nYour Lambda function contains a Python function named lambda_handler. A Lambda function in Python can contain more than one Python function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method.\n\nWhen you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to lambda_handler. Be sure not to edit the name of this Python function. If you do, Lambda wonât be able to run your code when you invoke your function.\n\nTo learn more about the Lambda handler in Python, see Define Lambda function handler in Python.\n\nThe Lambda event object:\n\nThe function lambda_handler takes two arguments, event and context. An event in Lambda is a JSON formatted document that contains data for your function to process.\n\nIf your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key.\n\nIn this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs.\n\nThe Lambda context object:\n\nThe second argument that your function takes is context. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment.\n\nYou can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the log_group_name parameter to output the name of its CloudWatch log group.\n\nTo learn more about the Lambda context object in Python, see Using the Lambda context object to retrieve Python function information.\n\nWith Python, you can use either a print statement or a Python logging library to send information to your function's log. To illustrate the difference in what's captured, the example code uses both methods. In a production application, we recommend that you use a logging library.\n\nTo learn more, see Log and monitor Python Lambda functions. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in.\n\n• Open the Functions page of the Lambda console.\n• Choose Create function.\n• Select Author from scratch.\n• In the Basic information pane, for Function name, enter myLambdaFunction.\n• For Runtime, choose either Node.js 22 or Python 3.13.\n• Leave architecture set to x86_64, and then choose Create function.\n\n• Choose the Code tab. In the console's built-in code editor, you should see the function code that Lambda created. If you don't see the index.mjs tab in the code editor, select index.mjs in the file explorer as shown on the following diagram.\n• Paste the following code into the index.mjs tab, replacing the code that Lambda created. export const handler = async (event, context) => { const length = event.length; const width = event.width; let area = calculateArea(length, width); console.log(`The area is ${area}`); console.log('CloudWatch log group: ', context.logGroupName); let data = { \"area\": area, }; return JSON.stringify(data); function calculateArea(length, width) { return length * width; } };\n• In the DEPLOY section, choose Deploy to update your function's code:\n\n• The Lambda handler: Your Lambda function contains a Node.js function named handler. A Lambda function in Node.js can contain more than one Node.js function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method. When you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to handler. Be sure not to edit the name of this Node.js function. If you do, Lambda wonât be able to run your code when you invoke your function. To learn more about the Lambda handler in Node.js, see Define Lambda function handler in Node.js.\n• The Lambda event object: The function handler takes two arguments, event and context. An event in Lambda is a JSON formatted document that contains data for your function to process. If your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key. In this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs.\n• The Lambda context object: The second argument that your function takes is context. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment. You can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the logGroupName parameter to output the name of its CloudWatch log group. To learn more about the Lambda context object in Node.js, see Using the Lambda context object to retrieve Node.js function information.\n• Logging in Lambda: With Node.js, you can use console methods like console.log and console.error to send information to your function's log. The example code uses console.log statements to output the calculated area and the name of the function's CloudWatch Logs group. You can also use any logging library that writes to stdout or stderr. To learn more, see Log and monitor Node.js Lambda functions. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in.\n\n• Choose the Code tab. In the console's built-in code editor, you should see the function code that Lambda created. If you don't see the lambda_function.py tab in the code editor, select lambda_function.py in the file explorer as shown on the following diagram.\n• Paste the following code into the lambda_function.py tab, replacing the code that Lambda created. import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): # Get the length and width parameters from the event object. The # runtime converts the event object to a Python dictionary length = event['length'] width = event['width'] area = calculate_area(length, width) print(f\"The area is {area}\") logger.info(f\"CloudWatch logs group: {context.log_group_name}\") # return the calculated area as a JSON string data = {\"area\": area} return json.dumps(data) def calculate_area(length, width): return length*width\n• In the DEPLOY section, choose Deploy to update your function's code:\n\n• The Lambda handler: Your Lambda function contains a Python function named lambda_handler. A Lambda function in Python can contain more than one Python function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method. When you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to lambda_handler. Be sure not to edit the name of this Python function. If you do, Lambda wonât be able to run your code when you invoke your function. To learn more about the Lambda handler in Python, see Define Lambda function handler in Python.\n• The Lambda event object: The function lambda_handler takes two arguments, event and context. An event in Lambda is a JSON formatted document that contains data for your function to process. If your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key. In this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs.\n• The Lambda context object: The second argument that your function takes is context. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment. You can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the log_group_name parameter to output the name of its CloudWatch log group. To learn more about the Lambda context object in Python, see Using the Lambda context object to retrieve Python function information.\n• Logging in Lambda: With Python, you can use either a print statement or a Python logging library to send information to your function's log. To illustrate the difference in what's captured, the example code uses both methods. In a production application, we recommend that you use a logging library. To learn more, see Log and monitor Python Lambda functions. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in.\n\n**Node.js**: To modify the code in the console Choose the Code tab. In the console's built-in code editor, you should see the function code that Lambda created. If you don't see the index.mjs tab in the code editor, select index.mjs in the file explorer as shown on the following diagram. Paste the following code into the index.mjs tab, replacing the code that Lambda created. export const handler = async (event, context) => { const length = event.length; const width = event.width; let area = calculateArea(length, width); console.log(`The area is ${area}`); console.log('CloudWatch log group: ', context.logGroupName); let data = { \"area\": area, }; return JSON.stringify(data); function calculateArea(length, width) { return length * width; } }; In the DEPLOY section, choose Deploy to update your function's code: Understanding your function code Before you move to the next step, let's take a moment to look at the function code and understand some key Lambda concepts. The Lambda handler: Your Lambda function contains a Node.js function named handler. A Lambda function in Node.js can contain more than one Node.js function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method. When you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to handler. Be sure not to edit the name of this Node.js function. If you do, Lambda wonât be able to run your code when you invoke your function. To learn more about the Lambda handler in Node.js, see Define Lambda function handler in Node.js. The Lambda event object: The function handler takes two arguments, event and context. An event in Lambda is a JSON formatted document that contains data for your function to process. If your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key. In this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs. The Lambda context object: The second argument that your function takes is context. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment. You can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the logGroupName parameter to output the name of its CloudWatch log group. To learn more about the Lambda context object in Node.js, see Using the Lambda context object to retrieve Node.js function information. Logging in Lambda: With Node.js, you can use console methods like console.log and console.error to send information to your function's log. The example code uses console.log statements to output the calculated area and the name of the function's CloudWatch Logs group. You can also use any logging library that writes to stdout or stderr. To learn more, see Log and monitor Node.js Lambda functions. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in.\n**Python**: To modify the code in the console Choose the Code tab. In the console's built-in code editor, you should see the function code that Lambda created. If you don't see the lambda_function.py tab in the code editor, select lambda_function.py in the file explorer as shown on the following diagram. Paste the following code into the lambda_function.py tab, replacing the code that Lambda created. import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def lambda_handler(event, context): # Get the length and width parameters from the event object. The # runtime converts the event object to a Python dictionary length = event['length'] width = event['width'] area = calculate_area(length, width) print(f\"The area is {area}\") logger.info(f\"CloudWatch logs group: {context.log_group_name}\") # return the calculated area as a JSON string data = {\"area\": area} return json.dumps(data) def calculate_area(length, width): return length*width In the DEPLOY section, choose Deploy to update your function's code: Understanding your function code Before you move to the next step, let's take a moment to look at the function code and understand some key Lambda concepts. The Lambda handler: Your Lambda function contains a Python function named lambda_handler. A Lambda function in Python can contain more than one Python function, but the handler function is always the entry point to your code. When your function is invoked, Lambda runs this method. When you created your Hello world function using the console, Lambda automatically set the name of the handler method for your function to lambda_handler. Be sure not to edit the name of this Python function. If you do, Lambda wonât be able to run your code when you invoke your function. To learn more about the Lambda handler in Python, see Define Lambda function handler in Python. The Lambda event object: The function lambda_handler takes two arguments, event and context. An event in Lambda is a JSON formatted document that contains data for your function to process. If your function is invoked by another AWS service, the event object contains information about the event that caused the invocation. For example, if your function is invoked when an object is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket, the event contains the name of the bucket and the object key. In this example, youâll create an event in the console by entering a JSON formatted document with two key-value pairs. The Lambda context object: The second argument that your function takes is context. Lambda passes the context object to your function automatically. The context object contains information about the function invocation and execution environment. You can use the context object to output information about your function's invocation for monitoring purposes. In this example, your function uses the log_group_name parameter to output the name of its CloudWatch log group. To learn more about the Lambda context object in Python, see Using the Lambda context object to retrieve Python function information. Logging in Lambda: With Python, you can use either a print statement or a Python logging library to send information to your function's log. To illustrate the difference in what's captured, the example code uses both methods. In a production application, we recommend that you use a logging library. To learn more, see Log and monitor Python Lambda functions. To learn about logging in other runtimes, see the 'Building with' pages for the runtimes you're interested in.",
        "code_examples": [
          "```\nimport json\nimport logging\n\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ndef lambda_handler(event, context):\n    \n    # Get the length and width parameters from the event object. The \n    # runtime converts the event object to a Python dictionary\n    length = event['length']\n    width = event['width']\n    \n    area = calculate_area(length, width)\n    print(f\"The area is{area}\")\n        \n    logger.info(f\"CloudWatch logs group:{context.log_group_name}\")\n    \n    # return the calculated area as a JSON string\n    data ={\"area\": area}\n    return json.dumps(data)\n    \ndef calculate_area(length, width):\n    return length*width\n```"
        ],
        "usage_examples": [
          "```\nexport const handler = async (event, context) =>{const length = event.length;\n  const width = event.width;\n  let area = calculateArea(length, width);\n  console.log(`The area is ${area}`);\n        \n  console.log('CloudWatch log group: ', context.logGroupName);\n  \n  let data ={\"area\": area,\n  };\n    return JSON.stringify(data);\n    \n  function calculateArea(length, width){return length * width;\n  }\n};\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 46,
          "content_length": 20863
        }
      },
      {
        "header": "Invoke the Lambda function using the console code editor",
        "content": "To invoke your function using the Lambda console code editor, create a test event to send to your function. The event is a JSON formatted document containing two key-value pairs with the keys \"length\" and \"width\".\n\nIn the TEST EVENTS section of the console code editor, choose Create test event.\n\nFor Event Name, enter myTestEvent.\n\nIn the Event JSON section, replace the default JSON with the following:\n\nTo test your function and view invocation records\n\nIn the TEST EVENTS section of the console code editor, choose the run icon next to your test event:\n\nWhen your function finishes running, the response and function logs are displayed in the OUTPUT tab. You should see results similar to the following:\n\nWhen you invoke your function outside of the Lambda console, you must use CloudWatch Logs to view your function's execution results.\n\nOpen the Log groups page of the CloudWatch console.\n\nChoose the log group for your function (/aws/lambda/myLambdaFunction). This is the log group name that your function printed to the console.\n\nScroll down and choose the Log stream for the function invocations you want to look at.\n\nYou should see output similar to the following:\n\n• In the TEST EVENTS section of the console code editor, choose Create test event.\n• For Event Name, enter myTestEvent.\n• In the Event JSON section, replace the default JSON with the following: { \"length\": 6, \"width\": 7 }\n• Choose Save.\n\n• Open the Log groups page of the CloudWatch console.\n• Choose the log group for your function (/aws/lambda/myLambdaFunction). This is the log group name that your function printed to the console.\n• Scroll down and choose the Log stream for the function invocations you want to look at. You should see output similar to the following: Node.js INIT_START Runtime Version: nodejs:22.v13 Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:e3aaabf6b92ef8755eaae2f4bfdcb7eb8c4536a5e044900570a42bdba7b869d9 START RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 Version: $LATEST 2024-08-23T22:04:15.809Z 5c012b0a-18f7-4805-b2f6-40912935034a INFO The area is 42 2024-08-23T22:04:15.810Z aba6c0fc-cf99-49d7-a77d-26d805dacd20 INFO CloudWatch log group: /aws/lambda/myLambdaFunction END RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 REPORT RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 Duration: 17.77 ms Billed Duration: 18 ms Memory Size: 128 MB Max Memory Used: 67 MB Init Duration: 178.85 ms Python INIT_START Runtime Version: python:3.13.v16 Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:ca202755c87b9ec2b58856efb7374b4f7b655a0ea3deb1d5acc9aee9e297b072 START RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e Version: $LATEST The area is 42 [INFO] 2024-09-01T00:05:22.464Z 9315ab6b-354a-486e-884a-2fb2972b7d84 CloudWatch logs group: /aws/lambda/myLambdaFunction END RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e REPORT RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e Duration: 1.15 ms Billed Duration: 2 ms Memory Size: 128 MB Max Memory Used: 40 MB\n\n**Node.js**: Status: Succeeded Test Event Name: myTestEvent Response \"{\\\"area\\\":42}\" Function Logs START RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a Version: $LATEST 2024-08-31T23:39:45.313Z 5c012b0a-18f7-4805-b2f6-40912935034a INFO The area is 42 2024-08-31T23:39:45.331Z 5c012b0a-18f7-4805-b2f6-40912935034a INFO CloudWatch log group: /aws/lambda/myLambdaFunction END RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a REPORT RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a Duration: 20.67 ms Billed Duration: 21 ms Memory Size: 128 MB Max Memory Used: 66 MB Init Duration: 163.87 ms Request ID 5c012b0a-18f7-4805-b2f6-40912935034a\n**Python**: Status: Succeeded Test Event Name: myTestEvent Response \"{\\\"area\\\": 42}\" Function Logs START RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b Version: $LATEST The area is 42 [INFO] 2024-08-31T23:43:26.428Z 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b CloudWatch logs group: /aws/lambda/myLambdaFunction END RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b REPORT RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b Duration: 1.42 ms Billed Duration: 2 ms Memory Size: 128 MB Max Memory Used: 39 MB Init Duration: 123.74 ms Request ID 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b\n**Node.js**: INIT_START Runtime Version: nodejs:22.v13 Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:e3aaabf6b92ef8755eaae2f4bfdcb7eb8c4536a5e044900570a42bdba7b869d9 START RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 Version: $LATEST 2024-08-23T22:04:15.809Z 5c012b0a-18f7-4805-b2f6-40912935034a INFO The area is 42 2024-08-23T22:04:15.810Z aba6c0fc-cf99-49d7-a77d-26d805dacd20 INFO CloudWatch log group: /aws/lambda/myLambdaFunction END RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 REPORT RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 Duration: 17.77 ms Billed Duration: 18 ms Memory Size: 128 MB Max Memory Used: 67 MB Init Duration: 178.85 ms\n**Python**: INIT_START Runtime Version: python:3.13.v16 Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:ca202755c87b9ec2b58856efb7374b4f7b655a0ea3deb1d5acc9aee9e297b072 START RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e Version: $LATEST The area is 42 [INFO] 2024-09-01T00:05:22.464Z 9315ab6b-354a-486e-884a-2fb2972b7d84 CloudWatch logs group: /aws/lambda/myLambdaFunction END RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e REPORT RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e Duration: 1.15 ms Billed Duration: 2 ms Memory Size: 128 MB Max Memory Used: 40 MB",
        "code_examples": [
          "```\n{\"length\": 6,\n  \"width\": 7\n}\n```"
        ],
        "usage_examples": [
          "```\nStatus: Succeeded\nTest Event Name: myTestEvent\n\nResponse\n\"{\\\"area\\\":42}\"\n\nFunction Logs\nSTART RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a Version: $LATEST\n2024-08-31T23:39:45.313Z\t5c012b0a-18f7-4805-b2f6-40912935034a\tINFO\tThe area is 42\n2024-08-31T23:39:45.331Z\t5c012b0a-18f7-4805-b2f6-40912935034a\tINFO\tCloudWatch log group:  /aws/lambda/myLambdaFunction\nEND RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a\nREPORT RequestId: 5c012b0a-18f7-4805-b2f6-40912935034a\tDuration: 20.67 ms\tBilled Duration: 21 ms\tMemory Size: 128 MB\tMax Memory Used: 66 MB\tInit Duration: 163.87 ms\n\nRequest ID\n5c012b0a-18f7-4805-b2f6-40912935034a\n```",
          "```\nStatus: Succeeded\nTest Event Name: myTestEvent\n\nResponse\n\"{\\\"area\\\": 42}\"\n\nFunction Logs\nSTART RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b Version: $LATEST\nThe area is 42\n[INFO]\t2024-08-31T23:43:26.428Z\t2d0b1579-46fb-4bf7-a6e1-8e08840eae5b\tCloudWatch logs group: /aws/lambda/myLambdaFunction\nEND RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b\nREPORT RequestId: 2d0b1579-46fb-4bf7-a6e1-8e08840eae5b\tDuration: 1.42 ms\tBilled Duration: 2 ms\tMemory Size: 128 MB\tMax Memory Used: 39 MB\tInit Duration: 123.74 ms\n\nRequest ID\n2d0b1579-46fb-4bf7-a6e1-8e08840eae5b\n```",
          "```\nINIT_START Runtime Version: nodejs:22.v13    Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:e3aaabf6b92ef8755eaae2f4bfdcb7eb8c4536a5e044900570a42bdba7b869d9\nSTART RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20 Version: $LATEST\n2024-08-23T22:04:15.809Z    5c012b0a-18f7-4805-b2f6-40912935034a  INFO\tThe area is 42\n2024-08-23T22:04:15.810Z    aba6c0fc-cf99-49d7-a77d-26d805dacd20  INFO  CloudWatch log group:  /aws/lambda/myLambdaFunction\nEND RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20\nREPORT RequestId: aba6c0fc-cf99-49d7-a77d-26d805dacd20    Duration: 17.77 ms    Billed Duration: 18 ms    Memory Size: 128 MB    Max Memory Used: 67 MB    Init Duration: 178.85 ms\n```",
          "```\nINIT_START Runtime Version: python:3.13.v16    Runtime Version ARN: arn:aws:lambda:us-west-2::runtime:ca202755c87b9ec2b58856efb7374b4f7b655a0ea3deb1d5acc9aee9e297b072\nSTART RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e Version: $LATEST\nThe area is 42\n[INFO]\t2024-09-01T00:05:22.464Z\t9315ab6b-354a-486e-884a-2fb2972b7d84\tCloudWatch logs group: /aws/lambda/myLambdaFunction\nEND RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e \nREPORT RequestId: 9d4096ee-acb3-4c25-be10-8a210f0a9d8e    Duration: 1.15 ms    Billed Duration: 2 ms    Memory Size: 128 MB    Max Memory Used: 40 MB\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 5415
        }
      },
      {
        "header": "Clean up",
        "content": "When you're finished working with the example function, delete it. You can also delete the log group that stores the function's logs, and the execution role that the console created.\n\nOpen the Functions page of the Lambda console.\n\nSelect the function that you created.\n\nChoose Actions, Delete.\n\nType confirm in the text input field and choose Delete.\n\nOpen the Log groups page of the CloudWatch console.\n\nSelect the function's log group (/aws/lambda/myLambdaFunction).\n\nChoose Actions, Delete log group(s).\n\nIn the Delete log group(s) dialog box, choose Delete.\n\nOpen the Roles page of the AWS Identity and Access Management (IAM) console.\n\nSelect the function's execution role (for example, myLambdaFunction-role-31exxmpl).\n\nIn the Delete role dialog box, enter the role name, and then choose Delete.\n\n• Open the Functions page of the Lambda console.\n• Select the function that you created.\n• Choose Actions, Delete.\n• Type confirm in the text input field and choose Delete.\n\n• Open the Log groups page of the CloudWatch console.\n• Select the function's log group (/aws/lambda/myLambdaFunction).\n• Choose Actions, Delete log group(s).\n• In the Delete log group(s) dialog box, choose Delete.\n\n• Open the Roles page of the AWS Identity and Access Management (IAM) console.\n• Select the function's execution role (for example, myLambdaFunction-role-31exxmpl).\n• Choose Delete.\n• In the Delete role dialog box, enter the role name, and then choose Delete.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 1453
        }
      },
      {
        "header": "Additional resources and next steps",
        "content": "Now that youâve created and tested a simple Lambda function using the console, take these next steps:\n\nLearn to add dependencies to your function and deploy it using a .zip deployment package. Choose your preferred language from the following links.\n\nDeploy Node.js Lambda functions with .zip file archives\n\nDeploy transpiled TypeScript code in Lambda with .zip file archives\n\nWorking with .zip file archives for Python Lambda functions\n\nDeploy Ruby Lambda functions with .zip file archives\n\nDeploy Java Lambda functions with .zip or JAR file archives\n\nDeploy Go Lambda functions with .zip file archives\n\nBuild and deploy C# Lambda functions with .zip file archives\n\nTo learn how to invoke a Lambda function using another AWS service, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.\n\nChoose one of the following tutorials for more complex examples of using Lambda with other AWS services.\n\nTutorial: Using Lambda with API Gateway: Create an Amazon API Gateway REST API that invokes a Lambda function.\n\nUsing a Lambda function to access an Amazon RDS database: Use a Lambda function to write data to an Amazon Relational Database Service (Amazon RDS) database through RDS Proxy.\n\nUsing an Amazon S3 trigger to create thumbnail images: Use a Lambda function to create a thumbnail every time an image file is uploaded to an Amazon S3 bucket.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• Learn to add dependencies to your function and deploy it using a .zip deployment package. Choose your preferred language from the following links. Node.js Deploy Node.js Lambda functions with .zip file archives Typescript Deploy transpiled TypeScript code in Lambda with .zip file archives Python Working with .zip file archives for Python Lambda functions Ruby Deploy Ruby Lambda functions with .zip file archives Java Deploy Java Lambda functions with .zip or JAR file archives Go Deploy Go Lambda functions with .zip file archives C# Build and deploy C# Lambda functions with .zip file archives\n• To learn how to invoke a Lambda function using another AWS service, see Tutorial: Using an Amazon S3 trigger to invoke a Lambda function.\n• Choose one of the following tutorials for more complex examples of using Lambda with other AWS services. Tutorial: Using Lambda with API Gateway: Create an Amazon API Gateway REST API that invokes a Lambda function. Using a Lambda function to access an Amazon RDS database: Use a Lambda function to write data to an Amazon Relational Database Service (Amazon RDS) database through RDS Proxy. Using an Amazon S3 trigger to create thumbnail images: Use a Lambda function to create a thumbnail every time an image file is uploaded to an Amazon S3 bucket.\n\n• Tutorial: Using Lambda with API Gateway: Create an Amazon API Gateway REST API that invokes a Lambda function.\n• Using a Lambda function to access an Amazon RDS database: Use a Lambda function to write data to an Amazon Relational Database Service (Amazon RDS) database through RDS Proxy.\n• Using an Amazon S3 trigger to create thumbnail images: Use a Lambda function to create a thumbnail every time an image file is uploaded to an Amazon S3 bucket.\n\n**Node.js**: Deploy Node.js Lambda functions with .zip file archives\n**Typescript**: Deploy transpiled TypeScript code in Lambda with .zip file archives\n**Python**: Working with .zip file archives for Python Lambda functions\n**Ruby**: Deploy Ruby Lambda functions with .zip file archives\n**Java**: Deploy Java Lambda functions with .zip or JAR file archives\n**Go**: Deploy Go Lambda functions with .zip file archives\n**C#**: Build and deploy C# Lambda functions with .zip file archives",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 3894
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html",
    "doc_type": "aws",
    "total_sections": 6
  },
  {
    "title": "Building Lambda functions with Python",
    "summary": "Runtime-included SDK versionsDisabled Python featuresResponse formatGraceful shutdown for extensions\n\nBuilding Lambda functions with PythonYou can run Python code in AWS Lambda. Lambda provides runtimes for Python that run your code to process events. Your code runs in an environment that includes the SDK for Python (Boto3), with credentials from an AWS Identity and Access Management (IAM) role that you manage. To learn more about the SDK versions included with the Python runtimes, see Runtime-i",
    "sections": [
      {
        "header": "",
        "content": "You can run Python code in AWS Lambda. Lambda provides runtimes for Python that run your code to process events. Your code runs in an environment that includes the SDK for Python (Boto3), with credentials from an AWS Identity and Access Management (IAM) role that you manage. To learn more about the SDK versions included with the Python runtimes, see Runtime-included SDK versions.\n\nLambda supports the following Python runtimes.\n\nOpen the Lambda console.\n\nChoose Create function.\n\nConfigure the following settings:\n\nFunction name: Enter a name for the function.\n\nRuntime: Choose Python 3.13.\n\nChoose Create function.\n\nThe console creates a Lambda function with a single source file named lambda_function. You can edit this file and add more files in the built-in code editor. In the DEPLOY section, choose Deploy to update your function's code. Then, to run your code, choose Create test event in the TEST EVENTS section.\n\nYour Lambda function comes with a CloudWatch Logs log group. The function runtime sends details about each invocation to CloudWatch Logs. It relays any logs that your function outputs during invocation. If your function returns an error, Lambda formats the error and returns it to the invoker.\n\nRuntime-included SDK versions\n\nDisabled Python features\n\nGraceful shutdown for extensions\n\nDefine Lambda function handler in Python\n\nWorking with .zip file archives for Python Lambda functions\n\nDeploy Python Lambda functions with container images\n\nWorking with layers for Python Lambda functions\n\nUsing the Lambda context object to retrieve Python function information\n\nLog and monitor Python Lambda functions\n\nAWS Lambda function testing in Python\n\nInstrumenting Python code in AWS Lambda\n\n• Open the Lambda console.\n• Choose Create function.\n• Configure the following settings: Function name: Enter a name for the function. Runtime: Choose Python 3.13.\n• Choose Create function.\n\n• Function name: Enter a name for the function.\n• Runtime: Choose Python 3.13.\n\n• Runtime-included SDK versions\n• Disabled Python features\n• Response format\n• Graceful shutdown for extensions\n• Define Lambda function handler in Python\n• Working with .zip file archives for Python Lambda functions\n• Deploy Python Lambda functions with container images\n• Working with layers for Python Lambda functions\n• Using the Lambda context object to retrieve Python function information\n• Log and monitor Python Lambda functions\n• AWS Lambda function testing in Python\n• Instrumenting Python code in AWS Lambda\n\nName | Identifier | Operating system | Deprecation date | Block function create | Block function update\n--- | --- | --- | --- | --- | ---\nName | Identifier | Operating system | Deprecation date | Block function create | Block function update\nPython 3.14 | python3.14 | Amazon Linux 2023 | Jun 30, 2029 | Jul 31, 2029 | Aug 31, 2029\nPython 3.13 | python3.13 | Amazon Linux 2023 | Jun 30, 2029 | Jul 31, 2029 | Aug 31, 2029\nPython 3.12 | python3.12 | Amazon Linux 2023 | Oct 31, 2028 | Nov 30, 2028 | Jan 10, 2029\nPython 3.11 | python3.11 | Amazon Linux 2 | Jun 30, 2026 | Jul 31, 2026 | Aug 31, 2026\nPython 3.10 | python3.10 | Amazon Linux 2 | Jun 30, 2026 | Jul 31, 2026 | Aug 31, 2026\nPython 3.9 | python3.9 | Amazon Linux 2 | Dec 15, 2025 | Jun 1, 2026 | Jul 1, 2026",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 21,
          "content_length": 3271
        }
      },
      {
        "header": "Runtime-included SDK versions",
        "content": "The version of the AWS SDK included in the Python runtime depends on the runtime version and your AWS Region. To find the version of the SDK included in the runtime you're using, create a Lambda function with the following code.",
        "code_examples": [
          "```\nimport boto3\nimport botocore\n\ndef lambda_handler(event, context):\n   print(f'boto3 version:{boto3.__version__}')\n   print(f'botocore version:{botocore.__version__}')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 228
        }
      },
      {
        "header": "Disabled Python features",
        "content": "The following table lists Python features which are disabled in the Lambda managed runtimes and container base images for Python. These features must be enabled when the Python runtime executable is compiled and cannot be enabled by using an execution-time flag. To use these features in Lambda, you can deploy your own Python runtime build with these features enabled, using a container image or custom runtime.\n\nPython feature | Affected Python versions | Status\n--- | --- | ---\nPython feature | Affected Python versions | Status\nJust-in-Time (JIT) compiler | Python 3.13 and later | The JIT compiler is experimental and is not recommended for production workloads. It is therefore disabled in the Lambda Python runtimes.\nFree-threading | Python 3.13 and later | Free threading (option to disable the global interpreter lock) is disabled in Lambda Python builds due to the performance impact on single-threaded code.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 1,
          "content_length": 918
        }
      },
      {
        "header": "Response format",
        "content": "In Python 3.12 and later Python runtimes, functions return Unicode characters as part of their JSON response. Earlier Python runtimes return escaped sequences for Unicode characters in responses. For example, in Python 3.11, if you return a Unicode string such as \"ããã«ã¡ã¯\", it escapes the Unicode characters and returns \"\\u3053\\u3093\\u306b\\u3061\\u306f\". The Python 3.12 runtime returns the original \"ããã«ã¡ã¯\".\n\nUsing Unicode responses reduces the size of Lambda responses, making it easier to fit larger responses into the 6 MB maximum payload size for synchronous functions. In the previous example, the escaped version is 32 bytesâcompared to 17 bytes with the Unicode string.\n\nWhen you upgrade to Python 3.12 or later Python runtimes, you might need to adjust your code to account for the new response format. If the caller expects escaped Unicode, you must either add code to the returning function to escape the Unicode manually, or adjust the caller to handle the Unicode return.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1004
        }
      },
      {
        "header": "Graceful shutdown for extensions",
        "content": "Python 3.12 and later Python runtimes offer improved graceful shutdown capabilities for functions with external extensions. When Lambda shuts down an execution environment, it sends a SIGTERM signal to the runtime and then a SHUTDOWN event to each registered external extension. You can catch the SIGTERM signal in your Lambda function and clean up resources such as database connections that were created by the function.\n\nTo learn more about the execution environment lifecycle, see Understanding the Lambda execution environment lifecycle. For examples of how to use graceful shutdown with extensions, see the AWS Samples GitHub repository.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 939
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "What is Amazon Relational Database Service (Amazon RDS)?",
    "summary": "Advantages of Amazon RDSComparison of responsibilitiesAmazon RDS shared responsibility modelDB instancesAWS Regions and Availability ZonesAccess control with security groupsAmazon RDS monitoringUser interfaces to Amazon RDSHow you are charged for Amazon RDSAWS Free Tier on Amazon RDSWhat's next?\n\nWhat is Amazon Relational Database Service (Amazon RDS)?Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the A",
    "sections": [
      {
        "header": "",
        "content": "Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the AWS Cloud. It provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.\n\nThis guide covers Amazon RDS database engines other than Amazon Aurora. For information about using Amazon Aurora, see the Amazon Aurora User Guide.\n\nIf you are new to AWS products and services, begin learning more with the following resources:\n\nFor an overview of all AWS products, see What is cloud computing?\n\nAmazon Web Services provides a number of database services. To learn more about the variety of database options available on AWS, see Choosing an AWS database service and Running databases on AWS.\n\n• For an overview of all AWS products, see What is cloud computing?\n• Amazon Web Services provides a number of database services. To learn more about the variety of database options available on AWS, see Choosing an AWS database service and Running databases on AWS.\n\n[Note] NoteThis guide covers Amazon RDS database engines other than Amazon Aurora. For information about using Amazon Aurora, see the Amazon Aurora User Guide.\n\n[Note] This guide covers Amazon RDS database engines other than Amazon Aurora. For information about using Amazon Aurora, see the Amazon Aurora User Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1394
        }
      },
      {
        "header": "Advantages of Amazon RDS",
        "content": "Amazon RDS is a managed database service. It's responsible for most management tasks. By eliminating tedious manual processes, Amazon RDS frees you to focus on your application and your users.\n\nAmazon RDS provides the following principal advantages over database deployments that aren't fully managed:\n\nYou can use database engines that you are already familiar with: IBM Db2, MariaDB, Microsoft SQL Server, MySQL, Oracle Database, and PostgreSQL.\n\nAmazon RDS manages backups, software patching, automatic failure detection, and recovery.\n\nYou can turn on automated backups, or manually create your own backup snapshots. You can use these backups to restore a database. The Amazon RDS restore process works reliably and efficiently.\n\nYou can get high availability with a primary DB instance and a synchronous secondary DB instance that you can fail over to when problems occur. You can also use read replicas to increase read scaling.\n\nIn addition to the security in your database package, you can control access by using AWS Identity and Access Management (IAM) to define users and permissions. You can also help protect your databases by putting them in a virtual private cloud (VPC).\n\n• You can use database engines that you are already familiar with: IBM Db2, MariaDB, Microsoft SQL Server, MySQL, Oracle Database, and PostgreSQL.\n• Amazon RDS manages backups, software patching, automatic failure detection, and recovery.\n• You can turn on automated backups, or manually create your own backup snapshots. You can use these backups to restore a database. The Amazon RDS restore process works reliably and efficiently.\n• You can get high availability with a primary DB instance and a synchronous secondary DB instance that you can fail over to when problems occur. You can also use read replicas to increase read scaling.\n• In addition to the security in your database package, you can control access by using AWS Identity and Access Management (IAM) to define users and permissions. You can also help protect your databases by putting them in a virtual private cloud (VPC).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 2077
        }
      },
      {
        "header": "Comparison of responsibilities with Amazon EC2 and on-premises deployments",
        "content": "We recommend Amazon RDS as your default choice for most relational database deployments. The following alternatives have the disadvantage of making you spend more time managing software and hardware:\n\nWhen you buy an on-premises server, you get CPU, memory, storage, and IOPS, all bundled together. You assume full responsibility for the server, operating system, and database software.\n\nAmazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the AWS Cloud. Unlike in an on-premises server, CPU, memory, storage, and IOPS are separated so that you can scale them independently. AWS manages the hardware layers, which eliminates some of the burden of managing an on-premises database server.\n\nThe disadvantage to running a database on Amazon EC2 is that you're more prone to user errors. For example, when you update the operating system or database software manually, you might accidentally cause application downtime. You might spend hours checking every change to identify and fix an issue.\n\nThe following table compares the management models for on-premises databases, Amazon EC2, and Amazon RDS.\n\nOn-premises management\n\nAmazon EC2 management\n\nAmazon RDS management\n\nApplication optimization\n\nDatabase software patching\n\nDatabase software install\n\nOperating system (OS) patching\n\nPower, network, and cooling\n\n**On-premises deployment**: When you buy an on-premises server, you get CPU, memory, storage, and IOPS, all bundled together. You assume full responsibility for the server, operating system, and database software.\n**Amazon EC2**: Amazon Elastic Compute Cloud (Amazon EC2) provides scalable computing capacity in the AWS Cloud. Unlike in an on-premises server, CPU, memory, storage, and IOPS are separated so that you can scale them independently. AWS manages the hardware layers, which eliminates some of the burden of managing an on-premises database server. The disadvantage to running a database on Amazon EC2 is that you're more prone to user errors. For example, when you update the operating system or database software manually, you might accidentally cause application downtime. You might spend hours checking every change to identify and fix an issue.\n\nFeature | On-premises management | Amazon EC2 management | Amazon RDS management\n--- | --- | --- | ---\nFeature | On-premises management | Amazon EC2 management | Amazon RDS management\nApplication optimization | Customer | Customer | Customer\nScaling | Customer | Customer | AWS\nHigh availability | Customer | Customer | AWS\nDatabase backups | Customer | Customer | AWS\nDatabase software patching | Customer | Customer | AWS\nDatabase software install | Customer | Customer | AWS\nOperating system (OS) patching | Customer | Customer | AWS\nOS installation | Customer | Customer | AWS\nServer maintenance | Customer | AWS | AWS\nHardware lifecycle | Customer | AWS | AWS\nPower, network, and cooling | Customer | AWS | AWS",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 13,
          "content_length": 2917
        }
      },
      {
        "header": "Amazon RDS shared responsibility model",
        "content": "Amazon RDS is responsible for hosting the software components and infrastructure of DB instances and DB clusters. You are responsible for query tuning, which is the process of adjusting SQL queries to improve performance. Query performance is highly dependent on database design, data size, data distribution, application workload, and query patterns, which can vary greatly. Monitoring and tuning are highly individualized processes that you own for your RDS databases. You can use Amazon RDS Performance Insights and other tools to identify problematic queries.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 563
        }
      },
      {
        "header": "Amazon RDS DB instances",
        "content": "A DB instance is an isolated database environment in the AWS Cloud. The basic building block of Amazon RDS is the DB instance. Your DB instance can contain one or more user-created databases. The following diagram shows a virtual private cloud (VPC) that contains two Availability Zones, with each AZ containing two DB instances.\n\nYou can access your DB instances by using the same tools and applications that you use with a standalone database instance. You can create and modify a DB instance by using the AWS Command Line Interface (AWS CLI), the Amazon RDS API, or the AWS Management Console.\n\nAmazon RDS application architecture: example\n\nDB instances in an Amazon Virtual Private Cloud (Amazon VPC)\n\n• Amazon RDS application architecture: example\n• DB instance classes\n• DB instance storage\n• DB instances in an Amazon Virtual Private Cloud (Amazon VPC)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 859
        }
      },
      {
        "header": "Amazon RDS application architecture: example",
        "content": "The following image shows a typical use case of a dynamic website that uses Amazon RDS DB instances for database storage:\n\nThe primary components of the preceding architecture are as follows:\n\nAWS routes user traffic through Elastic Load Balancing. A load balancer distributes workloads across multiple compute resources, such as virtual servers. In this sample use case, the Elastic Load Balancer forwards client requests to application servers.\n\nApplication servers interact with RDS DB instances. An application server in AWS is typically hosted on EC2 instances, which provide scalable computing capacity. The application servers reside in public subnets with different Availability Zones (AZs) within the same Virtual Private Cloud (VPC).\n\nThe EC2 application servers interact with RDS DB instances. The DB instances reside in private subnets within different Availability Zones (AZs) within the same Virtual Private Cloud (VPC). Because the subnets are private, no requests from the internet are permitted.\n\nThe primary DB instance replicates to another DB instance, called a read replica. Both DB instances are in private subnets within the VPC, which means that Internet users can't access them directly.\n\n**Elastic Load Balancing**: AWS routes user traffic through Elastic Load Balancing. A load balancer distributes workloads across multiple compute resources, such as virtual servers. In this sample use case, the Elastic Load Balancer forwards client requests to application servers.\n**Application servers**: Application servers interact with RDS DB instances. An application server in AWS is typically hosted on EC2 instances, which provide scalable computing capacity. The application servers reside in public subnets with different Availability Zones (AZs) within the same Virtual Private Cloud (VPC).\n**RDS DB instances**: The EC2 application servers interact with RDS DB instances. The DB instances reside in private subnets within different Availability Zones (AZs) within the same Virtual Private Cloud (VPC). Because the subnets are private, no requests from the internet are permitted. The primary DB instance replicates to another DB instance, called a read replica. Both DB instances are in private subnets within the VPC, which means that Internet users can't access them directly.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2305
        }
      },
      {
        "header": "DB engines",
        "content": "A DB engine is the specific relational database software that runs on your DB instance. Amazon RDS supports the following database engines:\n\nFor more information, see Amazon RDS for Db2.\n\nFor more information, see Amazon RDS for MariaDB.\n\nMicrosoft SQL Server\n\nFor more information, see Amazon RDS for Microsoft SQL Server.\n\nFor more information, see Amazon RDS for MySQL.\n\nFor more information, see Amazon RDS for Oracle.\n\nFor more information, see Amazon RDS for PostgreSQL.\n\nEach DB engine has its own supported features, and each version of a DB engine can include specific features. Support for Amazon RDS features varies across AWS Regions and specific versions of each DB engine. To check feature support in different engine versions and Regions, see Supported features in Amazon RDS by AWS Region and DB engine.\n\nAdditionally, each DB engine has a set of parameters in a DB parameter group that control the behavior of the databases that it manages. For more information about parameter groups, see Parameter groups for Amazon RDS.\n\n• IBM Db2 For more information, see Amazon RDS for Db2.\n• MariaDB For more information, see Amazon RDS for MariaDB.\n• Microsoft SQL Server For more information, see Amazon RDS for Microsoft SQL Server.\n• MySQL For more information, see Amazon RDS for MySQL.\n• Oracle Database For more information, see Amazon RDS for Oracle.\n• PostgreSQL For more information, see Amazon RDS for PostgreSQL.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 1431
        }
      },
      {
        "header": "DB instance classes",
        "content": "A DB instance class determines the computation and memory capacity of a DB instance. A DB instance class consists of both the DB instance class type and the size. Amazon RDS supports the following instance class types, where the asterisk (*) represents the generation, optional attribute, and size:\n\nGeneral purpose â db.m*\n\nMemory optimized â db.z*, db.x*, db.r*\n\nCompute optimized â db.c*\n\nBurstable performance â db.t*\n\nEach instance class offers different compute, memory, and storage capabilities. For example, db.m7g is a 7th-generation, general-purpose DB instance class type powered by AWS Graviton3 processors. When you create a DB instance, you specify a DB instance class such as db.m7g.2xlarge, where 2xlarge is the size. For more information about the hardware specifications for the different instance classes, see Hardware specifications for DB instance classes.\n\nYou can select the DB instance class that best meets your requirements. If your requirements change over time, you can change your DB instance class. For example, you might scale up your db.m7g.2xlarge instance to db.m7g.4xlarge. For more information, see DB instance classes.\n\nFor pricing information on DB instance classes, see the Pricing section of the Amazon RDS product page.\n\n• General purpose â db.m*\n• Memory optimized â db.z*, db.x*, db.r*\n• Compute optimized â db.c*\n• Burstable performance â db.t*\n\n[Note] NoteFor pricing information on DB instance classes, see the Pricing section of the Amazon RDS product page.\n\n[Note] For pricing information on DB instance classes, see the Pricing section of the Amazon RDS product page.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1632
        }
      },
      {
        "header": "DB instance storage",
        "content": "Amazon EBS provides durable, block-level storage volumes that you can attach to a running instance. DB instance storage comes in the following types:\n\nGeneral Purpose (SSD)\n\nThis cost-effective storage type is ideal for a broad range of workloads running on medium-sized DB instances. General Purpose storage is best suited for development and testing environments.\n\nProvisioned IOPS (PIOPS)\n\nThis storage type is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that require low I/O latency and consistent I/O throughput. Provisioned IOPS storage is best suited for production environments.\n\nAmazon RDS supports magnetic storage for backward compatibility. We recommend that you use General Purpose SSD or Provisioned IOPS SSD for any new storage needs.\n\nThe storage types differ in performance characteristics and price. You can tailor your storage performance and cost to the requirements of your database.\n\nEach DB instance has minimum and maximum storage requirements depending on the storage type and the database engine it supports. It's important to have sufficient storage so that your databases have room to grow. Also, sufficient storage makes sure that features for the DB engine have room to write content or log entries. For more information, see Amazon RDS DB instance storage.\n\n• General Purpose (SSD) This cost-effective storage type is ideal for a broad range of workloads running on medium-sized DB instances. General Purpose storage is best suited for development and testing environments.\n• Provisioned IOPS (PIOPS) This storage type is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that require low I/O latency and consistent I/O throughput. Provisioned IOPS storage is best suited for production environments.\n• Magnetic Amazon RDS supports magnetic storage for backward compatibility. We recommend that you use General Purpose SSD or Provisioned IOPS SSD for any new storage needs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1986
        }
      },
      {
        "header": "DB instances in an Amazon Virtual Private Cloud (Amazon VPC)",
        "content": "You can run a DB instance on a virtual private cloud (VPC) using the Amazon Virtual Private Cloud (Amazon VPC) service. When you use a VPC, you have control over your virtual networking environment. You can choose your own IP address range, create subnets, and configure routing and access control lists.\n\nThe basic functionality of Amazon RDS is the same whether it's running in a VPC or not. Amazon RDS manages backups, software patching, automatic failure detection, and recovery. There's no additional cost to run your DB instance in a VPC. For more information on using Amazon VPC with RDS, see Amazon VPC and Amazon RDS.\n\nAmazon RDS uses Network Time Protocol (NTP) to synchronize the time on DB instances.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 712
        }
      },
      {
        "header": "AWS Regions and Availability Zones",
        "content": "Amazon cloud computing resources are housed in highly available data center facilities in different areas of the world (for example, North America, Europe, or Asia). Each data center location is called an AWS Region. With Amazon RDS, you can create your DB instances in multiple Regions.\n\nThe following scenario shows an RDS DB instance in one Region that replicates asynchronously to a standby DB instance in a different Region. If one Region becomes unavailable, the instance in the other Region is still available.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 517
        }
      },
      {
        "header": "Availability Zones",
        "content": "Each AWS Region contains multiple distinct locations called Availability Zones, or AZs. Each Availability Zone is engineered to be isolated from failures in other Availability Zones. Each is engineered to provide inexpensive, low-latency network connectivity to other Availability Zones in the same AWS Region. By launching DB instances in separate Availability Zones, you can protect your applications from the failure of a single location. For more information, see Regions, Availability Zones, and Local Zones.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 513
        }
      },
      {
        "header": "Multi-AZ deployments",
        "content": "You can run your DB instance in several Availability Zones, an option called a Multi-AZ deployment. When you choose this option, Amazon automatically provisions and maintains one or more secondary standby DB instances in a different AZ. Your primary DB instance is replicated across Availability Zones to each secondary DB instance.\n\nA Multi-AZ deployment provides the following advantages:\n\nProviding data redundancy and failover support\n\nEliminating I/O freezes\n\nMinimizing latency spikes during system backups\n\nServing read traffic on secondary DB instances (Multi-AZ DB clusters deployment only)\n\nThe following diagram depicts a Multi-AZ DB instance deployment, where Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The replica database doesn't serve read traffic.\n\nThe following diagram depicts a Multi-AZ DB cluster deployment, which has a writer DB instance and two reader DB instances in three separate Availability Zones in the same AWS Region. All three DB instances can serve read traffic.\n\nFor more information, see Configuring and managing a Multi-AZ deployment for Amazon RDS.\n\n• Providing data redundancy and failover support\n• Eliminating I/O freezes\n• Minimizing latency spikes during system backups\n• Serving read traffic on secondary DB instances (Multi-AZ DB clusters deployment only)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1370
        }
      },
      {
        "header": "Access control with security groups",
        "content": "A security group controls the access to a DB instance by allowing access to IP address ranges or Amazon EC2 instances that you specify. You can apply a security group to one or more DB instances.\n\nA common use of a DB instance in a VPC is to share data with an application server in the same VPC. The following example uses VPC security group ec2-rds-x to define inbound rules that use the IP addresses of the client application as the source. The application server belongs to this security group. A second security group named rds-ec2-x specifies ec2-rds-x as the source and attaches to an RDS DB instance. According to the security group rules, client applications can't directly access the DB instance, but the EC2 instance can access the DB instance.\n\nFor more information about security groups, see Security in Amazon RDS.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 828
        }
      },
      {
        "header": "Amazon RDS monitoring",
        "content": "Monitoring is an important part of maintaining the reliability, availability, and performance of Amazon RDS and your other AWS solutions. AWS provides various monitoring tools to watch Amazon RDS, report when something is wrong, and take automatic actions when appropriate.\n\nYou can track the performance and health of your DB instances using various automated and manual tools:\n\nView details about the current status of your instance by using the Amazon RDS console, AWS CLI, or RDS API. You can also respond to automated recommendations for database resources, such as DB instances, read replicas, and DB parameter groups. For more information, see Recommendations from Amazon RDS.\n\nYou can use the Amazon CloudWatch service to monitor the performance and health of a DB instance. CloudWatch performance charts are shown in the Amazon RDS console. Amazon RDS automatically sends metrics to CloudWatch every minute for each active database. You don't get additional charges for Amazon RDS metrics in CloudWatch.\n\nUsing Amazon CloudWatch alarms, you can watch a single Amazon RDS metric over a specific time period. You can then perform one or more actions based on the value of the metric relative to a threshold that you set. For more information, see Monitoring Amazon RDS metrics with Amazon CloudWatch.\n\nPerformance Insights assesses the load on your database, and determine when and where to take action. For more information, see Monitoring DB load with Performance Insights on Amazon RDS. Amazon RDS Enhanced Monitoring looks at metrics in real time for the operating system. For more information, see Monitoring OS metrics with Enhanced Monitoring.\n\nAmazon RDS is integrated with Amazon EventBridge, Amazon CloudWatch Logs, and Amazon DevOpsÂ Guru. For more information, see Monitoring metrics in an Amazon RDS instance.\n\n**Amazon RDS DB instance status and recommendations**: View details about the current status of your instance by using the Amazon RDS console, AWS CLI, or RDS API. You can also respond to automated recommendations for database resources, such as DB instances, read replicas, and DB parameter groups. For more information, see Recommendations from Amazon RDS.\n**Amazon CloudWatch metrics for Amazon RDS**: You can use the Amazon CloudWatch service to monitor the performance and health of a DB instance. CloudWatch performance charts are shown in the Amazon RDS console. Amazon RDS automatically sends metrics to CloudWatch every minute for each active database. You don't get additional charges for Amazon RDS metrics in CloudWatch. Using Amazon CloudWatch alarms, you can watch a single Amazon RDS metric over a specific time period. You can then perform one or more actions based on the value of the metric relative to a threshold that you set. For more information, see Monitoring Amazon RDS metrics with Amazon CloudWatch.\n**Amazon RDS Performance Insights and operating-system monitoring**: Performance Insights assesses the load on your database, and determine when and where to take action. For more information, see Monitoring DB load with Performance Insights on Amazon RDS. Amazon RDS Enhanced Monitoring looks at metrics in real time for the operating system. For more information, see Monitoring OS metrics with Enhanced Monitoring.\n**Integrated AWS services**: Amazon RDS is integrated with Amazon EventBridge, Amazon CloudWatch Logs, and Amazon DevOpsÂ Guru. For more information, see Monitoring metrics in an Amazon RDS instance.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 3475
        }
      },
      {
        "header": "User interfaces to Amazon RDS",
        "content": "You can interact with Amazon RDS in multiple ways.\n\nAWS Management Console\n\nCommand line interface\n\n• AWS Management Console\n• Command line interface\n• Amazon RDS APIs",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 167
        }
      },
      {
        "header": "AWS Management Console",
        "content": "The AWS Management Console is a simple web-based user interface. You can manage your DB instances from the console with no programming required. To access the Amazon RDS console, sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 288
        }
      },
      {
        "header": "Command line interface",
        "content": "You can use the AWS Command Line Interface (AWS CLI) to access the Amazon RDS API interactively. To install the AWS CLI, see Installing the AWS Command Line Interface. To begin using the AWS CLI for RDS, see AWS Command Line Interface reference for Amazon RDS.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 260
        }
      },
      {
        "header": "Amazon RDS APIs",
        "content": "If you are a developer, you can access the Amazon RDS programmatically using APIs. For more information, see Amazon RDS API reference.\n\nFor application development, we recommend that you use one of the AWS Software Development Kits (SDKs). The AWS SDKs handle low-level details such as authentication, retry logic, and error handling, so that you can focus on your application logic. AWS SDKs are available for a wide variety of languages. For more information, see Tools for Amazon web services .\n\nAWS also provides libraries, sample code, tutorials, and other resources to help you get started more easily. For more information, see Sample code & libraries.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 659
        }
      },
      {
        "header": "How you are charged for Amazon RDS",
        "content": "When you use Amazon RDS, you can choose to use on-demand DB instances or reserved DB instances. For more information, see DB instance billing for Amazon RDS.\n\nFor Amazon RDS pricing information, see the Amazon RDS product page.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 227
        }
      },
      {
        "header": "AWS Free Tier on Amazon RDS",
        "content": "You can use AWS Free Tier on Amazon RDS with the following engines and DB instance classes:\n\nEngine types â MariaDB, MySQL, PostgreSQL, or SQL Server Express Edition\n\nt3.micro â For all engine types\n\nt4g.micro â For all engine types except SQL Server Express Edition\n\nYou can't use the following features, resources, and actions with AWS Free Tier on Amazon RDS:\n\nAmazon RDS custom engine options\n\nReserved DB instances\n\nDeployment options other than Single-AZ\n\nCreating Aurora read replicas\n\nThese limitations are subject to change as the AWS Free Tier program evolves. For more information about AWS Free Tier, see Explore AWS services with AWS Free Tier. For more information about Amazon RDS Free Tier, see Amazon RDS Free Tier.\n\nIf you were an AWS Free Tier customer before July 17, 2025, you can continue to use Amazon RDS Free Tier until the 12 months of your free usage expires. During this time period, you remain eligible for the following usage:\n\n750 hours each month of Single-AZ db.t3.micro and db.t4g.micro instance classes running MySQL, MariaDB, or PostgreSQL on Amazon RDS\n\n750 hours each month of a db.t3.micro instance class running SQL Server Express Edition on Amazon RDS\n\nAfter July 17, 2025, any new AWS Free Tier resources that you create on AWS will use the new AWS Free Tier offering.\n\n• Engine types â MariaDB, MySQL, PostgreSQL, or SQL Server Express Edition\n• DB instance classes t3.micro â For all engine types t4g.micro â For all engine types except SQL Server Express Edition\n\n• t3.micro â For all engine types\n• t4g.micro â For all engine types except SQL Server Express Edition\n\n• Amazon RDS custom engine options\n• Reserved DB instances\n• Deployment options other than Single-AZ\n• Migrating snapshots\n• Query editor\n• Creating Aurora read replicas\n\n• 750 hours each month of Single-AZ db.t3.micro and db.t4g.micro instance classes running MySQL, MariaDB, or PostgreSQL on Amazon RDS\n• 750 hours each month of a db.t3.micro instance class running SQL Server Express Edition on Amazon RDS",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2037
        }
      },
      {
        "header": "What's next?",
        "content": "The preceding section introduced you to the basic infrastructure components that RDS offers. What should you do next?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 117
        }
      },
      {
        "header": "Getting started",
        "content": "Create a DB instance using instructions in Getting started with Amazon RDS.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 75
        }
      },
      {
        "header": "Topics specific to database engines",
        "content": "You can review information specific to a particular DB engine in the following sections:\n\nAmazon RDS for MariaDB\n\nAmazon RDS for Microsoft SQL Server\n\nAmazon RDS for MySQL\n\nAmazon RDS for Oracle\n\nAmazon RDS for PostgreSQL\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• Amazon RDS for Db2\n• Amazon RDS for MariaDB\n• Amazon RDS for Microsoft SQL Server\n• Amazon RDS for MySQL\n• Amazon RDS for Oracle\n• Amazon RDS for PostgreSQL",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 677
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html",
    "doc_type": "aws",
    "total_sections": 24
  },
  {
    "title": "Getting started with Amazon RDS",
    "summary": "Getting started with Amazon RDSIn the following examples, you can find how to create and connect to a DB instance using Amazon Relational Database Service (Amazon RDS). You can create a DB instance that uses Db2, MariaDB, MySQL, Microsoft SQL Server, Oracle, or PostgreSQL.ImportantBefore you can create or connect to a DB instance, make sure to complete the tasks in Setting up your Amazon RDS environment.Creating a DB instance and connecting to a database on a DB instance is slightly different fo",
    "sections": [
      {
        "header": "",
        "content": "In the following examples, you can find how to create and connect to a DB instance using Amazon Relational Database Service (Amazon RDS). You can create a DB instance that uses Db2, MariaDB, MySQL, Microsoft SQL Server, Oracle, or PostgreSQL.\n\nBefore you can create or connect to a DB instance, make sure to complete the tasks in Setting up your Amazon RDS environment.\n\nCreating a DB instance and connecting to a database on a DB instance is slightly different for each of the DB engines. Choose one of the following DB engines that you want to use for detailed information on creating and connecting to the DB instance. After you have created and connected to your DB instance, there are instructions to help you delete the DB instance.\n\nCreating and connecting to a MariaDB DB instance\n\nCreating and connecting to a Microsoft SQL Server DB instance\n\nCreating and connecting to a MySQL DB instance\n\nCreating and connecting to an Oracle DB instance\n\nCreating and connecting to a PostgreSQL DB instance\n\nTutorial: Create a web server and an Amazon RDS DB instance\n\nTutorial: Using a Lambda function to access an Amazon RDS database\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n• Creating and connecting to a MariaDB DB instance\n• Creating and connecting to a Microsoft SQL Server DB instance\n• Creating and connecting to a MySQL DB instance\n• Creating and connecting to an Oracle DB instance\n• Creating and connecting to a PostgreSQL DB instance\n• Tutorial: Create a web server and an Amazon RDS DB instance\n• Tutorial: Using a Lambda function to access an Amazon RDS database\n\n[Note] ImportantBefore you can create or connect to a DB instance, make sure to complete the tasks in Setting up your Amazon RDS environment.\n\n[Note] Before you can create or connect to a DB instance, make sure to complete the tasks in Setting up your Amazon RDS environment.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2105
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is IAM?",
    "summary": "What is IAM?AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. With IAM, you can manage permissions that control which AWS resources users can access. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. IAM provides the infrastructure necessary to control authentication and authorization for your AWS accounts. Identities When you create an AWS account, you begin with one sign-in",
    "sections": [
      {
        "header": "",
        "content": "AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. With IAM, you can manage permissions that control which AWS resources users can access. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. IAM provides the infrastructure necessary to control authentication and authorization for your AWS accounts.\n\nWhen you create an AWS account, you begin with one sign-in identity called the AWS account root user that has complete access to all AWS services and resources. We strongly recommend that you don't use the root user for everyday tasks. For tasks that require root user credentials, see Tasks that require root user credentials in the IAM User Guide.\n\nUse IAM to set up other identities in addition to your root user, such as administrators, analysts, and developers, and grant them access to the resources they need to succeed in their tasks.\n\nAfter a user is set up in IAM, they use their sign-in credentials to authenticate with AWS. Authentication is provided by matching the sign-in credentials to a principal (an IAM user, AWS STS federated principal, IAM role, or application) trusted by the AWS account. Next, a request is made to grant the principal access to resources. Access is granted in response to an authorization request if the user has been given permission to the resource. For example, when you first sign in to the console and are on the console Home page, you aren't accessing a specific service. When you select a service, the request for authorization is sent to that service and it looks to see if your identity is on the list of authorized users, what policies are being enforced to control the level of access granted, and any other policies that might be in effect. Authorization requests can be made by principals within your AWS account or from another AWS account that you trust.\n\nOnce authorized, the principal can take action or perform operations on resources in your AWS account. For example, the principal could launch a new Amazon Elastic Compute Cloud instance, modify IAM group membership, or delete Amazon Simple Storage Service buckets.\n\nAWS Training and Certification provides a 10-minute video introduction to IAM:\n\nIntroduction to AWS Identity and Access Management.\n\nService availability\n\nIAM, like many other AWS services, is eventually consistent. IAM achieves high availability by replicating data across multiple servers within Amazon's data centers around the world. If a request to change some data is successful, the change is committed and safely stored. However, the change must be replicated across IAM, which can take some time. Such changes include creating or updating users, groups, roles, or policies. We recommend that you do not include such IAM changes in the critical, high-availability code paths of your application. Instead, make IAM changes in a separate initialization or setup routine that you run less frequently. Also, be sure to verify that the changes have been propagated before production workflows depend on them. For more information, see Changes that I make are not always immediately visible.\n\nService cost information\n\nAWS Identity and Access Management (IAM), AWS IAM Identity Center and AWS Security Token Service (AWS STS) are features of your AWS account offered at no additional charge. You are charged only when you access other AWS services using your IAM users or AWS STS temporary security credentials.\n\nIAM Access Analyzer external access analysis is offered at no additional charge. However, you will incur charges for unused access analysis and customer policy checks. For a complete list of charges and prices for IAM Access Analyzer, see IAM Access Analyzer pricing.\n\nFor information about the pricing of other AWS products, see the Amazon Web Services pricing page.\n\nIntegration with other AWS services\n\nIAM is integrated with many AWS services. For a list of AWS services that work with IAM and the IAM features the services support, see AWS services that work with IAM.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n[Note] TipAWS Training and Certification provides a 10-minute video introduction to IAM:Introduction to AWS Identity and Access Management.\n\n[Note] AWS Training and Certification provides a 10-minute video introduction to IAM:Introduction to AWS Identity and Access Management.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 19,
          "content_length": 4661
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "Getting started with IAM",
    "summary": "Getting started with IAMAWS Identity and Access Management (IAM) helps you securely control access to Amazon Web Services (AWS) and your account resources. IAM can also keep your sign-in credentials private. You don't specifically sign up to use IAM. There is no charge to use IAM. Use IAM to give identities, such as users and roles, access to resources in your account. For example, you can use IAM with existing users in your corporate directory that you manage external to AWS or you can create u",
    "sections": [
      {
        "header": "",
        "content": "AWS Identity and Access Management (IAM) helps you securely control access to Amazon Web Services (AWS) and your account resources. IAM can also keep your sign-in credentials private. You don't specifically sign up to use IAM. There is no charge to use IAM.\n\nUse IAM to give identities, such as users and roles, access to resources in your account. For example, you can use IAM with existing users in your corporate directory that you manage external to AWS or you can create users in AWS using AWS IAM Identity Center. Federated identities assume defined IAM roles to access the resources they need. For more information about IAM Identity Center, see What is IAM Identity Center? in the AWS IAM Identity Center User Guide.\n\nIAM is integrated with several AWS products. For a list of services that support IAM, see AWS services that work with IAM.\n\nTo learn about getting started with AWS, creating an administrative user, an AWS Organizations, and using multiple services to solve a problem such as building and launching your first project, see the Getting Started Resource Center.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.\n\n[Note] NoteIAM is integrated with several AWS products. For a list of services that support IAM, see AWS services that work with IAM.\n\n[Note] IAM is integrated with several AWS products. For a list of services that support IAM, see AWS services that work with IAM.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1646
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "Security best practices in IAM",
    "summary": "Require human users to use federation with an identity provider to access AWS using temporary credentialsRequire workloads to use temporary credentials with IAM roles to access AWSRequire multi-factor authentication (MFA)Update access keys when needed for use cases that require long-term credentialsFollow best practices to protect your root user credentialsApply least-privilege permissionsGet started with AWS managed policies and move toward least-privilege permissionsUse IAM Access Analyzer to ",
    "sections": [
      {
        "header": "",
        "content": "To help secure your AWS resources, follow these best practices for AWS Identity and Access Management (IAM).\n\nRequire human users to use federation with an identity provider to access AWS using temporary credentials\n\nRequire workloads to use temporary credentials with IAM roles to access AWS\n\nRequire multi-factor authentication (MFA)\n\nUpdate access keys when needed for use cases that require long-term credentials\n\nFollow best practices to protect your root user credentials\n\nApply least-privilege permissions\n\nGet started with AWS managed policies and move toward least-privilege permissions\n\nUse IAM Access Analyzer to generate least-privilege policies based on access activity\n\nRegularly review and remove unused users, roles, permissions, policies, and credentials\n\nUse conditions in IAM policies to further restrict access\n\nVerify public and cross-account access to resources with IAM Access Analyzer\n\nUse IAM Access Analyzer to validate your IAM policies to ensure secure and functional permissions\n\nEstablish permissions guardrails across multiple accounts\n\nUse permissions boundaries to delegate permissions management within an account\n\n• Require human users to use federation with an identity provider to access AWS using temporary credentials\n• Require workloads to use temporary credentials with IAM roles to access AWS\n• Require multi-factor authentication (MFA)\n• Update access keys when needed for use cases that require long-term credentials\n• Follow best practices to protect your root user credentials\n• Apply least-privilege permissions\n• Get started with AWS managed policies and move toward least-privilege permissions\n• Use IAM Access Analyzer to generate least-privilege policies based on access activity\n• Regularly review and remove unused users, roles, permissions, policies, and credentials\n• Use conditions in IAM policies to further restrict access\n• Verify public and cross-account access to resources with IAM Access Analyzer\n• Use IAM Access Analyzer to validate your IAM policies to ensure secure and functional permissions\n• Establish permissions guardrails across multiple accounts\n• Use permissions boundaries to delegate permissions management within an account",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 2201
        }
      },
      {
        "header": "Require human users to use federation with an identity provider to access AWS using temporary credentials",
        "content": "Human users, also known as human identities, are the people, administrators, developers, operators, and consumers of your applications. They must have an identity to access your AWS environments and applications. Human users that are members of your organization are also known as workforce identities. Human users can also be external users with whom you collaborate, and who interact with your AWS resources. They can do this via a web browser, client application, mobile app, or interactive command-line tools.\n\nRequire your human users to use temporary credentials when accessing AWS. You can use an identity provider for your human users to provide federated access to AWS accounts by assuming roles, which provide temporary credentials. For centralized access management, we recommend that you use AWS IAM Identity Center (IAM Identity Center) to manage access to your accounts and permissions within those accounts. You can manage your user identities with IAM Identity Center, or manage access permissions for user identities in IAM Identity Center from an external identity provider. For more information, see What is AWS IAM Identity Center in the AWS IAM Identity Center User Guide.\n\nFor more information about roles, see Roles terms and concepts.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1258
        }
      },
      {
        "header": "Require workloads to use temporary credentials with IAM roles to access AWS",
        "content": "A workload is a collection of resources and code that delivers business value, such as an application or backend process. Your workload can have applications, operational tools, and components that require credentials to make requests to AWS services, such as requests to read data from Amazon S3.\n\nWhen you're building on an AWS compute service, such as Amazon EC2 or Lambda, AWS delivers the temporary credentials of an IAM role to that compute resource. Applications written using an AWS SDK will discover and use these temporary credentials to access AWS resources, and there is no need to distribute long lived credentials for an IAM user to your workloads running on AWS.\n\nWorkloads that run on outside of AWS, such as your on-premises servers, servers from other cloud providers, or managed continuous integration and continuous delivery (CI/CD) platforms, can still use temporary credentials. However, you'll need to deliver these temporary credentials to your workload. The following are ways you can deliver temporary credentials to your workloads:\n\nYou can use IAM Roles Anywhere to request temporary AWS credentials for your workload using an X.509 Certificate from your public key infrastructure (PKI).\n\nYou can call the AWS AWS STSAssumeRoleWithSAML API to request temporary AWS credentials for your workload using a SAML assertion from an external identity provider (IdP) that is configured within your AWS account.\n\nYou can call the AWS AWS STS AssumeRoleWithWebIdentity API to request temporary AWS credentials for your workload using a JSON web token (JWT) from an IdP that is configured within your AWS account.\n\nYou can request temporary AWS credentials from your IoT device using Mutual Transport Layer Security (MTLS) authentication using AWS IoT Core.\n\nSome AWS services also support integrations to deliver temporary credentials to your workloads running outside of AWS:\n\nAmazon Elastic Container Service (Amazon ECS) Anywhere lets you run Amazon ECS tasks on your own compute resources, and delivers temporary AWS credentials to your Amazon ECS tasks running on those compute resources.\n\nAmazon Elastic Kubernetes Service Hybrid Nodes lets you join your compute resources running outside of AWS as nodes to an Amazon EKS cluster. Amazon EKS can deliver temporary credentials to the Amazon EKS pods running on your compute resources.\n\nAWS Systems ManagerHybrid Activations lets you manage your compute resources running outside of AWS using SSM, and delivers temporary AWS credentials to the SSM agent running on your compute resources.\n\n• You can use IAM Roles Anywhere to request temporary AWS credentials for your workload using an X.509 Certificate from your public key infrastructure (PKI).\n• You can call the AWS AWS STSAssumeRoleWithSAML API to request temporary AWS credentials for your workload using a SAML assertion from an external identity provider (IdP) that is configured within your AWS account.\n• You can call the AWS AWS STS AssumeRoleWithWebIdentity API to request temporary AWS credentials for your workload using a JSON web token (JWT) from an IdP that is configured within your AWS account.\n• You can request temporary AWS credentials from your IoT device using Mutual Transport Layer Security (MTLS) authentication using AWS IoT Core.\n\n• Amazon Elastic Container Service (Amazon ECS) Anywhere lets you run Amazon ECS tasks on your own compute resources, and delivers temporary AWS credentials to your Amazon ECS tasks running on those compute resources.\n• Amazon Elastic Kubernetes Service Hybrid Nodes lets you join your compute resources running outside of AWS as nodes to an Amazon EKS cluster. Amazon EKS can deliver temporary credentials to the Amazon EKS pods running on your compute resources.\n• AWS Systems ManagerHybrid Activations lets you manage your compute resources running outside of AWS using SSM, and delivers temporary AWS credentials to the SSM agent running on your compute resources.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 3951
        }
      },
      {
        "header": "Require multi-factor authentication (MFA)",
        "content": "We recommend using IAM roles for human users and workloads that access your AWS resources so that they use temporary credentials. However, for scenarios in which you need an IAM user or root user in your account, require MFA for additional security. With MFA, users have a device that generates a response to an authentication challenge. Each user's credentials and device-generated response are required to complete the sign-in process. For more information, see AWS Multi-factor authentication in IAM.\n\nIf you use IAM Identity Center for centralized access management for human users, you can use the IAM Identity Center MFA capabilities when your identity source is configured with the IAM Identity Center identity store, AWS Managed Microsoft AD, or AD Connector. For more information about MFA in IAM Identity Center see Multi-factor authentication in the AWS IAM Identity Center User Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 896
        }
      },
      {
        "header": "Update access keys when needed for use cases that require long-term credentials",
        "content": "Where possible, we recommend relying on temporary credentials instead of creating long-term credentials such as access keys. However, for scenarios in which you need IAM users with programmatic access and long-term credentials, we recommend that you update the access keys when needed, such as when an employee leaves your company. We recommend that you use IAM access last used information to update and remove access keys safely. For more information, see Update access keys.\n\nThere are specific use cases that require long-term credentials with IAM users in AWS. Some of the use cases include the following:\n\nWorkloads that cannot use IAM roles â You might run a workload from a location that needs to access AWS. In some situations, you can't use IAM roles to provide temporary credentials, such as for WordPress plugins. In these situations, use IAM user long-term access keys for that workload to authenticate to AWS.\n\nThird-party AWS clients â If you are using tools that donât support access with IAM Identity Center, such as third-party AWS clients or vendors that are not hosted on AWS, use IAM user long-term access keys.\n\nAWS CodeCommit access â If you are using CodeCommit to store your code, you can use an IAM user with either SSH keys or service-specific credentials for CodeCommit to authenticate to your repositories. We recommend that you do this in addition to using a user in IAM Identity Center for normal authentication. Users in IAM Identity Center are the people in your workforce who need access to your AWS accounts or to your cloud applications. To give users access to your CodeCommit repositories without configuring IAM users, you can configure the git-remote-codecommit utility. For more information about IAM and CodeCommit, see IAM credentials for CodeCommit: Git credentials, SSH keys, and AWS access keys. For more information about configuring the git-remote-codecommit utility, see Connecting to AWS CodeCommit repositories with rotating credentials in the AWS CodeCommit User Guide.\n\nAmazon Keyspaces (for Apache Cassandra) access â In a situation where you are unable to use users in IAM Identity Center, such as for testing purposes for Cassandra compatibility, you can use an IAM user with service-specific credentials to authenticate with Amazon Keyspaces. Users in IAM Identity Center are the people in your workforce who need access to your AWS accounts or to your cloud applications. You can also connect to Amazon Keyspaces using temporary credentials. For more information, see Using temporary credentials to connect to Amazon Keyspaces using an IAM role and the SigV4 plugin in the Amazon Keyspaces (for Apache Cassandra) Developer Guide.\n\n• Workloads that cannot use IAM roles â You might run a workload from a location that needs to access AWS. In some situations, you can't use IAM roles to provide temporary credentials, such as for WordPress plugins. In these situations, use IAM user long-term access keys for that workload to authenticate to AWS.\n• Third-party AWS clients â If you are using tools that donât support access with IAM Identity Center, such as third-party AWS clients or vendors that are not hosted on AWS, use IAM user long-term access keys.\n• AWS CodeCommit access â If you are using CodeCommit to store your code, you can use an IAM user with either SSH keys or service-specific credentials for CodeCommit to authenticate to your repositories. We recommend that you do this in addition to using a user in IAM Identity Center for normal authentication. Users in IAM Identity Center are the people in your workforce who need access to your AWS accounts or to your cloud applications. To give users access to your CodeCommit repositories without configuring IAM users, you can configure the git-remote-codecommit utility. For more information about IAM and CodeCommit, see IAM credentials for CodeCommit: Git credentials, SSH keys, and AWS access keys. For more information about configuring the git-remote-codecommit utility, see Connecting to AWS CodeCommit repositories with rotating credentials in the AWS CodeCommit User Guide.\n• Amazon Keyspaces (for Apache Cassandra) access â In a situation where you are unable to use users in IAM Identity Center, such as for testing purposes for Cassandra compatibility, you can use an IAM user with service-specific credentials to authenticate with Amazon Keyspaces. Users in IAM Identity Center are the people in your workforce who need access to your AWS accounts or to your cloud applications. You can also connect to Amazon Keyspaces using temporary credentials. For more information, see Using temporary credentials to connect to Amazon Keyspaces using an IAM role and the SigV4 plugin in the Amazon Keyspaces (for Apache Cassandra) Developer Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 4789
        }
      },
      {
        "header": "Follow best practices to protect your root user credentials",
        "content": "When you create an AWS account, you establish root user credentials to sign in to the AWS Management Console. Safeguard your root user credentials the same way you would protect other sensitive personal information. To better understand how to secure and scale your root user processes, see Root user best practices for your AWS account.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 337
        }
      },
      {
        "header": "Apply least-privilege permissions",
        "content": "When you set permissions with IAM policies, grant only the permissions required to perform a task. You do this by defining the actions that can be taken on specific resources under specific conditions, also known as least-privilege permissions. You might start with broad permissions while you explore the permissions that are required for your workload or use case. As your use case matures, you can work to reduce the permissions that you grant to work toward least privilege. For more information about using IAM to apply permissions, see Policies and permissions in AWS Identity and Access Management.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 605
        }
      },
      {
        "header": "Get started with AWS managed policies and move toward least-privilege permissions",
        "content": "To get started granting permissions to your users and workloads, use the AWS managed policies that grant permissions for many common use cases. They are available in your AWS account. Keep in mind that AWS managed policies might not grant least-privilege permissions for your specific use cases because they are available for use by all AWS customers. As a result, we recommend that you reduce permissions further by defining customer managed policies that are specific to your use cases. For more information, see AWS managed policies. For more information about AWS managed policies that are designed for specific job functions, see AWS managed policies for job functions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 674
        }
      },
      {
        "header": "Use IAM Access Analyzer to generate least-privilege policies based on access activity",
        "content": "To grant only the permissions required to perform a task, you can generate policies based on your access activity that is logged in AWS CloudTrail. IAM Access Analyzer analyzes the services and actions that your IAM roles use, and then generates a fine-grained policy that you can use. After you test each generated policy, you can deploy the policy to your production environment. This ensures that you grant only the required permissions to your workloads. For more information about policy generation, see IAM Access Analyzer policy generation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 547
        }
      },
      {
        "header": "Regularly review and remove unused users, roles, permissions, policies, and credentials",
        "content": "You might have IAM users, roles, permissions, policies, or credentials that you no longer need in your AWS account. IAM provides last accessed information to help you identify the users, roles, permissions, policies, and credentials that you no longer need so that you can remove them. This helps you reduce the number of users, roles, permissions, policies, and credentials that you have to monitor. You can also use this information to refine your IAM policies to better adhere to least-privilege permissions. For more information, see Refine permissions in AWS using last accessed information.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 596
        }
      },
      {
        "header": "Use conditions in IAM policies to further restrict access",
        "content": "You can specify conditions under which a policy statement is in effect. That way, you can grant access to actions and resources, but only if the access request meets specific conditions. For example, you can write a policy condition to specify that all requests must be sent using TLS. You can also use conditions to grant access to service actions, but only if they are used through a specific AWS service, such as AWS CloudFormation. For more information, see IAM JSON policy elements: Condition.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 498
        }
      },
      {
        "header": "Verify public and cross-account access to resources with IAM Access Analyzer",
        "content": "Before you grant permissions for public or cross-account access in AWS, we recommend that you verify if such access is required. You can use IAM Access Analyzer to help you preview and analyze public and cross-account access for supported resource types. You do this by reviewing the findings that IAM Access Analyzer generates. These findings help you verify that your resource access controls grant the access that you expect. Additionally, as you update public and cross-account permissions, you can verify the effect of your changes before deploying new access controls to your resources. IAM Access Analyzer also monitors supported resource types continuously and generates a finding for resources that allow public or cross-account access. For more information, see Previewing access with IAM Access Analyzer APIs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 820
        }
      },
      {
        "header": "Use IAM Access Analyzer to validate your IAM policies to ensure secure and functional permissions",
        "content": "Validate the policies you create to ensure that they adhere to the IAM policy language (JSON) and IAM best practices. You can validate your policies by using IAM Access Analyzer policy validation. IAM Access Analyzer provides more than 100 policy checks and actionable recommendations to help you author secure and functional policies. As you author new policies or edit existing policies in the console, IAM Access Analyzer provides recommendations to help you refine and validate your policies before you save them. Additionally, we recommend that you review and validate all of your existing policies. For more information, see IAM Access Analyzer policy validation. For more information about policy checks provided by IAM Access Analyzer, see IAM Access Analyzer policy check reference.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 791
        }
      },
      {
        "header": "Establish permissions guardrails across multiple accounts",
        "content": "As you scale your workloads, separate them by using multiple accounts that are managed with AWS Organizations. We recommend that you use AWS Organizations service control policies (SCPs) to establish permissions guardrails to control access for all principals (IAM roles and users) across your accounts. We recommend that you use AWS Organizations resource control policies (RCPs) to establish permissions guardrails to control access for AWS resources across your organization. SCPs and RCPs are types of organization policies that you can use to manage permissions in your organization at the AWS organization, organizational unit (OU), or account level.\n\nHowever, SCPs and RCPs alone are insufficient to grant permissions to principals and resources in your organization. No permissions are granted by SCPs and RCPs. To grant permissions, you must attach identity-based or resource-based policies to IAM users, IAM roles, or the resources in your accounts. For more information, see SRA building blocks â AWS Organizations, accounts, and guardrails.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1054
        }
      },
      {
        "header": "Use permissions boundaries to delegate permissions management within an account",
        "content": "In some scenarios, you might want to delegate permissions management within an account to others. For example, you could allow developers to create and manage roles for their workloads. When you delegate permissions to others, use permissions boundaries to set the maximum permissions that you delegate. A permissions boundary is an advanced feature for using a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM role. A permissions boundary does not grant permissions on its own. For more information, see Permissions boundaries for IAM entities.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 888
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html",
    "doc_type": "aws",
    "total_sections": 15
  },
  {
    "title": "AWS Well-Architected",
    "summary": "AWS Architecture Center Technology Categories Video Series AWS Well-Architected Libraries More Resources Products› Well architected AWS Well-Architected Learn, measure, and build using architectural best practices Get started with the AWS Well-Architected Tool Overview AWS Well-Architected helps cloud architects build secure, high-performing, resilient, and efficient infrastructure for a variety of applications and workloads. Built around six pillars—operational excellence, security, reliability",
    "sections": [
      {
        "header": "",
        "content": "AWS Architecture Center\n\nLearn, measure, and build using architectural best practices\n\n• Technology Categories\n• Video Series\n• AWS Well-Architected\n• More Resources\n\n• Well architected",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 185
        }
      },
      {
        "header": "Overview",
        "content": "AWS Well-Architected helps cloud architects build secure, high-performing, resilient, and efficient infrastructure for a variety of applications and workloads. Built around six pillars—operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability—AWS Well-Architected provides a consistent approach for customers and partners to evaluate architectures and implement scalable designs.\n\nThe AWS Well-Architected Framework includes domain-specific lenses, hands-on labs, and the AWS Well-Architected Tool. The AWS Well-Architected Tool, available at no cost in the AWS Management Console, provides a mechanism for regularly evaluating workloads, identifying high-risk issues, and recording improvements.\n\nAWS also provides access to an ecosystem of hundreds of members in the AWS Well-Architected Partner Program. Engage a partner in your area to help analyze and review your applications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 933
        }
      },
      {
        "header": "Framework Overview",
        "content": "The AWS Well-Architected Framework describes key concepts, design principles, and architectural best practices for designing and running workloads in the cloud. By answering a few foundational questions, learn how well your architecture aligns with cloud best practices and gain guidance for making improvements.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 312
        }
      },
      {
        "header": "Operational Excellence Pillar",
        "content": "The operational excellence pillar focuses on running and monitoring systems, and continually improving processes and procedures. Key topics include automating changes, responding to events, and defining standards to manage daily operations.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 240
        }
      },
      {
        "header": "Security Pillar",
        "content": "The security pillar focuses on protecting information and systems. Key topics include confidentiality and integrity of data, managing user permissions, and establishing controls to detect security events.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 204
        }
      },
      {
        "header": "Reliability Pillar",
        "content": "The reliability pillar focuses on workloads performing their intended functions and how to recover quickly from failure to meet demands. Key topics include distributed system design, recovery planning, and adapting to changing requirements.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 240
        }
      },
      {
        "header": "Performance Efficiency Pillar",
        "content": "The performance efficiency pillar focuses on structured and streamlined allocation of IT and computing resources. Key topics include selecting resource types and sizes optimized for workload requirements, monitoring performance, and maintaining efficiency as business needs evol",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 278
        }
      },
      {
        "header": "Cost Optimization Pillar",
        "content": "The cost optimization pillar focuses on avoiding unnecessary costs. Key topics include understanding spending over time and controlling fund allocation, selecting resources of the right type and quantity, and scaling to meet business needs without overspending.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 261
        }
      },
      {
        "header": "Sustainability Pillar",
        "content": "The sustainability pillar focuses on minimizing the environmental impacts of running cloud workloads. Key topics include a shared responsibility model for sustainability, understanding impact, and maximizing utilization to minimize required resources and reduce downstream impacts.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 281
        }
      },
      {
        "header": "AWS Well-Architected Lenses",
        "content": "AWS Well-Architected Lenses extend the guidance offered by AWS Well-Architected to specific industry and technology domains, such as machine learning (ML), data analytics, serverless, high performance computing (HPC), IoT, SAP, streaming media, the games industry, hybrid networking, and financial services. To fully evaluate workloads, use applicable lenses together with the AWS Well-Architected Framework and its six pillars.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 428
        }
      },
      {
        "header": "AWS Well-Architected Guidance",
        "content": "Unlike the Framework and Lenses, which are aligned with all six pillars of the Well-Architected Framework, AWS Well-Architected Guidance focuses on a specific use case, technology, or implementation scenario.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 208
        }
      }
    ],
    "url": "https://aws.amazon.com/architecture/well-architected/",
    "doc_type": "aws",
    "total_sections": 11
  },
  {
    "title": "awsÂ¶",
    "summary": "DescriptionÂ¶ The AWS Command Line Interface is a unified tool to manage your AWS services.\n\nThe AWS Command Line Interface is a unified tool to manage your AWS services.",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "The AWS Command Line Interface is a unified tool to manage your AWS services.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 77
        }
      },
      {
        "header": "SynopsisÂ¶",
        "content": "Use aws command help for information on a specific command. Use aws help topics to view a list of available help topics. The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets.",
        "code_examples": [
          "```\naws[options]<command><subcommand>[parameters]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 238
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "Available ServicesÂ¶",
        "content": "• accessanalyzer\n• amplifybackend\n• amplifyuibuilder\n• apigatewaymanagementapi\n• apigatewayv2\n• appconfigdata\n• appintegrations\n• application-autoscaling\n• application-insights\n• application-signals\n• applicationcostprofiler\n• arc-region-switch\n• arc-zonal-shift\n• auditmanager\n• autoscaling\n• autoscaling-plans\n• backup-gateway\n• backupsearch\n• bcm-dashboards\n• bcm-data-exports\n• bcm-pricing-calculator\n• bcm-recommended-actions\n• bedrock-agent\n• bedrock-agent-runtime\n• bedrock-agentcore\n• bedrock-agentcore-control\n• bedrock-data-automation\n• bedrock-data-automation-runtime\n• bedrock-runtime\n• billingconductor\n• chime-sdk-identity\n• chime-sdk-media-pipelines\n• chime-sdk-meetings\n• chime-sdk-messaging\n• chime-sdk-voice\n• cleanroomsml\n• cloudcontrol\n• clouddirectory\n• cloudformation\n• cloudfront-keyvaluestore\n• cloudsearch\n• cloudsearchdomain\n• cloudtrail-data\n• codeartifact\n• codecatalyst\n• codeconnections\n• codeguru-reviewer\n• codeguru-security\n• codeguruprofiler\n• codepipeline\n• codestar-connections\n• codestar-notifications\n• cognito-identity\n• cognito-idp\n• cognito-sync\n• comprehendmedical\n• compute-optimizer\n• configservice\n• connect-contact-lens\n• connectcampaigns\n• connectcampaignsv2\n• connectcases\n• connectparticipant\n• controlcatalog\n• controltower\n• cost-optimization-hub\n• customer-profiles\n• dataexchange\n• datapipeline\n• devops-guru\n• directconnect\n• docdb-elastic\n• dynamodbstreams\n• ec2-instance-connect\n• elasticache\n• elasticbeanstalk\n• elastictranscoder\n• emr-containers\n• emr-serverless\n• entityresolution\n• finspace-data\n• forecastquery\n• frauddetector\n• gameliftstreams\n• globalaccelerator\n• greengrassv2\n• groundstation\n• identitystore\n• imagebuilder\n• importexport\n• inspector-scan\n• internetmonitor\n• iot-jobs-data\n• iot-managed-integrations\n• iotanalytics\n• iotdeviceadvisor\n• iotevents-data\n• iotfleetwise\n• iotsecuretunneling\n• iotsitewise\n• iotthingsgraph\n• iottwinmaker\n• iotwireless\n• ivs-realtime\n• kafkaconnect\n• kendra-ranking\n• keyspacesstreams\n• kinesis-video-archived-media\n• kinesis-video-media\n• kinesis-video-signaling\n• kinesis-video-webrtc-storage\n• kinesisanalytics\n• kinesisanalyticsv2\n• kinesisvideo\n• lakeformation\n• launch-wizard\n• lex-runtime\n• lexv2-models\n• lexv2-runtime\n• license-manager\n• license-manager-linux-subscriptions\n• license-manager-user-subscriptions\n• lookoutequipment\n• machinelearning\n• mailmanager\n• managedblockchain\n• managedblockchain-query\n• marketplace-agreement\n• marketplace-catalog\n• marketplace-deployment\n• marketplace-entitlement\n• marketplace-reporting\n• marketplacecommerceanalytics\n• mediaconnect\n• mediaconvert\n• mediapackage\n• mediapackage-vod\n• mediapackagev2\n• mediastore-data\n• mediatailor\n• medical-imaging\n• meteringmarketplace\n• migration-hub-refactor-spaces\n• migrationhub-config\n• migrationhuborchestrator\n• migrationhubstrategy\n• neptune-graph\n• neptunedata\n• network-firewall\n• networkflowmonitor\n• networkmanager\n• networkmonitor\n• notifications\n• notificationscontacts\n• observabilityadmin\n• opensearchserverless\n• organizations\n• partnercentral-selling\n• payment-cryptography\n• payment-cryptography-data\n• pca-connector-ad\n• pca-connector-scep\n• personalize\n• personalize-events\n• personalize-runtime\n• pinpoint-email\n• pinpoint-sms-voice\n• pinpoint-sms-voice-v2\n• redshift-data\n• redshift-serverless\n• rekognition\n• repostspace\n• resiliencehub\n• resource-explorer-2\n• resource-groups\n• resourcegroupstaggingapi\n• rolesanywhere\n• route53-recovery-cluster\n• route53-recovery-control-config\n• route53-recovery-readiness\n• route53domains\n• route53profiles\n• route53resolver\n• sagemaker-a2i-runtime\n• sagemaker-edge\n• sagemaker-featurestore-runtime\n• sagemaker-geospatial\n• sagemaker-metrics\n• sagemaker-runtime\n• savingsplans\n• secretsmanager\n• security-ir\n• securityhub\n• securitylake\n• serverlessrepo\n• service-quotas\n• servicecatalog\n• servicecatalog-appregistry\n• servicediscovery\n• simspaceweaver\n• snow-device-management\n• socialmessaging\n• ssm-contacts\n• ssm-guiconnect\n• ssm-incidents\n• ssm-quicksetup\n• stepfunctions\n• storagegateway\n• supplychain\n• support-app\n• taxsettings\n• timestream-influxdb\n• timestream-query\n• timestream-write\n• trustedadvisor\n• verifiedpermissions\n• vpc-lattice\n• waf-regional\n• wellarchitected\n• workmailmessageflow\n• workspaces-instances\n• workspaces-thin-client\n• workspaces-web",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 4326
        }
      },
      {
        "header": "See AlsoÂ¶",
        "content": "• aws help topics",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 17
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/index.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "ec2Â¶",
    "summary": "DescriptionÂ¶ You can access the features of Amazon Elastic Compute Cloud (Amazon EC2) programmatically. For more information, see the Amazon EC2 Developer Guide .\n\nYou can access the features of Amazon Elastic Compute Cloud (Amazon EC2) programmatically. For more information, see the Amazon EC2 Developer Guide .",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "You can access the features of Amazon Elastic Compute Cloud (Amazon EC2) programmatically. For more information, see the Amazon EC2 Developer Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 149
        }
      },
      {
        "header": "Available CommandsÂ¶",
        "content": "• accept-address-transfer\n• accept-capacity-reservation-billing-ownership\n• accept-reserved-instances-exchange-quote\n• accept-transit-gateway-multicast-domain-associations\n• accept-transit-gateway-peering-attachment\n• accept-transit-gateway-vpc-attachment\n• accept-vpc-endpoint-connections\n• accept-vpc-peering-connection\n• advertise-byoip-cidr\n• allocate-address\n• allocate-hosts\n• allocate-ipam-pool-cidr\n• apply-security-groups-to-client-vpn-target-network\n• assign-ipv6-addresses\n• assign-private-ip-addresses\n• assign-private-nat-gateway-address\n• associate-address\n• associate-capacity-reservation-billing-owner\n• associate-client-vpn-target-network\n• associate-dhcp-options\n• associate-enclave-certificate-iam-role\n• associate-iam-instance-profile\n• associate-instance-event-window\n• associate-ipam-byoasn\n• associate-ipam-resource-discovery\n• associate-nat-gateway-address\n• associate-route-server\n• associate-route-table\n• associate-security-group-vpc\n• associate-subnet-cidr-block\n• associate-transit-gateway-multicast-domain\n• associate-transit-gateway-policy-table\n• associate-transit-gateway-route-table\n• associate-trunk-interface\n• associate-vpc-cidr-block\n• attach-classic-link-vpc\n• attach-internet-gateway\n• attach-network-interface\n• attach-verified-access-trust-provider\n• attach-volume\n• attach-vpn-gateway\n• authorize-client-vpn-ingress\n• authorize-security-group-egress\n• authorize-security-group-ingress\n• bundle-instance\n• cancel-bundle-task\n• cancel-capacity-reservation\n• cancel-capacity-reservation-fleets\n• cancel-conversion-task\n• cancel-declarative-policies-report\n• cancel-export-task\n• cancel-image-launch-permission\n• cancel-import-task\n• cancel-reserved-instances-listing\n• cancel-spot-fleet-requests\n• cancel-spot-instance-requests\n• confirm-product-instance\n• copy-fpga-image\n• copy-snapshot\n• copy-volumes\n• create-capacity-manager-data-export\n• create-capacity-reservation\n• create-capacity-reservation-by-splitting\n• create-capacity-reservation-fleet\n• create-carrier-gateway\n• create-client-vpn-endpoint\n• create-client-vpn-route\n• create-coip-cidr\n• create-coip-pool\n• create-customer-gateway\n• create-default-subnet\n• create-default-vpc\n• create-delegate-mac-volume-ownership-task\n• create-dhcp-options\n• create-egress-only-internet-gateway\n• create-fleet\n• create-flow-logs\n• create-fpga-image\n• create-image\n• create-image-usage-report\n• create-instance-connect-endpoint\n• create-instance-event-window\n• create-instance-export-task\n• create-internet-gateway\n• create-ipam\n• create-ipam-external-resource-verification-token\n• create-ipam-pool\n• create-ipam-prefix-list-resolver\n• create-ipam-prefix-list-resolver-target\n• create-ipam-resource-discovery\n• create-ipam-scope\n• create-key-pair\n• create-launch-template\n• create-launch-template-version\n• create-local-gateway-route\n• create-local-gateway-route-table\n• create-local-gateway-route-table-virtual-interface-group-association\n• create-local-gateway-route-table-vpc-association\n• create-local-gateway-virtual-interface\n• create-local-gateway-virtual-interface-group\n• create-mac-system-integrity-protection-modification-task\n• create-managed-prefix-list\n• create-nat-gateway\n• create-network-acl\n• create-network-acl-entry\n• create-network-insights-access-scope\n• create-network-insights-path\n• create-network-interface\n• create-network-interface-permission\n• create-placement-group\n• create-public-ipv4-pool\n• create-replace-root-volume-task\n• create-reserved-instances-listing\n• create-restore-image-task\n• create-route\n• create-route-server\n• create-route-server-endpoint\n• create-route-server-peer\n• create-route-table\n• create-security-group\n• create-snapshot\n• create-snapshots\n• create-spot-datafeed-subscription\n• create-store-image-task\n• create-subnet\n• create-subnet-cidr-reservation\n• create-tags\n• create-traffic-mirror-filter\n• create-traffic-mirror-filter-rule\n• create-traffic-mirror-session\n• create-traffic-mirror-target\n• create-transit-gateway\n• create-transit-gateway-connect\n• create-transit-gateway-connect-peer\n• create-transit-gateway-multicast-domain\n• create-transit-gateway-peering-attachment\n• create-transit-gateway-policy-table\n• create-transit-gateway-prefix-list-reference\n• create-transit-gateway-route\n• create-transit-gateway-route-table\n• create-transit-gateway-route-table-announcement\n• create-transit-gateway-vpc-attachment\n• create-verified-access-endpoint\n• create-verified-access-group\n• create-verified-access-instance\n• create-verified-access-trust-provider\n• create-volume\n• create-vpc-block-public-access-exclusion\n• create-vpc-endpoint\n• create-vpc-endpoint-connection-notification\n• create-vpc-endpoint-service-configuration\n• create-vpc-peering-connection\n• create-vpn-connection\n• create-vpn-connection-route\n• create-vpn-gateway\n• delete-capacity-manager-data-export\n• delete-carrier-gateway\n• delete-client-vpn-endpoint\n• delete-client-vpn-route\n• delete-coip-cidr\n• delete-coip-pool\n• delete-customer-gateway\n• delete-dhcp-options\n• delete-egress-only-internet-gateway\n• delete-fleets\n• delete-flow-logs\n• delete-fpga-image\n• delete-image-usage-report\n• delete-instance-connect-endpoint\n• delete-instance-event-window\n• delete-internet-gateway\n• delete-ipam\n• delete-ipam-external-resource-verification-token\n• delete-ipam-pool\n• delete-ipam-prefix-list-resolver\n• delete-ipam-prefix-list-resolver-target\n• delete-ipam-resource-discovery\n• delete-ipam-scope\n• delete-key-pair\n• delete-launch-template\n• delete-launch-template-versions\n• delete-local-gateway-route\n• delete-local-gateway-route-table\n• delete-local-gateway-route-table-virtual-interface-group-association\n• delete-local-gateway-route-table-vpc-association\n• delete-local-gateway-virtual-interface\n• delete-local-gateway-virtual-interface-group\n• delete-managed-prefix-list\n• delete-nat-gateway\n• delete-network-acl\n• delete-network-acl-entry\n• delete-network-insights-access-scope\n• delete-network-insights-access-scope-analysis\n• delete-network-insights-analysis\n• delete-network-insights-path\n• delete-network-interface\n• delete-network-interface-permission\n• delete-placement-group\n• delete-public-ipv4-pool\n• delete-queued-reserved-instances\n• delete-route\n• delete-route-server\n• delete-route-server-endpoint\n• delete-route-server-peer\n• delete-route-table\n• delete-security-group\n• delete-snapshot\n• delete-spot-datafeed-subscription\n• delete-subnet\n• delete-subnet-cidr-reservation\n• delete-tags\n• delete-traffic-mirror-filter\n• delete-traffic-mirror-filter-rule\n• delete-traffic-mirror-session\n• delete-traffic-mirror-target\n• delete-transit-gateway\n• delete-transit-gateway-connect\n• delete-transit-gateway-connect-peer\n• delete-transit-gateway-multicast-domain\n• delete-transit-gateway-peering-attachment\n• delete-transit-gateway-policy-table\n• delete-transit-gateway-prefix-list-reference\n• delete-transit-gateway-route\n• delete-transit-gateway-route-table\n• delete-transit-gateway-route-table-announcement\n• delete-transit-gateway-vpc-attachment\n• delete-verified-access-endpoint\n• delete-verified-access-group\n• delete-verified-access-instance\n• delete-verified-access-trust-provider\n• delete-volume\n• delete-vpc-block-public-access-exclusion\n• delete-vpc-endpoint-connection-notifications\n• delete-vpc-endpoint-service-configurations\n• delete-vpc-endpoints\n• delete-vpc-peering-connection\n• delete-vpn-connection\n• delete-vpn-connection-route\n• delete-vpn-gateway\n• deprovision-byoip-cidr\n• deprovision-ipam-byoasn\n• deprovision-ipam-pool-cidr\n• deprovision-public-ipv4-pool-cidr\n• deregister-image\n• deregister-instance-event-notification-attributes\n• deregister-transit-gateway-multicast-group-members\n• deregister-transit-gateway-multicast-group-sources\n• describe-account-attributes\n• describe-address-transfers\n• describe-addresses\n• describe-addresses-attribute\n• describe-aggregate-id-format\n• describe-availability-zones\n• describe-aws-network-performance-metric-subscriptions\n• describe-bundle-tasks\n• describe-byoip-cidrs\n• describe-capacity-block-extension-history\n• describe-capacity-block-extension-offerings\n• describe-capacity-block-offerings\n• describe-capacity-block-status\n• describe-capacity-blocks\n• describe-capacity-manager-data-exports\n• describe-capacity-reservation-billing-requests\n• describe-capacity-reservation-fleets\n• describe-capacity-reservation-topology\n• describe-capacity-reservations\n• describe-carrier-gateways\n• describe-classic-link-instances\n• describe-client-vpn-authorization-rules\n• describe-client-vpn-connections\n• describe-client-vpn-endpoints\n• describe-client-vpn-routes\n• describe-client-vpn-target-networks\n• describe-coip-pools\n• describe-conversion-tasks\n• describe-customer-gateways\n• describe-declarative-policies-reports\n• describe-dhcp-options\n• describe-egress-only-internet-gateways\n• describe-elastic-gpus\n• describe-export-image-tasks\n• describe-export-tasks\n• describe-fast-launch-images\n• describe-fast-snapshot-restores\n• describe-fleet-history\n• describe-fleet-instances\n• describe-fleets\n• describe-flow-logs\n• describe-fpga-image-attribute\n• describe-fpga-images\n• describe-host-reservation-offerings\n• describe-host-reservations\n• describe-hosts\n• describe-iam-instance-profile-associations\n• describe-id-format\n• describe-identity-id-format\n• describe-image-attribute\n• describe-image-references\n• describe-image-usage-report-entries\n• describe-image-usage-reports\n• describe-images\n• describe-import-image-tasks\n• describe-import-snapshot-tasks\n• describe-instance-attribute\n• describe-instance-connect-endpoints\n• describe-instance-credit-specifications\n• describe-instance-event-notification-attributes\n• describe-instance-event-windows\n• describe-instance-image-metadata\n• describe-instance-status\n• describe-instance-topology\n• describe-instance-type-offerings\n• describe-instance-types\n• describe-instances\n• describe-internet-gateways\n• describe-ipam-byoasn\n• describe-ipam-external-resource-verification-tokens\n• describe-ipam-pools\n• describe-ipam-prefix-list-resolver-targets\n• describe-ipam-prefix-list-resolvers\n• describe-ipam-resource-discoveries\n• describe-ipam-resource-discovery-associations\n• describe-ipam-scopes\n• describe-ipams\n• describe-ipv6-pools\n• describe-key-pairs\n• describe-launch-template-versions\n• describe-launch-templates\n• describe-local-gateway-route-table-virtual-interface-group-associations\n• describe-local-gateway-route-table-vpc-associations\n• describe-local-gateway-route-tables\n• describe-local-gateway-virtual-interface-groups\n• describe-local-gateway-virtual-interfaces\n• describe-local-gateways\n• describe-locked-snapshots\n• describe-mac-hosts\n• describe-mac-modification-tasks\n• describe-managed-prefix-lists\n• describe-moving-addresses\n• describe-nat-gateways\n• describe-network-acls\n• describe-network-insights-access-scope-analyses\n• describe-network-insights-access-scopes\n• describe-network-insights-analyses\n• describe-network-insights-paths\n• describe-network-interface-attribute\n• describe-network-interface-permissions\n• describe-network-interfaces\n• describe-outpost-lags\n• describe-placement-groups\n• describe-prefix-lists\n• describe-principal-id-format\n• describe-public-ipv4-pools\n• describe-regions\n• describe-replace-root-volume-tasks\n• describe-reserved-instances\n• describe-reserved-instances-listings\n• describe-reserved-instances-modifications\n• describe-reserved-instances-offerings\n• describe-route-server-endpoints\n• describe-route-server-peers\n• describe-route-servers\n• describe-route-tables\n• describe-scheduled-instance-availability\n• describe-scheduled-instances\n• describe-security-group-references\n• describe-security-group-rules\n• describe-security-group-vpc-associations\n• describe-security-groups\n• describe-service-link-virtual-interfaces\n• describe-snapshot-attribute\n• describe-snapshot-tier-status\n• describe-snapshots\n• describe-spot-datafeed-subscription\n• describe-spot-fleet-instances\n• describe-spot-fleet-request-history\n• describe-spot-fleet-requests\n• describe-spot-instance-requests\n• describe-spot-price-history\n• describe-stale-security-groups\n• describe-store-image-tasks\n• describe-subnets\n• describe-tags\n• describe-traffic-mirror-filter-rules\n• describe-traffic-mirror-filters\n• describe-traffic-mirror-sessions\n• describe-traffic-mirror-targets\n• describe-transit-gateway-attachments\n• describe-transit-gateway-connect-peers\n• describe-transit-gateway-connects\n• describe-transit-gateway-multicast-domains\n• describe-transit-gateway-peering-attachments\n• describe-transit-gateway-policy-tables\n• describe-transit-gateway-route-table-announcements\n• describe-transit-gateway-route-tables\n• describe-transit-gateway-vpc-attachments\n• describe-transit-gateways\n• describe-trunk-interface-associations\n• describe-verified-access-endpoints\n• describe-verified-access-groups\n• describe-verified-access-instance-logging-configurations\n• describe-verified-access-instances\n• describe-verified-access-trust-providers\n• describe-volume-attribute\n• describe-volume-status\n• describe-volumes\n• describe-volumes-modifications\n• describe-vpc-attribute\n• describe-vpc-block-public-access-exclusions\n• describe-vpc-block-public-access-options\n• describe-vpc-classic-link\n• describe-vpc-classic-link-dns-support\n• describe-vpc-endpoint-associations\n• describe-vpc-endpoint-connection-notifications\n• describe-vpc-endpoint-connections\n• describe-vpc-endpoint-service-configurations\n• describe-vpc-endpoint-service-permissions\n• describe-vpc-endpoint-services\n• describe-vpc-endpoints\n• describe-vpc-peering-connections\n• describe-vpcs\n• describe-vpn-connections\n• describe-vpn-gateways\n• detach-classic-link-vpc\n• detach-internet-gateway\n• detach-network-interface\n• detach-verified-access-trust-provider\n• detach-volume\n• detach-vpn-gateway\n• disable-address-transfer\n• disable-allowed-images-settings\n• disable-aws-network-performance-metric-subscription\n• disable-capacity-manager\n• disable-ebs-encryption-by-default\n• disable-fast-launch\n• disable-fast-snapshot-restores\n• disable-image\n• disable-image-block-public-access\n• disable-image-deprecation\n• disable-image-deregistration-protection\n• disable-ipam-organization-admin-account\n• disable-route-server-propagation\n• disable-serial-console-access\n• disable-snapshot-block-public-access\n• disable-transit-gateway-route-table-propagation\n• disable-vgw-route-propagation\n• disable-vpc-classic-link\n• disable-vpc-classic-link-dns-support\n• disassociate-address\n• disassociate-capacity-reservation-billing-owner\n• disassociate-client-vpn-target-network\n• disassociate-enclave-certificate-iam-role\n• disassociate-iam-instance-profile\n• disassociate-instance-event-window\n• disassociate-ipam-byoasn\n• disassociate-ipam-resource-discovery\n• disassociate-nat-gateway-address\n• disassociate-route-server\n• disassociate-route-table\n• disassociate-security-group-vpc\n• disassociate-subnet-cidr-block\n• disassociate-transit-gateway-multicast-domain\n• disassociate-transit-gateway-policy-table\n• disassociate-transit-gateway-route-table\n• disassociate-trunk-interface\n• disassociate-vpc-cidr-block\n• enable-address-transfer\n• enable-allowed-images-settings\n• enable-aws-network-performance-metric-subscription\n• enable-capacity-manager\n• enable-ebs-encryption-by-default\n• enable-fast-launch\n• enable-fast-snapshot-restores\n• enable-image\n• enable-image-block-public-access\n• enable-image-deprecation\n• enable-image-deregistration-protection\n• enable-ipam-organization-admin-account\n• enable-reachability-analyzer-organization-sharing\n• enable-route-server-propagation\n• enable-serial-console-access\n• enable-snapshot-block-public-access\n• enable-transit-gateway-route-table-propagation\n• enable-vgw-route-propagation\n• enable-volume-io\n• enable-vpc-classic-link\n• enable-vpc-classic-link-dns-support\n• export-client-vpn-client-certificate-revocation-list\n• export-client-vpn-client-configuration\n• export-image\n• export-transit-gateway-routes\n• export-verified-access-instance-client-configuration\n• get-active-vpn-tunnel-status\n• get-allowed-images-settings\n• get-associated-enclave-certificate-iam-roles\n• get-associated-ipv6-pool-cidrs\n• get-aws-network-performance-data\n• get-capacity-manager-attributes\n• get-capacity-manager-metric-data\n• get-capacity-manager-metric-dimensions\n• get-capacity-reservation-usage\n• get-coip-pool-usage\n• get-console-output\n• get-console-screenshot\n• get-declarative-policies-report-summary\n• get-default-credit-specification\n• get-ebs-default-kms-key-id\n• get-ebs-encryption-by-default\n• get-flow-logs-integration-template\n• get-groups-for-capacity-reservation\n• get-host-reservation-purchase-preview\n• get-image-ancestry\n• get-image-block-public-access-state\n• get-instance-metadata-defaults\n• get-instance-tpm-ek-pub\n• get-instance-types-from-instance-requirements\n• get-instance-uefi-data\n• get-ipam-address-history\n• get-ipam-discovered-accounts\n• get-ipam-discovered-public-addresses\n• get-ipam-discovered-resource-cidrs\n• get-ipam-pool-allocations\n• get-ipam-pool-cidrs\n• get-ipam-prefix-list-resolver-rules\n• get-ipam-prefix-list-resolver-version-entries\n• get-ipam-prefix-list-resolver-versions\n• get-ipam-resource-cidrs\n• get-launch-template-data\n• get-managed-prefix-list-associations\n• get-managed-prefix-list-entries\n• get-network-insights-access-scope-analysis-findings\n• get-network-insights-access-scope-content\n• get-password-data\n• get-reserved-instances-exchange-quote\n• get-route-server-associations\n• get-route-server-propagations\n• get-route-server-routing-database\n• get-security-groups-for-vpc\n• get-serial-console-access-status\n• get-snapshot-block-public-access-state\n• get-spot-placement-scores\n• get-subnet-cidr-reservations\n• get-transit-gateway-attachment-propagations\n• get-transit-gateway-multicast-domain-associations\n• get-transit-gateway-policy-table-associations\n• get-transit-gateway-policy-table-entries\n• get-transit-gateway-prefix-list-references\n• get-transit-gateway-route-table-associations\n• get-transit-gateway-route-table-propagations\n• get-verified-access-endpoint-policy\n• get-verified-access-endpoint-targets\n• get-verified-access-group-policy\n• get-vpn-connection-device-sample-configuration\n• get-vpn-connection-device-types\n• get-vpn-tunnel-replacement-status\n• import-client-vpn-client-certificate-revocation-list\n• import-image\n• import-key-pair\n• import-snapshot\n• list-images-in-recycle-bin\n• list-snapshots-in-recycle-bin\n• lock-snapshot\n• modify-address-attribute\n• modify-availability-zone-group\n• modify-capacity-reservation\n• modify-capacity-reservation-fleet\n• modify-client-vpn-endpoint\n• modify-default-credit-specification\n• modify-ebs-default-kms-key-id\n• modify-fleet\n• modify-fpga-image-attribute\n• modify-hosts\n• modify-id-format\n• modify-identity-id-format\n• modify-image-attribute\n• modify-instance-attribute\n• modify-instance-capacity-reservation-attributes\n• modify-instance-connect-endpoint\n• modify-instance-cpu-options\n• modify-instance-credit-specification\n• modify-instance-event-start-time\n• modify-instance-event-window\n• modify-instance-maintenance-options\n• modify-instance-metadata-defaults\n• modify-instance-metadata-options\n• modify-instance-network-performance-options\n• modify-instance-placement\n• modify-ipam\n• modify-ipam-pool\n• modify-ipam-prefix-list-resolver\n• modify-ipam-prefix-list-resolver-target\n• modify-ipam-resource-cidr\n• modify-ipam-resource-discovery\n• modify-ipam-scope\n• modify-launch-template\n• modify-local-gateway-route\n• modify-managed-prefix-list\n• modify-network-interface-attribute\n• modify-private-dns-name-options\n• modify-public-ip-dns-name-options\n• modify-reserved-instances\n• modify-route-server\n• modify-security-group-rules\n• modify-snapshot-attribute\n• modify-snapshot-tier\n• modify-spot-fleet-request\n• modify-subnet-attribute\n• modify-traffic-mirror-filter-network-services\n• modify-traffic-mirror-filter-rule\n• modify-traffic-mirror-session\n• modify-transit-gateway\n• modify-transit-gateway-prefix-list-reference\n• modify-transit-gateway-vpc-attachment\n• modify-verified-access-endpoint\n• modify-verified-access-endpoint-policy\n• modify-verified-access-group\n• modify-verified-access-group-policy\n• modify-verified-access-instance\n• modify-verified-access-instance-logging-configuration\n• modify-verified-access-trust-provider\n• modify-volume\n• modify-volume-attribute\n• modify-vpc-attribute\n• modify-vpc-block-public-access-exclusion\n• modify-vpc-block-public-access-options\n• modify-vpc-endpoint\n• modify-vpc-endpoint-connection-notification\n• modify-vpc-endpoint-service-configuration\n• modify-vpc-endpoint-service-payer-responsibility\n• modify-vpc-endpoint-service-permissions\n• modify-vpc-peering-connection-options\n• modify-vpc-tenancy\n• modify-vpn-connection\n• modify-vpn-connection-options\n• modify-vpn-tunnel-certificate\n• modify-vpn-tunnel-options\n• monitor-instances\n• move-address-to-vpc\n• move-byoip-cidr-to-ipam\n• move-capacity-reservation-instances\n• provision-byoip-cidr\n• provision-ipam-byoasn\n• provision-ipam-pool-cidr\n• provision-public-ipv4-pool-cidr\n• purchase-capacity-block\n• purchase-capacity-block-extension\n• purchase-host-reservation\n• purchase-reserved-instances-offering\n• purchase-scheduled-instances\n• reboot-instances\n• register-image\n• register-instance-event-notification-attributes\n• register-transit-gateway-multicast-group-members\n• register-transit-gateway-multicast-group-sources\n• reject-capacity-reservation-billing-ownership\n• reject-transit-gateway-multicast-domain-associations\n• reject-transit-gateway-peering-attachment\n• reject-transit-gateway-vpc-attachment\n• reject-vpc-endpoint-connections\n• reject-vpc-peering-connection\n• release-address\n• release-hosts\n• release-ipam-pool-allocation\n• replace-iam-instance-profile-association\n• replace-image-criteria-in-allowed-images-settings\n• replace-network-acl-association\n• replace-network-acl-entry\n• replace-route\n• replace-route-table-association\n• replace-transit-gateway-route\n• replace-vpn-tunnel\n• report-instance-status\n• request-spot-fleet\n• request-spot-instances\n• reset-address-attribute\n• reset-ebs-default-kms-key-id\n• reset-fpga-image-attribute\n• reset-image-attribute\n• reset-instance-attribute\n• reset-network-interface-attribute\n• reset-snapshot-attribute\n• restore-address-to-classic\n• restore-image-from-recycle-bin\n• restore-managed-prefix-list-version\n• restore-snapshot-from-recycle-bin\n• restore-snapshot-tier\n• revoke-client-vpn-ingress\n• revoke-security-group-egress\n• revoke-security-group-ingress\n• run-instances\n• run-scheduled-instances\n• search-local-gateway-routes\n• search-transit-gateway-multicast-groups\n• search-transit-gateway-routes\n• send-diagnostic-interrupt\n• start-declarative-policies-report\n• start-instances\n• start-network-insights-access-scope-analysis\n• start-network-insights-analysis\n• start-vpc-endpoint-service-private-dns-verification\n• stop-instances\n• terminate-client-vpn-connections\n• terminate-instances\n• unassign-ipv6-addresses\n• unassign-private-ip-addresses\n• unassign-private-nat-gateway-address\n• unlock-snapshot\n• unmonitor-instances\n• update-capacity-manager-organizations-access\n• update-security-group-rule-descriptions-egress\n• update-security-group-rule-descriptions-ingress\n• withdraw-byoip-cidr",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 23152
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/ec2/index.html",
    "doc_type": "aws",
    "total_sections": 2
  },
  {
    "title": "run-instancesÂ¶",
    "summary": "DescriptionÂ¶ Launches the specified number of instances using an AMI for which you have permissions. You can specify a number of options, or leave the default options. The following rules apply: If you donât specify a subnet ID, we choose a default subnet from your default VPC for you. If you donât have a default VPC, you must specify a subnet ID in the request. All instances have a network interface with a primary private IPv4 address. If you donât specify this address, we choose one fro",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Launches the specified number of instances using an AMI for which you have permissions.\n\nYou can specify a number of options, or leave the default options. The following rules apply:\n\nYou can create a launch template , which is a resource that contains the parameters to launch an instance. When you launch an instance using RunInstances , you can specify the launch template instead of specifying the launch parameters.\n\nTo ensure faster instance launches, break up large requests into smaller batches. For example, create five separate launch requests for 100 instances each instead of one launch request for 500 instances.\n\nAn instance is ready for you to use when itâs in the running state. You can check the state of your instance using DescribeInstances . You can tag instances and EBS volumes during launch, after launch, or both. For more information, see CreateTags and Tagging your Amazon EC2 resources .\n\nLinux instances have access to the public key of the key pair at boot. You can use this key to provide secure access to the instance. Amazon EC2 public images use this feature to provide secure access without passwords. For more information, see Key pairs .\n\nFor troubleshooting, see What to do if an instance immediately terminates , and Troubleshooting connecting to your instance .\n\nSee also: AWS API Documentation\n\n• If you donât specify a subnet ID, we choose a default subnet from your default VPC for you. If you donât have a default VPC, you must specify a subnet ID in the request.\n• All instances have a network interface with a primary private IPv4 address. If you donât specify this address, we choose one from the IPv4 range of your subnet.\n• Not all instance types support IPv6 addresses. For more information, see Instance types .\n• If you donât specify a security group ID, we use the default security group for the VPC. For more information, see Security groups .\n• If any of the AMIs have a product code attached for which the user has not subscribed, the request fails.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2014
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--block-device-mappings (list)\n\nThe block device mapping, which defines the EBS volumes and instance store volumes to attach to the instance at launch. For more information, see Block device mappings in the Amazon EC2 User Guide .\n\nDescribes a block device mapping, which defines the EBS volumes and instance store volumes to attach to an instance at launch.\n\nParameters used to automatically set up EBS volumes when the instance is launched.\n\nDeleteOnTermination -> (boolean)\n\nThe number of I/O operations per second (IOPS). For gp3 , io1 , and io2 volumes, this represents the number of IOPS that are provisioned for the volume. For gp2 volumes, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting.\n\nThe following are the supported values for each volume type:\n\nFor io2 volumes, you can achieve up to 256,000 IOPS on instances built on the Nitro System . On other instances, you can achieve performance up to 32,000 IOPS.\n\nThis parameter is required for io1 and io2 volumes. The default for gp3 volumes is 3,000 IOPS.\n\nSnapshotId -> (string)\n\nVolumeSize -> (integer)\n\nThe size of the volume, in GiBs. You must specify either a snapshot ID or a volume size. If you specify a snapshot, the default is the snapshot size. You can specify a volume size that is equal to or larger than the snapshot size.\n\nThe following are the supported sizes for each volume type:\n\nVolumeType -> (string)\n\nThe volume type. For more information, see Amazon EBS volume types in the Amazon EBS User Guide .\n\nKmsKeyId -> (string)\n\nIdentifier (key ID, key alias, key ARN, or alias ARN) of the customer managed KMS key to use for EBS encryption.\n\nThis parameter is only supported on BlockDeviceMapping objects called by RunInstances , RequestSpotFleet , and RequestSpotInstances .\n\nThroughput -> (integer)\n\nThe throughput that the volume supports, in MiB/s.\n\nThis parameter is valid only for gp3 volumes.\n\nValid Range: Minimum value of 125. Maximum value of 2,000.\n\nOutpostArn -> (string)\n\nThe ARN of the Outpost on which the snapshot is stored.\n\nThis parameter is not supported when using CreateImage .\n\nAvailabilityZone -> (string)\n\nThe Availability Zone where the EBS volume will be created (for example, us-east-1a ).\n\nEither AvailabilityZone or AvailabilityZoneId can be specified, but not both. If neither is specified, Amazon EC2 automatically selects an Availability Zone within the Region.\n\nThis parameter is not supported when using CreateFleet , CreateImage , DescribeImages , RequestSpotFleet , RequestSpotInstances , and RunInstances .\n\nEncrypted -> (boolean)\n\nIndicates whether the encryption state of an EBS volume is changed while being restored from a backing snapshot. The effect of setting the encryption state to true depends on the volume origin (new or from a snapshot), starting encryption state, ownership, and whether encryption by default is enabled. For more information, see Amazon EBS encryption in the Amazon EBS User Guide .\n\nIn no case can you remove encryption from an encrypted volume.\n\nEncrypted volumes can only be attached to instances that support Amazon EBS encryption. For more information, see Supported instance types .\n\nThis parameter is not returned by DescribeImageAttribute .\n\nFor CreateImage and RegisterImage , whether you can include this parameter, and the allowed values differ depending on the type of block device mapping you are creating.\n\nVolumeInitializationRate -> (integer)\n\nSpecifies the Amazon EBS Provisioned Rate for Volume Initialization (volume initialization rate), in MiB/s, at which to download the snapshot blocks from Amazon S3 to the volume. This is also known as volume initialization . Specifying a volume initialization rate ensures that the volume is initialized at a predictable and consistent rate after creation. For more information, see Initialize Amazon EBS volumes in the Amazon EC2 User Guide .\n\nThis parameter is supported only for volumes created from snapshots. Omit this parameter if:\n\n• gp3 : 3,000 - 80,000 IOPS\n• io1 : 100 - 64,000 IOPS\n• io2 : 100 - 256,000 IOPS\n\n• gp2 : 1 - 16,384 GiB\n• gp3 : 1 - 65,536 GiB\n• io1 : 4 - 16,384 GiB\n• io2 : 4 - 65,536 GiB\n• st1 and sc1 : 125 - 16,384 GiB\n• standard : 1 - 1024 GiB\n\n• If you are creating a block device mapping for a new (empty) volume , you can include this parameter, and specify either true for an encrypted volume, or false for an unencrypted volume. If you omit this parameter, it defaults to false (unencrypted).\n• If you are creating a block device mapping from an existing encrypted or unencrypted snapshot , you must omit this parameter. If you include this parameter, the request will fail, regardless of the value that you specify.\n• If you are creating a block device mapping from an existing unencrypted volume , you can include this parameter, but you must specify false . If you specify true , the request will fail. In this case, we recommend that you omit the parameter.\n• If you are creating a block device mapping from an existing encrypted volume , you can include this parameter, and specify either true or false . However, if you specify false , the parameter is ignored and the block device mapping is always encrypted. In this case, we recommend that you omit the parameter.\n\n• You want to create the volume using fast snapshot restore. You must specify a snapshot that is enabled for fast snapshot restore. In this case, the volume is fully initialized at creation.\n\n[Note] NoteIf you specify a snapshot that is enabled for fast snapshot restore and a volume initialization rate, the volume will be initialized at the specified rate instead of fast snapshot restore.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 38,
          "content_length": 5679
        }
      },
      {
        "header": "Note",
        "content": "This parameter is not supported when using CreateImage and DescribeImages .\n\nValid range: 100 - 300 MiB/s\n\nAvailabilityZoneId -> (string)\n\nThe ID of the Availability Zone where the EBS volume will be created (for example, use1-az1 ).\n\nEither AvailabilityZone or AvailabilityZoneId can be specified, but not both. If neither is specified, Amazon EC2 automatically selects an Availability Zone within the Region.\n\nThis parameter is not supported when using CreateFleet , CreateImage , DescribeImages , RequestSpotFleet , RequestSpotInstances , and RunInstances .\n\nNoDevice -> (string)\n\nDeviceName -> (string)\n\nVirtualName -> (string)\n\nThe virtual device name (ephemeral N). Instance store volumes are numbered starting from 0. An instance type with 2 available instance store volumes can specify mappings for ephemeral0 and ephemeral1 . The number of available instance store volumes depends on the instance type. After you connect to the instance, you must mount the volume.\n\nNVMe instance store volumes are automatically enumerated and assigned a device name. Including them in your block device mapping has no effect.\n\nConstraints: For M3 instances, you must specify instance store volumes in the block device mapping for the instance. When you launch an M3 instance, we ignore any instance store volumes specified in the block device mapping for the AMI.\n\n--instance-type (string)\n\nThe instance type. For more information, see Amazon EC2 Instance Types Guide .\n\n--ipv6-address-count (integer)\n\nThe number of IPv6 addresses to associate with the primary network interface. Amazon EC2 chooses the IPv6 addresses from the range of your subnet. You cannot specify this option and the option to assign specific IPv6 addresses in the same request. You can specify this option if youâve specified a minimum number of instances to launch.\n\nYou cannot specify this option and the network interfaces option in the same request.\n\n--ipv6-addresses (list)\n\nThe IPv6 addresses from the range of the subnet to associate with the primary network interface. You cannot specify this option and the option to assign a number of IPv6 addresses in the same request. You cannot specify this option if youâve specified a minimum number of instances to launch.\n\nYou cannot specify this option and the network interfaces option in the same request.\n\nDescribes an IPv6 address.\n\nIpv6Address -> (string)\n\nIsPrimaryIpv6 -> (boolean)\n\n--kernel-id (string)\n\nThe ID of the kernel.\n\n• You want to create a volume that is initialized at the default rate.\n\n• c5.12xlarge\n• c5.18xlarge\n• c5.24xlarge\n• c5a.2xlarge\n• c5a.4xlarge\n• c5a.8xlarge\n• c5a.12xlarge\n• c5a.16xlarge\n• c5a.24xlarge\n• c5ad.xlarge\n• c5ad.2xlarge\n• c5ad.4xlarge\n• c5ad.8xlarge\n• c5ad.12xlarge\n• c5ad.16xlarge\n• c5ad.24xlarge\n• c5d.2xlarge\n• c5d.4xlarge\n• c5d.9xlarge\n• c5d.12xlarge\n• c5d.18xlarge\n• c5d.24xlarge\n• c5n.2xlarge\n• c5n.4xlarge\n• c5n.9xlarge\n• c5n.18xlarge\n• c6g.2xlarge\n• c6g.4xlarge\n• c6g.8xlarge\n• c6g.12xlarge\n• c6g.16xlarge\n• c6gd.medium\n• c6gd.xlarge\n• c6gd.2xlarge\n• c6gd.4xlarge\n• c6gd.8xlarge\n• c6gd.12xlarge\n• c6gd.16xlarge\n• c6gn.medium\n• c6gn.xlarge\n• c6gn.2xlarge\n• c6gn.4xlarge\n• c6gn.8xlarge\n• c6gn.12xlarge\n• c6gn.16xlarge\n• c6i.2xlarge\n• c6i.4xlarge\n• c6i.8xlarge\n• c6i.12xlarge\n• c6i.16xlarge\n• c6i.24xlarge\n• c6i.32xlarge\n• cc1.4xlarge\n• cc2.8xlarge\n• cg1.4xlarge\n• cr1.8xlarge\n• d3en.xlarge\n• d3en.2xlarge\n• d3en.4xlarge\n• d3en.6xlarge\n• d3en.8xlarge\n• d3en.12xlarge\n• dl1.24xlarge\n• f1.16xlarge\n• g3.16xlarge\n• g4ad.xlarge\n• g4ad.2xlarge\n• g4ad.4xlarge\n• g4ad.8xlarge\n• g4ad.16xlarge\n• g4dn.xlarge\n• g4dn.2xlarge\n• g4dn.4xlarge\n• g4dn.8xlarge\n• g4dn.12xlarge\n• g4dn.16xlarge\n• g5.12xlarge\n• g5.16xlarge\n• g5.24xlarge\n• g5.48xlarge\n• g5g.2xlarge\n• g5g.4xlarge\n• g5g.8xlarge\n• g5g.16xlarge\n• hi1.4xlarge\n• hpc6a.48xlarge\n• hs1.8xlarge\n• h1.16xlarge\n• i3.16xlarge\n• i3en.xlarge\n• i3en.2xlarge\n• i3en.3xlarge\n• i3en.6xlarge\n• i3en.12xlarge\n• i3en.24xlarge\n• im4gn.large\n• im4gn.xlarge\n• im4gn.2xlarge\n• im4gn.4xlarge\n• im4gn.8xlarge\n• im4gn.16xlarge\n• inf1.xlarge\n• inf1.2xlarge\n• inf1.6xlarge\n• inf1.24xlarge\n• is4gen.medium\n• is4gen.large\n• is4gen.xlarge\n• is4gen.2xlarge\n• is4gen.4xlarge\n• is4gen.8xlarge\n• m4.10xlarge\n• m4.16xlarge\n• m5.12xlarge\n• m5.16xlarge\n• m5.24xlarge\n• m5a.2xlarge\n• m5a.4xlarge\n• m5a.8xlarge\n• m5a.12xlarge\n• m5a.16xlarge\n• m5a.24xlarge\n• m5ad.xlarge\n• m5ad.2xlarge\n• m5ad.4xlarge\n• m5ad.8xlarge\n• m5ad.12xlarge\n• m5ad.16xlarge\n• m5ad.24xlarge\n• m5d.2xlarge\n• m5d.4xlarge\n• m5d.8xlarge\n• m5d.12xlarge\n• m5d.16xlarge\n• m5d.24xlarge\n• m5dn.xlarge\n• m5dn.2xlarge\n• m5dn.4xlarge\n• m5dn.8xlarge\n• m5dn.12xlarge\n• m5dn.16xlarge\n• m5dn.24xlarge\n• m5n.2xlarge\n• m5n.4xlarge\n• m5n.8xlarge\n• m5n.12xlarge\n• m5n.16xlarge\n• m5n.24xlarge\n• m5zn.xlarge\n• m5zn.2xlarge\n• m5zn.3xlarge\n• m5zn.6xlarge\n• m5zn.12xlarge\n• m6a.2xlarge\n• m6a.4xlarge\n• m6a.8xlarge\n• m6a.12xlarge\n• m6a.16xlarge\n• m6a.24xlarge\n• m6a.32xlarge\n• m6a.48xlarge\n• m6g.2xlarge\n• m6g.4xlarge\n• m6g.8xlarge\n• m6g.12xlarge\n• m6g.16xlarge\n• m6gd.medium\n• m6gd.xlarge\n• m6gd.2xlarge\n• m6gd.4xlarge\n• m6gd.8xlarge\n• m6gd.12xlarge\n• m6gd.16xlarge\n• m6i.2xlarge\n• m6i.4xlarge\n• m6i.8xlarge\n• m6i.12xlarge\n• m6i.16xlarge\n• m6i.24xlarge\n• m6i.32xlarge\n• p2.16xlarge\n• p3.16xlarge\n• p3dn.24xlarge\n• p4d.24xlarge\n• r4.16xlarge\n• r5.12xlarge\n• r5.16xlarge\n• r5.24xlarge\n• r5a.2xlarge\n• r5a.4xlarge\n• r5a.8xlarge\n• r5a.12xlarge\n• r5a.16xlarge\n• r5a.24xlarge\n• r5ad.xlarge\n• r5ad.2xlarge\n• r5ad.4xlarge\n• r5ad.8xlarge\n• r5ad.12xlarge\n• r5ad.16xlarge\n• r5ad.24xlarge\n• r5b.2xlarge\n• r5b.4xlarge\n• r5b.8xlarge\n• r5b.12xlarge\n• r5b.16xlarge\n• r5b.24xlarge\n• r5d.2xlarge\n• r5d.4xlarge\n• r5d.8xlarge\n• r5d.12xlarge\n• r5d.16xlarge\n• r5d.24xlarge\n• r5dn.xlarge\n• r5dn.2xlarge\n• r5dn.4xlarge\n• r5dn.8xlarge\n• r5dn.12xlarge\n• r5dn.16xlarge\n• r5dn.24xlarge\n• r5n.2xlarge\n• r5n.4xlarge\n• r5n.8xlarge\n• r5n.12xlarge\n• r5n.16xlarge\n• r5n.24xlarge\n• r6g.2xlarge\n• r6g.4xlarge\n• r6g.8xlarge\n• r6g.12xlarge\n• r6g.16xlarge\n• r6gd.medium\n• r6gd.xlarge\n• r6gd.2xlarge\n• r6gd.4xlarge\n• r6gd.8xlarge\n• r6gd.12xlarge\n• r6gd.16xlarge\n• r6i.2xlarge\n• r6i.4xlarge\n• r6i.8xlarge\n• r6i.12xlarge\n• r6i.16xlarge\n• r6i.24xlarge\n• r6i.32xlarge\n• t3a.2xlarge\n• t4g.2xlarge\n• u-6tb1.56xlarge\n• u-6tb1.112xlarge\n• u-9tb1.112xlarge\n• u-12tb1.112xlarge\n• u-6tb1.metal\n• u-9tb1.metal\n• u-12tb1.metal\n• u-18tb1.metal\n• u-24tb1.metal\n• vt1.3xlarge\n• vt1.6xlarge\n• vt1.24xlarge\n• x1.16xlarge\n• x1.32xlarge\n• x1e.2xlarge\n• x1e.4xlarge\n• x1e.8xlarge\n• x1e.16xlarge\n• x1e.32xlarge\n• x2iezn.2xlarge\n• x2iezn.4xlarge\n• x2iezn.6xlarge\n• x2iezn.8xlarge\n• x2iezn.12xlarge\n• x2iezn.metal\n• x2gd.medium\n• x2gd.xlarge\n• x2gd.2xlarge\n• x2gd.4xlarge\n• x2gd.8xlarge\n• x2gd.12xlarge\n• x2gd.16xlarge\n• z1d.2xlarge\n• z1d.3xlarge\n• z1d.6xlarge\n• z1d.12xlarge\n• x2idn.16xlarge\n• x2idn.24xlarge\n• x2idn.32xlarge\n• x2iedn.xlarge\n• x2iedn.2xlarge\n• x2iedn.4xlarge\n• x2iedn.8xlarge\n• x2iedn.16xlarge\n• x2iedn.24xlarge\n• x2iedn.32xlarge\n• c6a.2xlarge\n• c6a.4xlarge\n• c6a.8xlarge\n• c6a.12xlarge\n• c6a.16xlarge\n• c6a.24xlarge\n• c6a.32xlarge\n• c6a.48xlarge\n• i4i.2xlarge\n• i4i.4xlarge\n• i4i.8xlarge\n• i4i.16xlarge\n• i4i.32xlarge\n• x2idn.metal\n• x2iedn.metal\n• c7g.2xlarge\n• c7g.4xlarge\n• c7g.8xlarge\n• c7g.12xlarge\n• c7g.16xlarge\n• c6id.xlarge\n• c6id.2xlarge\n• c6id.4xlarge\n• c6id.8xlarge\n• c6id.12xlarge\n• c6id.16xlarge\n• c6id.24xlarge\n• c6id.32xlarge\n• m6id.xlarge\n• m6id.2xlarge\n• m6id.4xlarge\n• m6id.8xlarge\n• m6id.12xlarge\n• m6id.16xlarge\n• m6id.24xlarge\n• m6id.32xlarge\n• r6id.xlarge\n• r6id.2xlarge\n• r6id.4xlarge\n• r6id.8xlarge\n• r6id.12xlarge\n• r6id.16xlarge\n• r6id.24xlarge\n• r6id.32xlarge\n• r6a.2xlarge\n• r6a.4xlarge\n• r6a.8xlarge\n• r6a.12xlarge\n• r6a.16xlarge\n• r6a.24xlarge\n• r6a.32xlarge\n• r6a.48xlarge\n• p4de.24xlarge\n• u-3tb1.56xlarge\n• u-18tb1.112xlarge\n• u-24tb1.112xlarge\n• trn1.2xlarge\n• trn1.32xlarge\n• hpc6id.32xlarge\n• c6in.xlarge\n• c6in.2xlarge\n• c6in.4xlarge\n• c6in.8xlarge\n• c6in.12xlarge\n• c6in.16xlarge\n• c6in.24xlarge\n• c6in.32xlarge\n• m6in.xlarge\n• m6in.2xlarge\n• m6in.4xlarge\n• m6in.8xlarge\n• m6in.12xlarge\n• m6in.16xlarge\n• m6in.24xlarge\n• m6in.32xlarge\n• m6idn.large\n• m6idn.xlarge\n• m6idn.2xlarge\n• m6idn.4xlarge\n• m6idn.8xlarge\n• m6idn.12xlarge\n• m6idn.16xlarge\n• m6idn.24xlarge\n• m6idn.32xlarge\n• r6in.xlarge\n• r6in.2xlarge\n• r6in.4xlarge\n• r6in.8xlarge\n• r6in.12xlarge\n• r6in.16xlarge\n• r6in.24xlarge\n• r6in.32xlarge\n• r6idn.large\n• r6idn.xlarge\n• r6idn.2xlarge\n• r6idn.4xlarge\n• r6idn.8xlarge\n• r6idn.12xlarge\n• r6idn.16xlarge\n• r6idn.24xlarge\n• r6idn.32xlarge\n• m7g.2xlarge\n• m7g.4xlarge\n• m7g.8xlarge\n• m7g.12xlarge\n• m7g.16xlarge\n• r7g.2xlarge\n• r7g.4xlarge\n• r7g.8xlarge\n• r7g.12xlarge\n• r7g.16xlarge\n• m6idn.metal\n• r6idn.metal\n• inf2.xlarge\n• inf2.8xlarge\n• inf2.24xlarge\n• inf2.48xlarge\n• trn1n.32xlarge\n• i4g.2xlarge\n• i4g.4xlarge\n• i4g.8xlarge\n• i4g.16xlarge\n• hpc7g.4xlarge\n• hpc7g.8xlarge\n• hpc7g.16xlarge\n• c7gn.medium\n• c7gn.xlarge\n• c7gn.2xlarge\n• c7gn.4xlarge\n• c7gn.8xlarge\n• c7gn.12xlarge\n• c7gn.16xlarge\n• p5.48xlarge\n• m7i.2xlarge\n• m7i.4xlarge\n• m7i.8xlarge\n• m7i.12xlarge\n• m7i.16xlarge\n• m7i.24xlarge\n• m7i.48xlarge\n• m7i-flex.large\n• m7i-flex.xlarge\n• m7i-flex.2xlarge\n• m7i-flex.4xlarge\n• m7i-flex.8xlarge\n• m7a.2xlarge\n• m7a.4xlarge\n• m7a.8xlarge\n• m7a.12xlarge\n• m7a.16xlarge\n• m7a.24xlarge\n• m7a.32xlarge\n• m7a.48xlarge\n• m7a.metal-48xl\n• hpc7a.12xlarge\n• hpc7a.24xlarge\n• hpc7a.48xlarge\n• hpc7a.96xlarge\n• c7gd.medium\n• c7gd.xlarge\n• c7gd.2xlarge\n• c7gd.4xlarge\n• c7gd.8xlarge\n• c7gd.12xlarge\n• c7gd.16xlarge\n• m7gd.medium\n• m7gd.xlarge\n• m7gd.2xlarge\n• m7gd.4xlarge\n• m7gd.8xlarge\n• m7gd.12xlarge\n• m7gd.16xlarge\n• r7gd.medium\n• r7gd.xlarge\n• r7gd.2xlarge\n• r7gd.4xlarge\n• r7gd.8xlarge\n• r7gd.12xlarge\n• r7gd.16xlarge\n• r7a.2xlarge\n• r7a.4xlarge\n• r7a.8xlarge\n• r7a.12xlarge\n• r7a.16xlarge\n• r7a.24xlarge\n• r7a.32xlarge\n• r7a.48xlarge\n• c7i.2xlarge\n• c7i.4xlarge\n• c7i.8xlarge\n• c7i.12xlarge\n• c7i.16xlarge\n• c7i.24xlarge\n• c7i.48xlarge\n• mac2-m2pro.metal\n• r7iz.xlarge\n• r7iz.2xlarge\n• r7iz.4xlarge\n• r7iz.8xlarge\n• r7iz.12xlarge\n• r7iz.16xlarge\n• r7iz.32xlarge\n• c7a.2xlarge\n• c7a.4xlarge\n• c7a.8xlarge\n• c7a.12xlarge\n• c7a.16xlarge\n• c7a.24xlarge\n• c7a.32xlarge\n• c7a.48xlarge\n• c7a.metal-48xl\n• r7a.metal-48xl\n• r7i.2xlarge\n• r7i.4xlarge\n• r7i.8xlarge\n• r7i.12xlarge\n• r7i.16xlarge\n• r7i.24xlarge\n• r7i.48xlarge\n• dl2q.24xlarge\n• mac2-m2.metal\n• i4i.12xlarge\n• i4i.24xlarge\n• c7i.metal-24xl\n• c7i.metal-48xl\n• m7i.metal-24xl\n• m7i.metal-48xl\n• r7i.metal-24xl\n• r7i.metal-48xl\n• r7iz.metal-16xl\n• r7iz.metal-32xl\n• g6.12xlarge\n• g6.16xlarge\n• g6.24xlarge\n• g6.48xlarge\n• gr6.4xlarge\n• gr6.8xlarge\n• c7i-flex.large\n• c7i-flex.xlarge\n• c7i-flex.2xlarge\n• c7i-flex.4xlarge\n• c7i-flex.8xlarge\n• u7i-12tb.224xlarge\n• u7in-16tb.224xlarge\n• u7in-24tb.224xlarge\n• u7in-32tb.224xlarge\n• u7ib-12tb.224xlarge\n• r8g.2xlarge\n• r8g.4xlarge\n• r8g.8xlarge\n• r8g.12xlarge\n• r8g.16xlarge\n• r8g.24xlarge\n• r8g.48xlarge\n• r8g.metal-24xl\n• r8g.metal-48xl\n• mac2-m1ultra.metal\n• g6e.2xlarge\n• g6e.4xlarge\n• g6e.8xlarge\n• g6e.12xlarge\n• g6e.16xlarge\n• g6e.24xlarge\n• g6e.48xlarge\n• c8g.2xlarge\n• c8g.4xlarge\n• c8g.8xlarge\n• c8g.12xlarge\n• c8g.16xlarge\n• c8g.24xlarge\n• c8g.48xlarge\n• c8g.metal-24xl\n• c8g.metal-48xl\n• m8g.2xlarge\n• m8g.4xlarge\n• m8g.8xlarge\n• m8g.12xlarge\n• m8g.16xlarge\n• m8g.24xlarge\n• m8g.48xlarge\n• m8g.metal-24xl\n• m8g.metal-48xl\n• x8g.2xlarge\n• x8g.4xlarge\n• x8g.8xlarge\n• x8g.12xlarge\n• x8g.16xlarge\n• x8g.24xlarge\n• x8g.48xlarge\n• x8g.metal-24xl\n• x8g.metal-48xl\n• i7ie.xlarge\n• i7ie.2xlarge\n• i7ie.3xlarge\n• i7ie.6xlarge\n• i7ie.12xlarge\n• i7ie.18xlarge\n• i7ie.24xlarge\n• i7ie.48xlarge\n• i8g.2xlarge\n• i8g.4xlarge\n• i8g.8xlarge\n• i8g.12xlarge\n• i8g.16xlarge\n• i8g.24xlarge\n• i8g.metal-24xl\n• u7i-6tb.112xlarge\n• u7i-8tb.112xlarge\n• u7inh-32tb.480xlarge\n• p5e.48xlarge\n• p5en.48xlarge\n• f2.12xlarge\n• f2.48xlarge\n• trn2.48xlarge\n• c7i-flex.12xlarge\n• c7i-flex.16xlarge\n• m7i-flex.12xlarge\n• m7i-flex.16xlarge\n• i7ie.metal-24xl\n• i7ie.metal-48xl\n• i8g.48xlarge\n• c8gd.medium\n• c8gd.xlarge\n• c8gd.2xlarge\n• c8gd.4xlarge\n• c8gd.8xlarge\n• c8gd.12xlarge\n• c8gd.16xlarge\n• c8gd.24xlarge\n• c8gd.48xlarge\n• c8gd.metal-24xl\n• c8gd.metal-48xl\n• i7i.2xlarge\n• i7i.4xlarge\n• i7i.8xlarge\n• i7i.12xlarge\n• i7i.16xlarge\n• i7i.24xlarge\n• i7i.48xlarge\n• i7i.metal-24xl\n• i7i.metal-48xl\n• p6-b200.48xlarge\n• m8gd.medium\n• m8gd.xlarge\n• m8gd.2xlarge\n• m8gd.4xlarge\n• m8gd.8xlarge\n• m8gd.12xlarge\n• m8gd.16xlarge\n• m8gd.24xlarge\n• m8gd.48xlarge\n• m8gd.metal-24xl\n• m8gd.metal-48xl\n• r8gd.medium\n• r8gd.xlarge\n• r8gd.2xlarge\n• r8gd.4xlarge\n• r8gd.8xlarge\n• r8gd.12xlarge\n• r8gd.16xlarge\n• r8gd.24xlarge\n• r8gd.48xlarge\n• r8gd.metal-24xl\n• r8gd.metal-48xl\n• c8gn.medium\n• c8gn.xlarge\n• c8gn.2xlarge\n• c8gn.4xlarge\n• c8gn.8xlarge\n• c8gn.12xlarge\n• c8gn.16xlarge\n• c8gn.24xlarge\n• c8gn.48xlarge\n• c8gn.metal-24xl\n• c8gn.metal-48xl\n• p6e-gb200.36xlarge\n• g6f.2xlarge\n• g6f.4xlarge\n• gr6f.4xlarge\n• r8i.2xlarge\n• r8i.4xlarge\n• r8i.8xlarge\n• r8i.12xlarge\n• r8i.16xlarge\n• r8i.24xlarge\n• r8i.32xlarge\n• r8i.48xlarge\n• r8i.96xlarge\n• r8i.metal-48xl\n• r8i.metal-96xl\n• r8i-flex.large\n• r8i-flex.xlarge\n• r8i-flex.2xlarge\n• r8i-flex.4xlarge\n• r8i-flex.8xlarge\n• r8i-flex.12xlarge\n• r8i-flex.16xlarge\n• m8i.2xlarge\n• m8i.4xlarge\n• m8i.8xlarge\n• m8i.12xlarge\n• m8i.16xlarge\n• m8i.24xlarge\n• m8i.32xlarge\n• m8i.48xlarge\n• m8i.96xlarge\n• m8i.metal-48xl\n• m8i.metal-96xl\n• m8i-flex.large\n• m8i-flex.xlarge\n• m8i-flex.2xlarge\n• m8i-flex.4xlarge\n• m8i-flex.8xlarge\n• m8i-flex.12xlarge\n• m8i-flex.16xlarge\n• i8ge.xlarge\n• i8ge.2xlarge\n• i8ge.3xlarge\n• i8ge.6xlarge\n• i8ge.12xlarge\n• i8ge.18xlarge\n• i8ge.24xlarge\n• i8ge.48xlarge\n• i8ge.metal-24xl\n• i8ge.metal-48xl\n• mac-m4.metal\n• mac-m4pro.metal\n• r8gn.medium\n• r8gn.xlarge\n• r8gn.2xlarge\n• r8gn.4xlarge\n• r8gn.8xlarge\n• r8gn.12xlarge\n• r8gn.16xlarge\n• r8gn.24xlarge\n• r8gn.48xlarge\n• r8gn.metal-24xl\n• r8gn.metal-48xl\n• c8i.2xlarge\n• c8i.4xlarge\n• c8i.8xlarge\n• c8i.12xlarge\n• c8i.16xlarge\n• c8i.24xlarge\n• c8i.32xlarge\n• c8i.48xlarge\n• c8i.96xlarge\n• c8i.metal-48xl\n• c8i.metal-96xl\n• c8i-flex.large\n• c8i-flex.xlarge\n• c8i-flex.2xlarge\n• c8i-flex.4xlarge\n• c8i-flex.8xlarge\n• c8i-flex.12xlarge\n• c8i-flex.16xlarge\n• r8gb.medium\n• r8gb.xlarge\n• r8gb.2xlarge\n• r8gb.4xlarge\n• r8gb.8xlarge\n• r8gb.12xlarge\n• r8gb.16xlarge\n• r8gb.24xlarge\n• r8gb.metal-24xl\n• m8a.2xlarge\n• m8a.4xlarge\n• m8a.8xlarge\n• m8a.12xlarge\n• m8a.16xlarge\n• m8a.24xlarge\n• m8a.48xlarge\n• m8a.metal-24xl\n• m8a.metal-48xl\n• trn2.3xlarge\n• r8a.2xlarge\n• r8a.4xlarge\n• r8a.8xlarge\n• r8a.12xlarge\n• r8a.16xlarge\n• r8a.24xlarge\n• r8a.48xlarge\n• r8a.metal-24xl\n• r8a.metal-48xl\n\n[Warning] WarningWe recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon EC2 User Guide .",
        "code_examples": [
          "```\nEbs={DeleteOnTermination=boolean,Iops=integer,SnapshotId=string,VolumeSize=integer,VolumeType=string,KmsKeyId=string,Throughput=integer,OutpostArn=string,AvailabilityZone=string,Encrypted=boolean,VolumeInitializationRate=integer,AvailabilityZoneId=string},NoDevice=string,DeviceName=string,VirtualName=string...\n```",
          "```\n[{\"Ebs\":{\"DeleteOnTermination\":true|false,\"Iops\":integer,\"SnapshotId\":\"string\",\"VolumeSize\":integer,\"VolumeType\":\"standard\"|\"io1\"|\"io2\"|\"gp2\"|\"sc1\"|\"st1\"|\"gp3\",\"KmsKeyId\":\"string\",\"Throughput\":integer,\"OutpostArn\":\"string\",\"AvailabilityZone\":\"string\",\"Encrypted\":true|false,\"VolumeInitializationRate\":integer,\"AvailabilityZoneId\":\"string\"},\"NoDevice\":\"string\",\"DeviceName\":\"string\",\"VirtualName\":\"string\"}...]\n```",
          "```\nIpv6Address=string,IsPrimaryIpv6=boolean...\n```",
          "```\n[{\"Ipv6Address\":\"string\",\"IsPrimaryIpv6\":true|false}...]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 25,
          "content_length": 14645
        }
      },
      {
        "header": "Warning",
        "content": "The name of the key pair. For more information, see Create a key pair for your EC2 instance .\n\n[Warning] WarningIf you do not specify a key pair, you canât connect to the instance unless you choose an AMI that is configured to allow users another way to log in.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 263
        }
      },
      {
        "header": "Warning",
        "content": "--monitoring (structure)\n\nSpecifies whether detailed monitoring is enabled for the instance.\n\nEnabled -> (boolean) [required]\n\n--placement (structure)\n\nThe placement for the instance.\n\nAvailabilityZoneId -> (string)\n\nThe ID of the Availability Zone of the instance.\n\nOn input, you can specify AvailabilityZone or AvailabilityZoneId , but not both. If you specify neither one, Amazon EC2 automatically selects an Availability Zone for you.\n\nThis parameter is not supported for CreateFleet .\n\nAffinity -> (string)\n\nThe affinity setting for the instance on the Dedicated Host.\n\nThis parameter is not supported for CreateFleet or ImportInstance .\n\nGroupName -> (string)\n\nThe name of the placement group that the instance is in.\n\nOn input, you can specify GroupId or GroupName , but not both.\n\nPartitionNumber -> (integer)\n\nThe number of the partition that the instance is in. Valid only if the placement group strategy is set to partition .\n\nThis parameter is not supported for CreateFleet .\n\nThe ID of the Dedicated Host on which the instance resides.\n\nThis parameter is not supported for CreateFleet or ImportInstance .\n\nThe tenancy of the instance. An instance with a tenancy of dedicated runs on single-tenant hardware.\n\nThis parameter is not supported for CreateFleet . The host tenancy is not supported for ImportInstance or for T3 instances that are configured for the unlimited CPU credit option.\n\nSpreadDomain -> (string)\n\nHostResourceGroupArn -> (string)\n\nThe ARN of the host resource group in which to launch the instances.\n\nOn input, if you specify this parameter, either omit the Tenancy parameter or set it to host .\n\nThis parameter is not supported for CreateFleet .\n\nThe ID of the placement group that the instance is in.\n\nOn input, you can specify GroupId or GroupName , but not both.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone of the instance.\n\nOn input, you can specify AvailabilityZone or AvailabilityZoneId , but not both. If you specify neither one, Amazon EC2 automatically selects an Availability Zone for you.\n\nThis parameter is not supported for CreateFleet .\n\n--ramdisk-id (string)\n\nThe ID of the RAM disk to select. Some kernels require additional drivers at launch. Check the kernel requirements for information about whether you need to specify a RAM disk. To find kernel requirements, go to the Amazon Web Services Resource Center and search for the kernel ID.\n\n[Warning] WarningWe recommend that you use PV-GRUB instead of kernels and RAM disks. For more information, see PV-GRUB in the Amazon EC2 User Guide .",
        "code_examples": [
          "```\nEnabled=boolean\n```",
          "```\n{\"Enabled\":true|false}\n```",
          "```\nAvailabilityZoneId=string,Affinity=string,GroupName=string,PartitionNumber=integer,HostId=string,Tenancy=string,SpreadDomain=string,HostResourceGroupArn=string,GroupId=string,AvailabilityZone=string\n```",
          "```\n{\"AvailabilityZoneId\":\"string\",\"Affinity\":\"string\",\"GroupName\":\"string\",\"PartitionNumber\":integer,\"HostId\":\"string\",\"Tenancy\":\"default\"|\"dedicated\"|\"host\",\"SpreadDomain\":\"string\",\"HostResourceGroupArn\":\"string\",\"GroupId\":\"string\",\"AvailabilityZone\":\"string\"}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 35,
          "content_length": 2548
        }
      },
      {
        "header": "Warning",
        "content": "--security-group-ids (list)\n\nThe IDs of the security groups.\n\nIf you specify a network interface, you must specify any security groups as part of the network interface instead of using this parameter.\n\n--security-groups (list)\n\n[Default VPC] The names of the security groups.\n\nIf you specify a network interface, you must specify any security groups as part of the network interface instead of using this parameter.\n\nDefault: Amazon EC2 uses the default security group.\n\n--subnet-id (string)\n\nThe ID of the subnet to launch the instance into.\n\nIf you specify a network interface, you must specify any subnets as part of the network interface instead of using this parameter.\n\n--user-data (string)\n\n--elastic-gpu-specification (list)\n\nAn elastic GPU to associate with the instance.\n\n[Note] NoteAmazon Elastic Graphics reached end of life on January 8, 2024.",
        "code_examples": [
          "```\n\"string\"\"string\"...\n```",
          "```\n\"string\"\"string\"...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 13,
          "content_length": 856
        }
      },
      {
        "header": "Note",
        "content": "[Note] NoteAmazon Elastic Graphics reached end of life on January 8, 2024.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 74
        }
      },
      {
        "header": "Note",
        "content": "A specification for an Elastic Graphics accelerator.\n\nType -> (string) [required]\n\n--elastic-inference-accelerators (list)\n\nAn elastic inference accelerator to associate with the instance.\n\n[Note] NoteAmazon Elastic Inference is no longer available.",
        "code_examples": [
          "```\nType=string...\n```",
          "```\n[{\"Type\":\"string\"}...]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 249
        }
      },
      {
        "header": "Note",
        "content": "[Note] NoteAmazon Elastic Inference is no longer available.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 59
        }
      },
      {
        "header": "Note",
        "content": "Describes an elastic inference accelerator.\n\nType -> (string) [required]\n\nThe number of elastic inference accelerators to attach to the instance.\n\n--tag-specifications (list)\n\nThe tags to apply to the resources that are created during instance launch.\n\nYou can specify tags for the following resources only:\n\nTo tag a resource after it has been created, see CreateTags .\n\nThe tags to apply to a resource when the resource is being created. When you specify a tag, you must specify the resource type to tag, otherwise the request will fail.\n\n• Spot Instance requests\n• Network interfaces\n\n[Note] NoteThe Valid Values lists all the resource types that can be tagged. However, the action youâre using might not support tagging all of these resource types. If you try to tag a resource type that is unsupported for the action youâre using, youâll get an error.",
        "code_examples": [
          "```\nType=string,Count=integer...\n```",
          "```\n[{\"Type\":\"string\",\"Count\":integer}...]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 862
        }
      },
      {
        "header": "Note",
        "content": "ResourceType -> (string)\n\nThe type of resource to tag on creation.\n\nThe tags to apply to the resource.\n\nConstraints: Tag keys are case-sensitive and accept a maximum of 127 Unicode characters. May not begin with aws: .\n\nThe value of the tag.\n\nConstraints: Tag values are case-sensitive and accept a maximum of 256 Unicode characters.\n\n--launch-template (structure)\n\nThe launch template. Any additional parameters that you specify for the new instance overwrite the corresponding parameters included in the launch template.\n\nLaunchTemplateId -> (string)\n\nThe ID of the launch template.\n\nYou must specify either the launch template ID or the launch template name, but not both.\n\nLaunchTemplateName -> (string)\n\nThe name of the launch template.\n\nYou must specify either the launch template ID or the launch template name, but not both.\n\nThe launch template version number, $Latest , or $Default .\n\nA value of $Latest uses the latest version of the launch template.\n\nA value of $Default uses the default version of the launch template.\n\nDefault: The default version of the launch template.\n\n--instance-market-options (structure)\n\nThe market (purchasing) option for the instances.\n\nFor RunInstances , persistent Spot Instance requests are only supported when InstanceInterruptionBehavior is set to either hibernate or stop .\n\nMarketType -> (string)\n\nSpotOptions -> (structure)\n\nThe options for Spot Instances.\n\nMaxPrice -> (string)\n\nThe maximum hourly price that youâre willing to pay for a Spot Instance. We do not recommend using this parameter because it can lead to increased interruptions. If you do not specify this parameter, you will pay the current Spot price.\n\n• capacity-reservation\n• client-vpn-endpoint\n• customer-gateway\n• carrier-gateway\n• declarative-policies-report\n• dedicated-host\n• dhcp-options\n• egress-only-internet-gateway\n• elastic-gpu\n• export-image-task\n• export-instance-task\n• host-reservation\n• image-usage-report\n• import-image-task\n• import-snapshot-task\n• instance-event-window\n• internet-gateway\n• ipv4pool-ec2\n• ipv6pool-ec2\n• launch-template\n• local-gateway\n• local-gateway-route-table\n• local-gateway-virtual-interface\n• local-gateway-virtual-interface-group\n• local-gateway-route-table-vpc-association\n• local-gateway-route-table-virtual-interface-group-association\n• network-acl\n• network-interface\n• network-insights-analysis\n• network-insights-path\n• network-insights-access-scope\n• network-insights-access-scope-analysis\n• outpost-lag\n• placement-group\n• prefix-list\n• replace-root-volume-task\n• reserved-instances\n• route-table\n• security-group\n• security-group-rule\n• service-link-virtual-interface\n• spot-fleet-request\n• spot-instances-request\n• subnet-cidr-reservation\n• traffic-mirror-filter\n• traffic-mirror-session\n• traffic-mirror-target\n• transit-gateway\n• transit-gateway-attachment\n• transit-gateway-connect-peer\n• transit-gateway-multicast-domain\n• transit-gateway-policy-table\n• transit-gateway-route-table\n• transit-gateway-route-table-announcement\n• vpc-endpoint\n• vpc-endpoint-connection\n• vpc-endpoint-service\n• vpc-endpoint-service-permission\n• vpc-peering-connection\n• vpn-connection\n• vpn-gateway\n• vpc-flow-log\n• capacity-reservation-fleet\n• traffic-mirror-filter-rule\n• vpc-endpoint-connection-device-type\n• verified-access-instance\n• verified-access-group\n• verified-access-endpoint\n• verified-access-policy\n• verified-access-trust-provider\n• vpn-connection-device-type\n• vpc-block-public-access-exclusion\n• route-server\n• route-server-endpoint\n• route-server-peer\n• ipam-resource-discovery\n• ipam-resource-discovery-association\n• instance-connect-endpoint\n• verified-access-endpoint-target\n• ipam-external-resource-verification-token\n• capacity-block\n• mac-modification-task\n• ipam-prefix-list-resolver\n• ipam-prefix-list-resolver-target\n• capacity-manager-data-export\n\n• capacity-block\n\n[Warning] WarningIf you specify a maximum price, your Spot Instances will be interrupted more frequently than if you do not specify this parameter. If you specify a maximum price, it must be more than USD $0.001. Specifying a value below USD $0.001 will result in an InvalidParameterValue error message.",
        "code_examples": [
          "```\nResourceType=string,Tags=[{Key=string,Value=string},{Key=string,Value=string}]...\n```",
          "```\n[{\"ResourceType\":\"capacity-reservation\"|\"client-vpn-endpoint\"|\"customer-gateway\"|\"carrier-gateway\"|\"coip-pool\"|\"declarative-policies-report\"|\"dedicated-host\"|\"dhcp-options\"|\"egress-only-internet-gateway\"|\"elastic-ip\"|\"elastic-gpu\"|\"export-image-task\"|\"export-instance-task\"|\"fleet\"|\"fpga-image\"|\"host-reservation\"|\"image\"|\"image-usage-report\"|\"import-image-task\"|\"import-snapshot-task\"|\"instance\"|\"instance-event-window\"|\"internet-gateway\"|\"ipam\"|\"ipam-pool\"|\"ipam-scope\"|\"ipv4pool-ec2\"|\"ipv6pool-ec2\"|\"key-pair\"|\"launch-template\"|\"local-gateway\"|\"local-gateway-route-table\"|\"local-gateway-virtual-interface\"|\"local-gateway-virtual-interface-group\"|\"local-gateway-route-table-vpc-association\"|\"local-gateway-route-table-virtual-interface-group-association\"|\"natgateway\"|\"network-acl\"|\"network-interface\"|\"network-insights-analysis\"|\"network-insights-path\"|\"network-insights-access-scope\"|\"network-insights-access-scope-analysis\"|\"outpost-lag\"|\"placement-group\"|\"prefix-list\"|\"replace-root-volume-task\"|\"reserved-instances\"|\"route-table\"|\"security-group\"|\"security-group-rule\"|\"service-link-virtual-interface\"|\"snapshot\"|\"spot-fleet-request\"|\"spot-instances-request\"|\"subnet\"|\"subnet-cidr-reservation\"|\"traffic-mirror-filter\"|\"traffic-mirror-session\"|\"traffic-mirror-target\"|\"transit-gateway\"|\"transit-gateway-attachment\"|\"transit-gateway-connect-peer\"|\"transit-gateway-multicast-domain\"|\"transit-gateway-policy-table\"|\"transit-gateway-route-table\"|\"transit-gateway-route-table-announcement\"|\"volume\"|\"vpc\"|\"vpc-endpoint\"|\"vpc-endpoint-connection\"|\"vpc-endpoint-service\"|\"vpc-endpoint-service-permission\"|\"vpc-peering-connection\"|\"vpn-connection\"|\"vpn-gateway\"|\"vpc-flow-log\"|\"capacity-reservation-fleet\"|\"traffic-mirror-filter-rule\"|\"vpc-endpoint-connection-device-type\"|\"verified-access-instance\"|\"verified-access-group\"|\"verified-access-endpoint\"|\"verified-access-policy\"|\"verified-access-trust-provider\"|\"vpn-connection-device-type\"|\"vpc-block-public-access-exclusion\"|\"route-server\"|\"route-server-endpoint\"|\"route-server-peer\"|\"ipam-resource-discovery\"|\"ipam-resource-discovery-association\"|\"instance-connect-endpoint\"|\"verified-access-endpoint-target\"|\"ipam-external-resource-verification-token\"|\"capacity-block\"|\"mac-modification-task\"|\"ipam-prefix-list-resolver\"|\"ipam-prefix-list-resolver-target\"|\"capacity-manager-data-export\",\"Tags\":[{\"Key\":\"string\",\"Value\":\"string\"}...]}...]\n```",
          "```\nLaunchTemplateId=string,LaunchTemplateName=string,Version=string\n```",
          "```\n{\"LaunchTemplateId\":\"string\",\"LaunchTemplateName\":\"string\",\"Version\":\"string\"}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 26,
          "content_length": 4153
        }
      },
      {
        "header": "Warning",
        "content": "If you specify a maximum price, your Spot Instances will be interrupted more frequently than if you do not specify this parameter.\n\nIf you specify a maximum price, it must be more than USD $0.001. Specifying a value below USD $0.001 will result in an InvalidParameterValue error message.\n\nSpotInstanceType -> (string)\n\nThe Spot Instance request type. For RunInstances , persistent Spot Instance requests are only supported when the instance interruption behavior is either hibernate or stop .\n\nBlockDurationMinutes -> (integer)\n\nValidUntil -> (timestamp)\n\nThe end date of the request, in UTC format (YYYY -MM -DD T*HH* :MM :SS Z). Supported only for persistent requests.\n\nInstanceInterruptionBehavior -> (string)\n\nThe behavior when a Spot Instance is interrupted.\n\nIf Configured (for ` HibernationOptions https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_HibernationOptionsRequest.html`__ ) is set to true , the InstanceInterruptionBehavior parameter is automatically set to hibernate . If you set it to stop or terminate , youâll get an error.\n\nIf Configured (for ` HibernationOptions https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_HibernationOptionsRequest.html`__ ) is set to false or null , the InstanceInterruptionBehavior parameter is automatically set to terminate . You can also set it to stop or hibernate .\n\nFor more information, see Interruption behavior in the Amazon EC2 User Guide .\n\n--credit-specification (structure)\n\nThe credit option for CPU usage of the burstable performance instance. Valid values are standard and unlimited . To change this attribute after launch, use ModifyInstanceCreditSpecification . For more information, see Burstable performance instances in the Amazon EC2 User Guide .\n\nDefault: standard (T2 instances) or unlimited (T3/T3a/T4g instances)\n\nFor T3 instances with host tenancy, only standard is supported.\n\nCpuCredits -> (string) [required]\n\nThe credit option for CPU usage of a T instance.\n\nValid values: standard | unlimited\n\n--cpu-options (structure)\n\nThe CPU options for the instance. For more information, see Optimize CPU options in the Amazon EC2 User Guide .\n\nCoreCount -> (integer)\n\nThreadsPerCore -> (integer)\n\nAmdSevSnp -> (string)\n\nIndicates whether to enable the instance for AMD SEV-SNP. AMD SEV-SNP is supported with M6a, R6a, and C6a instance types only. For more information, see AMD SEV-SNP .\n\n--capacity-reservation-specification (structure)\n\nInformation about the Capacity Reservation targeting option. If you do not specify this parameter, the instanceâs Capacity Reservation preference defaults to open , which enables it to run in any open Capacity Reservation that has matching attributes (instance type, platform, Availability Zone, and tenancy).\n\nCapacityReservationPreference -> (string)\n\nIndicates the instanceâs Capacity Reservation preferences. Possible preferences include:\n\nCapacityReservationTarget -> (structure)\n\nInformation about the target Capacity Reservation or Capacity Reservation group.\n\nCapacityReservationId -> (string)\n\nCapacityReservationResourceGroupArn -> (string)\n\n--hibernation-options (structure)\n\nIndicates whether an instance is enabled for hibernation. This parameter is valid only if the instance meets the hibernation prerequisites . For more information, see Hibernate your Amazon EC2 instance in the Amazon EC2 User Guide .\n\nYou canât enable hibernation and Amazon Web Services Nitro Enclaves on the same instance.\n\nConfigured -> (boolean)\n\nSet to true to enable your instance for hibernation.\n\nFor Spot Instances, if you set Configured to true , either omit the InstanceInterruptionBehavior parameter (for ` SpotMarketOptions https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_SpotMarketOptions.html`__ ), or set it to hibernate . When Configured is true:\n\n--license-specifications (list)\n\nThe license configurations.\n\nDescribes a license configuration.\n\nLicenseConfigurationArn -> (string)\n\n--metadata-options (structure)\n\nThe metadata options for the instance. For more information, see Configure the Instance Metadata Service options .\n\nHttpTokens -> (string)\n\nIndicates whether IMDSv2 is required.\n\nThe default value can also be affected by other combinations of parameters. For more information, see Order of precedence for instance metadata options in the Amazon EC2 User Guide .\n\nHttpPutResponseHopLimit -> (integer)\n\nThe maximum number of hops that the metadata token can travel.\n\nPossible values: Integers from 1 to 64\n\nHttpEndpoint -> (string)\n\nEnables or disables the HTTP metadata endpoint on your instances.\n\nIf you specify a value of disabled , you cannot access your instance metadata.\n\nHttpProtocolIpv6 -> (string)\n\nEnables or disables the IPv6 endpoint for the instance metadata service.\n\nInstanceMetadataTags -> (string)\n\nSet to enabled to allow access to instance tags from the instance metadata. Set to disabled to turn off access to instance tags from the instance metadata. For more information, see Work with instance tags using the instance metadata .\n\n--enclave-options (structure)\n\nIndicates whether the instance is enabled for Amazon Web Services Nitro Enclaves. For more information, see Amazon Web Services Nitro Enclaves User Guide .\n\nYou canât enable Amazon Web Services Nitro Enclaves and hibernation on the same instance.\n\nEnabled -> (boolean)\n\n--private-dns-name-options (structure)\n\nThe options for the instance hostname. The default values are inherited from the subnet. Applies only if creating a network interface, not attaching an existing one.\n\nHostnameType -> (string)\n\nThe type of hostname for EC2 instances. For IPv4 only subnets, an instance DNS name must be based on the instance IPv4 address. For IPv6 only subnets, an instance DNS name must be based on the instance ID. For dual-stack subnets, you can specify whether DNS names use the instance IPv4 address or the instance ID.\n\nEnableResourceNameDnsARecord -> (boolean)\n\nEnableResourceNameDnsAAAARecord -> (boolean)\n\n--maintenance-options (structure)\n\nThe maintenance and recovery options for the instance.\n\nAutoRecovery -> (string)\n\nDisables the automatic recovery behavior of your instance or sets it to default. For more information, see Simplified automatic recovery .\n\n--disable-api-stop | --no-disable-api-stop (boolean)\n\n--enable-primary-ipv6 | --no-enable-primary-ipv6 (boolean)\n\n--network-performance-options (structure)\n\nContains settings for the network performance options for the instance.\n\nBandwidthWeighting -> (string)\n\nSpecify the bandwidth weighting option to boost the associated type of baseline bandwidth, as follows:\n\nThis option uses the standard bandwidth configuration for your instance type.\n\nThis option boosts your networking baseline bandwidth and reduces your EBS baseline bandwidth.\n\nThis option boosts your EBS baseline bandwidth and reduces your networking baseline bandwidth.\n\n--operator (structure)\n\nReserved for internal use.\n\nPrincipal -> (string)\n\n--dry-run | --no-dry-run (boolean)\n\n--disable-api-termination | --enable-api-termination (boolean)\n\n--instance-initiated-shutdown-behavior (string)\n\nIndicates whether an instance stops or terminates when you initiate shutdown from the instance (using the operating system command for system shutdown).\n\n--private-ip-address (string)\n\nThe primary IPv4 address. You must specify a value from the IPv4 address range of the subnet.\n\nOnly one private IP address can be designated as primary. You canât specify this option if youâve specified the option to designate a private IP address as the primary IP address in a network interface specification. You cannot specify this option if youâre launching more than one instance in the request.\n\nYou cannot specify this option and the network interfaces option in the same request.\n\n--client-token (string)\n\nUnique, case-sensitive identifier you provide to ensure the idempotency of the request. If you do not specify a client token, a randomly generated token is used for the request to ensure idempotency.\n\nFor more information, see Ensuring idempotency in Amazon EC2 API requests .\n\nConstraints: Maximum 64 ASCII characters\n\n--additional-info (string)\n\n--network-interfaces (list)\n\nThe network interfaces to associate with the instance.\n\nDescribes a network interface.\n\nAssociatePublicIpAddress -> (boolean)\n\nIndicates whether to assign a public IPv4 address to an instance you launch in a VPC. The public IP address can only be assigned to a network interface for eth0, and can only be assigned to a new network interface, not an existing one. You cannot specify more than one network interface in the request. If launching into a default subnet, the default value is true .\n\nAmazon Web Services charges for all public IPv4 addresses, including public IPv4 addresses associated with running instances and Elastic IP addresses. For more information, see the Public IPv4 Address tab on the Amazon VPC pricing page .\n\nDeleteOnTermination -> (boolean)\n\nDescription -> (string)\n\nDeviceIndex -> (integer)\n\nThe position of the network interface in the attachment order. A primary network interface has a device index of 0.\n\nIf you specify a network interface when launching an instance, you must specify the device index.\n\nThe IDs of the security groups for the network interface. Applies only if creating a network interface when launching an instance.\n\nIpv6AddressCount -> (integer)\n\nIpv6Addresses -> (list)\n\nThe IPv6 addresses to assign to the network interface. You cannot specify this option and the option to assign a number of IPv6 addresses in the same request. You cannot specify this option if youâve specified a minimum number of instances to launch.\n\nDescribes an IPv6 address.\n\nIpv6Address -> (string)\n\nIsPrimaryIpv6 -> (boolean)\n\nNetworkInterfaceId -> (string)\n\nThe ID of the network interface.\n\nIf you are creating a Spot Fleet, omit this parameter because you canât specify a network interface ID in a launch specification.\n\nPrivateIpAddress -> (string)\n\nPrivateIpAddresses -> (list)\n\nThe private IPv4 addresses to assign to the network interface. Only one private IPv4 address can be designated as primary. You cannot specify this option if youâre launching more than one instance in a RunInstances request.\n\nDescribes a secondary private IPv4 address for a network interface.\n\nPrimary -> (boolean)\n\nPrivateIpAddress -> (string)\n\nSecondaryPrivateIpAddressCount -> (integer)\n\nSubnetId -> (string)\n\nAssociateCarrierIpAddress -> (boolean)\n\nIndicates whether to assign a carrier IP address to the network interface.\n\nYou can only assign a carrier IP address to a network interface that is in a subnet in a Wavelength Zone. For more information about carrier IP addresses, see Carrier IP address in the Amazon Web Services Wavelength Developer Guide .\n\nInterfaceType -> (string)\n\nThe type of network interface.\n\nIf you specify efa-only , do not assign any IP addresses to the network interface. EFA-only network interfaces do not support IP addresses.\n\nValid values: interface | efa | efa-only\n\nNetworkCardIndex -> (integer)\n\nThe index of the network card. Some instance types support multiple network cards. The primary network interface must be assigned to network card index 0. The default is network card index 0.\n\nIf you are using RequestSpotInstances to create Spot Instances, omit this parameter because you canât specify the network card index when using this API. To specify the network card index, use RunInstances .\n\nIpv4Prefixes -> (list)\n\nThe IPv4 delegated prefixes to be assigned to the network interface. You cannot use this option if you use the Ipv4PrefixCount option.\n\nDescribes the IPv4 prefix option for a network interface.\n\nIpv4Prefix -> (string)\n\nIpv4PrefixCount -> (integer)\n\nIpv6Prefixes -> (list)\n\nThe IPv6 delegated prefixes to be assigned to the network interface. You cannot use this option if you use the Ipv6PrefixCount option.\n\nDescribes the IPv6 prefix option for a network interface.\n\nIpv6Prefix -> (string)\n\nIpv6PrefixCount -> (integer)\n\nPrimaryIpv6 -> (boolean)\n\nEnaSrdSpecification -> (structure)\n\nSpecifies the ENA Express settings for the network interface thatâs attached to the instance.\n\nEnaSrdEnabled -> (boolean)\n\nEnaSrdUdpSpecification -> (structure)\n\nContains ENA Express settings for UDP network traffic for the network interface attached to the instance.\n\nEnaSrdUdpEnabled -> (boolean)\n\nConnectionTrackingSpecification -> (structure)\n\nA security group connection tracking specification that enables you to set the timeout for connection tracking on an Elastic network interface. For more information, see Connection tracking timeouts in the Amazon EC2 User Guide .\n\nTcpEstablishedTimeout -> (integer)\n\nUdpStreamTimeout -> (integer)\n\nUdpTimeout -> (integer)\n\nEnaQueueCount -> (integer)\n\n--iam-instance-profile (structure)\n\nThe name or Amazon Resource Name (ARN) of an IAM instance profile.\n\n--ebs-optimized | --no-ebs-optimized (boolean)\n\nIndicates whether the instance is optimized for Amazon EBS I/O. This optimization provides dedicated throughput to Amazon EBS and an optimized configuration stack to provide optimal Amazon EBS I/O performance. This optimization isnât available with all instance types. Additional usage charges apply when using an EBS-optimized instance.\n\n--secondary-private-ip-addresses (string) [EC2-VPC] A secondary private IP address for the network interface or instance. You can specify this multiple times to assign multiple secondary IP addresses. If you want additional private IP addresses but do not need a specific address, use the âsecondary-private-ip-address-count option.\n\n--secondary-private-ip-address-count (string) [EC2-VPC] The number of secondary IP addresses to assign to the network interface or instance.\n\n--associate-public-ip-address | --no-associate-public-ip-address (boolean) [EC2-VPC] If specified a public IP address will be assigned to the new instance in a VPC.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command. The generated JSON skeleton is not stable between versions of the AWS CLI and there are no backwards compatibility guarantees in the JSON skeleton generated.\n\n• For a persistent request, the request remains active until the ValidUntil date and time is reached. Otherwise, the request remains active until you cancel it.\n• For a one-time request, ValidUntil is not supported. The request remains active until all instances launch or you cancel the request.\n\n• capacity-reservations-only - The instance will only run in a Capacity Reservation or Capacity Reservation group. If capacity isnât available, the instance will fail to launch.\n• open - The instance can run in any open Capacity Reservation that has matching attributes (instance type, platform, Availability Zone, and tenancy). If capacity isnât available, the instance runs as an On-Demand Instance.\n• none - The instance doesnât run in a Capacity Reservation even if one is available. The instance runs as an On-Demand Instance.\n\n• capacity-reservations-only\n\n• If you omit InstanceInterruptionBehavior , it defaults to hibernate .\n• If you set InstanceInterruptionBehavior to a value other than hibernate , youâll get an error.\n\n• optional - IMDSv2 is optional, which means that you can use either IMDSv2 or IMDSv1.\n• required - IMDSv2 is required, which means that IMDSv1 is disabled, and you must use IMDSv2.\n\n• If the value of ImdsSupport for the Amazon Machine Image (AMI) for your instance is v2.0 and the account level default is set to no-preference , the default is required .\n• If the value of ImdsSupport for the Amazon Machine Image (AMI) for your instance is v2.0 , but the account level default is set to V1 or V2 , the default is optional .\n\n• resource-name",
        "code_examples": [
          "```\nMarketType=string,SpotOptions={MaxPrice=string,SpotInstanceType=string,BlockDurationMinutes=integer,ValidUntil=timestamp,InstanceInterruptionBehavior=string}\n```",
          "```\n{\"MarketType\":\"spot\"|\"capacity-block\",\"SpotOptions\":{\"MaxPrice\":\"string\",\"SpotInstanceType\":\"one-time\"|\"persistent\",\"BlockDurationMinutes\":integer,\"ValidUntil\":timestamp,\"InstanceInterruptionBehavior\":\"hibernate\"|\"stop\"|\"terminate\"}}\n```",
          "```\nCpuCredits=string\n```",
          "```\n{\"CpuCredits\":\"string\"}\n```",
          "```\nCoreCount=integer,ThreadsPerCore=integer,AmdSevSnp=string\n```",
          "```\n{\"CoreCount\":integer,\"ThreadsPerCore\":integer,\"AmdSevSnp\":\"enabled\"|\"disabled\"}\n```",
          "```\nCapacityReservationPreference=string,CapacityReservationTarget={CapacityReservationId=string,CapacityReservationResourceGroupArn=string}\n```",
          "```\n{\"CapacityReservationPreference\":\"capacity-reservations-only\"|\"open\"|\"none\",\"CapacityReservationTarget\":{\"CapacityReservationId\":\"string\",\"CapacityReservationResourceGroupArn\":\"string\"}}\n```",
          "```\nConfigured=boolean\n```",
          "```\n{\"Configured\":true|false}\n```",
          "```\nLicenseConfigurationArn=string...\n```",
          "```\n[{\"LicenseConfigurationArn\":\"string\"}...]\n```",
          "```\nHttpTokens=string,HttpPutResponseHopLimit=integer,HttpEndpoint=string,HttpProtocolIpv6=string,InstanceMetadataTags=string\n```",
          "```\n{\"HttpTokens\":\"optional\"|\"required\",\"HttpPutResponseHopLimit\":integer,\"HttpEndpoint\":\"disabled\"|\"enabled\",\"HttpProtocolIpv6\":\"disabled\"|\"enabled\",\"InstanceMetadataTags\":\"disabled\"|\"enabled\"}\n```",
          "```\nEnabled=boolean\n```",
          "```\n{\"Enabled\":true|false}\n```",
          "```\nHostnameType=string,EnableResourceNameDnsARecord=boolean,EnableResourceNameDnsAAAARecord=boolean\n```",
          "```\n{\"HostnameType\":\"ip-name\"|\"resource-name\",\"EnableResourceNameDnsARecord\":true|false,\"EnableResourceNameDnsAAAARecord\":true|false}\n```",
          "```\nAutoRecovery=string\n```",
          "```\n{\"AutoRecovery\":\"disabled\"|\"default\"}\n```",
          "```\nBandwidthWeighting=string\n```",
          "```\n{\"BandwidthWeighting\":\"default\"|\"vpc-1\"|\"ebs-1\"}\n```",
          "```\nPrincipal=string\n```",
          "```\n{\"Principal\":\"string\"}\n```",
          "```\nAssociatePublicIpAddress=boolean,DeleteOnTermination=boolean,Description=string,DeviceIndex=integer,Groups=string,string,Ipv6AddressCount=integer,Ipv6Addresses=[{Ipv6Address=string,IsPrimaryIpv6=boolean},{Ipv6Address=string,IsPrimaryIpv6=boolean}],NetworkInterfaceId=string,PrivateIpAddress=string,PrivateIpAddresses=[{Primary=boolean,PrivateIpAddress=string},{Primary=boolean,PrivateIpAddress=string}],SecondaryPrivateIpAddressCount=integer,SubnetId=string,AssociateCarrierIpAddress=boolean,InterfaceType=string,NetworkCardIndex=integer,Ipv4Prefixes=[{Ipv4Prefix=string},{Ipv4Prefix=string}],Ipv4PrefixCount=integer,Ipv6Prefixes=[{Ipv6Prefix=string},{Ipv6Prefix=string}],Ipv6PrefixCount=integer,PrimaryIpv6=boolean,EnaSrdSpecification={EnaSrdEnabled=boolean,EnaSrdUdpSpecification={EnaSrdUdpEnabled=boolean}},ConnectionTrackingSpecification={TcpEstablishedTimeout=integer,UdpStreamTimeout=integer,UdpTimeout=integer},EnaQueueCount=integer...\n```",
          "```\n[{\"AssociatePublicIpAddress\":true|false,\"DeleteOnTermination\":true|false,\"Description\":\"string\",\"DeviceIndex\":integer,\"Groups\":[\"string\",...],\"Ipv6AddressCount\":integer,\"Ipv6Addresses\":[{\"Ipv6Address\":\"string\",\"IsPrimaryIpv6\":true|false}...],\"NetworkInterfaceId\":\"string\",\"PrivateIpAddress\":\"string\",\"PrivateIpAddresses\":[{\"Primary\":true|false,\"PrivateIpAddress\":\"string\"}...],\"SecondaryPrivateIpAddressCount\":integer,\"SubnetId\":\"string\",\"AssociateCarrierIpAddress\":true|false,\"InterfaceType\":\"string\",\"NetworkCardIndex\":integer,\"Ipv4Prefixes\":[{\"Ipv4Prefix\":\"string\"}...],\"Ipv4PrefixCount\":integer,\"Ipv6Prefixes\":[{\"Ipv6Prefix\":\"string\"}...],\"Ipv6PrefixCount\":integer,\"PrimaryIpv6\":true|false,\"EnaSrdSpecification\":{\"EnaSrdEnabled\":true|false,\"EnaSrdUdpSpecification\":{\"EnaSrdUdpEnabled\":true|false}},\"ConnectionTrackingSpecification\":{\"TcpEstablishedTimeout\":integer,\"UdpStreamTimeout\":integer,\"UdpTimeout\":integer},\"EnaQueueCount\":integer}...]\n```",
          "```\nArn=string,Name=string\n```",
          "```\n{\"Arn\":\"string\",\"Name\":\"string\"}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 168,
          "content_length": 16508
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: To launch an instance into a default subnet\n\nThe following run-instances example launches a single instance of type t2.micro into the default subnet for the current Region and associates it with the default subnet for the default VPC for the Region. The key pair is optional if you do not plan to connect to your instance using SSH (Linux) or RDP (Windows).\n\nExample 2: To launch an instance into a non-default subnet and add a public IP address\n\nThe following run-instances example requests a public IP address for an instance that youâre launching into a nondefault subnet. The instance is associated with the specified security group.\n\nFor an example of the output for run-instances, see Example 1.\n\nExample 3: To launch an instance with additional volumes\n\nThe following run-instances example uses a block device mapping, specified in mapping.json, to attach additional volumes at launch. A block device mapping can specify EBS volumes, instance store volumes, or both EBS volumes and instance store volumes.\n\nContents of mapping.json. This example adds /dev/sdh an empty EBS volume with a size of 100 GiB.\n\nContents of mapping.json. This example adds ephemeral1 as an instance store volume.\n\nFor an example of the output for run-instances, see Example 1.\n\nFor more information about block device mappings, see Block device mapping in the Amazon EC2 User Guide.\n\nExample 4: To launch an instance and add tags on creation\n\nThe following run-instances example adds a tag with a key of webserver and value of production to the instance. The command also applies a tag with a key of cost-center and a value of cc123 to any EBS volume thatâs created (in this case, the root volume).\n\nFor an example of the output for run-instances, see Example 1.\n\nExample 5: To launch an instance with user data\n\nThe following run-instances example passes user data in a file called my_script.txt that contains a configuration script for your instance. The script runs at launch.\n\nFor an example of the output for run-instances, see Example 1.\n\nFor more information about instance user data, see Working with instance user data in the Amazon EC2 User Guide.\n\nExample 6: To launch a burstable performance instance\n\nThe following run-instances example launches a t2.micro instance with the unlimited credit option. When you launch a T2 instance, if you do not specify --credit-specification, the default is the standard credit option. When you launch a T3 instance, the default is the unlimited credit option.\n\nFor an example of the output for run-instances, see Example 1.\n\nFor more information about burstable performance instances, see Burstable performance instances in the Amazon EC2 User Guide.",
        "code_examples": [
          "```\nawsec2run-instances\\--image-idami-0abcdef1234567890\\--instance-typet2.micro\\--key-nameMyKeyPair\n```",
          "```\n{\"Instances\":[{\"AmiLaunchIndex\":0,\"ImageId\":\"ami-0abcdef1234567890\",\"InstanceId\":\"i-1231231230abcdef0\",\"InstanceType\":\"t2.micro\",\"KeyName\":\"MyKeyPair\",\"LaunchTime\":\"2018-05-10T08:05:20.000Z\",\"Monitoring\":{\"State\":\"disabled\"},\"Placement\":{\"AvailabilityZone\":\"us-east-2a\",\"GroupName\":\"\",\"Tenancy\":\"default\"},\"PrivateDnsName\":\"ip-10-0-0-157.us-east-2.compute.internal\",\"PrivateIpAddress\":\"10.0.0.157\",\"ProductCodes\":[],\"PublicDnsName\":\"\",\"State\":{\"Code\":0,\"Name\":\"pending\"},\"StateTransitionReason\":\"\",\"SubnetId\":\"subnet-04a636d18e83cfacb\",\"VpcId\":\"vpc-1234567890abcdef0\",\"Architecture\":\"x86_64\",\"BlockDeviceMappings\":[],\"ClientToken\":\"\",\"EbsOptimized\":false,\"Hypervisor\":\"xen\",\"NetworkInterfaces\":[{\"Attachment\":{\"AttachTime\":\"2018-05-10T08:05:20.000Z\",\"AttachmentId\":\"eni-attach-0e325c07e928a0405\",\"DeleteOnTermination\":true,\"DeviceIndex\":0,\"Status\":\"attaching\"},\"Description\":\"\",\"Groups\":[{\"GroupName\":\"MySecurityGroup\",\"GroupId\":\"sg-0598c7d356eba48d7\"}],\"Ipv6Addresses\":[],\"MacAddress\":\"0a:ab:58:e0:67:e2\",\"NetworkInterfaceId\":\"eni-0c0a29997760baee7\",\"OwnerId\":\"123456789012\",\"PrivateDnsName\":\"ip-10-0-0-157.us-east-2.compute.internal\",\"PrivateIpAddress\":\"10.0.0.157\",\"PrivateIpAddresses\":[{\"Primary\":true,\"PrivateDnsName\":\"ip-10-0-0-157.us-east-2.compute.internal\",\"PrivateIpAddress\":\"10.0.0.157\"}],\"SourceDestCheck\":true,\"Status\":\"in-use\",\"SubnetId\":\"subnet-04a636d18e83cfacb\",\"VpcId\":\"vpc-1234567890abcdef0\",\"InterfaceType\":\"interface\"}],\"RootDeviceName\":\"/dev/xvda\",\"RootDeviceType\":\"ebs\",\"SecurityGroups\":[{\"GroupName\":\"MySecurityGroup\",\"GroupId\":\"sg-0598c7d356eba48d7\"}],\"SourceDestCheck\":true,\"StateReason\":{\"Code\":\"pending\",\"Message\":\"pending\"},\"Tags\":[],\"VirtualizationType\":\"hvm\",\"CpuOptions\":{\"CoreCount\":1,\"ThreadsPerCore\":1},\"CapacityReservationSpecification\":{\"CapacityReservationPreference\":\"open\"},\"MetadataOptions\":{\"State\":\"pending\",\"HttpTokens\":\"optional\",\"HttpPutResponseHopLimit\":1,\"HttpEndpoint\":\"enabled\"}}],\"OwnerId\":\"123456789012\",\"ReservationId\":\"r-02a3f596d91211712\"}\n```",
          "```\nawsec2run-instances\\--image-idami-0abcdef1234567890\\--instance-typet2.micro\\--subnet-idsubnet-08fc749671b2d077c\\--security-group-idssg-0b0384b66d7d692f9\\--associate-public-ip-address\\--key-nameMyKeyPair\n```",
          "```\nawsec2run-instances\\--image-idami-0abcdef1234567890\\--instance-typet2.micro\\--subnet-idsubnet-08fc749671b2d077c\\--security-group-idssg-0b0384b66d7d692f9\\--key-nameMyKeyPair\\--block-device-mappingsfile://mapping.json\n```",
          "```\n[{\"DeviceName\":\"/dev/sdh\",\"Ebs\":{\"VolumeSize\":100}}]\n```",
          "```\n[{\"DeviceName\":\"/dev/sdc\",\"VirtualName\":\"ephemeral1\"}]\n```",
          "```\nawsec2run-instances\\--image-idami-0abcdef1234567890\\--instance-typet2.micro\\--count1\\--subnet-idsubnet-08fc749671b2d077c\\--key-nameMyKeyPair\\--security-group-idssg-0b0384b66d7d692f9\\--tag-specifications'ResourceType=instance,Tags=[{Key=webserver,Value=production}]''ResourceType=volume,Tags=[{Key=cost-center,Value=cc123}]'\n```",
          "```\nawsec2run-instances\\--image-idami-0abcdef1234567890\\--instance-typet2.micro\\--count1\\--subnet-idsubnet-08fc749671b2d077c\\--key-nameMyKeyPair\\--security-group-idssg-0b0384b66d7d692f9\\--user-datafile://my_script.txt\n```",
          "```\nawsec2run-instances\\--image-idami-0abcdef1234567890\\--instance-typet2.micro\\--count1\\--subnet-idsubnet-08fc749671b2d077c\\--key-nameMyKeyPair\\--security-group-idssg-0b0384b66d7d692f9\\--credit-specificationCpuCredits=unlimited\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 24,
          "content_length": 3071
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "ReservationId -> (string)\n\nRequesterId -> (string)\n\nDescribes a security group.\n\nGroupName -> (string)\n\nDescribes an instance.\n\nArchitecture -> (string)\n\nThe architecture of the image.\n\nBlockDeviceMappings -> (list)\n\nAny block device mapping entries for the instance.\n\nDescribes a block device mapping.\n\nDeviceName -> (string)\n\nParameters used to automatically set up EBS volumes when the instance is launched.\n\nAttachTime -> (timestamp)\n\nDeleteOnTermination -> (boolean)\n\nThe attachment state.\n\nVolumeId -> (string)\n\nAssociatedResource -> (string)\n\nVolumeOwnerId -> (string)\n\nThe ID of the Amazon Web Services account that owns the volume.\n\nThis parameter is returned only for volumes that are attached to Amazon Web Services-managed resources.\n\nOperator -> (structure)\n\nThe service provider that manages the EBS volume.\n\nManaged -> (boolean)\n\nPrincipal -> (string)\n\nClientToken -> (string)\n\nEbsOptimized -> (boolean)\n\nEnaSupport -> (boolean)\n\nHypervisor -> (string)\n\nThe hypervisor type of the instance. The value xen is used for both Xen and Nitro hypervisors.\n\nIamInstanceProfile -> (structure)\n\nThe IAM instance profile associated with the instance, if applicable.\n\nInstanceLifecycle -> (string)\n\nIndicates whether this is a Spot Instance or a Scheduled Instance.\n\nElasticGpuAssociations -> (list)\n\n• capacity-block\n\n[Note] NoteAmazon Elastic Graphics reached end of life on January 8, 2024.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 34,
          "content_length": 1396
        }
      },
      {
        "header": "Note",
        "content": "[Note] NoteAmazon Elastic Graphics reached end of life on January 8, 2024.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 74
        }
      },
      {
        "header": "Note",
        "content": "Describes the association between an instance and an Elastic Graphics accelerator.\n\nElasticGpuId -> (string)\n\nElasticGpuAssociationId -> (string)\n\nElasticGpuAssociationState -> (string)\n\nElasticGpuAssociationTime -> (string)\n\nElasticInferenceAcceleratorAssociations -> (list)\n\n[Note] NoteAmazon Elastic Inference is no longer available.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 336
        }
      },
      {
        "header": "Note",
        "content": "[Note] NoteAmazon Elastic Inference is no longer available.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 59
        }
      },
      {
        "header": "Note",
        "content": "Describes the association between an instance and an elastic inference accelerator.\n\nElasticInferenceAcceleratorArn -> (string)\n\nElasticInferenceAcceleratorAssociationId -> (string)\n\nElasticInferenceAcceleratorAssociationState -> (string)\n\nElasticInferenceAcceleratorAssociationTime -> (timestamp)\n\nNetworkInterfaces -> (list)\n\nThe network interfaces for the instance.\n\nDescribes a network interface.\n\nAssociation -> (structure)\n\nThe association information for an Elastic IPv4 associated with the network interface.\n\nCarrierIp -> (string)\n\nCustomerOwnedIp -> (string)\n\nIpOwnerId -> (string)\n\nPublicDnsName -> (string)\n\nPublicIp -> (string)\n\nAttachment -> (structure)\n\nThe network interface attachment.\n\nAttachTime -> (timestamp)\n\nAttachmentId -> (string)\n\nDeleteOnTermination -> (boolean)\n\nDeviceIndex -> (integer)\n\nThe attachment state.\n\nNetworkCardIndex -> (integer)\n\nEnaSrdSpecification -> (structure)\n\nContains the ENA Express settings for the network interface thatâs attached to the instance.\n\nEnaSrdEnabled -> (boolean)\n\nEnaSrdUdpSpecification -> (structure)\n\nConfigures ENA Express for UDP network traffic.\n\nEnaSrdUdpEnabled -> (boolean)\n\nEnaQueueCount -> (integer)\n\nDescription -> (string)\n\nThe security groups.\n\nDescribes a security group.\n\nGroupName -> (string)\n\nIpv6Addresses -> (list)\n\nThe IPv6 addresses associated with the network interface.\n\nDescribes an IPv6 address.\n\nIpv6Address -> (string)\n\nIsPrimaryIpv6 -> (boolean)\n\nMacAddress -> (string)\n\nNetworkInterfaceId -> (string)\n\nPrivateDnsName -> (string)\n\nPrivateIpAddress -> (string)\n\nPrivateIpAddresses -> (list)\n\nThe private IPv4 addresses associated with the network interface.\n\nDescribes a private IPv4 address.\n\nAssociation -> (structure)\n\nThe association information for an Elastic IP address for the network interface.\n\nCarrierIp -> (string)\n\nCustomerOwnedIp -> (string)\n\nIpOwnerId -> (string)\n\nPublicDnsName -> (string)\n\nPublicIp -> (string)\n\nPrimary -> (boolean)\n\nPrivateDnsName -> (string)\n\nPrivateIpAddress -> (string)\n\nSourceDestCheck -> (boolean)\n\nThe status of the network interface.\n\nSubnetId -> (string)\n\nInterfaceType -> (string)\n\nThe type of network interface.\n\nValid values: interface | efa | efa-only | evs | trunk\n\nIpv4Prefixes -> (list)\n\nThe IPv4 delegated prefixes that are assigned to the network interface.\n\nInformation about an IPv4 prefix.\n\nIpv4Prefix -> (string)\n\nIpv6Prefixes -> (list)\n\nThe IPv6 delegated prefixes that are assigned to the network interface.\n\nInformation about an IPv6 prefix.\n\nIpv6Prefix -> (string)\n\nConnectionTrackingConfiguration -> (structure)\n\nA security group connection tracking configuration that enables you to set the timeout for connection tracking on an Elastic network interface. For more information, see Connection tracking timeouts in the Amazon EC2 User Guide .\n\nTcpEstablishedTimeout -> (integer)\n\nUdpStreamTimeout -> (integer)\n\nUdpTimeout -> (integer)\n\nOperator -> (structure)\n\nThe service provider that manages the network interface.\n\nManaged -> (boolean)\n\nPrincipal -> (string)\n\nOutpostArn -> (string)\n\nRootDeviceName -> (string)\n\nRootDeviceType -> (string)\n\nThe root device type used by the AMI. The AMI can use an EBS volume or an instance store volume.\n\nSecurityGroups -> (list)\n\nThe security groups for the instance.\n\nDescribes a security group.\n\nGroupName -> (string)\n\nSourceDestCheck -> (boolean)\n\nSpotInstanceRequestId -> (string)\n\nSriovNetSupport -> (string)\n\nStateReason -> (structure)\n\nThe reason for the most recent state transition.\n\nThe message for the state change.\n\nAny tags assigned to the instance.\n\nConstraints: Tag keys are case-sensitive and accept a maximum of 127 Unicode characters. May not begin with aws: .\n\nThe value of the tag.\n\nConstraints: Tag values are case-sensitive and accept a maximum of 256 Unicode characters.\n\nVirtualizationType -> (string)\n\nThe virtualization type of the instance.\n\nCpuOptions -> (structure)\n\nThe CPU options for the instance.\n\nCoreCount -> (integer)\n\nThreadsPerCore -> (integer)\n\nAmdSevSnp -> (string)\n\nIndicates whether the instance is enabled for AMD SEV-SNP. For more information, see AMD SEV-SNP .\n\nCapacityBlockId -> (string)\n\nThe ID of the Capacity Block.\n\n• instance-store\n\n• Server.InsufficientInstanceCapacity : There was insufficient capacity available to satisfy the launch request.\n• Server.InternalError : An internal error caused the instance to terminate during launch.\n• Server.ScheduledStop : The instance was stopped due to a scheduled retirement.\n• Server.SpotInstanceShutdown : The instance was stopped because the number of Spot requests with a maximum price equal to or higher than the Spot price exceeded available capacity or because of an increase in the Spot price.\n• Server.SpotInstanceTermination : The instance was terminated because the number of Spot requests with a maximum price equal to or higher than the Spot price exceeded available capacity or because of an increase in the Spot price.\n• Client.InstanceInitiatedShutdown : The instance was shut down from the operating system of the instance.\n• Client.InstanceTerminated : The instance was terminated or rebooted during AMI creation.\n• Client.InternalError : A client error caused the instance to terminate during launch.\n• Client.InvalidSnapshot.NotFound : The specified snapshot was not found.\n• Client.UserInitiatedHibernate : Hibernation was initiated on the instance.\n• Client.UserInitiatedShutdown : The instance was shut down using the Amazon EC2 API.\n• Client.VolumeLimitExceeded : The limit on the number of EBS volumes or total storage was exceeded. Decrease usage or request an increase in your account limits.\n\n• paravirtual\n\n[Note] NoteFor P5 instances, a Capacity Block ID refers to a group of instances. For Trn2u instances, a capacity block ID refers to an EC2 UltraServer.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 107,
          "content_length": 5773
        }
      },
      {
        "header": "Note",
        "content": "CapacityReservationId -> (string)\n\nCapacityReservationSpecification -> (structure)\n\nInformation about the Capacity Reservation targeting option.\n\nCapacityReservationPreference -> (string)\n\nDescribes the instanceâs Capacity Reservation preferences. Possible preferences include:\n\nCapacityReservationTarget -> (structure)\n\nInformation about the targeted Capacity Reservation or Capacity Reservation group.\n\nCapacityReservationId -> (string)\n\nCapacityReservationResourceGroupArn -> (string)\n\nHibernationOptions -> (structure)\n\nIndicates whether the instance is enabled for hibernation.\n\nConfigured -> (boolean)\n\nThe license configurations for the instance.\n\nDescribes a license configuration.\n\nLicenseConfigurationArn -> (string)\n\nMetadataOptions -> (structure)\n\nThe metadata options for the instance.\n\nThe state of the metadata option changes.\n\npending - The metadata options are being updated and the instance is not ready to process metadata traffic with the new selection.\n\napplied - The metadata options have been successfully applied on the instance.\n\nHttpTokens -> (string)\n\nIndicates whether IMDSv2 is required.\n\nHttpPutResponseHopLimit -> (integer)\n\nThe maximum number of hops that the metadata token can travel.\n\nPossible values: Integers from 1 to 64\n\nHttpEndpoint -> (string)\n\nIndicates whether the HTTP metadata endpoint on your instances is enabled or disabled.\n\nIf the value is disabled , you cannot access your instance metadata.\n\nHttpProtocolIpv6 -> (string)\n\nIndicates whether the IPv6 endpoint for the instance metadata service is enabled or disabled.\n\nInstanceMetadataTags -> (string)\n\nIndicates whether access to instance tags from the instance metadata is enabled or disabled. For more information, see Work with instance tags using the instance metadata .\n\nEnclaveOptions -> (structure)\n\nIndicates whether the instance is enabled for Amazon Web Services Nitro Enclaves.\n\nEnabled -> (boolean)\n\nBootMode -> (string)\n\nThe boot mode that was specified by the AMI. If the value is uefi-preferred , the AMI supports both UEFI and Legacy BIOS. The currentInstanceBootMode parameter is the boot mode that is used to boot the instance at launch or start.\n\n• open - The instance can run in any open Capacity Reservation that has matching attributes (instance type, platform, Availability Zone).\n• none - The instance avoids running in a Capacity Reservation even if one is available. The instance runs in On-Demand capacity.\n\n• capacity-reservations-only\n\n• optional - IMDSv2 is optional, which means that you can use either IMDSv2 or IMDSv1.\n• required - IMDSv2 is required, which means that IMDSv1 is disabled, and you must use IMDSv2.\n\n[Note] NoteThe operating system contained in the AMI must be configured to support the specified boot mode.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 37,
          "content_length": 2758
        }
      },
      {
        "header": "Note",
        "content": "For more information, see Boot modes in the Amazon EC2 User Guide .\n\nPlatformDetails -> (string)\n\nUsageOperation -> (string)\n\nUsageOperationUpdateTime -> (timestamp)\n\nPrivateDnsNameOptions -> (structure)\n\nThe options for the instance hostname.\n\nHostnameType -> (string)\n\nThe type of hostname to assign to an instance.\n\nEnableResourceNameDnsARecord -> (boolean)\n\nEnableResourceNameDnsAAAARecord -> (boolean)\n\nIpv6Address -> (string)\n\nTpmSupport -> (string)\n\nMaintenanceOptions -> (structure)\n\nProvides information on the recovery and maintenance options of your instance.\n\nAutoRecovery -> (string)\n\nProvides information on the current automatic recovery behavior of your instance.\n\nRebootMigration -> (string)\n\nSpecifies whether to attempt reboot migration during a user-initiated reboot of an instance that has a scheduled system-reboot event:\n\nThis setting only applies to supported instances that have a scheduled reboot event. For more information, see Enable or disable reboot migration in the Amazon EC2 User Guide .\n\nCurrentInstanceBootMode -> (string)\n\nThe boot mode that is used to boot the instance at launch or start. For more information, see Boot modes in the Amazon EC2 User Guide .\n\nNetworkPerformanceOptions -> (structure)\n\nContains settings for the network performance options for your instance.\n\nBandwidthWeighting -> (string)\n\nWhen you configure network bandwidth weighting, you can boost your baseline bandwidth for either networking or EBS by up to 25%. The total available baseline bandwidth for your instance remains the same. The default option uses the standard bandwidth configuration for your instance type.\n\nOperator -> (structure)\n\nThe service provider that manages the instance.\n\nManaged -> (boolean)\n\nPrincipal -> (string)\n\nInstanceId -> (string)\n\nState -> (structure)\n\nThe current state of the instance.\n\nThe state of the instance as a 16-bit unsigned integer.\n\nThe high byte is all of the bits between 2^8 and (2^16)-1, which equals decimal values between 256 and 65,535. These numerical values are used for internal purposes and should be ignored.\n\nThe low byte is all of the bits between 2^0 and (2^8)-1, which equals decimal values between 0 and 255.\n\nThe valid values for instance-state-code will all be in the range of the low byte and they are:\n\nYou can ignore the high byte value by zeroing out all of the bits above 2^8 or 256 in decimal.\n\nThe current state of the instance.\n\nPrivateDnsName -> (string)\n\n[IPv4 only] The private DNS hostname name assigned to the instance. This DNS hostname can only be used inside the Amazon EC2 network. This name is not available until the instance enters the running state.\n\nThe Amazon-provided DNS server resolves Amazon-provided private DNS hostnames if youâve enabled DNS resolution and DNS hostnames in your VPC. If you are not using the Amazon-provided DNS server in your VPC, your custom domain name servers must resolve the hostname as appropriate.\n\nPublicDnsName -> (string)\n\nStateTransitionReason -> (string)\n\nAmiLaunchIndex -> (integer)\n\nProductCodes -> (list)\n\nThe product codes attached to this instance, if applicable.\n\nDescribes a product code.\n\nProductCodeId -> (string)\n\nProductCodeType -> (string)\n\nThe type of product code.\n\nInstanceType -> (string)\n\nLaunchTime -> (timestamp)\n\nPlacement -> (structure)\n\nThe location where the instance launched, if applicable.\n\nAvailabilityZoneId -> (string)\n\nThe ID of the Availability Zone of the instance.\n\nOn input, you can specify AvailabilityZone or AvailabilityZoneId , but not both. If you specify neither one, Amazon EC2 automatically selects an Availability Zone for you.\n\nThis parameter is not supported for CreateFleet .\n\nAffinity -> (string)\n\nThe affinity setting for the instance on the Dedicated Host.\n\nThis parameter is not supported for CreateFleet or ImportInstance .\n\nGroupName -> (string)\n\nThe name of the placement group that the instance is in.\n\nOn input, you can specify GroupId or GroupName , but not both.\n\nPartitionNumber -> (integer)\n\nThe number of the partition that the instance is in. Valid only if the placement group strategy is set to partition .\n\nThis parameter is not supported for CreateFleet .\n\nThe ID of the Dedicated Host on which the instance resides.\n\nThis parameter is not supported for CreateFleet or ImportInstance .\n\nThe tenancy of the instance. An instance with a tenancy of dedicated runs on single-tenant hardware.\n\nThis parameter is not supported for CreateFleet . The host tenancy is not supported for ImportInstance or for T3 instances that are configured for the unlimited CPU credit option.\n\nSpreadDomain -> (string)\n\nHostResourceGroupArn -> (string)\n\nThe ARN of the host resource group in which to launch the instances.\n\nOn input, if you specify this parameter, either omit the Tenancy parameter or set it to host .\n\nThis parameter is not supported for CreateFleet .\n\nThe ID of the placement group that the instance is in.\n\nOn input, you can specify GroupId or GroupName , but not both.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone of the instance.\n\nOn input, you can specify AvailabilityZone or AvailabilityZoneId , but not both. If you specify neither one, Amazon EC2 automatically selects an Availability Zone for you.\n\nThis parameter is not supported for CreateFleet .\n\nKernelId -> (string)\n\nRamdiskId -> (string)\n\nPlatform -> (string)\n\nThe platform. This value is windows for Windows instances; otherwise, it is empty.\n\nMonitoring -> (structure)\n\nThe monitoring for the instance.\n\nIndicates whether detailed monitoring is enabled. Otherwise, basic monitoring is enabled.\n\nSubnetId -> (string)\n\nPrivateIpAddress -> (string)\n\nPublicIpAddress -> (string)\n\nThe public IPv4 address, or the Carrier IP address assigned to the instance, if applicable.\n\nA Carrier IP address only applies to an instance launched in a subnet associated with a Wavelength Zone.\n\n• legacy-bios\n• uefi-preferred\n\n• resource-name\n\n• default - Amazon EC2 attempts to migrate the instance to new hardware (reboot migration). If successful, the system-reboot event is cleared. If unsuccessful, an in-place reboot occurs and the event remains scheduled.\n• disabled - Amazon EC2 keeps the instance on the same hardware (in-place reboot). The system-reboot event remains scheduled.\n\n• legacy-bios\n\n• 0 : pending\n• 16 : running\n• 32 : shutting-down\n• 48 : terminated\n• 64 : stopping\n• 80 : stopped\n\n• shutting-down\n\n• marketplace\n\n• c5.12xlarge\n• c5.18xlarge\n• c5.24xlarge\n• c5a.2xlarge\n• c5a.4xlarge\n• c5a.8xlarge\n• c5a.12xlarge\n• c5a.16xlarge\n• c5a.24xlarge\n• c5ad.xlarge\n• c5ad.2xlarge\n• c5ad.4xlarge\n• c5ad.8xlarge\n• c5ad.12xlarge\n• c5ad.16xlarge\n• c5ad.24xlarge\n• c5d.2xlarge\n• c5d.4xlarge\n• c5d.9xlarge\n• c5d.12xlarge\n• c5d.18xlarge\n• c5d.24xlarge\n• c5n.2xlarge\n• c5n.4xlarge\n• c5n.9xlarge\n• c5n.18xlarge\n• c6g.2xlarge\n• c6g.4xlarge\n• c6g.8xlarge\n• c6g.12xlarge\n• c6g.16xlarge\n• c6gd.medium\n• c6gd.xlarge\n• c6gd.2xlarge\n• c6gd.4xlarge\n• c6gd.8xlarge\n• c6gd.12xlarge\n• c6gd.16xlarge\n• c6gn.medium\n• c6gn.xlarge\n• c6gn.2xlarge\n• c6gn.4xlarge\n• c6gn.8xlarge\n• c6gn.12xlarge\n• c6gn.16xlarge\n• c6i.2xlarge\n• c6i.4xlarge\n• c6i.8xlarge\n• c6i.12xlarge\n• c6i.16xlarge\n• c6i.24xlarge\n• c6i.32xlarge\n• cc1.4xlarge\n• cc2.8xlarge\n• cg1.4xlarge\n• cr1.8xlarge\n• d3en.xlarge\n• d3en.2xlarge\n• d3en.4xlarge\n• d3en.6xlarge\n• d3en.8xlarge\n• d3en.12xlarge\n• dl1.24xlarge\n• f1.16xlarge\n• g3.16xlarge\n• g4ad.xlarge\n• g4ad.2xlarge\n• g4ad.4xlarge\n• g4ad.8xlarge\n• g4ad.16xlarge\n• g4dn.xlarge\n• g4dn.2xlarge\n• g4dn.4xlarge\n• g4dn.8xlarge\n• g4dn.12xlarge\n• g4dn.16xlarge\n• g5.12xlarge\n• g5.16xlarge\n• g5.24xlarge\n• g5.48xlarge\n• g5g.2xlarge\n• g5g.4xlarge\n• g5g.8xlarge\n• g5g.16xlarge\n• hi1.4xlarge\n• hpc6a.48xlarge\n• hs1.8xlarge\n• h1.16xlarge\n• i3.16xlarge\n• i3en.xlarge\n• i3en.2xlarge\n• i3en.3xlarge\n• i3en.6xlarge\n• i3en.12xlarge\n• i3en.24xlarge\n• im4gn.large\n• im4gn.xlarge\n• im4gn.2xlarge\n• im4gn.4xlarge\n• im4gn.8xlarge\n• im4gn.16xlarge\n• inf1.xlarge\n• inf1.2xlarge\n• inf1.6xlarge\n• inf1.24xlarge\n• is4gen.medium\n• is4gen.large\n• is4gen.xlarge\n• is4gen.2xlarge\n• is4gen.4xlarge\n• is4gen.8xlarge\n• m4.10xlarge\n• m4.16xlarge\n• m5.12xlarge\n• m5.16xlarge\n• m5.24xlarge\n• m5a.2xlarge\n• m5a.4xlarge\n• m5a.8xlarge\n• m5a.12xlarge\n• m5a.16xlarge\n• m5a.24xlarge\n• m5ad.xlarge\n• m5ad.2xlarge\n• m5ad.4xlarge\n• m5ad.8xlarge\n• m5ad.12xlarge\n• m5ad.16xlarge\n• m5ad.24xlarge\n• m5d.2xlarge\n• m5d.4xlarge\n• m5d.8xlarge\n• m5d.12xlarge\n• m5d.16xlarge\n• m5d.24xlarge\n• m5dn.xlarge\n• m5dn.2xlarge\n• m5dn.4xlarge\n• m5dn.8xlarge\n• m5dn.12xlarge\n• m5dn.16xlarge\n• m5dn.24xlarge\n• m5n.2xlarge\n• m5n.4xlarge\n• m5n.8xlarge\n• m5n.12xlarge\n• m5n.16xlarge\n• m5n.24xlarge\n• m5zn.xlarge\n• m5zn.2xlarge\n• m5zn.3xlarge\n• m5zn.6xlarge\n• m5zn.12xlarge\n• m6a.2xlarge\n• m6a.4xlarge\n• m6a.8xlarge\n• m6a.12xlarge\n• m6a.16xlarge\n• m6a.24xlarge\n• m6a.32xlarge\n• m6a.48xlarge\n• m6g.2xlarge\n• m6g.4xlarge\n• m6g.8xlarge\n• m6g.12xlarge\n• m6g.16xlarge\n• m6gd.medium\n• m6gd.xlarge\n• m6gd.2xlarge\n• m6gd.4xlarge\n• m6gd.8xlarge\n• m6gd.12xlarge\n• m6gd.16xlarge\n• m6i.2xlarge\n• m6i.4xlarge\n• m6i.8xlarge\n• m6i.12xlarge\n• m6i.16xlarge\n• m6i.24xlarge\n• m6i.32xlarge\n• p2.16xlarge\n• p3.16xlarge\n• p3dn.24xlarge\n• p4d.24xlarge\n• r4.16xlarge\n• r5.12xlarge\n• r5.16xlarge\n• r5.24xlarge\n• r5a.2xlarge\n• r5a.4xlarge\n• r5a.8xlarge\n• r5a.12xlarge\n• r5a.16xlarge\n• r5a.24xlarge\n• r5ad.xlarge\n• r5ad.2xlarge\n• r5ad.4xlarge\n• r5ad.8xlarge\n• r5ad.12xlarge\n• r5ad.16xlarge\n• r5ad.24xlarge\n• r5b.2xlarge\n• r5b.4xlarge\n• r5b.8xlarge\n• r5b.12xlarge\n• r5b.16xlarge\n• r5b.24xlarge\n• r5d.2xlarge\n• r5d.4xlarge\n• r5d.8xlarge\n• r5d.12xlarge\n• r5d.16xlarge\n• r5d.24xlarge\n• r5dn.xlarge\n• r5dn.2xlarge\n• r5dn.4xlarge\n• r5dn.8xlarge\n• r5dn.12xlarge\n• r5dn.16xlarge\n• r5dn.24xlarge\n• r5n.2xlarge\n• r5n.4xlarge\n• r5n.8xlarge\n• r5n.12xlarge\n• r5n.16xlarge\n• r5n.24xlarge\n• r6g.2xlarge\n• r6g.4xlarge\n• r6g.8xlarge\n• r6g.12xlarge\n• r6g.16xlarge\n• r6gd.medium\n• r6gd.xlarge\n• r6gd.2xlarge\n• r6gd.4xlarge\n• r6gd.8xlarge\n• r6gd.12xlarge\n• r6gd.16xlarge\n• r6i.2xlarge\n• r6i.4xlarge\n• r6i.8xlarge\n• r6i.12xlarge\n• r6i.16xlarge\n• r6i.24xlarge\n• r6i.32xlarge\n• t3a.2xlarge\n• t4g.2xlarge\n• u-6tb1.56xlarge\n• u-6tb1.112xlarge\n• u-9tb1.112xlarge\n• u-12tb1.112xlarge\n• u-6tb1.metal\n• u-9tb1.metal\n• u-12tb1.metal\n• u-18tb1.metal\n• u-24tb1.metal\n• vt1.3xlarge\n• vt1.6xlarge\n• vt1.24xlarge\n• x1.16xlarge\n• x1.32xlarge\n• x1e.2xlarge\n• x1e.4xlarge\n• x1e.8xlarge\n• x1e.16xlarge\n• x1e.32xlarge\n• x2iezn.2xlarge\n• x2iezn.4xlarge\n• x2iezn.6xlarge\n• x2iezn.8xlarge\n• x2iezn.12xlarge\n• x2iezn.metal\n• x2gd.medium\n• x2gd.xlarge\n• x2gd.2xlarge\n• x2gd.4xlarge\n• x2gd.8xlarge\n• x2gd.12xlarge\n• x2gd.16xlarge\n• z1d.2xlarge\n• z1d.3xlarge\n• z1d.6xlarge\n• z1d.12xlarge\n• x2idn.16xlarge\n• x2idn.24xlarge\n• x2idn.32xlarge\n• x2iedn.xlarge\n• x2iedn.2xlarge\n• x2iedn.4xlarge\n• x2iedn.8xlarge\n• x2iedn.16xlarge\n• x2iedn.24xlarge\n• x2iedn.32xlarge\n• c6a.2xlarge\n• c6a.4xlarge\n• c6a.8xlarge\n• c6a.12xlarge\n• c6a.16xlarge\n• c6a.24xlarge\n• c6a.32xlarge\n• c6a.48xlarge\n• i4i.2xlarge\n• i4i.4xlarge\n• i4i.8xlarge\n• i4i.16xlarge\n• i4i.32xlarge\n• x2idn.metal\n• x2iedn.metal\n• c7g.2xlarge\n• c7g.4xlarge\n• c7g.8xlarge\n• c7g.12xlarge\n• c7g.16xlarge\n• c6id.xlarge\n• c6id.2xlarge\n• c6id.4xlarge\n• c6id.8xlarge\n• c6id.12xlarge\n• c6id.16xlarge\n• c6id.24xlarge\n• c6id.32xlarge\n• m6id.xlarge\n• m6id.2xlarge\n• m6id.4xlarge\n• m6id.8xlarge\n• m6id.12xlarge\n• m6id.16xlarge\n• m6id.24xlarge\n• m6id.32xlarge\n• r6id.xlarge\n• r6id.2xlarge\n• r6id.4xlarge\n• r6id.8xlarge\n• r6id.12xlarge\n• r6id.16xlarge\n• r6id.24xlarge\n• r6id.32xlarge\n• r6a.2xlarge\n• r6a.4xlarge\n• r6a.8xlarge\n• r6a.12xlarge\n• r6a.16xlarge\n• r6a.24xlarge\n• r6a.32xlarge\n• r6a.48xlarge\n• p4de.24xlarge\n• u-3tb1.56xlarge\n• u-18tb1.112xlarge\n• u-24tb1.112xlarge\n• trn1.2xlarge\n• trn1.32xlarge\n• hpc6id.32xlarge\n• c6in.xlarge\n• c6in.2xlarge\n• c6in.4xlarge\n• c6in.8xlarge\n• c6in.12xlarge\n• c6in.16xlarge\n• c6in.24xlarge\n• c6in.32xlarge\n• m6in.xlarge\n• m6in.2xlarge\n• m6in.4xlarge\n• m6in.8xlarge\n• m6in.12xlarge\n• m6in.16xlarge\n• m6in.24xlarge\n• m6in.32xlarge\n• m6idn.large\n• m6idn.xlarge\n• m6idn.2xlarge\n• m6idn.4xlarge\n• m6idn.8xlarge\n• m6idn.12xlarge\n• m6idn.16xlarge\n• m6idn.24xlarge\n• m6idn.32xlarge\n• r6in.xlarge\n• r6in.2xlarge\n• r6in.4xlarge\n• r6in.8xlarge\n• r6in.12xlarge\n• r6in.16xlarge\n• r6in.24xlarge\n• r6in.32xlarge\n• r6idn.large\n• r6idn.xlarge\n• r6idn.2xlarge\n• r6idn.4xlarge\n• r6idn.8xlarge\n• r6idn.12xlarge\n• r6idn.16xlarge\n• r6idn.24xlarge\n• r6idn.32xlarge\n• m7g.2xlarge\n• m7g.4xlarge\n• m7g.8xlarge\n• m7g.12xlarge\n• m7g.16xlarge\n• r7g.2xlarge\n• r7g.4xlarge\n• r7g.8xlarge\n• r7g.12xlarge\n• r7g.16xlarge\n• m6idn.metal\n• r6idn.metal\n• inf2.xlarge\n• inf2.8xlarge\n• inf2.24xlarge\n• inf2.48xlarge\n• trn1n.32xlarge\n• i4g.2xlarge\n• i4g.4xlarge\n• i4g.8xlarge\n• i4g.16xlarge\n• hpc7g.4xlarge\n• hpc7g.8xlarge\n• hpc7g.16xlarge\n• c7gn.medium\n• c7gn.xlarge\n• c7gn.2xlarge\n• c7gn.4xlarge\n• c7gn.8xlarge\n• c7gn.12xlarge\n• c7gn.16xlarge\n• p5.48xlarge\n• m7i.2xlarge\n• m7i.4xlarge\n• m7i.8xlarge\n• m7i.12xlarge\n• m7i.16xlarge\n• m7i.24xlarge\n• m7i.48xlarge\n• m7i-flex.large\n• m7i-flex.xlarge\n• m7i-flex.2xlarge\n• m7i-flex.4xlarge\n• m7i-flex.8xlarge\n• m7a.2xlarge\n• m7a.4xlarge\n• m7a.8xlarge\n• m7a.12xlarge\n• m7a.16xlarge\n• m7a.24xlarge\n• m7a.32xlarge\n• m7a.48xlarge\n• m7a.metal-48xl\n• hpc7a.12xlarge\n• hpc7a.24xlarge\n• hpc7a.48xlarge\n• hpc7a.96xlarge\n• c7gd.medium\n• c7gd.xlarge\n• c7gd.2xlarge\n• c7gd.4xlarge\n• c7gd.8xlarge\n• c7gd.12xlarge\n• c7gd.16xlarge\n• m7gd.medium\n• m7gd.xlarge\n• m7gd.2xlarge\n• m7gd.4xlarge\n• m7gd.8xlarge\n• m7gd.12xlarge\n• m7gd.16xlarge\n• r7gd.medium\n• r7gd.xlarge\n• r7gd.2xlarge\n• r7gd.4xlarge\n• r7gd.8xlarge\n• r7gd.12xlarge\n• r7gd.16xlarge\n• r7a.2xlarge\n• r7a.4xlarge\n• r7a.8xlarge\n• r7a.12xlarge\n• r7a.16xlarge\n• r7a.24xlarge\n• r7a.32xlarge\n• r7a.48xlarge\n• c7i.2xlarge\n• c7i.4xlarge\n• c7i.8xlarge\n• c7i.12xlarge\n• c7i.16xlarge\n• c7i.24xlarge\n• c7i.48xlarge\n• mac2-m2pro.metal\n• r7iz.xlarge\n• r7iz.2xlarge\n• r7iz.4xlarge\n• r7iz.8xlarge\n• r7iz.12xlarge\n• r7iz.16xlarge\n• r7iz.32xlarge\n• c7a.2xlarge\n• c7a.4xlarge\n• c7a.8xlarge\n• c7a.12xlarge\n• c7a.16xlarge\n• c7a.24xlarge\n• c7a.32xlarge\n• c7a.48xlarge\n• c7a.metal-48xl\n• r7a.metal-48xl\n• r7i.2xlarge\n• r7i.4xlarge\n• r7i.8xlarge\n• r7i.12xlarge\n• r7i.16xlarge\n• r7i.24xlarge\n• r7i.48xlarge\n• dl2q.24xlarge\n• mac2-m2.metal\n• i4i.12xlarge\n• i4i.24xlarge\n• c7i.metal-24xl\n• c7i.metal-48xl\n• m7i.metal-24xl\n• m7i.metal-48xl\n• r7i.metal-24xl\n• r7i.metal-48xl\n• r7iz.metal-16xl\n• r7iz.metal-32xl\n• g6.12xlarge\n• g6.16xlarge\n• g6.24xlarge\n• g6.48xlarge\n• gr6.4xlarge\n• gr6.8xlarge\n• c7i-flex.large\n• c7i-flex.xlarge\n• c7i-flex.2xlarge\n• c7i-flex.4xlarge\n• c7i-flex.8xlarge\n• u7i-12tb.224xlarge\n• u7in-16tb.224xlarge\n• u7in-24tb.224xlarge\n• u7in-32tb.224xlarge\n• u7ib-12tb.224xlarge\n• r8g.2xlarge\n• r8g.4xlarge\n• r8g.8xlarge\n• r8g.12xlarge\n• r8g.16xlarge\n• r8g.24xlarge\n• r8g.48xlarge\n• r8g.metal-24xl\n• r8g.metal-48xl\n• mac2-m1ultra.metal\n• g6e.2xlarge\n• g6e.4xlarge\n• g6e.8xlarge\n• g6e.12xlarge\n• g6e.16xlarge\n• g6e.24xlarge\n• g6e.48xlarge\n• c8g.2xlarge\n• c8g.4xlarge\n• c8g.8xlarge\n• c8g.12xlarge\n• c8g.16xlarge\n• c8g.24xlarge\n• c8g.48xlarge\n• c8g.metal-24xl\n• c8g.metal-48xl\n• m8g.2xlarge\n• m8g.4xlarge\n• m8g.8xlarge\n• m8g.12xlarge\n• m8g.16xlarge\n• m8g.24xlarge\n• m8g.48xlarge\n• m8g.metal-24xl\n• m8g.metal-48xl\n• x8g.2xlarge\n• x8g.4xlarge\n• x8g.8xlarge\n• x8g.12xlarge\n• x8g.16xlarge\n• x8g.24xlarge\n• x8g.48xlarge\n• x8g.metal-24xl\n• x8g.metal-48xl\n• i7ie.xlarge\n• i7ie.2xlarge\n• i7ie.3xlarge\n• i7ie.6xlarge\n• i7ie.12xlarge\n• i7ie.18xlarge\n• i7ie.24xlarge\n• i7ie.48xlarge\n• i8g.2xlarge\n• i8g.4xlarge\n• i8g.8xlarge\n• i8g.12xlarge\n• i8g.16xlarge\n• i8g.24xlarge\n• i8g.metal-24xl\n• u7i-6tb.112xlarge\n• u7i-8tb.112xlarge\n• u7inh-32tb.480xlarge\n• p5e.48xlarge\n• p5en.48xlarge\n• f2.12xlarge\n• f2.48xlarge\n• trn2.48xlarge\n• c7i-flex.12xlarge\n• c7i-flex.16xlarge\n• m7i-flex.12xlarge\n• m7i-flex.16xlarge\n• i7ie.metal-24xl\n• i7ie.metal-48xl\n• i8g.48xlarge\n• c8gd.medium\n• c8gd.xlarge\n• c8gd.2xlarge\n• c8gd.4xlarge\n• c8gd.8xlarge\n• c8gd.12xlarge\n• c8gd.16xlarge\n• c8gd.24xlarge\n• c8gd.48xlarge\n• c8gd.metal-24xl\n• c8gd.metal-48xl\n• i7i.2xlarge\n• i7i.4xlarge\n• i7i.8xlarge\n• i7i.12xlarge\n• i7i.16xlarge\n• i7i.24xlarge\n• i7i.48xlarge\n• i7i.metal-24xl\n• i7i.metal-48xl\n• p6-b200.48xlarge\n• m8gd.medium\n• m8gd.xlarge\n• m8gd.2xlarge\n• m8gd.4xlarge\n• m8gd.8xlarge\n• m8gd.12xlarge\n• m8gd.16xlarge\n• m8gd.24xlarge\n• m8gd.48xlarge\n• m8gd.metal-24xl\n• m8gd.metal-48xl\n• r8gd.medium\n• r8gd.xlarge\n• r8gd.2xlarge\n• r8gd.4xlarge\n• r8gd.8xlarge\n• r8gd.12xlarge\n• r8gd.16xlarge\n• r8gd.24xlarge\n• r8gd.48xlarge\n• r8gd.metal-24xl\n• r8gd.metal-48xl\n• c8gn.medium\n• c8gn.xlarge\n• c8gn.2xlarge\n• c8gn.4xlarge\n• c8gn.8xlarge\n• c8gn.12xlarge\n• c8gn.16xlarge\n• c8gn.24xlarge\n• c8gn.48xlarge\n• c8gn.metal-24xl\n• c8gn.metal-48xl\n• p6e-gb200.36xlarge\n• g6f.2xlarge\n• g6f.4xlarge\n• gr6f.4xlarge\n• r8i.2xlarge\n• r8i.4xlarge\n• r8i.8xlarge\n• r8i.12xlarge\n• r8i.16xlarge\n• r8i.24xlarge\n• r8i.32xlarge\n• r8i.48xlarge\n• r8i.96xlarge\n• r8i.metal-48xl\n• r8i.metal-96xl\n• r8i-flex.large\n• r8i-flex.xlarge\n• r8i-flex.2xlarge\n• r8i-flex.4xlarge\n• r8i-flex.8xlarge\n• r8i-flex.12xlarge\n• r8i-flex.16xlarge\n• m8i.2xlarge\n• m8i.4xlarge\n• m8i.8xlarge\n• m8i.12xlarge\n• m8i.16xlarge\n• m8i.24xlarge\n• m8i.32xlarge\n• m8i.48xlarge\n• m8i.96xlarge\n• m8i.metal-48xl\n• m8i.metal-96xl\n• m8i-flex.large\n• m8i-flex.xlarge\n• m8i-flex.2xlarge\n• m8i-flex.4xlarge\n• m8i-flex.8xlarge\n• m8i-flex.12xlarge\n• m8i-flex.16xlarge\n• i8ge.xlarge\n• i8ge.2xlarge\n• i8ge.3xlarge\n• i8ge.6xlarge\n• i8ge.12xlarge\n• i8ge.18xlarge\n• i8ge.24xlarge\n• i8ge.48xlarge\n• i8ge.metal-24xl\n• i8ge.metal-48xl\n• mac-m4.metal\n• mac-m4pro.metal\n• r8gn.medium\n• r8gn.xlarge\n• r8gn.2xlarge\n• r8gn.4xlarge\n• r8gn.8xlarge\n• r8gn.12xlarge\n• r8gn.16xlarge\n• r8gn.24xlarge\n• r8gn.48xlarge\n• r8gn.metal-24xl\n• r8gn.metal-48xl\n• c8i.2xlarge\n• c8i.4xlarge\n• c8i.8xlarge\n• c8i.12xlarge\n• c8i.16xlarge\n• c8i.24xlarge\n• c8i.32xlarge\n• c8i.48xlarge\n• c8i.96xlarge\n• c8i.metal-48xl\n• c8i.metal-96xl\n• c8i-flex.large\n• c8i-flex.xlarge\n• c8i-flex.2xlarge\n• c8i-flex.4xlarge\n• c8i-flex.8xlarge\n• c8i-flex.12xlarge\n• c8i-flex.16xlarge\n• r8gb.medium\n• r8gb.xlarge\n• r8gb.2xlarge\n• r8gb.4xlarge\n• r8gb.8xlarge\n• r8gb.12xlarge\n• r8gb.16xlarge\n• r8gb.24xlarge\n• r8gb.metal-24xl\n• m8a.2xlarge\n• m8a.4xlarge\n• m8a.8xlarge\n• m8a.12xlarge\n• m8a.16xlarge\n• m8a.24xlarge\n• m8a.48xlarge\n• m8a.metal-24xl\n• m8a.metal-48xl\n• trn2.3xlarge\n• r8a.2xlarge\n• r8a.4xlarge\n• r8a.8xlarge\n• r8a.12xlarge\n• r8a.16xlarge\n• r8a.24xlarge\n• r8a.48xlarge\n• r8a.metal-24xl\n• r8a.metal-48xl",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 94,
          "content_length": 18356
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html",
    "doc_type": "aws",
    "total_sections": 22
  },
  {
    "title": "describe-instancesÂ¶",
    "summary": "DescriptionÂ¶ Describes the specified instances or all instances. If you specify instance IDs, the output includes information for only the specified instances. If you specify filters, the output includes information for only those instances that meet the filter criteria. If you do not specify instance IDs or filters, the output includes information for all instances, which can affect performance. We recommend that you use pagination to ensure that the operation returns quickly and successfully.",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Describes the specified instances or all instances.\n\nIf you specify instance IDs, the output includes information for only the specified instances. If you specify filters, the output includes information for only those instances that meet the filter criteria. If you do not specify instance IDs or filters, the output includes information for all instances, which can affect performance. We recommend that you use pagination to ensure that the operation returns quickly and successfully.\n\nIf you specify an instance ID that is not valid, an error is returned. If you specify an instance that you do not own, it is not included in the output.\n\nRecently terminated instances might appear in the returned results. This interval is usually less than one hour.\n\nIf you describe instances in the rare case where an Availability Zone is experiencing a service disruption and you specify instance IDs that are in the affected zone, or do not specify any instance IDs at all, the call fails. If you describe instances and specify only instance IDs that are in an unaffected zone, the call works normally.\n\nThe Amazon EC2 API follows an eventual consistency model. This means that the result of an API command you run that creates or modifies resources might not be immediately available to all subsequent commands you run. For guidance on how to manage eventual consistency, see Eventual consistency in the Amazon EC2 API in the Amazon EC2 Developer Guide .\n\n[Warning] WarningWe strongly recommend using only paginated requests. Unpaginated requests are susceptible to throttling and timeouts.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1584
        }
      },
      {
        "header": "Warning",
        "content": "[Note] NoteThe order of the elements in the response, including those within nested structures, might vary. Applications should not assume the elements appear in a particular order.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 181
        }
      },
      {
        "header": "Note",
        "content": "See also: AWS API Documentation\n\ndescribe-instances is a paginated operation. Multiple API calls may be issued in order to retrieve the entire data set of results. You can disable pagination by providing the --no-paginate argument. When using --output text and the --query argument on a paginated response, the --query argument must extract data from the results of the following query expressions: Reservations",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 411
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--instance-ids (list)\n\nDefault: Describes all your instances.\n\n--dry-run | --no-dry-run (boolean)\n\nA filter name and value pair that is used to return a more specific list of results from a describe operation. Filters can be used to match a set of resources by specific criteria, such as tags, attributes, or IDs.\n\nIf you specify multiple filters, the filters are joined with an AND , and the request returns only results that match all of the specified filters.\n\nFor more information, see List and filter using the CLI and API in the Amazon EC2 User Guide .\n\nThe filter values. Filter values are case-sensitive. If you specify multiple values for a filter, the values are joined with an OR , and the request returns all results that match any of the specified values.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--starting-token (string)\n\nA token to specify where to start paginating. This is the NextToken from a previously truncated response.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--page-size (integer)\n\nThe size of each page to get in the AWS service call. This does not affect the number of items returned in the commandâs output. Setting a smaller page size results in more calls to the AWS service, retrieving fewer items in each call. This can help prevent the AWS service calls from timing out.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--max-items (integer)\n\nThe total number of items to return in the commandâs output. If the total number of items available is more than the value specified, a NextToken is provided in the commandâs output. To resume pagination, provide the NextToken value in the starting-token argument of a subsequent command. Do not use the NextToken response element directly outside of the AWS CLI.\n\nFor usage examples, see Pagination in the AWS Command Line Interface User Guide .\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command. The generated JSON skeleton is not stable between versions of the AWS CLI and there are no backwards compatibility guarantees in the JSON skeleton generated.\n\n• affinity - The affinity setting for an instance running on a Dedicated Host (default | host ).\n• architecture - The instance architecture (i386 | x86_64 | arm64 ).\n• availability-zone - The Availability Zone of the instance.\n• availability-zone-id - The ID of the Availability Zone of the instance.\n• block-device-mapping.attach-time - The attach time for an EBS volume mapped to the instance, for example, 2022-09-15T17:15:20.000Z .\n• block-device-mapping.delete-on-termination - A Boolean that indicates whether the EBS volume is deleted on instance termination.\n• block-device-mapping.device-name - The device name specified in the block device mapping (for example, /dev/sdh or xvdh ).\n• block-device-mapping.status - The status for the EBS volume (attaching | attached | detaching | detached ).\n• block-device-mapping.volume-id - The volume ID of the EBS volume.\n• boot-mode - The boot mode that was specified by the AMI (legacy-bios | uefi | uefi-preferred ).\n• capacity-reservation-id - The ID of the Capacity Reservation into which the instance was launched.\n• capacity-reservation-specification.capacity-reservation-preference - The instanceâs Capacity Reservation preference (open | none ).\n• capacity-reservation-specification.capacity-reservation-target.capacity-reservation-id - The ID of the targeted Capacity Reservation.\n• capacity-reservation-specification.capacity-reservation-target.capacity-reservation-resource-group-arn - The ARN of the targeted Capacity Reservation group.\n• client-token - The idempotency token you provided when you launched the instance.\n• current-instance-boot-mode - The boot mode that is used to launch the instance at launch or start (legacy-bios | uefi ).\n• dns-name - The public DNS name of the instance.\n• ebs-optimized - A Boolean that indicates whether the instance is optimized for Amazon EBS I/O.\n• ena-support - A Boolean that indicates whether the instance is enabled for enhanced networking with ENA.\n• enclave-options.enabled - A Boolean that indicates whether the instance is enabled for Amazon Web Services Nitro Enclaves.\n• hibernation-options.configured - A Boolean that indicates whether the instance is enabled for hibernation. A value of true means that the instance is enabled for hibernation.\n• host-id - The ID of the Dedicated Host on which the instance is running, if applicable.\n• hypervisor - The hypervisor type of the instance (ovm | xen ). The value xen is used for both Xen and Nitro hypervisors.\n• iam-instance-profile.arn - The instance profile associated with the instance. Specified as an ARN.\n• iam-instance-profile.id - The instance profile associated with the instance. Specified as an ID.\n• image-id - The ID of the image used to launch the instance.\n• instance-id - The ID of the instance.\n• instance-lifecycle - Indicates whether this is a Spot Instance, a Scheduled Instance, or a Capacity Block (spot | scheduled | capacity-block ).\n• instance-state-code - The state of the instance, as a 16-bit unsigned integer. The high byte is used for internal purposes and should be ignored. The low byte is set based on the state represented. The valid values are: 0 (pending), 16 (running), 32 (shutting-down), 48 (terminated), 64 (stopping), and 80 (stopped).\n• instance-state-name - The state of the instance (pending | running | shutting-down | terminated | stopping | stopped ).\n• instance-type - The type of instance (for example, t2.micro ).\n• instance.group-id - The ID of the security group for the instance.\n• instance.group-name - The name of the security group for the instance.\n• ip-address - The public IPv4 address of the instance.\n• ipv6-address - The IPv6 address of the instance.\n• kernel-id - The kernel ID.\n• key-name - The name of the key pair used when the instance was launched.\n• launch-index - When launching multiple instances, this is the index for the instance in the launch group (for example, 0, 1, 2, and so on).\n• launch-time - The time when the instance was launched, in the ISO 8601 format in the UTC time zone (YYYY-MM-DDThh:mm:ss.sssZ), for example, 2021-09-29T11:04:43.305Z . You can use a wildcard (* ), for example, 2021-09-29T* , which matches an entire day.\n• maintenance-options.auto-recovery - The current automatic recovery behavior of the instance (disabled | default ).\n• metadata-options.http-endpoint - The status of access to the HTTP metadata endpoint on your instance (enabled | disabled )\n• metadata-options.http-protocol-ipv4 - Indicates whether the IPv4 endpoint is enabled (disabled | enabled ).\n• metadata-options.http-protocol-ipv6 - Indicates whether the IPv6 endpoint is enabled (disabled | enabled ).\n• metadata-options.http-put-response-hop-limit - The HTTP metadata request put response hop limit (integer, possible values 1 to 64 )\n• metadata-options.http-tokens - The metadata request authorization state (optional | required )\n• metadata-options.instance-metadata-tags - The status of access to instance tags from the instance metadata (enabled | disabled )\n• metadata-options.state - The state of the metadata option changes (pending | applied ).\n• monitoring-state - Indicates whether detailed monitoring is enabled (disabled | enabled ).\n• network-interface.addresses.association.allocation-id - The allocation ID.\n• network-interface.addresses.association.association-id - The association ID.\n• network-interface.addresses.association.carrier-ip - The carrier IP address.\n• network-interface.addresses.association.customer-owned-ip - The customer-owned IP address.\n• network-interface.addresses.association.ip-owner-id - The owner ID of the private IPv4 address associated with the network interface.\n• network-interface.addresses.association.public-dns-name - The public DNS name.\n• network-interface.addresses.association.public-ip - The ID of the association of an Elastic IP address (IPv4) with a network interface.\n• network-interface.addresses.primary - Specifies whether the IPv4 address of the network interface is the primary private IPv4 address.\n• network-interface.addresses.private-dns-name - The private DNS name.\n• network-interface.addresses.private-ip-address - The private IPv4 address associated with the network interface.\n• network-interface.association.allocation-id - The allocation ID returned when you allocated the Elastic IP address (IPv4) for your network interface.\n• network-interface.association.association-id - The association ID returned when the network interface was associated with an IPv4 address.\n• network-interface.association.carrier-ip - The customer-owned IP address.\n• network-interface.association.customer-owned-ip - The customer-owned IP address.\n• network-interface.association.ip-owner-id - The owner of the Elastic IP address (IPv4) associated with the network interface.\n• network-interface.association.public-dns-name - The public DNS name.\n• network-interface.association.public-ip - The address of the Elastic IP address (IPv4) bound to the network interface.\n• network-interface.attachment.attach-time - The time that the network interface was attached to an instance.\n• network-interface.attachment.attachment-id - The ID of the interface attachment.\n• network-interface.attachment.delete-on-termination - Specifies whether the attachment is deleted when an instance is terminated.\n• network-interface.attachment.device-index - The device index to which the network interface is attached.\n• network-interface.attachment.instance-id - The ID of the instance to which the network interface is attached.\n• network-interface.attachment.instance-owner-id - The owner ID of the instance to which the network interface is attached.\n• network-interface.attachment.network-card-index - The index of the network card.\n• network-interface.attachment.status - The status of the attachment (attaching | attached | detaching | detached ).\n• network-interface.availability-zone - The Availability Zone for the network interface.\n• network-interface.deny-all-igw-traffic - A Boolean that indicates whether a network interface with an IPv6 address is unreachable from the public internet.\n• network-interface.description - The description of the network interface.\n• network-interface.group-id - The ID of a security group associated with the network interface.\n• network-interface.group-name - The name of a security group associated with the network interface.\n• network-interface.ipv4-prefixes.ipv4-prefix - The IPv4 prefixes that are assigned to the network interface.\n• network-interface.ipv6-address - The IPv6 address associated with the network interface.\n• network-interface.ipv6-addresses.ipv6-address - The IPv6 address associated with the network interface.\n• network-interface.ipv6-addresses.is-primary-ipv6 - A Boolean that indicates whether this is the primary IPv6 address.\n• network-interface.ipv6-native - A Boolean that indicates whether this is an IPv6 only network interface.\n• network-interface.ipv6-prefixes.ipv6-prefix - The IPv6 prefix assigned to the network interface.\n• network-interface.mac-address - The MAC address of the network interface.\n• network-interface.network-interface-id - The ID of the network interface.\n• network-interface.operator.managed - A Boolean that indicates whether the instance has a managed network interface.\n• network-interface.operator.principal - The principal that manages the network interface. Only valid for instances with managed network interfaces, where managed is true .\n• network-interface.outpost-arn - The ARN of the Outpost.\n• network-interface.owner-id - The ID of the owner of the network interface.\n• network-interface.private-dns-name - The private DNS name of the network interface.\n• network-interface.private-ip-address - The private IPv4 address.\n• network-interface.public-dns-name - The public DNS name.\n• network-interface.requester-id - The requester ID for the network interface.\n• network-interface.requester-managed - Indicates whether the network interface is being managed by Amazon Web Services.\n• network-interface.status - The status of the network interface (available ) | in-use ).\n• network-interface.source-dest-check - Whether the network interface performs source/destination checking. A value of true means that checking is enabled, and false means that checking is disabled. The value must be false for the network interface to perform network address translation (NAT) in your VPC.\n• network-interface.subnet-id - The ID of the subnet for the network interface.\n• network-interface.tag-key - The key of a tag assigned to the network interface.\n• network-interface.tag-value - The value of a tag assigned to the network interface.\n• network-interface.vpc-id - The ID of the VPC for the network interface.\n• network-performance-options.bandwidth-weighting - Where the performance boost is applied, if applicable. Valid values: default , vpc-1 , ebs-1 .\n• operator.managed - A Boolean that indicates whether this is a managed instance.\n• operator.principal - The principal that manages the instance. Only valid for managed instances, where managed is true .\n• outpost-arn - The Amazon Resource Name (ARN) of the Outpost.\n• owner-id - The Amazon Web Services account ID of the instance owner.\n• placement-group-name - The name of the placement group for the instance.\n• placement-partition-number - The partition in which the instance is located.\n• platform - The platform. To list only Windows instances, use windows .\n• platform-details - The platform (Linux/UNIX | Red Hat BYOL Linux | Red Hat Enterprise Linux | Red Hat Enterprise Linux with HA | Red Hat Enterprise Linux with High Availability | Red Hat Enterprise Linux with SQL Server Standard and HA | Red Hat Enterprise Linux with SQL Server Enterprise and HA | Red Hat Enterprise Linux with SQL Server Standard | Red Hat Enterprise Linux with SQL Server Web | Red Hat Enterprise Linux with SQL Server Enterprise | SQL Server Enterprise | SQL Server Standard | SQL Server Web | SUSE Linux | Ubuntu Pro | Windows | Windows BYOL | Windows with SQL Server Enterprise | Windows with SQL Server Standard | Windows with SQL Server Web ).\n• private-dns-name - The private IPv4 DNS name of the instance.\n• private-dns-name-options.enable-resource-name-dns-a-record - A Boolean that indicates whether to respond to DNS queries for instance hostnames with DNS A records.\n• private-dns-name-options.enable-resource-name-dns-aaaa-record - A Boolean that indicates whether to respond to DNS queries for instance hostnames with DNS AAAA records.\n• private-dns-name-options.hostname-type - The type of hostname (ip-name | resource-name ).\n• private-ip-address - The private IPv4 address of the instance. This can only be used to filter by the primary IP address of the network interface attached to the instance. To filter by additional IP addresses assigned to the network interface, use the filter network-interface.addresses.private-ip-address .\n• product-code - The product code associated with the AMI used to launch the instance.\n• product-code.type - The type of product code (devpay | marketplace ).\n• ramdisk-id - The RAM disk ID.\n• reason - The reason for the current state of the instance (for example, shows âUser Initiated [date]â when you stop or terminate the instance). Similar to the state-reason-code filter.\n• requester-id - The ID of the entity that launched the instance on your behalf (for example, Amazon Web Services Management Console, Auto Scaling, and so on).\n• reservation-id - The ID of the instanceâs reservation. A reservation ID is created any time you launch an instance. A reservation ID has a one-to-one relationship with an instance launch request, but can be associated with more than one instance if you launch multiple instances using the same launch request. For example, if you launch one instance, you get one reservation ID. If you launch ten instances using the same launch request, you also get one reservation ID.\n• root-device-name - The device name of the root device volume (for example, /dev/sda1 ).\n• root-device-type - The type of the root device volume (ebs | instance-store ).\n• source-dest-check - Indicates whether the instance performs source/destination checking. A value of true means that checking is enabled, and false means that checking is disabled. The value must be false for the instance to perform network address translation (NAT) in your VPC.\n• spot-instance-request-id - The ID of the Spot Instance request.\n• state-reason-code - The reason code for the state change.\n• state-reason-message - A message that describes the state change.\n• subnet-id - The ID of the subnet for the instance.\n• tag:<key> - The key/value combination of a tag assigned to the resource. Use the tag key in the filter name and the tag value as the filter value. For example, to find all resources that have a tag with the key Owner and the value TeamA , specify tag:Owner for the filter name and TeamA for the filter value.\n• tag-key - The key of a tag assigned to the resource. Use this filter to find all resources that have a tag with a specific key, regardless of the tag value.\n• tenancy - The tenancy of an instance (dedicated | default | host ).\n• tpm-support - Indicates if the instance is configured for NitroTPM support (v2.0 ).\n• usage-operation - The usage operation value for the instance (RunInstances | RunInstances:00g0 | RunInstances:0010 | RunInstances:1010 | RunInstances:1014 | RunInstances:1110 | RunInstances:0014 | RunInstances:0210 | RunInstances:0110 | RunInstances:0100 | RunInstances:0004 | RunInstances:0200 | RunInstances:000g | RunInstances:0g00 | RunInstances:0002 | RunInstances:0800 | RunInstances:0102 | RunInstances:0006 | RunInstances:0202 ).\n• usage-operation-update-time - The time that the usage operation was last updated, for example, 2022-09-15T17:15:20.000Z .\n• virtualization-type - The virtualization type of the instance (paravirtual | hvm ).\n• vpc-id - The ID of the VPC that the instance is running in.",
        "code_examples": [
          "```\n\"string\"\"string\"...\n```",
          "```\nName=string,Values=string,string...\n```",
          "```\n[{\"Name\":\"string\",\"Values\":[\"string\",...]}...]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 18,
          "content_length": 18864
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: To describe an instance\n\nThe following describe-instances example describes the specified instance.\n\nExample 2: To filter for instances with the specified type\n\nThe following describe-instances example uses filters to scope the results to instances of the specified type.\n\nFor example output, see Example 1.\n\nFor more information, see List and filter using the CLI in the Amazon EC2 User Guide.\n\nExample 3: To filter for instances with the specified type and Availability Zone\n\nThe following describe-instances example uses multiple filters to scope the results to instances with the specified type that are also in the specified Availability Zone.\n\nFor example output, see Example 1.\n\nExample 4: To filter for instances with the specified type and Availability Zone using a JSON file\n\nThe following describe-instances example uses a JSON input file to perform the same filtering as the previous example. When filters get more complicated, they can be easier to specify in a JSON file.\n\nContents of filters.json:\n\nFor example output, see Example 1.\n\nExample 5: To filter for instances with the specified Owner tag\n\nThe following describe-instances example uses tag filters to scope the results to instances that have a tag with the specified tag key (Owner), regardless of the tag value.\n\nFor example output, see Example 1.\n\nExample 6: To filter for instances with the specified my-team tag value\n\nThe following describe-instances example uses tag filters to scope the results to instances that have a tag with the specified tag value (my-team), regardless of the tag key.\n\nFor example output, see Example 1.\n\nExample 7: To filter for instances with the specified Owner tag and my-team value\n\nThe following describe-instances example uses tag filters to scope the results to instances that have the specified tag (Owner=my-team).\n\nFor example output, see Example 1.\n\nExample 8: To display only instance and subnet IDs for all instances\n\nThe following describe-instances examples use the --query parameter to display only the instance and subnet IDs for all instances, in JSON format.\n\nExample 9: To filter instances of the specified type and only display their instance IDs\n\nThe following describe-instances example uses filters to scope the results to instances of the specified type and the --query parameter to display only the instance IDs.\n\nExample 10: To filter instances of the specified type and only display their instance IDs, Availability Zone, and the specified tag value\n\nThe following describe-instances examples display the instance ID, Availability Zone, and the value of the Name tag for instances that have a tag with the name tag-key, in table format.\n\nExample 11: To describe instances in a partition placement group\n\nThe following describe-instances example describes the specified instance. The output includes the placement information for the instance, which contains the placement group name and the partition number for the instance.\n\nFor more information, see Describing instances in a placement group in the Amazon EC2 User Guide.\n\nExample 12: To filter to instances with the specified placement group and partition number\n\nThe following describe-instances example filters the results to only those instances with the specified placement group and partition number.\n\nThe following shows only the relevant information from the output.\n\nFor more information, see Describing instances in a placement group in the Amazon EC2 User Guide.\n\nExample 13: To filter to instances that are configured to allow access to tags from instance metadata\n\nThe following describe-instances example filters the results to only those instances that are configured to allow access to instance tags from instance metadata.\n\nThe following shows the expected output.\n\nFor more information, see Work with instance tags in instance metadata in the Amazon EC2 User Guide.",
        "code_examples": [
          "```\nawsec2describe-instances\\--instance-idsi-1234567890abcdef0\n```",
          "```\n{\"Reservations\":[{\"Groups\":[],\"Instances\":[{\"AmiLaunchIndex\":0,\"ImageId\":\"ami-0abcdef1234567890\",\"InstanceId\":\"i-1234567890abcdef0\",\"InstanceType\":\"t3.nano\",\"KeyName\":\"my-key-pair\",\"LaunchTime\":\"2022-11-15T10:48:59+00:00\",\"Monitoring\":{\"State\":\"disabled\"},\"Placement\":{\"AvailabilityZone\":\"us-east-2a\",\"GroupName\":\"\",\"Tenancy\":\"default\"},\"PrivateDnsName\":\"ip-10-0-0-157.us-east-2.compute.internal\",\"PrivateIpAddress\":\"10-0-0-157\",\"ProductCodes\":[],\"PublicDnsName\":\"ec2-34-253-223-13.us-east-2.compute.amazonaws.com\",\"PublicIpAddress\":\"34.253.223.13\",\"State\":{\"Code\":16,\"Name\":\"running\"},\"StateTransitionReason\":\"\",\"SubnetId\":\"subnet-04a636d18e83cfacb\",\"VpcId\":\"vpc-1234567890abcdef0\",\"Architecture\":\"x86_64\",\"BlockDeviceMappings\":[{\"DeviceName\":\"/dev/xvda\",\"Ebs\":{\"AttachTime\":\"2022-11-15T10:49:00+00:00\",\"DeleteOnTermination\":true,\"Status\":\"attached\",\"VolumeId\":\"vol-02e6ccdca7de29cf2\"}}],\"ClientToken\":\"1234abcd-1234-abcd-1234-d46a8903e9bc\",\"EbsOptimized\":true,\"EnaSupport\":true,\"Hypervisor\":\"xen\",\"IamInstanceProfile\":{\"Arn\":\"arn:aws:iam::111111111111:instance-profile/AmazonSSMRoleForInstancesQuickSetup\",\"Id\":\"111111111111111111111\"},\"NetworkInterfaces\":[{\"Association\":{\"IpOwnerId\":\"amazon\",\"PublicDnsName\":\"ec2-34-253-223-13.us-east-2.compute.amazonaws.com\",\"PublicIp\":\"34.253.223.13\"},\"Attachment\":{\"AttachTime\":\"2022-11-15T10:48:59+00:00\",\"AttachmentId\":\"eni-attach-1234567890abcdefg\",\"DeleteOnTermination\":true,\"DeviceIndex\":0,\"Status\":\"attached\",\"NetworkCardIndex\":0},\"Description\":\"\",\"Groups\":[{\"GroupName\":\"launch-wizard-146\",\"GroupId\":\"sg-1234567890abcdefg\"}],\"Ipv6Addresses\":[],\"MacAddress\":\"00:11:22:33:44:55\",\"NetworkInterfaceId\":\"eni-1234567890abcdefg\",\"OwnerId\":\"104024344472\",\"PrivateDnsName\":\"ip-10-0-0-157.us-east-2.compute.internal\",\"PrivateIpAddress\":\"10-0-0-157\",\"PrivateIpAddresses\":[{\"Association\":{\"IpOwnerId\":\"amazon\",\"PublicDnsName\":\"ec2-34-253-223-13.us-east-2.compute.amazonaws.com\",\"PublicIp\":\"34.253.223.13\"},\"Primary\":true,\"PrivateDnsName\":\"ip-10-0-0-157.us-east-2.compute.internal\",\"PrivateIpAddress\":\"10-0-0-157\"}],\"SourceDestCheck\":true,\"Status\":\"in-use\",\"SubnetId\":\"subnet-1234567890abcdefg\",\"VpcId\":\"vpc-1234567890abcdefg\",\"InterfaceType\":\"interface\"}],\"RootDeviceName\":\"/dev/xvda\",\"RootDeviceType\":\"ebs\",\"SecurityGroups\":[{\"GroupName\":\"launch-wizard-146\",\"GroupId\":\"sg-1234567890abcdefg\"}],\"SourceDestCheck\":true,\"Tags\":[{\"Key\":\"Name\",\"Value\":\"my-instance\"}],\"VirtualizationType\":\"hvm\",\"CpuOptions\":{\"CoreCount\":1,\"ThreadsPerCore\":2},\"CapacityReservationSpecification\":{\"CapacityReservationPreference\":\"open\"},\"HibernationOptions\":{\"Configured\":false},\"MetadataOptions\":{\"State\":\"applied\",\"HttpTokens\":\"optional\",\"HttpPutResponseHopLimit\":1,\"HttpEndpoint\":\"enabled\",\"HttpProtocolIpv6\":\"disabled\",\"InstanceMetadataTags\":\"enabled\"},\"EnclaveOptions\":{\"Enabled\":false},\"PlatformDetails\":\"Linux/UNIX\",\"UsageOperation\":\"RunInstances\",\"UsageOperationUpdateTime\":\"2022-11-15T10:48:59+00:00\",\"PrivateDnsNameOptions\":{\"HostnameType\":\"ip-name\",\"EnableResourceNameDnsARecord\":true,\"EnableResourceNameDnsAAAARecord\":false},\"MaintenanceOptions\":{\"AutoRecovery\":\"default\"}}],\"OwnerId\":\"111111111111\",\"ReservationId\":\"r-1234567890abcdefg\"}]}\n```",
          "```\nawsec2describe-instances\\--filtersName=instance-type,Values=m5.large\n```",
          "```\nawsec2describe-instances\\--filtersName=instance-type,Values=t2.micro,t3.microName=availability-zone,Values=us-east-2c\n```",
          "```\nawsec2describe-instances\\--filtersfile://filters.json\n```",
          "```\n[{\"Name\":\"instance-type\",\"Values\":[\"t2.micro\",\"t3.micro\"]},{\"Name\":\"availability-zone\",\"Values\":[\"us-east-2c\"]}]\n```",
          "```\nawsec2describe-instances\\--filters\"Name=tag-key,Values=Owner\"\n```",
          "```\nawsec2describe-instances\\--filters\"Name=tag-value,Values=my-team\"\n```",
          "```\nawsec2describe-instances\\--filters\"Name=tag:Owner,Values=my-team\"\n```",
          "```\nawsec2describe-instances\\--query'Reservations[*].Instances[*].{Instance:InstanceId,Subnet:SubnetId}'\\--outputjson\n```",
          "```\nawsec2describe-instances^--query\"Reservations[*].Instances[*].{Instance:InstanceId,Subnet:SubnetId}\"^--outputjson\n```",
          "```\n[{\"Instance\":\"i-057750d42936e468a\",\"Subnet\":\"subnet-069beee9b12030077\"},{\"Instance\":\"i-001efd250faaa6ffa\",\"Subnet\":\"subnet-0b715c6b7db68927a\"},{\"Instance\":\"i-027552a73f021f3bd\",\"Subnet\":\"subnet-0250c25a1f4e15235\"}...]\n```",
          "```\nawsec2describe-instances\\--filters\"Name=instance-type,Values=t2.micro\"\\--query\"Reservations[*].Instances[*].[InstanceId]\"\\--outputtext\n```",
          "```\ni-031c0dc19de2fb70ci-00d8bff789a736b75i-0b715c6b7db68927ai-0626d4edd54f1286di-00b8ae04f9f99908ei-0fc71c25d2374130c\n```",
          "```\nawsec2describe-instances\\--filtersName=tag-key,Values=Name\\--query'Reservations[*].Instances[*].{Instance:InstanceId,AZ:Placement.AvailabilityZone,Name:Tags[?Key==`Name`]|[0].Value}'\\--outputtable\n```",
          "```\nawsec2describe-instances^--filtersName=tag-key,Values=Name^--query\"Reservations[*].Instances[*].{Instance:InstanceId,AZ:Placement.AvailabilityZone,Name:Tags[?Key=='Name']|[0].Value}\"^--outputtable\n```",
          "```\n-------------------------------------------------------------|DescribeInstances|+--------------+-----------------------+--------------------+|AZ|Instance|Name|+--------------+-----------------------+--------------------+|us-east-2b|i-057750d42936e468a|my-prod-server||us-east-2a|i-001efd250faaa6ffa|test-server-1||us-east-2a|i-027552a73f021f3bd|test-server-2|+--------------+-----------------------+--------------------+\n```",
          "```\nawsec2describe-instances\\--instance-idsi-0123a456700123456\\--query\"Reservations[*].Instances[*].Placement\"\n```",
          "```\n[[{\"AvailabilityZone\":\"us-east-1c\",\"GroupName\":\"HDFS-Group-A\",\"PartitionNumber\":3,\"Tenancy\":\"default\"}]]\n```",
          "```\nawsec2describe-instances\\--filters\"Name=placement-group-name,Values=HDFS-Group-A\"\"Name=placement-partition-number,Values=7\"\n```",
          "```\n\"Instances\":[{\"InstanceId\":\"i-0123a456700123456\",\"InstanceType\":\"r4.large\",\"Placement\":{\"AvailabilityZone\":\"us-east-1c\",\"GroupName\":\"HDFS-Group-A\",\"PartitionNumber\":7,\"Tenancy\":\"default\"}},{\"InstanceId\":\"i-9876a543210987654\",\"InstanceType\":\"r4.large\",\"Placement\":{\"AvailabilityZone\":\"us-east-1c\",\"GroupName\":\"HDFS-Group-A\",\"PartitionNumber\":7,\"Tenancy\":\"default\"}],\n```",
          "```\nawsec2describe-instances\\--filters\"Name=metadata-options.instance-metadata-tags,Values=enabled\"\\--query\"Reservations[*].Instances[*].InstanceId\"\\--outputtext\n```",
          "```\ni-1234567890abcdefgi-abcdefg1234567890i-11111111aaaaaaaaai-aaaaaaaa111111111\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 41,
          "content_length": 4256
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "NextToken -> (string)\n\nReservations -> (list)\n\nInformation about the reservations.\n\nDescribes a launch request for one or more instances, and includes owner, requester, and security group information that applies to all instances in the launch request.\n\nReservationId -> (string)\n\nRequesterId -> (string)\n\nDescribes a security group.\n\nGroupName -> (string)\n\nDescribes an instance.\n\nArchitecture -> (string)\n\nThe architecture of the image.\n\nBlockDeviceMappings -> (list)\n\nAny block device mapping entries for the instance.\n\nDescribes a block device mapping.\n\nDeviceName -> (string)\n\nParameters used to automatically set up EBS volumes when the instance is launched.\n\nAttachTime -> (timestamp)\n\nDeleteOnTermination -> (boolean)\n\nThe attachment state.\n\nVolumeId -> (string)\n\nAssociatedResource -> (string)\n\nVolumeOwnerId -> (string)\n\nThe ID of the Amazon Web Services account that owns the volume.\n\nThis parameter is returned only for volumes that are attached to Amazon Web Services-managed resources.\n\nOperator -> (structure)\n\nThe service provider that manages the EBS volume.\n\nManaged -> (boolean)\n\nPrincipal -> (string)\n\nClientToken -> (string)\n\nEbsOptimized -> (boolean)\n\nEnaSupport -> (boolean)\n\nHypervisor -> (string)\n\nThe hypervisor type of the instance. The value xen is used for both Xen and Nitro hypervisors.\n\nIamInstanceProfile -> (structure)\n\nThe IAM instance profile associated with the instance, if applicable.\n\nInstanceLifecycle -> (string)\n\nIndicates whether this is a Spot Instance or a Scheduled Instance.\n\nElasticGpuAssociations -> (list)\n\n• capacity-block\n\n[Note] NoteAmazon Elastic Graphics reached end of life on January 8, 2024.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 38,
          "content_length": 1650
        }
      },
      {
        "header": "Note",
        "content": "[Note] NoteAmazon Elastic Graphics reached end of life on January 8, 2024.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 74
        }
      },
      {
        "header": "Note",
        "content": "Describes the association between an instance and an Elastic Graphics accelerator.\n\nElasticGpuId -> (string)\n\nElasticGpuAssociationId -> (string)\n\nElasticGpuAssociationState -> (string)\n\nElasticGpuAssociationTime -> (string)\n\nElasticInferenceAcceleratorAssociations -> (list)\n\n[Note] NoteAmazon Elastic Inference is no longer available.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 336
        }
      },
      {
        "header": "Note",
        "content": "[Note] NoteAmazon Elastic Inference is no longer available.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 59
        }
      },
      {
        "header": "Note",
        "content": "Describes the association between an instance and an elastic inference accelerator.\n\nElasticInferenceAcceleratorArn -> (string)\n\nElasticInferenceAcceleratorAssociationId -> (string)\n\nElasticInferenceAcceleratorAssociationState -> (string)\n\nElasticInferenceAcceleratorAssociationTime -> (timestamp)\n\nNetworkInterfaces -> (list)\n\nThe network interfaces for the instance.\n\nDescribes a network interface.\n\nAssociation -> (structure)\n\nThe association information for an Elastic IPv4 associated with the network interface.\n\nCarrierIp -> (string)\n\nCustomerOwnedIp -> (string)\n\nIpOwnerId -> (string)\n\nPublicDnsName -> (string)\n\nPublicIp -> (string)\n\nAttachment -> (structure)\n\nThe network interface attachment.\n\nAttachTime -> (timestamp)\n\nAttachmentId -> (string)\n\nDeleteOnTermination -> (boolean)\n\nDeviceIndex -> (integer)\n\nThe attachment state.\n\nNetworkCardIndex -> (integer)\n\nEnaSrdSpecification -> (structure)\n\nContains the ENA Express settings for the network interface thatâs attached to the instance.\n\nEnaSrdEnabled -> (boolean)\n\nEnaSrdUdpSpecification -> (structure)\n\nConfigures ENA Express for UDP network traffic.\n\nEnaSrdUdpEnabled -> (boolean)\n\nEnaQueueCount -> (integer)\n\nDescription -> (string)\n\nThe security groups.\n\nDescribes a security group.\n\nGroupName -> (string)\n\nIpv6Addresses -> (list)\n\nThe IPv6 addresses associated with the network interface.\n\nDescribes an IPv6 address.\n\nIpv6Address -> (string)\n\nIsPrimaryIpv6 -> (boolean)\n\nMacAddress -> (string)\n\nNetworkInterfaceId -> (string)\n\nPrivateDnsName -> (string)\n\nPrivateIpAddress -> (string)\n\nPrivateIpAddresses -> (list)\n\nThe private IPv4 addresses associated with the network interface.\n\nDescribes a private IPv4 address.\n\nAssociation -> (structure)\n\nThe association information for an Elastic IP address for the network interface.\n\nCarrierIp -> (string)\n\nCustomerOwnedIp -> (string)\n\nIpOwnerId -> (string)\n\nPublicDnsName -> (string)\n\nPublicIp -> (string)\n\nPrimary -> (boolean)\n\nPrivateDnsName -> (string)\n\nPrivateIpAddress -> (string)\n\nSourceDestCheck -> (boolean)\n\nThe status of the network interface.\n\nSubnetId -> (string)\n\nInterfaceType -> (string)\n\nThe type of network interface.\n\nValid values: interface | efa | efa-only | evs | trunk\n\nIpv4Prefixes -> (list)\n\nThe IPv4 delegated prefixes that are assigned to the network interface.\n\nInformation about an IPv4 prefix.\n\nIpv4Prefix -> (string)\n\nIpv6Prefixes -> (list)\n\nThe IPv6 delegated prefixes that are assigned to the network interface.\n\nInformation about an IPv6 prefix.\n\nIpv6Prefix -> (string)\n\nConnectionTrackingConfiguration -> (structure)\n\nA security group connection tracking configuration that enables you to set the timeout for connection tracking on an Elastic network interface. For more information, see Connection tracking timeouts in the Amazon EC2 User Guide .\n\nTcpEstablishedTimeout -> (integer)\n\nUdpStreamTimeout -> (integer)\n\nUdpTimeout -> (integer)\n\nOperator -> (structure)\n\nThe service provider that manages the network interface.\n\nManaged -> (boolean)\n\nPrincipal -> (string)\n\nOutpostArn -> (string)\n\nRootDeviceName -> (string)\n\nRootDeviceType -> (string)\n\nThe root device type used by the AMI. The AMI can use an EBS volume or an instance store volume.\n\nSecurityGroups -> (list)\n\nThe security groups for the instance.\n\nDescribes a security group.\n\nGroupName -> (string)\n\nSourceDestCheck -> (boolean)\n\nSpotInstanceRequestId -> (string)\n\nSriovNetSupport -> (string)\n\nStateReason -> (structure)\n\nThe reason for the most recent state transition.\n\nThe message for the state change.\n\nAny tags assigned to the instance.\n\nConstraints: Tag keys are case-sensitive and accept a maximum of 127 Unicode characters. May not begin with aws: .\n\nThe value of the tag.\n\nConstraints: Tag values are case-sensitive and accept a maximum of 256 Unicode characters.\n\nVirtualizationType -> (string)\n\nThe virtualization type of the instance.\n\nCpuOptions -> (structure)\n\nThe CPU options for the instance.\n\nCoreCount -> (integer)\n\nThreadsPerCore -> (integer)\n\nAmdSevSnp -> (string)\n\nIndicates whether the instance is enabled for AMD SEV-SNP. For more information, see AMD SEV-SNP .\n\nCapacityBlockId -> (string)\n\nThe ID of the Capacity Block.\n\n• instance-store\n\n• Server.InsufficientInstanceCapacity : There was insufficient capacity available to satisfy the launch request.\n• Server.InternalError : An internal error caused the instance to terminate during launch.\n• Server.ScheduledStop : The instance was stopped due to a scheduled retirement.\n• Server.SpotInstanceShutdown : The instance was stopped because the number of Spot requests with a maximum price equal to or higher than the Spot price exceeded available capacity or because of an increase in the Spot price.\n• Server.SpotInstanceTermination : The instance was terminated because the number of Spot requests with a maximum price equal to or higher than the Spot price exceeded available capacity or because of an increase in the Spot price.\n• Client.InstanceInitiatedShutdown : The instance was shut down from the operating system of the instance.\n• Client.InstanceTerminated : The instance was terminated or rebooted during AMI creation.\n• Client.InternalError : A client error caused the instance to terminate during launch.\n• Client.InvalidSnapshot.NotFound : The specified snapshot was not found.\n• Client.UserInitiatedHibernate : Hibernation was initiated on the instance.\n• Client.UserInitiatedShutdown : The instance was shut down using the Amazon EC2 API.\n• Client.VolumeLimitExceeded : The limit on the number of EBS volumes or total storage was exceeded. Decrease usage or request an increase in your account limits.\n\n• paravirtual\n\n[Note] NoteFor P5 instances, a Capacity Block ID refers to a group of instances. For Trn2u instances, a capacity block ID refers to an EC2 UltraServer.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 107,
          "content_length": 5773
        }
      },
      {
        "header": "Note",
        "content": "CapacityReservationId -> (string)\n\nCapacityReservationSpecification -> (structure)\n\nInformation about the Capacity Reservation targeting option.\n\nCapacityReservationPreference -> (string)\n\nDescribes the instanceâs Capacity Reservation preferences. Possible preferences include:\n\nCapacityReservationTarget -> (structure)\n\nInformation about the targeted Capacity Reservation or Capacity Reservation group.\n\nCapacityReservationId -> (string)\n\nCapacityReservationResourceGroupArn -> (string)\n\nHibernationOptions -> (structure)\n\nIndicates whether the instance is enabled for hibernation.\n\nConfigured -> (boolean)\n\nThe license configurations for the instance.\n\nDescribes a license configuration.\n\nLicenseConfigurationArn -> (string)\n\nMetadataOptions -> (structure)\n\nThe metadata options for the instance.\n\nThe state of the metadata option changes.\n\npending - The metadata options are being updated and the instance is not ready to process metadata traffic with the new selection.\n\napplied - The metadata options have been successfully applied on the instance.\n\nHttpTokens -> (string)\n\nIndicates whether IMDSv2 is required.\n\nHttpPutResponseHopLimit -> (integer)\n\nThe maximum number of hops that the metadata token can travel.\n\nPossible values: Integers from 1 to 64\n\nHttpEndpoint -> (string)\n\nIndicates whether the HTTP metadata endpoint on your instances is enabled or disabled.\n\nIf the value is disabled , you cannot access your instance metadata.\n\nHttpProtocolIpv6 -> (string)\n\nIndicates whether the IPv6 endpoint for the instance metadata service is enabled or disabled.\n\nInstanceMetadataTags -> (string)\n\nIndicates whether access to instance tags from the instance metadata is enabled or disabled. For more information, see Work with instance tags using the instance metadata .\n\nEnclaveOptions -> (structure)\n\nIndicates whether the instance is enabled for Amazon Web Services Nitro Enclaves.\n\nEnabled -> (boolean)\n\nBootMode -> (string)\n\nThe boot mode that was specified by the AMI. If the value is uefi-preferred , the AMI supports both UEFI and Legacy BIOS. The currentInstanceBootMode parameter is the boot mode that is used to boot the instance at launch or start.\n\n• open - The instance can run in any open Capacity Reservation that has matching attributes (instance type, platform, Availability Zone).\n• none - The instance avoids running in a Capacity Reservation even if one is available. The instance runs in On-Demand capacity.\n\n• capacity-reservations-only\n\n• optional - IMDSv2 is optional, which means that you can use either IMDSv2 or IMDSv1.\n• required - IMDSv2 is required, which means that IMDSv1 is disabled, and you must use IMDSv2.\n\n[Note] NoteThe operating system contained in the AMI must be configured to support the specified boot mode.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 37,
          "content_length": 2758
        }
      },
      {
        "header": "Note",
        "content": "For more information, see Boot modes in the Amazon EC2 User Guide .\n\nPlatformDetails -> (string)\n\nUsageOperation -> (string)\n\nUsageOperationUpdateTime -> (timestamp)\n\nPrivateDnsNameOptions -> (structure)\n\nThe options for the instance hostname.\n\nHostnameType -> (string)\n\nThe type of hostname to assign to an instance.\n\nEnableResourceNameDnsARecord -> (boolean)\n\nEnableResourceNameDnsAAAARecord -> (boolean)\n\nIpv6Address -> (string)\n\nTpmSupport -> (string)\n\nMaintenanceOptions -> (structure)\n\nProvides information on the recovery and maintenance options of your instance.\n\nAutoRecovery -> (string)\n\nProvides information on the current automatic recovery behavior of your instance.\n\nRebootMigration -> (string)\n\nSpecifies whether to attempt reboot migration during a user-initiated reboot of an instance that has a scheduled system-reboot event:\n\nThis setting only applies to supported instances that have a scheduled reboot event. For more information, see Enable or disable reboot migration in the Amazon EC2 User Guide .\n\nCurrentInstanceBootMode -> (string)\n\nThe boot mode that is used to boot the instance at launch or start. For more information, see Boot modes in the Amazon EC2 User Guide .\n\nNetworkPerformanceOptions -> (structure)\n\nContains settings for the network performance options for your instance.\n\nBandwidthWeighting -> (string)\n\nWhen you configure network bandwidth weighting, you can boost your baseline bandwidth for either networking or EBS by up to 25%. The total available baseline bandwidth for your instance remains the same. The default option uses the standard bandwidth configuration for your instance type.\n\nOperator -> (structure)\n\nThe service provider that manages the instance.\n\nManaged -> (boolean)\n\nPrincipal -> (string)\n\nInstanceId -> (string)\n\nState -> (structure)\n\nThe current state of the instance.\n\nThe state of the instance as a 16-bit unsigned integer.\n\nThe high byte is all of the bits between 2^8 and (2^16)-1, which equals decimal values between 256 and 65,535. These numerical values are used for internal purposes and should be ignored.\n\nThe low byte is all of the bits between 2^0 and (2^8)-1, which equals decimal values between 0 and 255.\n\nThe valid values for instance-state-code will all be in the range of the low byte and they are:\n\nYou can ignore the high byte value by zeroing out all of the bits above 2^8 or 256 in decimal.\n\nThe current state of the instance.\n\nPrivateDnsName -> (string)\n\n[IPv4 only] The private DNS hostname name assigned to the instance. This DNS hostname can only be used inside the Amazon EC2 network. This name is not available until the instance enters the running state.\n\nThe Amazon-provided DNS server resolves Amazon-provided private DNS hostnames if youâve enabled DNS resolution and DNS hostnames in your VPC. If you are not using the Amazon-provided DNS server in your VPC, your custom domain name servers must resolve the hostname as appropriate.\n\nPublicDnsName -> (string)\n\nStateTransitionReason -> (string)\n\nAmiLaunchIndex -> (integer)\n\nProductCodes -> (list)\n\nThe product codes attached to this instance, if applicable.\n\nDescribes a product code.\n\nProductCodeId -> (string)\n\nProductCodeType -> (string)\n\nThe type of product code.\n\nInstanceType -> (string)\n\nLaunchTime -> (timestamp)\n\nPlacement -> (structure)\n\nThe location where the instance launched, if applicable.\n\nAvailabilityZoneId -> (string)\n\nThe ID of the Availability Zone of the instance.\n\nOn input, you can specify AvailabilityZone or AvailabilityZoneId , but not both. If you specify neither one, Amazon EC2 automatically selects an Availability Zone for you.\n\nThis parameter is not supported for CreateFleet .\n\nAffinity -> (string)\n\nThe affinity setting for the instance on the Dedicated Host.\n\nThis parameter is not supported for CreateFleet or ImportInstance .\n\nGroupName -> (string)\n\nThe name of the placement group that the instance is in.\n\nOn input, you can specify GroupId or GroupName , but not both.\n\nPartitionNumber -> (integer)\n\nThe number of the partition that the instance is in. Valid only if the placement group strategy is set to partition .\n\nThis parameter is not supported for CreateFleet .\n\nThe ID of the Dedicated Host on which the instance resides.\n\nThis parameter is not supported for CreateFleet or ImportInstance .\n\nThe tenancy of the instance. An instance with a tenancy of dedicated runs on single-tenant hardware.\n\nThis parameter is not supported for CreateFleet . The host tenancy is not supported for ImportInstance or for T3 instances that are configured for the unlimited CPU credit option.\n\nSpreadDomain -> (string)\n\nHostResourceGroupArn -> (string)\n\nThe ARN of the host resource group in which to launch the instances.\n\nOn input, if you specify this parameter, either omit the Tenancy parameter or set it to host .\n\nThis parameter is not supported for CreateFleet .\n\nThe ID of the placement group that the instance is in.\n\nOn input, you can specify GroupId or GroupName , but not both.\n\nAvailabilityZone -> (string)\n\nThe Availability Zone of the instance.\n\nOn input, you can specify AvailabilityZone or AvailabilityZoneId , but not both. If you specify neither one, Amazon EC2 automatically selects an Availability Zone for you.\n\nThis parameter is not supported for CreateFleet .\n\nKernelId -> (string)\n\nRamdiskId -> (string)\n\nPlatform -> (string)\n\nThe platform. This value is windows for Windows instances; otherwise, it is empty.\n\nMonitoring -> (structure)\n\nThe monitoring for the instance.\n\nIndicates whether detailed monitoring is enabled. Otherwise, basic monitoring is enabled.\n\nSubnetId -> (string)\n\nPrivateIpAddress -> (string)\n\nPublicIpAddress -> (string)\n\nThe public IPv4 address, or the Carrier IP address assigned to the instance, if applicable.\n\nA Carrier IP address only applies to an instance launched in a subnet associated with a Wavelength Zone.\n\n• legacy-bios\n• uefi-preferred\n\n• resource-name\n\n• default - Amazon EC2 attempts to migrate the instance to new hardware (reboot migration). If successful, the system-reboot event is cleared. If unsuccessful, an in-place reboot occurs and the event remains scheduled.\n• disabled - Amazon EC2 keeps the instance on the same hardware (in-place reboot). The system-reboot event remains scheduled.\n\n• legacy-bios\n\n• 0 : pending\n• 16 : running\n• 32 : shutting-down\n• 48 : terminated\n• 64 : stopping\n• 80 : stopped\n\n• shutting-down\n\n• marketplace\n\n• c5.12xlarge\n• c5.18xlarge\n• c5.24xlarge\n• c5a.2xlarge\n• c5a.4xlarge\n• c5a.8xlarge\n• c5a.12xlarge\n• c5a.16xlarge\n• c5a.24xlarge\n• c5ad.xlarge\n• c5ad.2xlarge\n• c5ad.4xlarge\n• c5ad.8xlarge\n• c5ad.12xlarge\n• c5ad.16xlarge\n• c5ad.24xlarge\n• c5d.2xlarge\n• c5d.4xlarge\n• c5d.9xlarge\n• c5d.12xlarge\n• c5d.18xlarge\n• c5d.24xlarge\n• c5n.2xlarge\n• c5n.4xlarge\n• c5n.9xlarge\n• c5n.18xlarge\n• c6g.2xlarge\n• c6g.4xlarge\n• c6g.8xlarge\n• c6g.12xlarge\n• c6g.16xlarge\n• c6gd.medium\n• c6gd.xlarge\n• c6gd.2xlarge\n• c6gd.4xlarge\n• c6gd.8xlarge\n• c6gd.12xlarge\n• c6gd.16xlarge\n• c6gn.medium\n• c6gn.xlarge\n• c6gn.2xlarge\n• c6gn.4xlarge\n• c6gn.8xlarge\n• c6gn.12xlarge\n• c6gn.16xlarge\n• c6i.2xlarge\n• c6i.4xlarge\n• c6i.8xlarge\n• c6i.12xlarge\n• c6i.16xlarge\n• c6i.24xlarge\n• c6i.32xlarge\n• cc1.4xlarge\n• cc2.8xlarge\n• cg1.4xlarge\n• cr1.8xlarge\n• d3en.xlarge\n• d3en.2xlarge\n• d3en.4xlarge\n• d3en.6xlarge\n• d3en.8xlarge\n• d3en.12xlarge\n• dl1.24xlarge\n• f1.16xlarge\n• g3.16xlarge\n• g4ad.xlarge\n• g4ad.2xlarge\n• g4ad.4xlarge\n• g4ad.8xlarge\n• g4ad.16xlarge\n• g4dn.xlarge\n• g4dn.2xlarge\n• g4dn.4xlarge\n• g4dn.8xlarge\n• g4dn.12xlarge\n• g4dn.16xlarge\n• g5.12xlarge\n• g5.16xlarge\n• g5.24xlarge\n• g5.48xlarge\n• g5g.2xlarge\n• g5g.4xlarge\n• g5g.8xlarge\n• g5g.16xlarge\n• hi1.4xlarge\n• hpc6a.48xlarge\n• hs1.8xlarge\n• h1.16xlarge\n• i3.16xlarge\n• i3en.xlarge\n• i3en.2xlarge\n• i3en.3xlarge\n• i3en.6xlarge\n• i3en.12xlarge\n• i3en.24xlarge\n• im4gn.large\n• im4gn.xlarge\n• im4gn.2xlarge\n• im4gn.4xlarge\n• im4gn.8xlarge\n• im4gn.16xlarge\n• inf1.xlarge\n• inf1.2xlarge\n• inf1.6xlarge\n• inf1.24xlarge\n• is4gen.medium\n• is4gen.large\n• is4gen.xlarge\n• is4gen.2xlarge\n• is4gen.4xlarge\n• is4gen.8xlarge\n• m4.10xlarge\n• m4.16xlarge\n• m5.12xlarge\n• m5.16xlarge\n• m5.24xlarge\n• m5a.2xlarge\n• m5a.4xlarge\n• m5a.8xlarge\n• m5a.12xlarge\n• m5a.16xlarge\n• m5a.24xlarge\n• m5ad.xlarge\n• m5ad.2xlarge\n• m5ad.4xlarge\n• m5ad.8xlarge\n• m5ad.12xlarge\n• m5ad.16xlarge\n• m5ad.24xlarge\n• m5d.2xlarge\n• m5d.4xlarge\n• m5d.8xlarge\n• m5d.12xlarge\n• m5d.16xlarge\n• m5d.24xlarge\n• m5dn.xlarge\n• m5dn.2xlarge\n• m5dn.4xlarge\n• m5dn.8xlarge\n• m5dn.12xlarge\n• m5dn.16xlarge\n• m5dn.24xlarge\n• m5n.2xlarge\n• m5n.4xlarge\n• m5n.8xlarge\n• m5n.12xlarge\n• m5n.16xlarge\n• m5n.24xlarge\n• m5zn.xlarge\n• m5zn.2xlarge\n• m5zn.3xlarge\n• m5zn.6xlarge\n• m5zn.12xlarge\n• m6a.2xlarge\n• m6a.4xlarge\n• m6a.8xlarge\n• m6a.12xlarge\n• m6a.16xlarge\n• m6a.24xlarge\n• m6a.32xlarge\n• m6a.48xlarge\n• m6g.2xlarge\n• m6g.4xlarge\n• m6g.8xlarge\n• m6g.12xlarge\n• m6g.16xlarge\n• m6gd.medium\n• m6gd.xlarge\n• m6gd.2xlarge\n• m6gd.4xlarge\n• m6gd.8xlarge\n• m6gd.12xlarge\n• m6gd.16xlarge\n• m6i.2xlarge\n• m6i.4xlarge\n• m6i.8xlarge\n• m6i.12xlarge\n• m6i.16xlarge\n• m6i.24xlarge\n• m6i.32xlarge\n• p2.16xlarge\n• p3.16xlarge\n• p3dn.24xlarge\n• p4d.24xlarge\n• r4.16xlarge\n• r5.12xlarge\n• r5.16xlarge\n• r5.24xlarge\n• r5a.2xlarge\n• r5a.4xlarge\n• r5a.8xlarge\n• r5a.12xlarge\n• r5a.16xlarge\n• r5a.24xlarge\n• r5ad.xlarge\n• r5ad.2xlarge\n• r5ad.4xlarge\n• r5ad.8xlarge\n• r5ad.12xlarge\n• r5ad.16xlarge\n• r5ad.24xlarge\n• r5b.2xlarge\n• r5b.4xlarge\n• r5b.8xlarge\n• r5b.12xlarge\n• r5b.16xlarge\n• r5b.24xlarge\n• r5d.2xlarge\n• r5d.4xlarge\n• r5d.8xlarge\n• r5d.12xlarge\n• r5d.16xlarge\n• r5d.24xlarge\n• r5dn.xlarge\n• r5dn.2xlarge\n• r5dn.4xlarge\n• r5dn.8xlarge\n• r5dn.12xlarge\n• r5dn.16xlarge\n• r5dn.24xlarge\n• r5n.2xlarge\n• r5n.4xlarge\n• r5n.8xlarge\n• r5n.12xlarge\n• r5n.16xlarge\n• r5n.24xlarge\n• r6g.2xlarge\n• r6g.4xlarge\n• r6g.8xlarge\n• r6g.12xlarge\n• r6g.16xlarge\n• r6gd.medium\n• r6gd.xlarge\n• r6gd.2xlarge\n• r6gd.4xlarge\n• r6gd.8xlarge\n• r6gd.12xlarge\n• r6gd.16xlarge\n• r6i.2xlarge\n• r6i.4xlarge\n• r6i.8xlarge\n• r6i.12xlarge\n• r6i.16xlarge\n• r6i.24xlarge\n• r6i.32xlarge\n• t3a.2xlarge\n• t4g.2xlarge\n• u-6tb1.56xlarge\n• u-6tb1.112xlarge\n• u-9tb1.112xlarge\n• u-12tb1.112xlarge\n• u-6tb1.metal\n• u-9tb1.metal\n• u-12tb1.metal\n• u-18tb1.metal\n• u-24tb1.metal\n• vt1.3xlarge\n• vt1.6xlarge\n• vt1.24xlarge\n• x1.16xlarge\n• x1.32xlarge\n• x1e.2xlarge\n• x1e.4xlarge\n• x1e.8xlarge\n• x1e.16xlarge\n• x1e.32xlarge\n• x2iezn.2xlarge\n• x2iezn.4xlarge\n• x2iezn.6xlarge\n• x2iezn.8xlarge\n• x2iezn.12xlarge\n• x2iezn.metal\n• x2gd.medium\n• x2gd.xlarge\n• x2gd.2xlarge\n• x2gd.4xlarge\n• x2gd.8xlarge\n• x2gd.12xlarge\n• x2gd.16xlarge\n• z1d.2xlarge\n• z1d.3xlarge\n• z1d.6xlarge\n• z1d.12xlarge\n• x2idn.16xlarge\n• x2idn.24xlarge\n• x2idn.32xlarge\n• x2iedn.xlarge\n• x2iedn.2xlarge\n• x2iedn.4xlarge\n• x2iedn.8xlarge\n• x2iedn.16xlarge\n• x2iedn.24xlarge\n• x2iedn.32xlarge\n• c6a.2xlarge\n• c6a.4xlarge\n• c6a.8xlarge\n• c6a.12xlarge\n• c6a.16xlarge\n• c6a.24xlarge\n• c6a.32xlarge\n• c6a.48xlarge\n• i4i.2xlarge\n• i4i.4xlarge\n• i4i.8xlarge\n• i4i.16xlarge\n• i4i.32xlarge\n• x2idn.metal\n• x2iedn.metal\n• c7g.2xlarge\n• c7g.4xlarge\n• c7g.8xlarge\n• c7g.12xlarge\n• c7g.16xlarge\n• c6id.xlarge\n• c6id.2xlarge\n• c6id.4xlarge\n• c6id.8xlarge\n• c6id.12xlarge\n• c6id.16xlarge\n• c6id.24xlarge\n• c6id.32xlarge\n• m6id.xlarge\n• m6id.2xlarge\n• m6id.4xlarge\n• m6id.8xlarge\n• m6id.12xlarge\n• m6id.16xlarge\n• m6id.24xlarge\n• m6id.32xlarge\n• r6id.xlarge\n• r6id.2xlarge\n• r6id.4xlarge\n• r6id.8xlarge\n• r6id.12xlarge\n• r6id.16xlarge\n• r6id.24xlarge\n• r6id.32xlarge\n• r6a.2xlarge\n• r6a.4xlarge\n• r6a.8xlarge\n• r6a.12xlarge\n• r6a.16xlarge\n• r6a.24xlarge\n• r6a.32xlarge\n• r6a.48xlarge\n• p4de.24xlarge\n• u-3tb1.56xlarge\n• u-18tb1.112xlarge\n• u-24tb1.112xlarge\n• trn1.2xlarge\n• trn1.32xlarge\n• hpc6id.32xlarge\n• c6in.xlarge\n• c6in.2xlarge\n• c6in.4xlarge\n• c6in.8xlarge\n• c6in.12xlarge\n• c6in.16xlarge\n• c6in.24xlarge\n• c6in.32xlarge\n• m6in.xlarge\n• m6in.2xlarge\n• m6in.4xlarge\n• m6in.8xlarge\n• m6in.12xlarge\n• m6in.16xlarge\n• m6in.24xlarge\n• m6in.32xlarge\n• m6idn.large\n• m6idn.xlarge\n• m6idn.2xlarge\n• m6idn.4xlarge\n• m6idn.8xlarge\n• m6idn.12xlarge\n• m6idn.16xlarge\n• m6idn.24xlarge\n• m6idn.32xlarge\n• r6in.xlarge\n• r6in.2xlarge\n• r6in.4xlarge\n• r6in.8xlarge\n• r6in.12xlarge\n• r6in.16xlarge\n• r6in.24xlarge\n• r6in.32xlarge\n• r6idn.large\n• r6idn.xlarge\n• r6idn.2xlarge\n• r6idn.4xlarge\n• r6idn.8xlarge\n• r6idn.12xlarge\n• r6idn.16xlarge\n• r6idn.24xlarge\n• r6idn.32xlarge\n• m7g.2xlarge\n• m7g.4xlarge\n• m7g.8xlarge\n• m7g.12xlarge\n• m7g.16xlarge\n• r7g.2xlarge\n• r7g.4xlarge\n• r7g.8xlarge\n• r7g.12xlarge\n• r7g.16xlarge\n• m6idn.metal\n• r6idn.metal\n• inf2.xlarge\n• inf2.8xlarge\n• inf2.24xlarge\n• inf2.48xlarge\n• trn1n.32xlarge\n• i4g.2xlarge\n• i4g.4xlarge\n• i4g.8xlarge\n• i4g.16xlarge\n• hpc7g.4xlarge\n• hpc7g.8xlarge\n• hpc7g.16xlarge\n• c7gn.medium\n• c7gn.xlarge\n• c7gn.2xlarge\n• c7gn.4xlarge\n• c7gn.8xlarge\n• c7gn.12xlarge\n• c7gn.16xlarge\n• p5.48xlarge\n• m7i.2xlarge\n• m7i.4xlarge\n• m7i.8xlarge\n• m7i.12xlarge\n• m7i.16xlarge\n• m7i.24xlarge\n• m7i.48xlarge\n• m7i-flex.large\n• m7i-flex.xlarge\n• m7i-flex.2xlarge\n• m7i-flex.4xlarge\n• m7i-flex.8xlarge\n• m7a.2xlarge\n• m7a.4xlarge\n• m7a.8xlarge\n• m7a.12xlarge\n• m7a.16xlarge\n• m7a.24xlarge\n• m7a.32xlarge\n• m7a.48xlarge\n• m7a.metal-48xl\n• hpc7a.12xlarge\n• hpc7a.24xlarge\n• hpc7a.48xlarge\n• hpc7a.96xlarge\n• c7gd.medium\n• c7gd.xlarge\n• c7gd.2xlarge\n• c7gd.4xlarge\n• c7gd.8xlarge\n• c7gd.12xlarge\n• c7gd.16xlarge\n• m7gd.medium\n• m7gd.xlarge\n• m7gd.2xlarge\n• m7gd.4xlarge\n• m7gd.8xlarge\n• m7gd.12xlarge\n• m7gd.16xlarge\n• r7gd.medium\n• r7gd.xlarge\n• r7gd.2xlarge\n• r7gd.4xlarge\n• r7gd.8xlarge\n• r7gd.12xlarge\n• r7gd.16xlarge\n• r7a.2xlarge\n• r7a.4xlarge\n• r7a.8xlarge\n• r7a.12xlarge\n• r7a.16xlarge\n• r7a.24xlarge\n• r7a.32xlarge\n• r7a.48xlarge\n• c7i.2xlarge\n• c7i.4xlarge\n• c7i.8xlarge\n• c7i.12xlarge\n• c7i.16xlarge\n• c7i.24xlarge\n• c7i.48xlarge\n• mac2-m2pro.metal\n• r7iz.xlarge\n• r7iz.2xlarge\n• r7iz.4xlarge\n• r7iz.8xlarge\n• r7iz.12xlarge\n• r7iz.16xlarge\n• r7iz.32xlarge\n• c7a.2xlarge\n• c7a.4xlarge\n• c7a.8xlarge\n• c7a.12xlarge\n• c7a.16xlarge\n• c7a.24xlarge\n• c7a.32xlarge\n• c7a.48xlarge\n• c7a.metal-48xl\n• r7a.metal-48xl\n• r7i.2xlarge\n• r7i.4xlarge\n• r7i.8xlarge\n• r7i.12xlarge\n• r7i.16xlarge\n• r7i.24xlarge\n• r7i.48xlarge\n• dl2q.24xlarge\n• mac2-m2.metal\n• i4i.12xlarge\n• i4i.24xlarge\n• c7i.metal-24xl\n• c7i.metal-48xl\n• m7i.metal-24xl\n• m7i.metal-48xl\n• r7i.metal-24xl\n• r7i.metal-48xl\n• r7iz.metal-16xl\n• r7iz.metal-32xl\n• g6.12xlarge\n• g6.16xlarge\n• g6.24xlarge\n• g6.48xlarge\n• gr6.4xlarge\n• gr6.8xlarge\n• c7i-flex.large\n• c7i-flex.xlarge\n• c7i-flex.2xlarge\n• c7i-flex.4xlarge\n• c7i-flex.8xlarge\n• u7i-12tb.224xlarge\n• u7in-16tb.224xlarge\n• u7in-24tb.224xlarge\n• u7in-32tb.224xlarge\n• u7ib-12tb.224xlarge\n• r8g.2xlarge\n• r8g.4xlarge\n• r8g.8xlarge\n• r8g.12xlarge\n• r8g.16xlarge\n• r8g.24xlarge\n• r8g.48xlarge\n• r8g.metal-24xl\n• r8g.metal-48xl\n• mac2-m1ultra.metal\n• g6e.2xlarge\n• g6e.4xlarge\n• g6e.8xlarge\n• g6e.12xlarge\n• g6e.16xlarge\n• g6e.24xlarge\n• g6e.48xlarge\n• c8g.2xlarge\n• c8g.4xlarge\n• c8g.8xlarge\n• c8g.12xlarge\n• c8g.16xlarge\n• c8g.24xlarge\n• c8g.48xlarge\n• c8g.metal-24xl\n• c8g.metal-48xl\n• m8g.2xlarge\n• m8g.4xlarge\n• m8g.8xlarge\n• m8g.12xlarge\n• m8g.16xlarge\n• m8g.24xlarge\n• m8g.48xlarge\n• m8g.metal-24xl\n• m8g.metal-48xl\n• x8g.2xlarge\n• x8g.4xlarge\n• x8g.8xlarge\n• x8g.12xlarge\n• x8g.16xlarge\n• x8g.24xlarge\n• x8g.48xlarge\n• x8g.metal-24xl\n• x8g.metal-48xl\n• i7ie.xlarge\n• i7ie.2xlarge\n• i7ie.3xlarge\n• i7ie.6xlarge\n• i7ie.12xlarge\n• i7ie.18xlarge\n• i7ie.24xlarge\n• i7ie.48xlarge\n• i8g.2xlarge\n• i8g.4xlarge\n• i8g.8xlarge\n• i8g.12xlarge\n• i8g.16xlarge\n• i8g.24xlarge\n• i8g.metal-24xl\n• u7i-6tb.112xlarge\n• u7i-8tb.112xlarge\n• u7inh-32tb.480xlarge\n• p5e.48xlarge\n• p5en.48xlarge\n• f2.12xlarge\n• f2.48xlarge\n• trn2.48xlarge\n• c7i-flex.12xlarge\n• c7i-flex.16xlarge\n• m7i-flex.12xlarge\n• m7i-flex.16xlarge\n• i7ie.metal-24xl\n• i7ie.metal-48xl\n• i8g.48xlarge\n• c8gd.medium\n• c8gd.xlarge\n• c8gd.2xlarge\n• c8gd.4xlarge\n• c8gd.8xlarge\n• c8gd.12xlarge\n• c8gd.16xlarge\n• c8gd.24xlarge\n• c8gd.48xlarge\n• c8gd.metal-24xl\n• c8gd.metal-48xl\n• i7i.2xlarge\n• i7i.4xlarge\n• i7i.8xlarge\n• i7i.12xlarge\n• i7i.16xlarge\n• i7i.24xlarge\n• i7i.48xlarge\n• i7i.metal-24xl\n• i7i.metal-48xl\n• p6-b200.48xlarge\n• m8gd.medium\n• m8gd.xlarge\n• m8gd.2xlarge\n• m8gd.4xlarge\n• m8gd.8xlarge\n• m8gd.12xlarge\n• m8gd.16xlarge\n• m8gd.24xlarge\n• m8gd.48xlarge\n• m8gd.metal-24xl\n• m8gd.metal-48xl\n• r8gd.medium\n• r8gd.xlarge\n• r8gd.2xlarge\n• r8gd.4xlarge\n• r8gd.8xlarge\n• r8gd.12xlarge\n• r8gd.16xlarge\n• r8gd.24xlarge\n• r8gd.48xlarge\n• r8gd.metal-24xl\n• r8gd.metal-48xl\n• c8gn.medium\n• c8gn.xlarge\n• c8gn.2xlarge\n• c8gn.4xlarge\n• c8gn.8xlarge\n• c8gn.12xlarge\n• c8gn.16xlarge\n• c8gn.24xlarge\n• c8gn.48xlarge\n• c8gn.metal-24xl\n• c8gn.metal-48xl\n• p6e-gb200.36xlarge\n• g6f.2xlarge\n• g6f.4xlarge\n• gr6f.4xlarge\n• r8i.2xlarge\n• r8i.4xlarge\n• r8i.8xlarge\n• r8i.12xlarge\n• r8i.16xlarge\n• r8i.24xlarge\n• r8i.32xlarge\n• r8i.48xlarge\n• r8i.96xlarge\n• r8i.metal-48xl\n• r8i.metal-96xl\n• r8i-flex.large\n• r8i-flex.xlarge\n• r8i-flex.2xlarge\n• r8i-flex.4xlarge\n• r8i-flex.8xlarge\n• r8i-flex.12xlarge\n• r8i-flex.16xlarge\n• m8i.2xlarge\n• m8i.4xlarge\n• m8i.8xlarge\n• m8i.12xlarge\n• m8i.16xlarge\n• m8i.24xlarge\n• m8i.32xlarge\n• m8i.48xlarge\n• m8i.96xlarge\n• m8i.metal-48xl\n• m8i.metal-96xl\n• m8i-flex.large\n• m8i-flex.xlarge\n• m8i-flex.2xlarge\n• m8i-flex.4xlarge\n• m8i-flex.8xlarge\n• m8i-flex.12xlarge\n• m8i-flex.16xlarge\n• i8ge.xlarge\n• i8ge.2xlarge\n• i8ge.3xlarge\n• i8ge.6xlarge\n• i8ge.12xlarge\n• i8ge.18xlarge\n• i8ge.24xlarge\n• i8ge.48xlarge\n• i8ge.metal-24xl\n• i8ge.metal-48xl\n• mac-m4.metal\n• mac-m4pro.metal\n• r8gn.medium\n• r8gn.xlarge\n• r8gn.2xlarge\n• r8gn.4xlarge\n• r8gn.8xlarge\n• r8gn.12xlarge\n• r8gn.16xlarge\n• r8gn.24xlarge\n• r8gn.48xlarge\n• r8gn.metal-24xl\n• r8gn.metal-48xl\n• c8i.2xlarge\n• c8i.4xlarge\n• c8i.8xlarge\n• c8i.12xlarge\n• c8i.16xlarge\n• c8i.24xlarge\n• c8i.32xlarge\n• c8i.48xlarge\n• c8i.96xlarge\n• c8i.metal-48xl\n• c8i.metal-96xl\n• c8i-flex.large\n• c8i-flex.xlarge\n• c8i-flex.2xlarge\n• c8i-flex.4xlarge\n• c8i-flex.8xlarge\n• c8i-flex.12xlarge\n• c8i-flex.16xlarge\n• r8gb.medium\n• r8gb.xlarge\n• r8gb.2xlarge\n• r8gb.4xlarge\n• r8gb.8xlarge\n• r8gb.12xlarge\n• r8gb.16xlarge\n• r8gb.24xlarge\n• r8gb.metal-24xl\n• m8a.2xlarge\n• m8a.4xlarge\n• m8a.8xlarge\n• m8a.12xlarge\n• m8a.16xlarge\n• m8a.24xlarge\n• m8a.48xlarge\n• m8a.metal-24xl\n• m8a.metal-48xl\n• trn2.3xlarge\n• r8a.2xlarge\n• r8a.4xlarge\n• r8a.8xlarge\n• r8a.12xlarge\n• r8a.16xlarge\n• r8a.24xlarge\n• r8a.48xlarge\n• r8a.metal-24xl\n• r8a.metal-48xl",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 94,
          "content_length": 18356
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-instances.html",
    "doc_type": "aws",
    "total_sections": 14
  },
  {
    "title": "stop-instancesÂ¶",
    "summary": "DescriptionÂ¶ Stops an Amazon EBS-backed instance. You can restart your instance at any time using the StartInstances API. For more information, see Stop and start Amazon EC2 instances in the Amazon EC2 User Guide . When you stop or hibernate an instance, we shut it down. By default, this includes a graceful operating system (OS) shutdown. To bypass the graceful shutdown, use the skipOsShutdown parameter; however, this might risk data integrity. You can use the StopInstances operation together w",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Stops an Amazon EBS-backed instance. You can restart your instance at any time using the StartInstances API. For more information, see Stop and start Amazon EC2 instances in the Amazon EC2 User Guide .\n\nWhen you stop or hibernate an instance, we shut it down. By default, this includes a graceful operating system (OS) shutdown. To bypass the graceful shutdown, use the skipOsShutdown parameter; however, this might risk data integrity.\n\nYou can use the StopInstances operation together with the Hibernate parameter to hibernate an instance if the instance is enabled for hibernation and meets the hibernation prerequisites . Stopping an instance doesnât preserve data stored in RAM, while hibernation does. If hibernation fails, a normal shutdown occurs. For more information, see Hibernate your Amazon EC2 instance in the Amazon EC2 User Guide .\n\nIf your instance appears stuck in the stopping state, there might be an issue with the underlying host computer. You can use the StopInstances operation together with the Force parameter to force stop your instance. For more information, see Troubleshoot Amazon EC2 instance stop issues in the Amazon EC2 User Guide .\n\nStopping and hibernating an instance differs from rebooting or terminating it. For example, a stopped or hibernated instance retains its root volume and any data volumes, unlike terminated instances where these volumes are automatically deleted. For more information about the differences between stopping, hibernating, rebooting, and terminating instances, see Amazon EC2 instance state changes in the Amazon EC2 User Guide .\n\nWe donât charge for instance usage or data transfer fees when an instance is stopped. However, the root volume and any data volumes remain and continue to persist your data, and youâre charged for volume usage. Every time you start your instance, Amazon EC2 charges a one-minute minimum for instance usage, followed by per-second billing.\n\nYou canât stop or hibernate instance store-backed instances.\n\nSee also: AWS API Documentation",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 2037
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--instance-ids (list) [required]\n\nThe IDs of the instances.\n\n--hibernate | --no-hibernate (boolean)\n\nHibernates the instance if the instance was enabled for hibernation at launch. If the instance cannot hibernate successfully, a normal shutdown occurs. For more information, see Hibernate your Amazon EC2 instance in the Amazon EC2 User Guide .\n\n--skip-os-shutdown | --no-skip-os-shutdown (boolean)\n\nSpecifies whether to bypass the graceful OS shutdown process when the instance is stopped.\n\n[Warning] WarningBypassing the graceful OS shutdown might result in data loss or corruption (for example, memory contents not flushed to disk or loss of in-flight IOs) or skipped shutdown scripts.",
        "code_examples": [
          "```\n\"string\"\"string\"...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 688
        }
      },
      {
        "header": "Warning",
        "content": "--dry-run | --no-dry-run (boolean)\n\n--force | --no-force (boolean)\n\nForces the instance to stop. The instance will first attempt a graceful shutdown, which includes flushing file system caches and metadata. If the graceful shutdown fails to complete within the timeout period, the instance shuts down forcibly without flushing the file system caches and metadata.\n\nAfter using this option, you must perform file system check and repair procedures. This option is not recommended for Windows instances. For more information, see Troubleshoot Amazon EC2 instance stop issues in the Amazon EC2 User Guide .\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command. The generated JSON skeleton is not stable between versions of the AWS CLI and there are no backwards compatibility guarantees in the JSON skeleton generated.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1666
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: To stop an Amazon EC2 instance\n\nThe following stop-instances example stops the specified Amazon EBS-backed instance.\n\nFor more information, see Stop and Start Your Instance in the Amazon Elastic Compute Cloud User Guide.\n\nExample 2: To hibernate an Amazon EC2 instance\n\nThe following stop-instances example hibernates Amazon EBS-backed instance if the instance is enabled for hibernation and meets the hibernation prerequisites. After the instance is put into hibernation the instance is stopped.\n\nFor more information, see Hibernate your On-Demand Linux instance in the Amazon Elastic Cloud Compute User Guide.",
        "code_examples": [
          "```\nawsec2stop-instances\\--instance-idsi-1234567890abcdef0\n```",
          "```\n{\"StoppingInstances\":[{\"InstanceId\":\"i-1234567890abcdef0\",\"CurrentState\":{\"Code\":64,\"Name\":\"stopping\"},\"PreviousState\":{\"Code\":16,\"Name\":\"running\"}}]}\n```",
          "```\nawsec2stop-instances\\--instance-idsi-1234567890abcdef0\\--hibernate\n```",
          "```\n{\"StoppingInstances\":[{\"CurrentState\":{\"Code\":64,\"Name\":\"stopping\"},\"InstanceId\":\"i-1234567890abcdef0\",\"PreviousState\":{\"Code\":16,\"Name\":\"running\"}}]}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 997
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "StoppingInstances -> (list)\n\nInformation about the stopped instances.\n\nDescribes an instance state change.\n\nInstanceId -> (string)\n\nCurrentState -> (structure)\n\nThe current state of the instance.\n\nThe state of the instance as a 16-bit unsigned integer.\n\nThe high byte is all of the bits between 2^8 and (2^16)-1, which equals decimal values between 256 and 65,535. These numerical values are used for internal purposes and should be ignored.\n\nThe low byte is all of the bits between 2^0 and (2^8)-1, which equals decimal values between 0 and 255.\n\nThe valid values for instance-state-code will all be in the range of the low byte and they are:\n\nYou can ignore the high byte value by zeroing out all of the bits above 2^8 or 256 in decimal.\n\nThe current state of the instance.\n\nPreviousState -> (structure)\n\nThe previous state of the instance.\n\nThe state of the instance as a 16-bit unsigned integer.\n\nThe high byte is all of the bits between 2^8 and (2^16)-1, which equals decimal values between 256 and 65,535. These numerical values are used for internal purposes and should be ignored.\n\nThe low byte is all of the bits between 2^0 and (2^8)-1, which equals decimal values between 0 and 255.\n\nThe valid values for instance-state-code will all be in the range of the low byte and they are:\n\nYou can ignore the high byte value by zeroing out all of the bits above 2^8 or 256 in decimal.\n\nThe current state of the instance.\n\n• 0 : pending\n• 16 : running\n• 32 : shutting-down\n• 48 : terminated\n• 64 : stopping\n• 80 : stopped\n\n• shutting-down\n\n• 0 : pending\n• 16 : running\n• 32 : shutting-down\n• 48 : terminated\n• 64 : stopping\n• 80 : stopped\n\n• shutting-down",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 1656
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/ec2/stop-instances.html",
    "doc_type": "aws",
    "total_sections": 7
  },
  {
    "title": "terminate-instancesÂ¶",
    "summary": "DescriptionÂ¶ Terminates (deletes) the specified instances. This operation is idempotent ; if you terminate an instance more than once, each call succeeds. Warning Terminating an instance is permanent and irreversible. After you terminate an instance, you can no longer connect to it, and it canât be recovered. All attached Amazon EBS volumes that are configured to be deleted on termination are also permanently deleted and canât be recovered. All data stored on instance store volumes is perma",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Terminates (deletes) the specified instances. This operation is idempotent ; if you terminate an instance more than once, each call succeeds.\n\n[Warning] Warning Terminating an instance is permanent and irreversible. After you terminate an instance, you can no longer connect to it, and it canât be recovered. All attached Amazon EBS volumes that are configured to be deleted on termination are also permanently deleted and canât be recovered. All data stored on instance store volumes is permanently lost. For more information, see How instance termination works . Before you terminate an instance, ensure that you have backed up all data that you need to retain after the termination to persistent storage.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 711
        }
      },
      {
        "header": "Warning",
        "content": "After you terminate an instance, you can no longer connect to it, and it canât be recovered. All attached Amazon EBS volumes that are configured to be deleted on termination are also permanently deleted and canât be recovered. All data stored on instance store volumes is permanently lost. For more information, see How instance termination works .\n\nBefore you terminate an instance, ensure that you have backed up all data that you need to retain after the termination to persistent storage.\n\nIf you specify multiple instances and the request fails (for example, because of a single incorrect instance ID), none of the instances are terminated.\n\nIf you terminate multiple instances across multiple Availability Zones, and one or more of the specified instances are enabled for termination protection, the request fails with the following results:\n\nFor example, say you have the following instances:\n\nIf you attempt to terminate all of these instances in the same request, the request reports failure with the following results:\n\nTerminated instances remain visible after termination (for approximately one hour).\n\nBy default, Amazon EC2 deletes all EBS volumes that were attached when the instance launched. Volumes attached after instance launch continue running.\n\nBy default, the TerminateInstances operation includes a graceful operating system (OS) shutdown. To bypass the graceful shutdown, use the skipOsShutdown parameter; however, this might risk data integrity.\n\nYou can stop, start, and terminate EBS-backed instances. You can only terminate instance store-backed instances. What happens to an instance differs if you stop or terminate it. For example, when you stop an instance, the root device and any other devices attached to the instance persist. When you terminate an instance, any attached EBS volumes with the DeleteOnTermination block device mapping parameter set to true are automatically deleted. For more information about the differences between stopping and terminating instances, see Amazon EC2 instance state changes in the Amazon EC2 User Guide .\n\nWhen you terminate an instance, we attempt to terminate it forcibly after a short while. If your instance appears stuck in the shutting-down state after a period of time, there might be an issue with the underlying host computer. For more information about terminating and troubleshooting terminating your instances, see Terminate Amazon EC2 instances and Troubleshooting terminating your instance in the Amazon EC2 User Guide .\n\nSee also: AWS API Documentation\n\n• The specified instances that are in the same Availability Zone as the protected instance are not terminated.\n• The specified instances that are in different Availability Zones, where no other specified instances are protected, are successfully terminated.\n\n• Instance A: us-east-1a ; Not protected\n• Instance B: us-east-1a ; Not protected\n• Instance C: us-east-1b ; Protected\n• Instance D: us-east-1b ; not protected\n\n• Instance A and Instance B are successfully terminated because none of the specified instances in us-east-1a are enabled for termination protection.\n• Instance C and Instance D fail to terminate because at least one of the specified instances in us-east-1b (Instance C) is enabled for termination protection.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 3272
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--instance-ids (list) [required]\n\nThe IDs of the instances.\n\nConstraints: Up to 1000 instance IDs. We recommend breaking up this request into smaller batches.\n\n--force | --no-force (boolean)\n\n--skip-os-shutdown | --no-skip-os-shutdown (boolean)\n\nSpecifies whether to bypass the graceful OS shutdown process when the instance is terminated.\n\n--dry-run | --no-dry-run (boolean)\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command. The generated JSON skeleton is not stable between versions of the AWS CLI and there are no backwards compatibility guarantees in the JSON skeleton generated.",
        "code_examples": [
          "```\n\"string\"\"string\"...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1438
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nTo terminate an Amazon EC2 instance\n\nThis example terminates the specified instance.\n\nFor more information, see Using Amazon EC2 Instances in the AWS Command Line Interface User Guide.",
        "code_examples": [
          "```\nawsec2terminate-instances--instance-idsi-1234567890abcdef0\n```",
          "```\n{\"TerminatingInstances\":[{\"InstanceId\":\"i-1234567890abcdef0\",\"CurrentState\":{\"Code\":32,\"Name\":\"shutting-down\"},\"PreviousState\":{\"Code\":16,\"Name\":\"running\"}}]}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 559
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "TerminatingInstances -> (list)\n\nInformation about the terminated instances.\n\nDescribes an instance state change.\n\nInstanceId -> (string)\n\nCurrentState -> (structure)\n\nThe current state of the instance.\n\nThe state of the instance as a 16-bit unsigned integer.\n\nThe high byte is all of the bits between 2^8 and (2^16)-1, which equals decimal values between 256 and 65,535. These numerical values are used for internal purposes and should be ignored.\n\nThe low byte is all of the bits between 2^0 and (2^8)-1, which equals decimal values between 0 and 255.\n\nThe valid values for instance-state-code will all be in the range of the low byte and they are:\n\nYou can ignore the high byte value by zeroing out all of the bits above 2^8 or 256 in decimal.\n\nThe current state of the instance.\n\nPreviousState -> (structure)\n\nThe previous state of the instance.\n\nThe state of the instance as a 16-bit unsigned integer.\n\nThe high byte is all of the bits between 2^8 and (2^16)-1, which equals decimal values between 256 and 65,535. These numerical values are used for internal purposes and should be ignored.\n\nThe low byte is all of the bits between 2^0 and (2^8)-1, which equals decimal values between 0 and 255.\n\nThe valid values for instance-state-code will all be in the range of the low byte and they are:\n\nYou can ignore the high byte value by zeroing out all of the bits above 2^8 or 256 in decimal.\n\nThe current state of the instance.\n\n• 0 : pending\n• 16 : running\n• 32 : shutting-down\n• 48 : terminated\n• 64 : stopping\n• 80 : stopped\n\n• shutting-down\n\n• 0 : pending\n• 16 : running\n• 32 : shutting-down\n• 48 : terminated\n• 64 : stopping\n• 80 : stopped\n\n• shutting-down",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 1662
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/ec2/terminate-instances.html",
    "doc_type": "aws",
    "total_sections": 7
  },
  {
    "title": "s3Â¶",
    "summary": "DescriptionÂ¶ This section explains prominent concepts and notations in the set of high-level S3 commands provided. If you are looking for the low level S3 commands for the CLI, please see the s3api command reference page. Path Argument TypeÂ¶ Whenever using a command, at least one path argument must be specified. There are two types of path arguments: LocalPath and S3Uri. LocalPath: represents the path of a local file or directory. It can be written as an absolute path or relative path. S3Uri: ",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "This section explains prominent concepts and notations in the set of high-level S3 commands provided.\n\nIf you are looking for the low level S3 commands for the CLI, please see the s3api command reference page.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 209
        }
      },
      {
        "header": "Path Argument TypeÂ¶",
        "content": "Whenever using a command, at least one path argument must be specified. There are two types of path arguments: LocalPath and S3Uri.\n\nLocalPath: represents the path of a local file or directory. It can be written as an absolute path or relative path.\n\nS3Uri: represents the location of a S3 object, prefix, or bucket. This must be written in the form s3://amzn-s3-demo-bucket/mykey where amzn-s3-demo-bucket is the specified S3 bucket, mykey is the specified S3 key. The path argument must begin with s3:// in order to denote that the path argument refers to a S3 object. Note that prefixes are separated by forward slashes. For example, if the S3 object myobject had the prefix myprefix, the S3 key would be myprefix/myobject, and if the object was in the bucket amzn-s3-demo-bucket, the S3Uri would be s3://amzn-s3-demo-bucket/myprefix/myobject.\n\nS3Uri also supports S3 access points. To specify an access point, this value must be of the form s3://<access-point-arn>/<key>. For example if the access point myaccesspoint to be used has the ARN: arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint and the object being accessed has the key mykey, then the S3URI used must be: s3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mykey. Similar to bucket names, you can also use prefixes with access point ARNs for the S3Uri. For example: s3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/myprefix/",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1432
        }
      },
      {
        "header": "Order of Path ArgumentsÂ¶",
        "content": "Every command takes one or two positional path arguments. The first path argument represents the source, which is the local file/directory or S3 object/prefix/bucket that is being referenced. If there is a second path argument, it represents the destination, which is the local file/directory or S3 object/prefix/bucket that is being operated on. Commands with only one path argument do not have a destination because the operation is being performed only on the source.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 470
        }
      },
      {
        "header": "Single Local File and S3 Object OperationsÂ¶",
        "content": "Some commands perform operations only on single files and S3 objects. The following commands are single file/object operations if no --recursive flag is provided.\n\nFor this type of operation, the first path argument, the source, must exist and be a local file or S3 object. The second path argument, the destination, can be the name of a local file, local directory, S3 object, S3 prefix, or S3 bucket.\n\nThe destination is indicated as a local directory, S3 prefix, or S3 bucket if it ends with a forward slash or back slash. The use of slash depends on the path argument type. If the path argument is a LocalPath, the type of slash is the separator used by the operating system. If the path is a S3Uri, the forward slash must always be used. If a slash is at the end of the destination, the destination file or object will adopt the name of the source file or object. Otherwise, if there is no slash at the end, the file or object will be saved under the name provided. See examples in cp and mv to illustrate this description.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1028
        }
      },
      {
        "header": "Directory and S3 Prefix OperationsÂ¶",
        "content": "Some commands only perform operations on the contents of a local directory or S3 prefix/bucket. Adding or omitting a forward slash or back slash to the end of any path argument, depending on its type, does not affect the results of the operation. The following commands will always result in a directory or S3 prefix/bucket operation:",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 334
        }
      },
      {
        "header": "Use of Exclude and Include FiltersÂ¶",
        "content": "Currently, there is no support for the use of UNIX style wildcards in a commandâs path arguments. However, most commands have --exclude \"<value>\" and --include \"<value>\" parameters that can achieve the desired result. These parameters perform pattern matching to either exclude or include a particular file or object. The following pattern symbols are supported.\n\nAny number of these parameters can be passed to a command. You can do this by providing an --exclude or --include argument multiple times, e.g. --include \"*.txt\" --include \"*.png\". When there are multiple filters, the rule is the filters that appear later in the command take precedence over filters that appear earlier in the command. For example, if the filter parameters passed to the command were\n\nAll files will be excluded from the command except for files ending with .txt However, if the order of the filter parameters was changed to\n\nAll files will be excluded from the command.\n\nEach filter is evaluated against the source directory. If the source location is a file instead of a directory, the directory containing the file is used as the source directory. For example, suppose you had the following directory structure:\n\nIn the command aws s3 sync /tmp/foo s3://bucket/ the source directory is /tmp/foo. Any include/exclude filters will be evaluated with the source directory prepended. Below are several examples to demonstrate this.\n\nGiven the directory structure above and the command aws s3 cp /tmp/foo s3://bucket/ --recursive --exclude \".git/*\", the files .git/config and .git/description will be excluded from the files to upload because the exclude filter .git/* will have the source prepended to the filter. This means that:\n\nThe command aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \"ba*\" will exclude /tmp/foo/bar.txt and /tmp/foo/baz.jpg:\n\nNote that, by default, all files are included. This means that providing only an --include filter will not change what files are transferred. --include will only re-include files that have been excluded from an --exclude filter. If you only want to upload files with a particular extension, you need to first exclude all files, then re-include the files with the particular extension. This command will upload only files ending with .jpg:\n\nIf you wanted to include both .jpg files as well as .txt files you can run:\n\n• *: Matches everything\n• ?: Matches any single character\n• [sequence]: Matches any character in sequence\n• [!sequence]: Matches any character not in sequence",
        "code_examples": [
          "```\n--exclude\"*\"--include\"*.txt\"\n```",
          "```\n--include\"*.txt\"--exclude\"*\"\n```",
          "```\nawss3cp/tmp/foo/s3://bucket/--recursive--exclude\"*\"--include\"*.jpg\"\n```",
          "```\nawss3cp/tmp/foo/s3://bucket/--recursive\\--exclude\"*\"--include\"*.jpg\"--include\"*.txt\"\n```"
        ],
        "usage_examples": [
          "```\n/tmp/foo/.git/|---config|---descriptionfoo.txtbar.txtbaz.jpg\n```",
          "```\n/tmp/foo/.git/*->/tmp/foo/.git/config(matches,shouldexclude)/tmp/foo/.git/*->/tmp/foo/.git/description(matches,shouldexclude)/tmp/foo/.git/*->/tmp/foo/foo.txt(doesnotmatch,shouldinclude)/tmp/foo/.git/*->/tmp/foo/bar.txt(doesnotmatch,shouldinclude)/tmp/foo/.git/*->/tmp/foo/baz.jpg(doesnotmatch,shouldinclude)\n```",
          "```\n/tmp/foo/ba*->/tmp/foo/.git/config(doesnotmatch,shouldinclude)/tmp/foo/ba*->/tmp/foo/.git/description(doesnotmatch,shouldinclude)/tmp/foo/ba*->/tmp/foo/foo.txt(doesnotmatch,shouldinclude)/tmp/foo/ba*->/tmp/foo/bar.txt(matches,shouldexclude)/tmp/foo/ba*->/tmp/foo/baz.jpg(matches,shouldexclude)\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2514
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/s3/index.html",
    "doc_type": "aws",
    "total_sections": 6
  },
  {
    "title": "lsÂ¶",
    "summary": "DescriptionÂ¶ List S3 objects and common prefixes under a prefix or all S3 buckets. Note that the âoutput and âno-paginate arguments are ignored for this command.\n\nList S3 objects and common prefixes under a prefix or all S3 buckets. Note that the âoutput and âno-paginate arguments are ignored for this command.",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "List S3 objects and common prefixes under a prefix or all S3 buckets. Note that the âoutput and âno-paginate arguments are ignored for this command.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 152
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--recursive (boolean) Command is performed on all files or objects under the specified directory or prefix.\n\n--page-size (integer) The number of results to return in each response to a list operation. The default value is 1000 (the maximum allowed). Using a lower value may help if an operation times out.\n\n--human-readable (boolean) Displays file sizes in human readable format.\n\n--summarize (boolean) Displays summary information (number of objects, total size).\n\n--request-payer (string) Confirms that the requester knows that they will be charged for the request. Bucket owners need not specify this parameter in their requests. Documentation on downloading objects from requester pays buckets can be found at http://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectsinRequesterPaysBuckets.html\n\n--bucket-name-prefix (string) Limits the response to bucket names that begin with the specified bucket name prefix.\n\n--bucket-region (string) Limits the response to buckets that are located in the specified Amazon Web Services Region. The Amazon Web Services Region must be expressed according to the Amazon Web Services Region code, such as us-west-2 for the US West (Oregon) Region. For a list of the valid values for all of the Amazon Web Services Regions, see https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1327
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: Listing all user owned buckets\n\nThe following ls command lists all of the bucket owned by the user. In this example, the user owns the buckets amzn-s3-demo-bucket and amzn-s3-demo-bucket2. The timestamp is the date the bucket was created, shown in your machineâs time zone. This date can change when making changes to your bucket, such as editing its bucket policy. Note if s3:// is used for the path argument <S3Uri>, it will list all of the buckets as well.\n\nExample 2: Listing all prefixes and objects in a bucket\n\nThe following ls command lists objects and common prefixes under a specified bucket and prefix. In this example, the user owns the bucket amzn-s3-demo-bucket with the objects test.txt and somePrefix/test.txt. The LastWriteTime and Length are arbitrary. Note that since the ls command has no interaction with the local filesystem, the s3:// URI scheme is not required to resolve ambiguity and may be omitted.\n\nExample 3: Listing all prefixes and objects in a specific bucket and prefix\n\nThe following ls command lists objects and common prefixes under a specified bucket and prefix. However, there are no objects nor common prefixes under the specified bucket and prefix.\n\nExample 4: Recursively listing all prefixes and objects in a bucket\n\nThe following ls command will recursively list objects in a bucket. Rather than showing PRE dirname/ in the output, all the content in a bucket will be listed in order.\n\nExample 5: Summarizing all prefixes and objects in a bucket\n\nThe following ls command demonstrates the same command using the âhuman-readable and âsummarize options. âhuman-readable displays file size in Bytes/MiB/KiB/GiB/TiB/PiB/EiB. âsummarize displays the total number of objects and total size at the end of the result listing:\n\nExample 6: Listing from an S3 access point\n\nThe following ls command list objects from access point (myaccesspoint):",
        "code_examples": [
          "```\nawss3ls\n```",
          "```\n2013-07-1117:08:50amzn-s3-demo-bucket2013-07-2414:55:44amzn-s3-demo-bucket2\n```",
          "```\nawss3lss3://amzn-s3-demo-bucket\n```",
          "```\nPREsomePrefix/2013-07-2517:06:2788test.txt\n```",
          "```\nawss3lss3://amzn-s3-demo-bucket/noExistPrefix\n```",
          "```\nawss3lss3://amzn-s3-demo-bucket\\--recursive\n```",
          "```\n2013-09-0221:37:5310a.txt2013-09-0221:37:532863288foo.zip2013-09-0221:32:5723foo/bar/.baz/a2013-09-0221:32:5841foo/bar/.baz/b2013-09-0221:32:57281foo/bar/.baz/c2013-09-0221:32:5773foo/bar/.baz/d2013-09-0221:32:57452foo/bar/.baz/e2013-09-0221:32:57896foo/bar/.baz/hooks/bar2013-09-0221:32:57189foo/bar/.baz/hooks/foo2013-09-0221:32:57398z.txt\n```",
          "```\nawss3lss3://amzn-s3-demo-bucket\\--recursive\\--human-readable\\--summarize\n```",
          "```\n2013-09-0221:37:5310Bytesa.txt2013-09-0221:37:532.9MiBfoo.zip2013-09-0221:32:5723Bytesfoo/bar/.baz/a2013-09-0221:32:5841Bytesfoo/bar/.baz/b2013-09-0221:32:57281Bytesfoo/bar/.baz/c2013-09-0221:32:5773Bytesfoo/bar/.baz/d2013-09-0221:32:57452Bytesfoo/bar/.baz/e2013-09-0221:32:57896Bytesfoo/bar/.baz/hooks/bar2013-09-0221:32:57189Bytesfoo/bar/.baz/hooks/foo2013-09-0221:32:57398Bytesz.txtTotalObjects:10TotalSize:2.9MiB\n```",
          "```\nawss3lss3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/\n```",
          "```\nPREsomePrefix/2013-07-2517:06:2788test.txt\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 2274
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/s3/ls.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "cpÂ¶",
    "summary": "DescriptionÂ¶ Copies a local file or S3 object to another location locally or in S3.\n\nCopies a local file or S3 object to another location locally or in S3.",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Copies a local file or S3 object to another location locally or in S3.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 70
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--dryrun (boolean) Displays the operations that would be performed using the specified command without actually running them.\n\n--quiet (boolean) Does not display the operations performed from the specified command.\n\n--include (string) Donât exclude files or objects in the command that match the specified pattern. See Use of Exclude and Include Filters for details.\n\n--exclude (string) Exclude all files or objects from the command that matches the specified pattern.\n\n--acl (string) Sets the ACL for the object when the command is performed. If you use this parameter you must have the âs3:PutObjectAclâ permission included in the list of actions for your IAM policy. Only accepts values of private, public-read, public-read-write, authenticated-read, aws-exec-read, bucket-owner-read, bucket-owner-full-control and log-delivery-write. See Canned ACL for details\n\n--follow-symlinks | --no-follow-symlinks (boolean) Symbolic links are followed only when uploading to S3 from the local filesystem. Note that S3 does not support symbolic links, so the contents of the link target are uploaded under the name of the link. When neither --follow-symlinks nor --no-follow-symlinks is specified, the default is to follow symlinks.\n\n--no-guess-mime-type (boolean) Do not try to guess the mime type for uploaded files. By default the mime type of a file is guessed when it is uploaded.\n\n--sse (string) Specifies server-side encryption of the object in S3. Valid values are AES256 and aws:kms. If the parameter is specified but no value is provided, AES256 is used.\n\n--sse-c (string) Specifies server-side encryption using customer provided keys of the the object in S3. AES256 is the only valid value. If the parameter is specified but no value is provided, AES256 is used. If you provide this value, --sse-c-key must be specified as well.\n\n--sse-c-key (blob) The customer-provided encryption key to use to server-side encrypt the object in S3. If you provide this value, --sse-c must be specified as well. The key provided should not be base64 encoded.\n\n--sse-kms-key-id (string) The customer-managed AWS Key Management Service (KMS) key ID that should be used to server-side encrypt the object in S3. You should only provide this parameter if you are using a customer managed customer master key (CMK) and not the AWS managed KMS CMK.\n\n--sse-c-copy-source (string) This parameter should only be specified when copying an S3 object that was encrypted server-side with a customer-provided key. It specifies the algorithm to use when decrypting the source object. AES256 is the only valid value. If the parameter is specified but no value is provided, AES256 is used. If you provide this value, --sse-c-copy-source-key must be specified as well.\n\n--sse-c-copy-source-key (blob) This parameter should only be specified when copying an S3 object that was encrypted server-side with a customer-provided key. Specifies the customer-provided encryption key for Amazon S3 to use to decrypt the source object. The encryption key provided must be one that was used when the source object was created. If you provide this value, --sse-c-copy-source be specified as well. The key provided should not be base64 encoded.\n\n--storage-class (string) The type of storage to use for the object. Valid choices are: STANDARD | REDUCED_REDUNDANCY | STANDARD_IA | ONEZONE_IA | INTELLIGENT_TIERING | GLACIER | DEEP_ARCHIVE | GLACIER_IR. Defaults to âSTANDARDâ\n\nGrant specific permissions to individual users or groups. You can supply a list of grants of the form\n\nTo specify the same permission type for multiple grantees, specify the permission as such as\n\nEach value contains the following elements:\n\nFor more information on Amazon S3 access control, see Access Control\n\n--website-redirect (string) If the bucket is configured as a website, redirects requests for this object to another object in the same bucket or to an external URL. Amazon S3 stores the value of this header in the object metadata.\n\n--content-type (string) Specify an explicit content type for this operation. This value overrides any guessed mime types.\n\n--cache-control (string) Specifies caching behavior along the request/reply chain.\n\n--content-disposition (string) Specifies presentational information for the object.\n\n--content-encoding (string) Specifies what content encodings have been applied to the object and thus what decoding mechanisms must be applied to obtain the media-type referenced by the Content-Type header field.\n\n--content-language (string) The language the content is in.\n\n--expires (string) The date and time at which the object is no longer cacheable.\n\n--source-region (string) When transferring objects from an s3 bucket to an s3 bucket, this specifies the region of the source bucket. Note the region specified by --region or through configuration of the CLI refers to the region of the destination bucket. If --source-region is not specified the region of the source will be the same as the region of the destination bucket.\n\n--only-show-errors (boolean) Only errors and warnings are displayed. All other output is suppressed.\n\n--no-progress (boolean) File transfer progress is not displayed. This flag is only applied when the quiet and only-show-errors flags are not provided.\n\n--progress-frequency (integer) Number of seconds to wait before updating file transfer progress. This flag is only applied when the quiet and only-show-errors flags are not provided.\n\n--progress-multiline (boolean) Show progress on multiple lines.\n\n--page-size (integer) The number of results to return in each response to a list operation. The default value is 1000 (the maximum allowed). Using a lower value may help if an operation times out.\n\n--ignore-glacier-warnings (boolean) Turns off glacier warnings. Warnings about an operation that cannot be performed because it involves copying, downloading, or moving a glacier object will no longer be printed to standard error and will no longer cause the return code of the command to be 2.\n\n--force-glacier-transfer (boolean) Forces a transfer request on all Glacier objects in a sync or recursive copy.\n\n--request-payer (string) Confirms that the requester knows that they will be charged for the request. Bucket owners need not specify this parameter in their requests. Documentation on downloading objects from requester pays buckets can be found at http://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectsinRequesterPaysBuckets.html\n\n--checksum-mode (string) To retrieve the checksum, this mode must be enabled. If the object has a checksum, it will be verified.\n\n--checksum-algorithm (string) Indicates the algorithm used to create the checksum for the object.\n\n--metadata (map) A map of metadata to store with the objects in S3. This will be applied to every object which is part of this request. In a sync, this means that files which havenât changed wonât receive the new metadata. key -> (string)\n\n--copy-props (string) Determines which properties are copied from the source S3 object. This parameter only applies for S3 to S3 copies. Valid values are:\n\nIn order to copy the appropriate properties for multipart copies, some of the options may require additional API calls if a multipart copy is involved. Specifically:\n\nIf you want to guarantee no additional API calls are made other than than the ones needed to perform the actual copy, set this option to none.\n\n--metadata-directive (string) Sets the x-amz-metadata-directive header for CopyObject operations. It is recommended to use the --copy-props parameter instead to control copying of metadata properties. If --metadata-directive is set, the --copy-props parameter will be disabled and will have no affect on the transfer.\n\n--expected-size (string) This argument specifies the expected size of a stream in terms of bytes. Note that this argument is needed only when a stream is being uploaded to s3 and the size is larger than 50GB. Failure to include this argument under these conditions may result in a failed upload due to too many parts in upload.\n\n--recursive (boolean) Command is performed on all files or objects under the specified directory or prefix.\n\n• Permission - Specifies the granted permissions, and can be set to read, readacl, writeacl, or full.\n• Grantee_Type - Specifies how the grantee is to be identified, and can be set to uri or id.\n• Grantee_ID - Specifies the grantee based on Grantee_Type. The Grantee_ID value can be one of: uri - The groupâs URI. For more information, see Who Is a Grantee? id - The accountâs canonical ID\n\n• uri - The groupâs URI. For more information, see Who Is a Grantee?\n• id - The accountâs canonical ID\n\n• none - Do not copy any of the properties from the source S3 object.\n• metadata-directive - Copies the following properties from the source S3 object: content-type, content-language, content-encoding, content-disposition, cache-control, --expires, and metadata\n• default - The default value. Copies tags and properties covered under the metadata-directive value from the source S3 object.\n\n• metadata-directive may require additional HeadObject API calls.\n• default may require additional HeadObject, GetObjectTagging, and PutObjectTagging API calls. Note this list of API calls may grow in the future in order to ensure multipart copies preserve the exact properties a CopyObject API call would preserve.",
        "code_examples": [
          "```\n--grantsPermission=Grantee_Type=Grantee_ID[Permission=Grantee_Type=Grantee_ID...]\n```",
          "```\n--grantsPermission=Grantee_Type=Grantee_ID,Grantee_Type=Grantee_ID,...\n```",
          "```\nKeyName1=string,KeyName2=string\n```",
          "```\n{\"string\":\"string\"...}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 43,
          "content_length": 9357
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: Copying a local file to S3\n\nThe following cp command copies a single file to a specified bucket and key:\n\nExample 2: Copying a local file to S3 with an expiration date\n\nThe following cp command copies a single file to a specified bucket and key that expires at the specified ISO 8601 timestamp:\n\nExample 3: Copying a file from S3 to S3\n\nThe following cp command copies a single s3 object to a specified bucket and key:\n\nExample 4: Copying an S3 object to a local file\n\nThe following cp command copies a single object to a specified file locally:\n\nExample 5: Copying an S3 object from one bucket to another\n\nThe following cp command copies a single object to a specified bucket while retaining its original name:\n\nExample 6: Recursively copying S3 objects to a local directory\n\nWhen passed with the parameter --recursive, the following cp command recursively copies all objects under a specified prefix and bucket to a specified directory. In this example, the bucket amzn-s3-demo-bucket has the objects test1.txt and test2.txt:\n\nExample 7: Recursively copying local files to S3\n\nWhen passed with the parameter --recursive, the following cp command recursively copies all files under a specified directory to a specified bucket and prefix while excluding some files by using an --exclude parameter. In this example, the directory myDir has the files test1.txt and test2.jpg:\n\nExample 8: Recursively copying S3 objects to another bucket\n\nWhen passed with the parameter --recursive, the following cp command recursively copies all objects under a specified bucket to another bucket while excluding some objects by using an --exclude parameter. In this example, the bucket amzn-s3-demo-bucket has the objects test1.txt and another/test1.txt:\n\nYou can combine --exclude and --include options to copy only objects that match a pattern, excluding all others:\n\nExample 9: Setting the Access Control List (ACL) while copying an S3 object\n\nThe following cp command copies a single object to a specified bucket and key while setting the ACL to public-read-write:\n\nNote that if youâre using the --acl option, ensure that any associated IAM policies include the \"s3:PutObjectAcl\" action:\n\nExample 10: Granting permissions for an S3 object\n\nThe following cp command illustrates the use of the --grants option to grant read access to all users identified by URI and full control to a specific user identified by their Canonical ID:\n\nExample 11: Uploading a local file stream to S3\n\n[Warning] WarningPowerShell may alter the encoding of or add a CRLF to piped input.",
        "code_examples": [
          "```\nawss3cptest.txts3://amzn-s3-demo-bucket/test2.txt\n```",
          "```\nupload:test.txttos3://amzn-s3-demo-bucket/test2.txt\n```",
          "```\nawss3cptest.txts3://amzn-s3-demo-bucket/test2.txt\\--expires2014-10-01T20:30:00Z\n```",
          "```\nupload:test.txttos3://amzn-s3-demo-bucket/test2.txt\n```",
          "```\nawss3cps3://amzn-s3-demo-bucket/test.txts3://amzn-s3-demo-bucket/test2.txt\n```",
          "```\ncopy:s3://amzn-s3-demo-bucket/test.txttos3://amzn-s3-demo-bucket/test2.txt\n```",
          "```\nawss3cps3://amzn-s3-demo-bucket/test.txttest2.txt\n```",
          "```\ndownload:s3://amzn-s3-demo-bucket/test.txttotest2.txt\n```",
          "```\nawss3cps3://amzn-s3-demo-bucket/test.txts3://amzn-s3-demo-bucket2/\n```",
          "```\ncopy:s3://amzn-s3-demo-bucket/test.txttos3://amzn-s3-demo-bucket2/test.txt\n```",
          "```\nawss3cps3://amzn-s3-demo-bucket.\\--recursive\n```",
          "```\ndownload:s3://amzn-s3-demo-bucket/test1.txttotest1.txtdownload:s3://amzn-s3-demo-bucket/test2.txttotest2.txt\n```",
          "```\nawss3cpmyDirs3://amzn-s3-demo-bucket/\\--recursive\\--exclude\"*.jpg\"\n```",
          "```\nupload:myDir/test1.txttos3://amzn-s3-demo-bucket/test1.txt\n```",
          "```\nawss3cps3://amzn-s3-demo-bucket/s3://amzn-s3-demo-bucket2/\\--recursive\\--exclude\"another/*\"\n```",
          "```\ncopy:s3://amzn-s3-demo-bucket/test1.txttos3://amzn-s3-demo-bucket2/test1.txt\n```",
          "```\nawss3cps3://amzn-s3-demo-bucket/logs/s3://amzn-s3-demo-bucket2/logs/\\--recursive\\--exclude\"*\"\\--include\"*.log\"\n```",
          "```\ncopy:s3://amzn-s3-demo-bucket/logs/test/test.logtos3://amzn-s3-demo-bucket2/logs/test/test.logcopy:s3://amzn-s3-demo-bucket/logs/test3.logtos3://amzn-s3-demo-bucket2/logs/test3.log\n```",
          "```\nawss3cps3://amzn-s3-demo-bucket/test.txts3://amzn-s3-demo-bucket/test2.txt\\--aclpublic-read-write\n```",
          "```\ncopy:s3://amzn-s3-demo-bucket/test.txttos3://amzn-s3-demo-bucket/test2.txt\n```",
          "```\nawsiamget-user-policy\\--user-namemyuser\\--policy-namemypolicy\n```",
          "```\n{\"UserName\":\"myuser\",\"PolicyName\":\"mypolicy\",\"PolicyDocument\":{\"Version\":\"2012-10-17\",\"Statement\":[{\"Action\":[\"s3:PutObject\",\"s3:PutObjectAcl\"],\"Resource\":[\"arn:aws:s3:::amzn-s3-demo-bucket/*\"],\"Effect\":\"Allow\",\"Sid\":\"Stmt1234567891234\"}]}}\n```",
          "```\nawss3cpfile.txts3://amzn-s3-demo-bucket/--grantsread=uri=http://acs.amazonaws.com/groups/global/AllUsersfull=id=79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be\n```",
          "```\nupload:file.txttos3://amzn-s3-demo-bucket/file.txt\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 25,
          "content_length": 2938
        }
      },
      {
        "header": "Warning",
        "content": "The following cp command uploads a local file stream from standard input to a specified bucket and key:\n\nExample 12: Uploading a local file stream that is larger than 50GB to S3\n\nThe following cp command uploads a 51GB local file stream from standard input to a specified bucket and key. The --expected-size option must be provided, or the upload may fail when it reaches the default part limit of 10,000:\n\nExample 13: Downloading an S3 object as a local file stream\n\n[Warning] WarningPowerShell may alter the encoding of or add a CRLF to piped or redirected output.",
        "code_examples": [
          "```\nawss3cp-s3://amzn-s3-demo-bucket/stream.txt\n```",
          "```\nawss3cp-s3://amzn-s3-demo-bucket/stream.txt--expected-size54760833024\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 566
        }
      },
      {
        "header": "Warning",
        "content": "The following cp command downloads an S3 object locally as a stream to standard output. Downloading as a stream is not currently compatible with the --recursive parameter:\n\nExample 14: Uploading to an S3 access point\n\nThe following cp command uploads a single file (mydoc.txt) to the access point (myaccesspoint) at the key (mykey):\n\nExample 15: Downloading from an S3 access point\n\nThe following cp command downloads a single object (mykey) from the access point (myaccesspoint) to the local file (mydoc.txt):",
        "code_examples": [
          "```\nawss3cps3://amzn-s3-demo-bucket/stream.txt-\n```",
          "```\nawss3cpmydoc.txts3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mykey\n```",
          "```\nupload:mydoc.txttos3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mykey\n```",
          "```\nawss3cps3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mykeymydoc.txt\n```",
          "```\ndownload:s3://arn:aws:s3:us-west-2:123456789012:accesspoint/myaccesspoint/mykeytomydoc.txt\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 510
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html",
    "doc_type": "aws",
    "total_sections": 7
  },
  {
    "title": "mbÂ¶",
    "summary": "SynopsisÂ¶ mb <S3Uri> [--debug] [--endpoint-url <value>] [--no-verify-ssl] [--no-paginate] [--output <value>] [--query <value>] [--profile <value>] [--region <value>] [--version <value>] [--color <value>] [--no-sign-request] [--ca-bundle <value>] [--cli-read-timeout <value>] [--cli-connect-timeout <value>] [--cli-binary-format <value>] [--no-cli-pager] [--cli-auto-prompt] [--no-cli-auto-prompt]\n\nmb <S3Uri> [--debug] [--endpoint-url <value>] [--no-verify-ssl] [--no-paginate] [--output <value>] [-",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Creates an S3 bucket.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 21
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: Create a bucket\n\nThe following mb command creates a bucket. In this example, the user makes the bucket amzn-s3-demo-bucket. The bucket is created in the region specified in the userâs configuration file:\n\nExample 2: Create a bucket in the specified region\n\nThe following mb command creates a bucket in a region specified by the --region parameter. In this example, the user makes the bucket amzn-s3-demo-bucket in the region us-west-1:",
        "code_examples": [
          "```\nawss3mbs3://amzn-s3-demo-bucket\n```",
          "```\nmake_bucket:s3://amzn-s3-demo-bucket\n```",
          "```\nawss3mbs3://amzn-s3-demo-bucket\\--regionus-west-1\n```",
          "```\nmake_bucket:s3://amzn-s3-demo-bucket\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 823
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/s3/mb.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "lambdaÂ¶",
    "summary": "DescriptionÂ¶ Overview Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring and logging. With Lambda, you can run code for virtually any type of application or backend service. For more information about the Lambda ",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code on a high-availability compute infrastructure and performs all of the administration of the compute resources, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring and logging. With Lambda, you can run code for virtually any type of application or backend service. For more information about the Lambda service, see What is Lambda in the Lambda Developer Guide .\n\nThe Lambda API Reference provides information about each of the API methods, including details about the parameters in each API request and response.\n\nYou can use Software Development Kits (SDKs), Integrated Development Environment (IDE) Toolkits, and command line tools to access the API. For installation instructions, see Tools for Amazon Web Services .\n\nFor a list of Region-specific endpoints that Lambda supports, see Lambda endpoints and quotas in the Amazon Web Services General Reference. .\n\nWhen making the API calls, you will need to authenticate your request by providing a signature. Lambda supports signature version 4. For more information, see Signature Version 4 signing process in the Amazon Web Services General Reference. .\n\nBecause Amazon Web Services SDKs use the CA certificates from your computer, changes to the certificates on the Amazon Web Services servers can cause connection failures when you attempt to use an SDK. You can prevent these failures by keeping your computerâs CA certificates and operating system up-to-date. If you encounter this issue in a corporate environment and do not manage your own computer, you might need to ask an administrator to assist with the update process. The following list shows minimum operating system and Java versions:\n\nWhen accessing the Lambda management console or Lambda API endpoints, whether through browsers or programmatically, you will need to ensure your client machines support any of the following CAs:\n\nRoot certificates from the first two authorities are available from Amazon trust services , but keeping your computer up-to-date is the more straightforward solution. To learn more about ACM-provided certificates, see Amazon Web Services Certificate Manager FAQs.\n\n• Microsoft Windows versions that have updates from January 2005 or later installed contain at least one of the required CAs in their trust list.\n• Mac OS X 10.4 with Java for Mac OS X 10.4 Release 5 (February 2007), Mac OS X 10.5 (October 2007), and later versions contain at least one of the required CAs in their trust list.\n• Red Hat Enterprise Linux 5 (March 2007), 6, and 7 and CentOS 5, 6, and 7 all contain at least one of the required CAs in their default trusted CA list.\n• Java 1.4.2_12 (May 2006), 5 Update 2 (March 2005), and all later versions, including Java 6 (December 2006), 7, and 8, contain at least one of the required CAs in their default trusted CA list.\n\n• Amazon Root CA 1\n• Starfield Services Root Certificate Authority - G2\n• Starfield Class 2 Certification Authority",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3085
        }
      },
      {
        "header": "Available CommandsÂ¶",
        "content": "• add-layer-version-permission\n• add-permission\n• create-alias\n• create-code-signing-config\n• create-event-source-mapping\n• create-function\n• create-function-url-config\n• delete-alias\n• delete-code-signing-config\n• delete-event-source-mapping\n• delete-function\n• delete-function-code-signing-config\n• delete-function-concurrency\n• delete-function-event-invoke-config\n• delete-function-url-config\n• delete-layer-version\n• delete-provisioned-concurrency-config\n• get-account-settings\n• get-code-signing-config\n• get-event-source-mapping\n• get-function\n• get-function-code-signing-config\n• get-function-concurrency\n• get-function-configuration\n• get-function-event-invoke-config\n• get-function-recursion-config\n• get-function-url-config\n• get-layer-version\n• get-layer-version-by-arn\n• get-layer-version-policy\n• get-provisioned-concurrency-config\n• get-runtime-management-config\n• list-aliases\n• list-code-signing-configs\n• list-event-source-mappings\n• list-function-event-invoke-configs\n• list-function-url-configs\n• list-functions\n• list-functions-by-code-signing-config\n• list-layer-versions\n• list-layers\n• list-provisioned-concurrency-configs\n• list-versions-by-function\n• publish-layer-version\n• publish-version\n• put-function-code-signing-config\n• put-function-concurrency\n• put-function-event-invoke-config\n• put-function-recursion-config\n• put-provisioned-concurrency-config\n• put-runtime-management-config\n• remove-layer-version-permission\n• remove-permission\n• tag-resource\n• untag-resource\n• update-alias\n• update-code-signing-config\n• update-event-source-mapping\n• update-function-code\n• update-function-configuration\n• update-function-event-invoke-config\n• update-function-url-config",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1695
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/lambda/index.html",
    "doc_type": "aws",
    "total_sections": 2
  },
  {
    "title": "create-functionÂ¶",
    "summary": "DescriptionÂ¶ Creates a Lambda function. To create a function, you need a deployment package and an execution role . The deployment package is a .zip file archive or container image that contains your function code. The execution role grants the function permission to use Amazon Web Services services, such as Amazon CloudWatch Logs for log streaming and X-Ray for request tracing. If the deployment package is a container image , then you set the package type to Image . For a container image, the ",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Creates a Lambda function. To create a function, you need a deployment package and an execution role . The deployment package is a .zip file archive or container image that contains your function code. The execution role grants the function permission to use Amazon Web Services services, such as Amazon CloudWatch Logs for log streaming and X-Ray for request tracing.\n\nIf the deployment package is a container image , then you set the package type to Image . For a container image, the code property must include the URI of a container image in the Amazon ECR registry. You do not need to specify the handler and runtime properties.\n\nIf the deployment package is a .zip file archive , then you set the package type to Zip . For a .zip file archive, the code property specifies the location of the .zip file. You must also specify the handler and runtime properties. The code in the deployment package must be compatible with the target instruction set architecture of the function (x86-64 or arm64 ). If you do not specify the architecture, then the default value is x86-64 .\n\nWhen you create a function, Lambda provisions an instance of the function and its supporting resources. If your function connects to a VPC, this process can take a minute or so. During this time, you canât invoke or modify the function. The State , StateReason , and StateReasonCode fields in the response from GetFunctionConfiguration indicate when the function is ready to invoke. For more information, see Lambda function states .\n\nA function has an unpublished version, and can have published versions and aliases. The unpublished version changes when you update your functionâs code and configuration. A published version is a snapshot of your function code and configuration that canât be changed. An alias is a named resource that maps to a version, and can be changed to map to a different version. Use the Publish parameter to create version 1 of your function from its initial configuration.\n\nThe other parameters let you configure version-specific and function-level settings. You can modify version-specific settings later with UpdateFunctionConfiguration . Function-level settings apply to both the unpublished and published versions of the function, and include tags ( TagResource ) and per-function concurrency limits ( PutFunctionConcurrency ).\n\nYou can use code signing if your deployment package is a .zip file archive. To enable code signing for this function, specify the ARN of a code-signing configuration. When a user attempts to deploy a code package with UpdateFunctionCode , Lambda checks that the code package has a valid signature from a trusted publisher. The code-signing configuration includes set of signing profiles, which define the trusted publishers for this function.\n\nIf another Amazon Web Services account or an Amazon Web Services service invokes your function, use AddPermission to grant permission by creating a resource-based Identity and Access Management (IAM) policy. You can grant permissions at the function level, on a version, or on an alias.\n\nTo invoke your function directly, use Invoke . To invoke your function in response to events in other Amazon Web Services services, create an event source mapping ( CreateEventSourceMapping ), or configure a function trigger in the other service. For more information, see Invoking Lambda functions .\n\nSee also: AWS API Documentation",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 3411
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--function-name (string) [required]\n\nThe name or ARN of the Lambda function.\n\nThe length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n\nThe identifier of the functionâs runtime . Runtime is required if the deployment package is a .zip file archive. Specifying a runtime results in an error if youâre deploying a function using a container image.\n\nThe following list includes deprecated runtimes. Lambda blocks creating new functions and updating existing functions shortly after each runtime is deprecated. For more information, see Runtime use after deprecation .\n\nFor a list of all currently supported runtimes, see Supported runtimes .\n\n--role (string) [required]\n\nThe Amazon Resource Name (ARN) of the functionâs execution role.\n\nThe name of the method within your code that Lambda calls to run your function. Handler is required if the deployment package is a .zip file archive. The format includes the file name. It can also include namespaces and other qualifiers, depending on the runtime. For more information, see Lambda programming model .\n\nThe code for the function.\n\nS3Bucket -> (string)\n\nAn Amazon S3 bucket in the same Amazon Web Services Region as your function. The bucket can be in a different Amazon Web Services account.\n\nThe Amazon S3 key of the deployment package.\n\nS3ObjectVersion -> (string)\n\nFor versioned objects, the version of the deployment package object to use.\n\nImageUri -> (string)\n\nSourceKMSKeyArn -> (string)\n\nThe ARN of the Key Management Service (KMS) customer managed key thatâs used to encrypt your functionâs .zip deployment package. If you donât provide a customer managed key, Lambda uses an Amazon Web Services owned key .\n\n--description (string)\n\nA description of the function.\n\nThe amount of time (in seconds) that Lambda allows a function to run before stopping it. The default is 3 seconds. The maximum allowed value is 900 seconds. For more information, see Lambda execution environment .\n\n--memory-size (integer)\n\nThe amount of memory available to the function at runtime. Increasing the function memory also increases its CPU allocation. The default value is 128 MB. The value can be any multiple of 1 MB.\n\n--publish | --no-publish (boolean)\n\n--vpc-config (structure)\n\nFor network connectivity to Amazon Web Services resources in a VPC, specify a list of security groups and subnets in the VPC. When you connect a function to a VPC, it can access resources and the internet only through that VPC. For more information, see Configuring a Lambda function to access resources in a VPC .\n\nA list of VPC subnet IDs.\n\nSecurityGroupIds -> (list)\n\nA list of VPC security group IDs.\n\nIpv6AllowedForDualStack -> (boolean)\n\n--package-type (string)\n\nThe type of deployment package. Set to Image for container image and set to Zip for .zip file archive.\n\n--dead-letter-config (structure)\n\nA dead-letter queue configuration that specifies the queue or topic where Lambda sends asynchronous events when they fail processing. For more information, see Dead-letter queues .\n\nTargetArn -> (string)\n\nThe Amazon Resource Name (ARN) of an Amazon SQS queue or Amazon SNS topic.\n\n--environment (structure)\n\nEnvironment variables that are accessible from function code during execution.\n\nEnvironment variable key-value pairs. For more information, see Using Lambda environment variables .\n\n--kms-key-arn (string)\n\nThe ARN of the Key Management Service (KMS) customer managed key thatâs used to encrypt the following resources:\n\nIf you donât provide a customer managed key, Lambda uses an Amazon Web Services owned key or an Amazon Web Services managed key .\n\n--tracing-config (structure)\n\nSet Mode to Active to sample and trace a subset of incoming requests with X-Ray .\n\nA list of tags to apply to the function.\n\nA list of function layers to add to the functionâs execution environment. Specify each layer by its ARN, including the version.\n\n--file-system-configs (list)\n\nConnection settings for an Amazon EFS file system.\n\nDetails about the connection between a Lambda function and an Amazon EFS file system .\n\nArn -> (string) [required]\n\nThe Amazon Resource Name (ARN) of the Amazon EFS access point that provides access to the file system.\n\nLocalMountPath -> (string) [required]\n\nThe path where the function can access the file system, starting with /mnt/ .\n\n--image-config (structure)\n\nContainer image configuration values that override the values in the container image Dockerfile.\n\nEntryPoint -> (list)\n\nSpecifies the entry point to their application, which is typically the location of the runtime executable.\n\nSpecifies parameters that you want to pass in with ENTRYPOINT.\n\nWorkingDirectory -> (string)\n\nSpecifies the working directory.\n\n--code-signing-config-arn (string)\n\nTo enable code signing for this function, specify the ARN of a code-signing configuration. A code-signing configuration includes a set of signing profiles, which define the trusted publishers for this function.\n\n--architectures (list)\n\nThe instruction set architecture that the function supports. Enter a string array with one of the valid values (arm64 or x86_64). The default value is x86_64 .\n\n--ephemeral-storage (structure)\n\nThe size of the functionâs /tmp directory in MB. The default value is 512, but can be any whole number between 512 and 10,240 MB. For more information, see Configuring ephemeral storage (console) .\n\nSize -> (integer) [required]\n\nThe size of the functionâs /tmp directory.\n\n--snap-start (structure)\n\nThe functionâs SnapStart setting.\n\nSet to PublishedVersions to create a snapshot of the initialized execution environment when you publish a function version.\n\n--logging-config (structure)\n\nThe functionâs Amazon CloudWatch Logs configuration settings.\n\nLogFormat -> (string)\n\nThe format in which Lambda sends your functionâs application and system logs to CloudWatch. Select between plain text and structured JSON.\n\nApplicationLogLevel -> (string)\n\nSet this property to filter the application logs for your function that Lambda sends to CloudWatch. Lambda only sends application logs at the selected level of detail and lower, where TRACE is the highest level and FATAL is the lowest.\n\nSystemLogLevel -> (string)\n\nSet this property to filter the system logs for your function that Lambda sends to CloudWatch. Lambda only sends system logs at the selected level of detail and lower, where DEBUG is the highest level and WARN is the lowest.\n\nLogGroup -> (string)\n\nThe name of the Amazon CloudWatch log group the function sends logs to. By default, Lambda functions send logs to a default log group named /aws/lambda/<function name> . To use a different log group, enter an existing log group or enter a new log group name.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command. The generated JSON skeleton is not stable between versions of the AWS CLI and there are no backwards compatibility guarantees in the JSON skeleton generated.\n\n• Function name â my-function .\n• Function ARN â arn:aws:lambda:us-west-2:123456789012:function:my-function .\n• Partial ARN â 123456789012:function:my-function .\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\\d{1}:)?(\\d{12}:)?(function:)?([a-zA-Z0-9-_]+)(:(\\$LATEST|[a-zA-Z0-9-_]+))?\n\n• dotnetcore1.0\n• dotnetcore2.0\n• dotnetcore2.1\n• dotnetcore3.1\n• nodejs4.3-edge\n• provided.al2\n• provided.al2023\n\n• pattern: arn:(aws[a-zA-Z-]*)?:iam::\\d{12}:role/?[a-zA-Z_0-9+=,.@\\-_/]+\n\n• pattern: [^\\s]+\n\n• pattern: [0-9A-Za-z\\.\\-_]*(?<!\\.)\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()\n\n• pattern: [a-zA-Z]([a-zA-Z0-9_])+\n\n• The functionâs environment variables .\n• The functionâs Lambda SnapStart snapshots.\n• When used with SourceKMSKeyArn , the unzipped version of the .zip deployment package thatâs used for function invocations. For more information, see Specifying a customer managed key for Lambda .\n• The optimized version of the container image thatâs used for function invocations. Note that this is not the same key thatâs used to protect your container image in the Amazon Elastic Container Registry (Amazon ECR). For more information, see Function lifecycle .\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()\n\n• PassThrough\n\n• pattern: arn:[a-zA-Z0-9-]+:lambda:[a-zA-Z0-9-]+:\\d{12}:layer:[a-zA-Z0-9-_]+:[0-9]+\n\n• pattern: arn:aws[a-zA-Z-]*:elasticfilesystem:[a-z]{2}((-gov)|(-iso(b?)))?-[a-z]+-\\d{1}:\\d{12}:access-point/fsap-[a-f0-9]{17}\n\n• pattern: /mnt/[a-zA-Z0-9-_.]+\n\n• pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}((-gov)|(-iso(b?)))?-[a-z]+-\\d{1}:\\d{12}:code-signing-config:csc-[a-z0-9]{17}\n\n• PublishedVersions\n\n• pattern: [\\.\\-_/#A-Za-z0-9]+",
        "code_examples": [
          "```\nS3Bucket=string,S3Key=string,S3ObjectVersion=string,ImageUri=string,SourceKMSKeyArn=string\n```",
          "```\n{\"S3Bucket\":\"string\",\"S3Key\":\"string\",\"S3ObjectVersion\":\"string\",\"ImageUri\":\"string\",\"SourceKMSKeyArn\":\"string\"}\n```",
          "```\nSubnetIds=string,string,SecurityGroupIds=string,string,Ipv6AllowedForDualStack=boolean\n```",
          "```\n{\"SubnetIds\":[\"string\",...],\"SecurityGroupIds\":[\"string\",...],\"Ipv6AllowedForDualStack\":true|false}\n```",
          "```\nTargetArn=string\n```",
          "```\n{\"TargetArn\":\"string\"}\n```",
          "```\nVariables={KeyName1=string,KeyName2=string}\n```",
          "```\n{\"Variables\":{\"string\":\"string\"...}}\n```",
          "```\nMode=string\n```",
          "```\n{\"Mode\":\"Active\"|\"PassThrough\"}\n```",
          "```\nKeyName1=string,KeyName2=string\n```",
          "```\n{\"string\":\"string\"...}\n```",
          "```\n\"string\"\"string\"...\n```",
          "```\nArn=string,LocalMountPath=string...\n```",
          "```\n[{\"Arn\":\"string\",\"LocalMountPath\":\"string\"}...]\n```",
          "```\nEntryPoint=string,string,Command=string,string,WorkingDirectory=string\n```",
          "```\n{\"EntryPoint\":[\"string\",...],\"Command\":[\"string\",...],\"WorkingDirectory\":\"string\"}\n```",
          "```\n\"string\"\"string\"...\n```",
          "```\nSize=integer\n```",
          "```\n{\"Size\":integer}\n```",
          "```\nApplyOn=string\n```",
          "```\n{\"ApplyOn\":\"PublishedVersions\"|\"None\"}\n```",
          "```\nLogFormat=string,ApplicationLogLevel=string,SystemLogLevel=string,LogGroup=string\n```",
          "```\n{\"LogFormat\":\"JSON\"|\"Text\",\"ApplicationLogLevel\":\"TRACE\"|\"DEBUG\"|\"INFO\"|\"WARN\"|\"ERROR\"|\"FATAL\",\"SystemLogLevel\":\"DEBUG\"|\"INFO\"|\"WARN\",\"LogGroup\":\"string\"}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 83,
          "content_length": 9579
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nTo create a Lambda function\n\nThe following create-function example creates a Lambda function named my-function.\n\nContents of my-function.zip:\n\nFor more information, see Configure Lambda function memory in the AWS Lambda Developer Guide.",
        "code_examples": [
          "```\nawslambdacreate-function\\--function-namemy-function\\--runtimenodejs22.x\\--zip-filefileb://my-function.zip\\--handlermy-function.handler\\--rolearn:aws:iam::123456789012:role/service-role/MyTestFunction-role-tges6bf4\n```",
          "```\nThisfileisadeploymentpackagethatcontainsyourfunctioncodeandanydependencies.\n```"
        ],
        "usage_examples": [
          "```\n{\"TracingConfig\":{\"Mode\":\"PassThrough\"},\"CodeSha256\":\"PFn4S+er27qk+UuZSTKEQfNKG/XNn7QJs90mJgq6oH8=\",\"FunctionName\":\"my-function\",\"CodeSize\":308,\"RevisionId\":\"873282ed-4cd3-4dc8-a069-d0c647e470c6\",\"MemorySize\":128,\"FunctionArn\":\"arn:aws:lambda:us-west-2:123456789012:function:my-function\",\"Version\":\"$LATEST\",\"Role\":\"arn:aws:iam::123456789012:role/service-role/MyTestFunction-role-zgur6bf4\",\"Timeout\":3,\"LastModified\":\"2025-10-14T22:26:11.234+0000\",\"Handler\":\"my-function.handler\",\"Runtime\":\"nodejs22.x\",\"Description\":\"\"}\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 611
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "FunctionName -> (string)\n\nThe name of the function.\n\nFunctionArn -> (string)\n\nThe functionâs Amazon Resource Name (ARN).\n\nThe identifier of the functionâs runtime . Runtime is required if the deployment package is a .zip file archive. Specifying a runtime results in an error if youâre deploying a function using a container image.\n\nThe following list includes deprecated runtimes. Lambda blocks creating new functions and updating existing functions shortly after each runtime is deprecated. For more information, see Runtime use after deprecation .\n\nFor a list of all currently supported runtimes, see Supported runtimes .\n\nThe functionâs execution role.\n\nThe function that Lambda calls to begin running your function.\n\nDescription -> (string)\n\nThe functionâs description.\n\nTimeout -> (integer)\n\nThe amount of time in seconds that Lambda allows a function to run before stopping it.\n\nMemorySize -> (integer)\n\nThe amount of memory available to the function at runtime.\n\nLastModified -> (string)\n\nCodeSha256 -> (string)\n\nThe version of the Lambda function.\n\nVpcConfig -> (structure)\n\nThe functionâs networking configuration.\n\nA list of VPC subnet IDs.\n\nSecurityGroupIds -> (list)\n\nA list of VPC security group IDs.\n\nIpv6AllowedForDualStack -> (boolean)\n\nDeadLetterConfig -> (structure)\n\nThe functionâs dead letter queue.\n\nTargetArn -> (string)\n\nThe Amazon Resource Name (ARN) of an Amazon SQS queue or Amazon SNS topic.\n\nEnvironment -> (structure)\n\nThe functionâs environment variables . Omitted from CloudTrail logs.\n\nEnvironment variable key-value pairs. Omitted from CloudTrail logs.\n\nError -> (structure)\n\nError messages for environment variables that couldnât be applied.\n\nErrorCode -> (string)\n\nKMSKeyArn -> (string)\n\nThe ARN of the Key Management Service (KMS) customer managed key thatâs used to encrypt the following resources:\n\nIf you donât provide a customer managed key, Lambda uses an Amazon Web Services owned key or an Amazon Web Services managed key .\n\nTracingConfig -> (structure)\n\nThe functionâs X-Ray tracing configuration.\n\nMasterArn -> (string)\n\nFor Lambda@Edge functions, the ARN of the main function.\n\nRevisionId -> (string)\n\nThe functionâs layers .\n\nThe Amazon Resource Name (ARN) of the function layer.\n\nSigningProfileVersionArn -> (string)\n\nThe Amazon Resource Name (ARN) for a signing profile version.\n\nSigningJobArn -> (string)\n\nThe Amazon Resource Name (ARN) of a signing job.\n\nThe current state of the function. When the state is Inactive , you can reactivate the function by invoking it.\n\nStateReason -> (string)\n\nStateReasonCode -> (string)\n\nThe reason code for the functionâs current state. When the code is Creating , you canât invoke or modify the function.\n\nLastUpdateStatus -> (string)\n\nThe status of the last update that was performed on the function. This is first set to Successful after function creation completes.\n\nLastUpdateStatusReason -> (string)\n\nLastUpdateStatusReasonCode -> (string)\n\nThe reason code for the last update that was performed on the function.\n\nFileSystemConfigs -> (list)\n\nConnection settings for an Amazon EFS file system .\n\nDetails about the connection between a Lambda function and an Amazon EFS file system .\n\nArn -> (string) [required]\n\nThe Amazon Resource Name (ARN) of the Amazon EFS access point that provides access to the file system.\n\nLocalMountPath -> (string) [required]\n\nThe path where the function can access the file system, starting with /mnt/ .\n\nPackageType -> (string)\n\nThe type of deployment package. Set to Image for container image and set Zip for .zip file archive.\n\nImageConfigResponse -> (structure)\n\nThe functionâs image configuration values.\n\nImageConfig -> (structure)\n\nConfiguration values that override the container image Dockerfile.\n\nEntryPoint -> (list)\n\nSpecifies the entry point to their application, which is typically the location of the runtime executable.\n\nSpecifies parameters that you want to pass in with ENTRYPOINT.\n\nWorkingDirectory -> (string)\n\nSpecifies the working directory.\n\nError -> (structure)\n\nError response to GetFunctionConfiguration .\n\nErrorCode -> (string)\n\nSigningProfileVersionArn -> (string)\n\nThe ARN of the signing profile version.\n\nSigningJobArn -> (string)\n\nThe ARN of the signing job.\n\nArchitectures -> (list)\n\nThe instruction set architecture that the function supports. Architecture is a string array with one of the valid values. The default architecture value is x86_64 .\n\nEphemeralStorage -> (structure)\n\nThe size of the functionâs /tmp directory in MB. The default value is 512, but can be any whole number between 512 and 10,240 MB. For more information, see Configuring ephemeral storage (console) .\n\nSize -> (integer) [required]\n\nThe size of the functionâs /tmp directory.\n\nSnapStart -> (structure)\n\nSet ApplyOn to PublishedVersions to create a snapshot of the initialized execution environment when you publish a function version. For more information, see Improving startup performance with Lambda SnapStart .\n\nWhen set to PublishedVersions , Lambda creates a snapshot of the execution environment when you publish a function version.\n\nOptimizationStatus -> (string)\n\nWhen you provide a qualified Amazon Resource Name (ARN) , this response element indicates whether SnapStart is activated for the specified function version.\n\nRuntimeVersionConfig -> (structure)\n\nThe ARN of the runtime and any errors that occured.\n\nRuntimeVersionArn -> (string)\n\nThe ARN of the runtime version you want the function to use.\n\nError -> (structure)\n\nError response when Lambda is unable to retrieve the runtime version for a function.\n\nErrorCode -> (string)\n\nLoggingConfig -> (structure)\n\nThe functionâs Amazon CloudWatch Logs configuration settings.\n\nLogFormat -> (string)\n\nThe format in which Lambda sends your functionâs application and system logs to CloudWatch. Select between plain text and structured JSON.\n\nApplicationLogLevel -> (string)\n\nSet this property to filter the application logs for your function that Lambda sends to CloudWatch. Lambda only sends application logs at the selected level of detail and lower, where TRACE is the highest level and FATAL is the lowest.\n\nSystemLogLevel -> (string)\n\nSet this property to filter the system logs for your function that Lambda sends to CloudWatch. Lambda only sends system logs at the selected level of detail and lower, where DEBUG is the highest level and WARN is the lowest.\n\nLogGroup -> (string)\n\nThe name of the Amazon CloudWatch log group the function sends logs to. By default, Lambda functions send logs to a default log group named /aws/lambda/<function name> . To use a different log group, enter an existing log group or enter a new log group name.\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\\d{1}:)?(\\d{12}:)?(function:)?([a-zA-Z0-9-_\\.]+)(:(\\$LATEST|[a-zA-Z0-9-_]+))?\n\n• pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\\d{1}:\\d{12}:function:[a-zA-Z0-9-_\\.]+(:(\\$LATEST|[a-zA-Z0-9-_]+))?\n\n• dotnetcore1.0\n• dotnetcore2.0\n• dotnetcore2.1\n• dotnetcore3.1\n• nodejs4.3-edge\n• provided.al2\n• provided.al2023\n\n• pattern: arn:(aws[a-zA-Z-]*)?:iam::\\d{12}:role/?[a-zA-Z_0-9+=,.@\\-_/]+\n\n• pattern: [^\\s]+\n\n• pattern: (\\$LATEST|[0-9]+)\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()\n\n• pattern: [a-zA-Z]([a-zA-Z0-9_])+\n\n• The functionâs environment variables .\n• The functionâs Lambda SnapStart snapshots.\n• When used with SourceKMSKeyArn , the unzipped version of the .zip deployment package thatâs used for function invocations. For more information, see Specifying a customer managed key for Lambda .\n• The optimized version of the container image thatâs used for function invocations. Note that this is not the same key thatâs used to protect your container image in the Amazon Elastic Container Registry (Amazon ECR). For more information, see Function lifecycle .\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:[a-z0-9-.]+:.*)|()\n\n• PassThrough\n\n• pattern: arn:(aws[a-zA-Z-]*)?:lambda:[a-z]{2}(-gov)?-[a-z]+-\\d{1}:\\d{12}:function:[a-zA-Z0-9-_]+(:(\\$LATEST|[a-zA-Z0-9-_]+))?\n\n• pattern: arn:[a-zA-Z0-9-]+:lambda:[a-zA-Z0-9-]+:\\d{12}:layer:[a-zA-Z0-9-_]+:[0-9]+\n\n• pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\\-])+:([a-z]{2}(-gov)?-[a-z]+-\\d{1})?:(\\d{12})?:(.*)\n\n• pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\\-])+:([a-z]{2}(-gov)?-[a-z]+-\\d{1})?:(\\d{12})?:(.*)\n\n• EniLimitExceeded\n• InsufficientRolePermissions\n• InvalidConfiguration\n• InternalError\n• SubnetOutOfIPAddresses\n• InvalidSubnet\n• InvalidSecurityGroup\n• ImageDeleted\n• ImageAccessDenied\n• InvalidImage\n• KMSKeyAccessDenied\n• KMSKeyNotFound\n• InvalidStateKMSKey\n• DisabledKMSKey\n• EFSMountConnectivityError\n• EFSMountFailure\n• EFSMountTimeout\n• InvalidRuntime\n• InvalidZipFileException\n• FunctionError\n\n• EniLimitExceeded\n• InsufficientRolePermissions\n• InvalidConfiguration\n• InternalError\n• SubnetOutOfIPAddresses\n• InvalidSubnet\n• InvalidSecurityGroup\n• ImageDeleted\n• ImageAccessDenied\n• InvalidImage\n• KMSKeyAccessDenied\n• KMSKeyNotFound\n• InvalidStateKMSKey\n• DisabledKMSKey\n• EFSMountConnectivityError\n• EFSMountFailure\n• EFSMountTimeout\n• InvalidRuntime\n• InvalidZipFileException\n• FunctionError\n\n• pattern: arn:aws[a-zA-Z-]*:elasticfilesystem:[a-z]{2}((-gov)|(-iso(b?)))?-[a-z]+-\\d{1}:\\d{12}:access-point/fsap-[a-f0-9]{17}\n\n• pattern: /mnt/[a-zA-Z0-9-_.]+\n\n• pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\\-])+:([a-z]{2}(-gov)?-[a-z]+-\\d{1})?:(\\d{12})?:(.*)\n\n• pattern: arn:(aws[a-zA-Z0-9-]*):([a-zA-Z0-9\\-])+:([a-z]{2}(-gov)?-[a-z]+-\\d{1})?:(\\d{12})?:(.*)\n\n• PublishedVersions\n\n• pattern: arn:(aws[a-zA-Z-]*):lambda:[a-z]{2}((-gov)|(-iso(b?)))?-[a-z]+-\\d{1}::runtime:.+\n\n• pattern: [\\.\\-_/#A-Za-z0-9]+",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 110,
          "content_length": 9632
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/lambda/create-function.html",
    "doc_type": "aws",
    "total_sections": 6
  },
  {
    "title": "invokeÂ¶",
    "summary": "DescriptionÂ¶ Invokes a Lambda function. You can invoke a function synchronously (and wait for the response), or asynchronously. By default, Lambda invokes your function synchronously (i.e. the``InvocationType`` is RequestResponse ). To invoke a function asynchronously, set InvocationType to Event . Lambda passes the ClientContext object to your function for synchronous invocations only. For synchronous invocations, the maximum payload size is 6 MB. For asynchronous invocations, the maximum payl",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Invokes a Lambda function. You can invoke a function synchronously (and wait for the response), or asynchronously. By default, Lambda invokes your function synchronously (i.e. the``InvocationType`` is RequestResponse ). To invoke a function asynchronously, set InvocationType to Event . Lambda passes the ClientContext object to your function for synchronous invocations only.\n\nFor synchronous invocations, the maximum payload size is 6 MB. For asynchronous invocations, the maximum payload size is 1 MB.\n\nFor synchronous invocation , details about the function response, including errors, are included in the response body and headers. For either invocation type, you can find more information in the execution log and trace .\n\nWhen an error occurs, your function may be invoked multiple times. Retry behavior varies by error type, client, event source, and invocation type. For example, if you invoke a function asynchronously and it returns an error, Lambda executes the function up to two more times. For more information, see Error handling and automatic retries in Lambda .\n\nFor asynchronous invocation , Lambda adds events to a queue before sending them to your function. If your function does not have enough capacity to keep up with the queue, events may be lost. Occasionally, your function may receive the same event multiple times, even if no error occurs. To retain events that were not processed, configure your function with a dead-letter queue .\n\nThe status code in the API response doesnât reflect function errors. Error codes are reserved for errors that prevent your function from executing, such as permissions errors, quota errors, or issues with your functionâs code and configuration. For example, Lambda returns TooManyRequestsException if running the function would cause you to exceed a concurrency limit at either the account level (ConcurrentInvocationLimitExceeded ) or function level (ReservedFunctionConcurrentInvocationLimitExceeded ).\n\nFor functions with a long timeout, your client might disconnect during synchronous invocation while it waits for a response. Configure your HTTP client, SDK, firewall, proxy, or operating system to allow for long connections with timeout or keep-alive settings.\n\nThis operation requires permission for the lambda:InvokeFunction action. For details on how to set up permissions for cross-account invocations, see Granting function access to other accounts .\n\nSee also: AWS API Documentation",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2462
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "--function-name (string) [required]\n\nThe name or ARN of the Lambda function, version, or alias.\n\nYou can append a version number or alias to any of the formats. The length constraint applies only to the full ARN. If you specify only the function name, it is limited to 64 characters in length.\n\n--invocation-type (string)\n\nChoose from the following options.\n\nSet to Tail to include the execution log in the response. Applies to synchronously invoked functions only.\n\n--client-context (string)\n\nThe JSON that you want to provide to your Lambda function as input. The maximum payload size is 6 MB for synchronous invocations and 1 MB for asynchronous invocations.\n\nYou can enter the JSON directly. For example, --payload '{ \"key\": \"value\" }' . You can also specify a file path. For example, --payload file://payload.json .\n\n--qualifier (string)\n\nSpecify a version or alias to invoke a published version of the function.\n\noutfile (string) [required] Filename where the content will be saved\n\n• Function name â my-function (name-only), my-function:v1 (with alias).\n• Function ARN â arn:aws:lambda:us-west-2:123456789012:function:my-function .\n• Partial ARN â 123456789012:function:my-function .\n\n• pattern: (arn:(aws[a-zA-Z-]*)?:lambda:)?([a-z]{2}(-gov)?-[a-z]+-\\d{1}:)?(\\d{12}:)?(function:)?([a-zA-Z0-9-_\\.]+)(:(\\$LATEST|[a-zA-Z0-9-_]+))?\n\n• RequestResponse (default) â Invoke the function synchronously. Keep the connection open until the function returns a response or times out. The API response includes the function response and additional data.\n• Event â Invoke the function asynchronously. Send events that fail multiple times to the functionâs dead-letter queue (if one is configured). The API response only includes a status code.\n• DryRun â Validate parameter values and verify that the user or role has permission to invoke the function.\n\n• RequestResponse\n\n• pattern: (|[a-zA-Z0-9$_-]+)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 1909
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: To invoke a Lambda function synchronously\n\nThe following invoke example invokes the my-function function synchronously. The cli-binary-format option is required if youâre using AWS CLI version 2. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide.\n\nFor more information, see Invoke a Lambda function synchronously in the AWS Lambda Developer Guide.\n\nExample 2: To invoke a Lambda function asynchronously\n\nThe following invoke example invokes the my-function function asynchronously. The cli-binary-format option is required if youâre using AWS CLI version 2. For more information, see AWS CLI supported global command line options in the AWS Command Line Interface User Guide.\n\nFor more information, see Invoking a Lambda function asynchronously in the AWS Lambda Developer Guide.",
        "code_examples": [
          "```\nawslambdainvoke\\--function-namemy-function\\--cli-binary-formatraw-in-base64-out\\--payload'{ \"name\": \"Bob\" }'\\response.json\n```",
          "```\nawslambdainvoke\\--function-namemy-function\\--invocation-typeEvent\\--cli-binary-formatraw-in-base64-out\\--payload'{ \"name\": \"Bob\" }'\\response.json\n```",
          "```\n{\"StatusCode\":202}\n```"
        ],
        "usage_examples": [
          "```\n{\"ExecutedVersion\":\"$LATEST\",\"StatusCode\":200}\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1236
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "StatusCode -> (integer)\n\nFunctionError -> (string)\n\nLogResult -> (string)\n\nExecutedVersion -> (string)\n\nThe version of the function that executed. When you invoke a function with an alias, this indicates which version the alias resolved to.\n\n• pattern: (\\$LATEST|[0-9]+)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 270
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/lambda/invoke.html",
    "doc_type": "aws",
    "total_sections": 6
  },
  {
    "title": "rdsÂ¶",
    "summary": "DescriptionÂ¶ Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. It provides cost-efficient, resizeable capacity for an industry-standard relational database and manages common database administration tasks, freeing up developers to focus on what makes their applications and businesses unique. Amazon RDS gives you access to the capabilities of a MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, O",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. It provides cost-efficient, resizeable capacity for an industry-standard relational database and manages common database administration tasks, freeing up developers to focus on what makes their applications and businesses unique.\n\nAmazon RDS gives you access to the capabilities of a MySQL, MariaDB, PostgreSQL, Microsoft SQL Server, Oracle, Db2, or Amazon Aurora database server. These capabilities mean that the code, applications, and tools you already use today with your existing databases work with Amazon RDS without modification. Amazon RDS automatically backs up your database and maintains the database software that powers your DB instance. Amazon RDS is flexible: you can scale your DB instanceâs compute resources and storage capacity to meet your applicationâs demand. As with all Amazon Web Services, there are no up-front investments, and you pay only for the resources you use.\n\nThis interface reference for Amazon RDS contains documentation for a programming or command line interface you can use to manage Amazon RDS. Amazon RDS is asynchronous, which means that some interfaces might require techniques such as polling or callback functions to determine when a command has been applied. In this reference, the parameter descriptions indicate whether a command is applied immediately, on the next instance reboot, or during the maintenance window. The reference structure is as follows, and we list following some related topics from the user guide.\n\n• For the alphabetical list of API actions, see API Actions .\n• For the alphabetical list of data types, see Data Types .\n• For a list of common query parameters, see Common Parameters .\n• For descriptions of the error codes, see Common Errors .\n\n• For a summary of the Amazon RDS interfaces, see Available RDS Interfaces .\n• For more information about how to use the Query API, see Using the Query API .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 2030
        }
      },
      {
        "header": "Available CommandsÂ¶",
        "content": "• add-option-to-option-group\n• add-role-to-db-cluster\n• add-role-to-db-instance\n• add-source-identifier-to-subscription\n• add-tags-to-resource\n• apply-pending-maintenance-action\n• authorize-db-security-group-ingress\n• backtrack-db-cluster\n• cancel-export-task\n• copy-db-cluster-parameter-group\n• copy-db-cluster-snapshot\n• copy-db-parameter-group\n• copy-db-snapshot\n• copy-option-group\n• create-blue-green-deployment\n• create-custom-db-engine-version\n• create-db-cluster\n• create-db-cluster-endpoint\n• create-db-cluster-parameter-group\n• create-db-cluster-snapshot\n• create-db-instance\n• create-db-instance-read-replica\n• create-db-parameter-group\n• create-db-proxy\n• create-db-proxy-endpoint\n• create-db-security-group\n• create-db-shard-group\n• create-db-snapshot\n• create-db-subnet-group\n• create-event-subscription\n• create-global-cluster\n• create-integration\n• create-option-group\n• create-tenant-database\n• delete-blue-green-deployment\n• delete-custom-db-engine-version\n• delete-db-cluster\n• delete-db-cluster-automated-backup\n• delete-db-cluster-endpoint\n• delete-db-cluster-parameter-group\n• delete-db-cluster-snapshot\n• delete-db-instance\n• delete-db-instance-automated-backup\n• delete-db-parameter-group\n• delete-db-proxy\n• delete-db-proxy-endpoint\n• delete-db-security-group\n• delete-db-shard-group\n• delete-db-snapshot\n• delete-db-subnet-group\n• delete-event-subscription\n• delete-global-cluster\n• delete-integration\n• delete-option-group\n• delete-tenant-database\n• deregister-db-proxy-targets\n• describe-account-attributes\n• describe-blue-green-deployments\n• describe-certificates\n• describe-db-cluster-automated-backups\n• describe-db-cluster-backtracks\n• describe-db-cluster-endpoints\n• describe-db-cluster-parameter-groups\n• describe-db-cluster-parameters\n• describe-db-cluster-snapshot-attributes\n• describe-db-cluster-snapshots\n• describe-db-clusters\n• describe-db-engine-versions\n• describe-db-instance-automated-backups\n• describe-db-instances\n• describe-db-log-files\n• describe-db-major-engine-versions\n• describe-db-parameter-groups\n• describe-db-parameters\n• describe-db-proxies\n• describe-db-proxy-endpoints\n• describe-db-proxy-target-groups\n• describe-db-proxy-targets\n• describe-db-recommendations\n• describe-db-security-groups\n• describe-db-shard-groups\n• describe-db-snapshot-attributes\n• describe-db-snapshot-tenant-databases\n• describe-db-snapshots\n• describe-db-subnet-groups\n• describe-engine-default-cluster-parameters\n• describe-engine-default-parameters\n• describe-event-categories\n• describe-event-subscriptions\n• describe-events\n• describe-export-tasks\n• describe-global-clusters\n• describe-integrations\n• describe-option-group-options\n• describe-option-groups\n• describe-orderable-db-instance-options\n• describe-pending-maintenance-actions\n• describe-reserved-db-instances\n• describe-reserved-db-instances-offerings\n• describe-source-regions\n• describe-tenant-databases\n• describe-valid-db-instance-modifications\n• disable-http-endpoint\n• download-db-log-file-portion\n• enable-http-endpoint\n• failover-db-cluster\n• failover-global-cluster\n• generate-db-auth-token\n• list-tags-for-resource\n• modify-activity-stream\n• modify-certificates\n• modify-current-db-cluster-capacity\n• modify-custom-db-engine-version\n• modify-db-cluster\n• modify-db-cluster-endpoint\n• modify-db-cluster-parameter-group\n• modify-db-cluster-snapshot-attribute\n• modify-db-instance\n• modify-db-parameter-group\n• modify-db-proxy\n• modify-db-proxy-endpoint\n• modify-db-proxy-target-group\n• modify-db-recommendation\n• modify-db-shard-group\n• modify-db-snapshot\n• modify-db-snapshot-attribute\n• modify-db-subnet-group\n• modify-event-subscription\n• modify-global-cluster\n• modify-integration\n• modify-tenant-database\n• promote-read-replica\n• promote-read-replica-db-cluster\n• purchase-reserved-db-instances-offering\n• reboot-db-cluster\n• reboot-db-instance\n• reboot-db-shard-group\n• register-db-proxy-targets\n• remove-from-global-cluster\n• remove-option-from-option-group\n• remove-role-from-db-cluster\n• remove-role-from-db-instance\n• remove-source-identifier-from-subscription\n• remove-tags-from-resource\n• reset-db-cluster-parameter-group\n• reset-db-parameter-group\n• restore-db-cluster-from-s3\n• restore-db-cluster-from-snapshot\n• restore-db-cluster-to-point-in-time\n• restore-db-instance-from-db-snapshot\n• restore-db-instance-from-s3\n• restore-db-instance-to-point-in-time\n• revoke-db-security-group-ingress\n• start-activity-stream\n• start-db-cluster\n• start-db-instance\n• start-db-instance-automated-backups-replication\n• start-export-task\n• stop-activity-stream\n• stop-db-cluster\n• stop-db-instance\n• stop-db-instance-automated-backups-replication\n• switchover-blue-green-deployment\n• switchover-global-cluster\n• switchover-read-replica",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 4744
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/rds/index.html",
    "doc_type": "aws",
    "total_sections": 2
  },
  {
    "title": "create-db-instanceÂ¶",
    "summary": "DescriptionÂ¶ Creates a new DB instance. The new DB instance can be an RDS DB instance, or it can be a DB instance in an Aurora DB cluster. For an Aurora DB cluster, you can call this operation multiple times to add more than one DB instance to the cluster. For more information about creating an RDS DB instance, see Creating an Amazon RDS DB instance in the Amazon RDS User Guide . For more information about creating a DB instance in an Aurora DB cluster, see Creating an Amazon Aurora DB cluster ",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Creates a new DB instance.\n\nThe new DB instance can be an RDS DB instance, or it can be a DB instance in an Aurora DB cluster. For an Aurora DB cluster, you can call this operation multiple times to add more than one DB instance to the cluster.\n\nFor more information about creating an RDS DB instance, see Creating an Amazon RDS DB instance in the Amazon RDS User Guide .\n\nFor more information about creating a DB instance in an Aurora DB cluster, see Creating an Amazon Aurora DB cluster in the Amazon Aurora User Guide .\n\nSee also: AWS API Documentation",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 555
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "The meaning of this parameter differs according to the database engine you use.\n\nThe name of the database to create when the primary DB instance of the Aurora MySQL DB cluster is created. If this parameter isnât specified for an Aurora MySQL DB cluster, no database is created in the DB cluster.\n\nMust contain 1 to 64 alphanumeric characters.\n\nMust begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n\nCanât be a word reserved by the database engine.\n\nAmazon Aurora PostgreSQL\n\nThe name of the database to create when the primary DB instance of the Aurora PostgreSQL DB cluster is created. A database named postgres is always created. If this parameter is specified, an additional database with this name is created.\n\nIt must contain 1 to 63 alphanumeric characters.\n\nMust begin with a letter. Subsequent characters can be letters, underscores, or digits (0 to 9).\n\nCanât be a word reserved by the database engine.\n\nAmazon RDS Custom for Oracle\n\nThe Oracle System ID (SID) of the created RDS Custom DB instance. If you donât specify a value, the default value is ORCL for non-CDBs and RDSCDB for CDBs.\n\nMust contain 1 to 8 alphanumeric characters.\n\nMust contain a letter.\n\nCanât be a word reserved by the database engine.\n\nAmazon RDS Custom for SQL Server\n\nNot applicable. Must be null.\n\nThe name of the database to create when the DB instance is created. If this parameter isnât specified, no database is created in the DB instance. In some cases, we recommend that you donât add a database name. For more information, see Additional considerations in the Amazon RDS User Guide .\n\nMust contain 1 to 64 letters or numbers.\n\nMust begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n\nCanât be a word reserved by the specified database engine.\n\nThe name of the database to create when the DB instance is created. If this parameter isnât specified, no database is created in the DB instance.\n\nMust contain 1 to 64 letters or numbers.\n\nMust begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n\nCanât be a word reserved by the specified database engine.\n\nThe name of the database to create when the DB instance is created. If this parameter isnât specified, no database is created in the DB instance.\n\nMust contain 1 to 64 letters or numbers.\n\nMust begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n\nCanât be a word reserved by the specified database engine.\n\nThe Oracle System ID (SID) of the created DB instance. If you donât specify a value, the default value is ORCL . You canât specify the string null , or any other reserved word, for DBName .\n\nCanât be longer than 8 characters.\n\nThe name of the database to create when the DB instance is created. A database named postgres is always created. If this parameter is specified, an additional database with this name is created.\n\nMust contain 1 to 63 letters, numbers, or underscores.\n\nMust begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n\nCanât be a word reserved by the specified database engine.\n\nNot applicable. Must be null.\n\n--db-instance-identifier (string) [required]\n\nThe identifier for this DB instance. This parameter is stored as a lowercase string.\n\nExample: mydbinstance\n\n--allocated-storage (integer)\n\nThe amount of storage in gibibytes (GiB) to allocate for the DB instance.\n\nThis setting doesnât apply to Amazon Aurora DB instances. Aurora cluster volumes automatically grow as the amount of data in your database increases, though you are only charged for the space that you use in an Aurora cluster volume.\n\nConstraints to the amount of storage for each storage type are the following:\n\nGeneral Purpose (SSD) storage (gp2, gp3): Must be an integer from 40 to 65536 for RDS Custom for Oracle, 16384 for RDS Custom for SQL Server.\n\nProvisioned IOPS storage (io1, io2): Must be an integer from 40 to 65536 for RDS Custom for Oracle, 16384 for RDS Custom for SQL Server.\n\nConstraints to the amount of storage for each storage type are the following:\n\nGeneral Purpose (SSD) storage (gp3): Must be an integer from 20 to 65536.\n\nProvisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n\nConstraints to the amount of storage for each storage type are the following:\n\nGeneral Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n\nProvisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n\nMagnetic storage (standard): Must be an integer from 5 to 3072.\n\nConstraints to the amount of storage for each storage type are the following:\n\nGeneral Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n\nProvisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n\nMagnetic storage (standard): Must be an integer from 5 to 3072.\n\nConstraints to the amount of storage for each storage type are the following:\n\nGeneral Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n\nProvisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n\nMagnetic storage (standard): Must be an integer from 10 to 3072.\n\nConstraints to the amount of storage for each storage type are the following:\n\nGeneral Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n\nProvisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n\nMagnetic storage (standard): Must be an integer from 5 to 3072.\n\nConstraints to the amount of storage for each storage type are the following:\n\n--db-instance-class (string) [required]\n\n--engine (string) [required]\n\nThe database engine to use for this DB instance.\n\nNot every database engine is available in every Amazon Web Services Region.\n\n--master-username (string)\n\nThe name for the master user.\n\nThis setting doesnât apply to Amazon Aurora DB instances. The name for the master user is managed by the DB cluster.\n\nThis setting is required for RDS DB instances.\n\n--master-user-password (string)\n\nThe password for the master user.\n\nThis setting doesnât apply to Amazon Aurora DB instances. The password for the master user is managed by the DB cluster.\n\n--db-security-groups (list)\n\nA list of DB security groups to associate with this DB instance.\n\nThis setting applies to the legacy EC2-Classic platform, which is no longer used to create new DB instances. Use the VpcSecurityGroupIds setting instead.\n\n--vpc-security-group-ids (list)\n\nA list of Amazon EC2 VPC security groups to associate with this DB instance.\n\nThis setting doesnât apply to Amazon Aurora DB instances. The associated list of EC2 VPC security groups is managed by the DB cluster.\n\nDefault: The default EC2 VPC security group for the DB subnet groupâs VPC.\n\n--availability-zone (string)\n\nThe Availability Zone (AZ) where the database will be created. For information on Amazon Web Services Regions and Availability Zones, see Regions and Availability Zones .\n\nFor Amazon Aurora, each Aurora DB cluster hosts copies of its storage in three separate Availability Zones. Specify one of these Availability Zones. Aurora automatically chooses an appropriate Availability Zone if you donât specify one.\n\nDefault: A random, system-chosen Availability Zone in the endpointâs Amazon Web Services Region.\n\n--db-subnet-group-name (string)\n\nA DB subnet group to associate with this DB instance.\n\nExample: mydbsubnetgroup\n\n--preferred-maintenance-window (string)\n\nThe time range each week during which system maintenance can occur. For more information, see Amazon RDS Maintenance Window in the Amazon RDS User Guide.\n\nThe default is a 30-minute window selected at random from an 8-hour block of time for each Amazon Web Services Region, occurring on a random day of the week.\n\n--db-parameter-group-name (string)\n\nThe name of the DB parameter group to associate with this DB instance. If you donât specify a value, then Amazon RDS uses the default DB parameter group for the specified DB engine and version.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\n--backup-retention-period (integer)\n\nThe number of days for which automated backups are retained. Setting this parameter to a positive number enables backups. Setting this parameter to 0 disables automated backups.\n\nThis setting doesnât apply to Amazon Aurora DB instances. The retention period for automated backups is managed by the DB cluster.\n\n--preferred-backup-window (string)\n\nThe daily time range during which automated backups are created if automated backups are enabled, using the BackupRetentionPeriod parameter. The default is a 30-minute window selected at random from an 8-hour block of time for each Amazon Web Services Region. For more information, see Backup window in the Amazon RDS User Guide .\n\nThis setting doesnât apply to Amazon Aurora DB instances. The daily time range for creating automated backups is managed by the DB cluster.\n\nThe port number on which the database accepts connections.\n\nThis setting doesnât apply to Aurora DB instances. The port number is managed by the cluster.\n\nValid Values: 1150-65535\n\n--multi-az | --no-multi-az (boolean)\n\nSpecifies whether the DB instance is a Multi-AZ deployment. You canât set the AvailabilityZone parameter if the DB instance is a Multi-AZ deployment.\n\nThis setting doesnât apply to Amazon Aurora because the DB instance Availability Zones (AZs) are managed by the DB cluster.\n\n--engine-version (string)\n\nThe version number of the database engine to use.\n\nThis setting doesnât apply to Amazon Aurora DB instances. The version number of the database engine the DB instance uses is managed by the DB cluster.\n\nFor a list of valid engine versions, use the DescribeDBEngineVersions operation.\n\nThe following are the database engines and links to information about the major and minor versions that are available with Amazon RDS. Not every database engine is available for every Amazon Web Services Region.\n\nA custom engine version (CEV) that you have previously created. This setting is required for RDS Custom for Oracle. The CEV name has the following format: 19.*customized_string* . A valid CEV name is 19.my_cev1 . For more information, see Creating an RDS Custom for Oracle DB instance in the Amazon RDS User Guide .\n\nSee RDS Custom for SQL Server general requirements in the Amazon RDS User Guide .\n\nFor information, see Db2 on Amazon RDS versions in the Amazon RDS User Guide .\n\nFor information, see MariaDB on Amazon RDS versions in the Amazon RDS User Guide .\n\nFor information, see Microsoft SQL Server versions on Amazon RDS in the Amazon RDS User Guide .\n\nFor information, see MySQL on Amazon RDS versions in the Amazon RDS User Guide .\n\nFor information, see Oracle Database Engine release notes in the Amazon RDS User Guide .\n\nFor information, see Amazon RDS for PostgreSQL versions and extensions in the Amazon RDS User Guide .\n\n--auto-minor-version-upgrade | --no-auto-minor-version-upgrade (boolean)\n\nSpecifies whether minor engine upgrades are applied automatically to the DB instance during the maintenance window. By default, minor engine upgrades are applied automatically.\n\nIf you create an RDS Custom DB instance, you must set AutoMinorVersionUpgrade to false .\n\nFor more information about automatic minor version upgrades, see Automatically upgrading the minor engine version .\n\n--license-model (string)\n\nThe license model information for this DB instance.\n\n• Must contain 1 to 64 alphanumeric characters.\n• Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n• Canât be a word reserved by the database engine. Amazon Aurora PostgreSQL\n\n• It must contain 1 to 63 alphanumeric characters.\n• Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0 to 9).\n• Canât be a word reserved by the database engine. Amazon RDS Custom for Oracle\n\n• Must contain 1 to 8 alphanumeric characters.\n• Must contain a letter.\n• Canât be a word reserved by the database engine. Amazon RDS Custom for SQL Server\n\n• Must contain 1 to 64 letters or numbers.\n• Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n• Canât be a word reserved by the specified database engine. RDS for MariaDB\n\n• Must contain 1 to 64 letters or numbers.\n• Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n• Canât be a word reserved by the specified database engine. RDS for MySQL\n\n• Must contain 1 to 64 letters or numbers.\n• Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n• Canât be a word reserved by the specified database engine. RDS for Oracle\n\n• Canât be longer than 8 characters. RDS for PostgreSQL\n\n• Must contain 1 to 63 letters, numbers, or underscores.\n• Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9).\n• Canât be a word reserved by the specified database engine. RDS for SQL Server\n\n• Must contain from 1 to 63 letters, numbers, or hyphens.\n• First character must be a letter.\n• Canât end with a hyphen or contain two consecutive hyphens.\n\n• General Purpose (SSD) storage (gp2, gp3): Must be an integer from 40 to 65536 for RDS Custom for Oracle, 16384 for RDS Custom for SQL Server.\n• Provisioned IOPS storage (io1, io2): Must be an integer from 40 to 65536 for RDS Custom for Oracle, 16384 for RDS Custom for SQL Server. RDS for Db2\n\n• General Purpose (SSD) storage (gp3): Must be an integer from 20 to 65536.\n• Provisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536. RDS for MariaDB\n\n• General Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n• Provisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n• Magnetic storage (standard): Must be an integer from 5 to 3072. RDS for MySQL\n\n• General Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n• Provisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n• Magnetic storage (standard): Must be an integer from 5 to 3072. RDS for Oracle\n\n• General Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n• Provisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n• Magnetic storage (standard): Must be an integer from 10 to 3072. RDS for PostgreSQL\n\n• General Purpose (SSD) storage (gp2, gp3): Must be an integer from 20 to 65536.\n• Provisioned IOPS storage (io1, io2): Must be an integer from 100 to 65536.\n• Magnetic storage (standard): Must be an integer from 5 to 3072. RDS for SQL Server\n\n• General Purpose (SSD) storage (gp2, gp3): Enterprise and Standard editions: Must be an integer from 20 to 16384. Web and Express editions: Must be an integer from 20 to 16384.\n• Provisioned IOPS storage (io1, io2): Enterprise and Standard editions: Must be an integer from 100 to 16384. Web and Express editions: Must be an integer from 100 to 16384.\n• Magnetic storage (standard): Enterprise and Standard editions: Must be an integer from 20 to 1024. Web and Express editions: Must be an integer from 20 to 1024.\n\n• Enterprise and Standard editions: Must be an integer from 20 to 16384.\n• Web and Express editions: Must be an integer from 20 to 16384.\n\n• Enterprise and Standard editions: Must be an integer from 100 to 16384.\n• Web and Express editions: Must be an integer from 100 to 16384.\n\n• Enterprise and Standard editions: Must be an integer from 20 to 1024.\n• Web and Express editions: Must be an integer from 20 to 1024.\n\n• aurora-mysql (for Aurora MySQL DB instances)\n• aurora-postgresql (for Aurora PostgreSQL DB instances)\n• custom-oracle-ee (for RDS Custom for Oracle DB instances)\n• custom-oracle-ee-cdb (for RDS Custom for Oracle DB instances)\n• custom-oracle-se2 (for RDS Custom for Oracle DB instances)\n• custom-oracle-se2-cdb (for RDS Custom for Oracle DB instances)\n• custom-sqlserver-ee (for RDS Custom for SQL Server DB instances)\n• custom-sqlserver-se (for RDS Custom for SQL Server DB instances)\n• custom-sqlserver-web (for RDS Custom for SQL Server DB instances)\n• custom-sqlserver-dev (for RDS Custom for SQL Server DB instances)\n• oracle-ee-cdb\n• oracle-se2-cdb\n• sqlserver-ee\n• sqlserver-se\n• sqlserver-ex\n• sqlserver-web\n\n• Must be 1 to 16 letters, numbers, or underscores.\n• First character must be a letter.\n• Canât be a reserved word for the chosen database engine.\n\n• Canât be specified if ManageMasterUserPassword is turned on.\n• Can include any printable ASCII character except â/â, âââ, or â@â. For RDS for Oracle, canât include the â&â (ampersand) or the âââ (single quotes) character.\n\n• RDS for Db2 - Must contain from 8 to 255 characters.\n• RDS for MariaDB - Must contain from 8 to 41 characters.\n• RDS for Microsoft SQL Server - Must contain from 8 to 128 characters.\n• RDS for MySQL - Must contain from 8 to 41 characters.\n• RDS for Oracle - Must contain from 8 to 30 characters.\n• RDS for PostgreSQL - Must contain from 8 to 128 characters.\n\n• The AvailabilityZone parameter canât be specified if the DB instance is a Multi-AZ deployment.\n• The specified Availability Zone must be in the same Amazon Web Services Region as the current endpoint.\n\n• Must match the name of an existing DB subnet group.\n\n• Must be in the format ddd:hh24:mi-ddd:hh24:mi .\n• The day values must be mon | tue | wed | thu | fri | sat | sun .\n• Must be in Universal Coordinated Time (UTC).\n• Must not conflict with the preferred backup window.\n• Must be at least 30 minutes.\n\n• Must be 1 to 255 letters, numbers, or hyphens.\n• The first character must be a letter.\n• Canât end with a hyphen or contain two consecutive hyphens.\n\n• Must be a value from 0 to 35.\n• Canât be set to 0 if the DB instance is a source to read replicas.\n• Canât be set to 0 for an RDS Custom for Oracle DB instance.\n\n• Must be in the format hh24:mi-hh24:mi .\n• Must be in Universal Coordinated Time (UTC).\n• Must not conflict with the preferred maintenance window.\n• Must be at least 30 minutes.\n\n• RDS for Db2 - 50000\n• RDS for MariaDB - 3306\n• RDS for Microsoft SQL Server - 1433\n• RDS for MySQL - 3306\n• RDS for Oracle - 1521\n• RDS for PostgreSQL - 5432\n\n• For RDS for Microsoft SQL Server, the value canât be 1234 , 1434 , 3260 , 3343 , 3389 , 47001 , or 49152-49156 .\n\n[Note] NoteLicense models for RDS for Db2 require additional configuration. The bring your own license (BYOL) model requires a custom parameter group and an Amazon Web Services License Manager self-managed license. The Db2 license through Amazon Web Services Marketplace model requires an Amazon Web Services Marketplace subscription. For more information, see Amazon RDS for Db2 licensing options in the Amazon RDS User Guide . The default for RDS for Db2 is bring-your-own-license .",
        "code_examples": [
          "```\n\"string\"\"string\"...\n```",
          "```\n\"string\"\"string\"...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 127,
          "content_length": 18778
        }
      },
      {
        "header": "Note",
        "content": "License models for RDS for Db2 require additional configuration. The bring your own license (BYOL) model requires a custom parameter group and an Amazon Web Services License Manager self-managed license. The Db2 license through Amazon Web Services Marketplace model requires an Amazon Web Services Marketplace subscription. For more information, see Amazon RDS for Db2 licensing options in the Amazon RDS User Guide .\n\nThe default for RDS for Db2 is bring-your-own-license .\n\nThis setting doesnât apply to Amazon Aurora or RDS Custom DB instances.\n\nThe amount of Provisioned IOPS (input/output operations per second) to initially allocate for the DB instance. For information about valid IOPS values, see Amazon RDS DB instance storage in the Amazon RDS User Guide .\n\nThis setting doesnât apply to Amazon Aurora DB instances. Storage is managed by the DB cluster.\n\n--storage-throughput (integer)\n\nThe storage throughput value, in mebibyte per second (MiBps), for the DB instance.\n\nThis setting applies only to the gp3 storage type.\n\nThis setting doesnât apply to Amazon Aurora or RDS Custom DB instances.\n\n--option-group-name (string)\n\nThe option group to associate the DB instance with.\n\nPermanent options, such as the TDE option for Oracle Advanced Security TDE, canât be removed from an option group. Also, that option group canât be removed from a DB instance after it is associated with a DB instance.\n\nThis setting doesnât apply to Amazon Aurora or RDS Custom DB instances.\n\n--character-set-name (string)\n\nFor supported engines, the character set (CharacterSet ) to associate the DB instance with.\n\nThis setting doesnât apply to the following DB instances:\n\n--nchar-character-set-name (string)\n\nThe name of the NCHAR character set for the Oracle DB instance.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\n--publicly-accessible | --no-publicly-accessible (boolean)\n\nSpecifies whether the DB instance is publicly accessible.\n\nWhen the DB instance is publicly accessible and you connect from outside of the DB instanceâs virtual private cloud (VPC), its Domain Name System (DNS) endpoint resolves to the public IP address. When you connect from within the same VPC as the DB instance, the endpoint resolves to the private IP address. Access to the DB instance is ultimately controlled by the security group it uses. That public access is not permitted if the security group assigned to the DB instance doesnât permit it.\n\nWhen the DB instance isnât publicly accessible, it is an internal DB instance with a DNS name that resolves to a private IP address.\n\nDefault: The default behavior varies depending on whether DBSubnetGroupName is specified.\n\nIf DBSubnetGroupName isnât specified, and PubliclyAccessible isnât specified, the following applies:\n\nIf DBSubnetGroupName is specified, and PubliclyAccessible isnât specified, the following applies:\n\nTags to assign to the DB instance.\n\nMetadata assigned to an Amazon RDS resource consisting of a key-value pair.\n\nFor more information, see Tagging Amazon RDS resources in the Amazon RDS User Guide or Tagging Amazon Aurora and Amazon RDS resources in the Amazon Aurora User Guide .\n\n--db-cluster-identifier (string)\n\nThe identifier of the DB cluster that this DB instance will belong to.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\n--storage-type (string)\n\nThe storage type to associate with the DB instance.\n\nIf you specify io1 , io2 , or gp3 , you must also include a value for the Iops parameter.\n\nThis setting doesnât apply to Amazon Aurora DB instances. Storage is managed by the DB cluster.\n\nValid Values: gp2 | gp3 | io1 | io2 | standard\n\nDefault: io1 , if the Iops parameter is specified. Otherwise, gp3 .\n\n--tde-credential-arn (string)\n\nThe ARN from the key store with which to associate the instance for TDE encryption.\n\nThis setting doesnât apply to Amazon Aurora or RDS Custom DB instances.\n\n--tde-credential-password (string)\n\nThe password for the given ARN from the key store in order to access the device.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\n--storage-encrypted | --no-storage-encrypted (boolean)\n\nSpecifes whether the DB instance is encrypted. By default, it isnât encrypted.\n\nFor RDS Custom DB instances, either enable this setting or leave it unset. Otherwise, Amazon RDS reports an error.\n\nThis setting doesnât apply to Amazon Aurora DB instances. The encryption for DB instances is managed by the DB cluster.\n\n--kms-key-id (string)\n\nThe Amazon Web Services KMS key identifier for an encrypted DB instance.\n\nThe Amazon Web Services KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key. To use a KMS key in a different Amazon Web Services account, specify the key ARN or alias ARN.\n\nThis setting doesnât apply to Amazon Aurora DB instances. The Amazon Web Services KMS key identifier is managed by the DB cluster. For more information, see CreateDBCluster .\n\nIf StorageEncrypted is enabled, and you do not specify a value for the KmsKeyId parameter, then Amazon RDS uses your default KMS key. There is a default KMS key for your Amazon Web Services account. Your Amazon Web Services account has a different default KMS key for each Amazon Web Services Region.\n\nFor Amazon RDS Custom, a KMS key is required for DB instances. For most RDS engines, if you leave this parameter empty while enabling StorageEncrypted , the engine uses the default KMS key. However, RDS Custom doesnât use the default key when this parameter is empty. You must explicitly specify a key.\n\nThe Active Directory directory ID to create the DB instance in. Currently, you can create only Db2, MySQL, Microsoft SQL Server, Oracle, and PostgreSQL DB instances in an Active Directory Domain.\n\nFor more information, see Kerberos Authentication in the Amazon RDS User Guide .\n\nThis setting doesnât apply to the following DB instances:\n\n--domain-fqdn (string)\n\nThe fully qualified domain name (FQDN) of an Active Directory domain.\n\nExample: mymanagedADtest.mymanagedAD.mydomain\n\n--domain-ou (string)\n\nThe Active Directory organizational unit for your DB instance to join.\n\nExample: OU=mymanagedADtestOU,DC=mymanagedADtest,DC=mymanagedAD,DC=mydomain\n\n--domain-auth-secret-arn (string)\n\nThe ARN for the Secrets Manager secret with the credentials for the user joining the domain.\n\nExample: arn:aws:secretsmanager:region:account-number:secret:myselfmanagedADtestsecret-123456\n\n--domain-dns-ips (list)\n\nThe IPv4 DNS IP addresses of your primary and secondary Active Directory domain controllers.\n\nExample: 123.124.125.126,234.235.236.237\n\n--copy-tags-to-snapshot | --no-copy-tags-to-snapshot (boolean)\n\nSpecifies whether to copy tags from the DB instance to snapshots of the DB instance. By default, tags are not copied.\n\nThis setting doesnât apply to Amazon Aurora DB instances. Copying tags to snapshots is managed by the DB cluster. Setting this value for an Aurora DB instance has no effect on the DB cluster setting.\n\n--monitoring-interval (integer)\n\nThe interval, in seconds, between points when Enhanced Monitoring metrics are collected for the DB instance. To disable collection of Enhanced Monitoring metrics, specify 0 .\n\nIf MonitoringRoleArn is specified, then you must set MonitoringInterval to a value other than 0 .\n\nThis setting doesnât apply to RDS Custom DB instances.\n\nValid Values: 0 | 1 | 5 | 10 | 15 | 30 | 60\n\n--monitoring-role-arn (string)\n\nThe ARN for the IAM role that permits RDS to send enhanced monitoring metrics to Amazon CloudWatch Logs. For example, arn:aws:iam:123456789012:role/emaccess . For information on creating a monitoring role, see Setting Up and Enabling Enhanced Monitoring in the Amazon RDS User Guide .\n\nIf MonitoringInterval is set to a value other than 0 , then you must supply a MonitoringRoleArn value.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\n--domain-iam-role-name (string)\n\nThe name of the IAM role to use when making API calls to the Directory Service.\n\nThis setting doesnât apply to the following DB instances:\n\n--promotion-tier (integer)\n\nThe order of priority in which an Aurora Replica is promoted to the primary instance after a failure of the existing primary instance. For more information, see Fault Tolerance for an Aurora DB Cluster in the Amazon Aurora User Guide .\n\nThis setting doesnât apply to RDS Custom DB instances.\n\nValid Values: 0 - 15\n\n--enable-iam-database-authentication | --no-enable-iam-database-authentication (boolean)\n\nSpecifies whether to enable mapping of Amazon Web Services Identity and Access Management (IAM) accounts to database accounts. By default, mapping isnât enabled.\n\nFor more information, see IAM Database Authentication for MySQL and PostgreSQL in the Amazon RDS User Guide .\n\nThis setting doesnât apply to the following DB instances:\n\n--database-insights-mode (string)\n\nThe mode of Database Insights to enable for the DB instance.\n\n• RDS for Db2 - bring-your-own-license | marketplace-license\n• RDS for MariaDB - general-public-license\n• RDS for Microsoft SQL Server - license-included\n• RDS for MySQL - general-public-license\n• RDS for Oracle - bring-your-own-license | license-included\n• RDS for PostgreSQL - postgresql-license\n\n• For RDS for Db2, MariaDB, MySQL, Oracle, and PostgreSQL - Must be a multiple between .5 and 50 of the storage amount for the DB instance.\n• For RDS for SQL Server - Must be a multiple between 1 and 50 of the storage amount for the DB instance.\n\n• Amazon Aurora - The character set is managed by the DB cluster. For more information, see CreateDBCluster .\n• RDS Custom - However, if you need to change the character set, you can change it on the database itself.\n\n• If the default VPC in the target Region doesnât have an internet gateway attached to it, the DB instance is private.\n• If the default VPC in the target Region has an internet gateway attached to it, the DB instance is public.\n\n• If the subnets are part of a VPC that doesnât have an internet gateway attached to it, the DB instance is private.\n• If the subnets are part of a VPC that has an internet gateway attached to it, the DB instance is public.\n\n• Amazon Aurora (The domain is managed by the DB cluster.)\n\n• Canât be longer than 64 characters.\n\n• Must be in the distinguished name format.\n• Canât be longer than 64 characters.\n\n• Two IP addresses must be provided. If there isnât a secondary domain controller, use the IP address of the primary domain controller for both entries in the list.\n\n• Amazon Aurora (The domain is managed by the DB cluster.)\n\n• Amazon Aurora (Mapping Amazon Web Services IAM accounts to database accounts is managed by the DB cluster.)\n\n[Note] NoteAurora DB instances inherit this value from the DB cluster, so you canât change this value.",
        "code_examples": [
          "```\nKey=string,Value=string...\n```",
          "```\n[{\"Key\":\"string\",\"Value\":\"string\"}...]\n```",
          "```\n\"string\"\"string\"...\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 94,
          "content_length": 10763
        }
      },
      {
        "header": "Note",
        "content": "--enable-performance-insights | --no-enable-performance-insights (boolean)\n\nSpecifies whether to enable Performance Insights for the DB instance. For more information, see Using Amazon Performance Insights in the Amazon RDS User Guide .\n\nThis setting doesnât apply to RDS Custom DB instances.\n\n--performance-insights-kms-key-id (string)\n\nThe Amazon Web Services KMS key identifier for encryption of Performance Insights data.\n\nThe Amazon Web Services KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key.\n\nIf you donât specify a value for PerformanceInsightsKMSKeyId , then Amazon RDS uses your default KMS key. There is a default KMS key for your Amazon Web Services account. Your Amazon Web Services account has a different default KMS key for each Amazon Web Services Region.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\n--performance-insights-retention-period (integer)\n\nThe number of days to retain Performance Insights data.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\nIf you specify a retention period that isnât valid, such as 94 , Amazon RDS returns an error.\n\n--enable-cloudwatch-logs-exports (list)\n\nThe list of log types to enable for exporting to CloudWatch Logs. For more information, see Publishing Database Logs to Amazon CloudWatch Logs in the Amazon RDS User Guide .\n\nThis setting doesnât apply to the following DB instances:\n\nThe following values are valid for each DB engine:\n\n--processor-features (list)\n\nThe number of CPU cores and the number of threads per core for the DB instance class of the DB instance.\n\nThis setting doesnât apply to Amazon Aurora or RDS Custom DB instances.\n\nContains the processor features of a DB instance class.\n\nTo specify the number of CPU cores, use the coreCount feature name for the Name parameter. To specify the number of threads per core, use the threadsPerCore feature name for the Name parameter.\n\nYou can set the processor features of the DB instance class for a DB instance when you call one of the following actions:\n\nYou can view the valid processor values for a particular instance class by calling the DescribeOrderableDBInstanceOptions action and specifying the instance class for the DBInstanceClass parameter.\n\nIn addition, you can use the following actions for DB instance class processor information:\n\nIf you call DescribeDBInstances , ProcessorFeature returns non-null values only if the following conditions are met:\n\nFor more information, see Configuring the processor for a DB instance class in RDS for Oracle in the Amazon RDS User Guide.\n\n--deletion-protection | --no-deletion-protection (boolean)\n\nSpecifies whether the DB instance has deletion protection enabled. The database canât be deleted when deletion protection is enabled. By default, deletion protection isnât enabled. For more information, see Deleting a DB Instance .\n\nThis setting doesnât apply to Amazon Aurora DB instances. You can enable or disable deletion protection for the DB cluster. For more information, see CreateDBCluster . DB instances in a DB cluster can be deleted even when deletion protection is enabled for the DB cluster.\n\n--max-allocated-storage (integer)\n\nThe upper limit in gibibytes (GiB) to which Amazon RDS can automatically scale the storage of the DB instance.\n\nFor more information about this setting, including limitations that apply to it, see Managing capacity automatically with Amazon RDS storage autoscaling in the Amazon RDS User Guide .\n\nThis setting doesnât apply to the following DB instances:\n\n--enable-customer-owned-ip | --no-enable-customer-owned-ip (boolean)\n\nSpecifies whether to enable a customer-owned IP address (CoIP) for an RDS on Outposts DB instance.\n\nA CoIP provides local or external connectivity to resources in your Outpost subnets through your on-premises network. For some use cases, a CoIP can provide lower latency for connections to the DB instance from outside of its virtual private cloud (VPC) on your local network.\n\nFor more information about RDS on Outposts, see Working with Amazon RDS on Amazon Web Services Outposts in the Amazon RDS User Guide .\n\nFor more information about CoIPs, see Customer-owned IP addresses in the Amazon Web Services Outposts User Guide .\n\n--network-type (string)\n\nThe network type of the DB instance.\n\nThe network type is determined by the DBSubnetGroup specified for the DB instance. A DBSubnetGroup can support only the IPv4 protocol or the IPv4 and the IPv6 protocols (DUAL ).\n\nFor more information, see Working with a DB instance in a VPC in the Amazon RDS User Guide.\n\nValid Values: IPV4 | DUAL\n\n--backup-target (string)\n\nThe location for storing automated backups and manual snapshots.\n\nFor more information, see Working with Amazon RDS on Amazon Web Services Outposts in the Amazon RDS User Guide .\n\n--custom-iam-instance-profile (string)\n\nThe instance profile associated with the underlying Amazon EC2 instance of an RDS Custom DB instance.\n\nThis setting is required for RDS Custom.\n\nFor the list of permissions required for the IAM role, see Configure IAM and your VPC in the Amazon RDS User Guide .\n\n--db-system-id (string)\n\n--ca-certificate-identifier (string)\n\nThe CA certificate identifier to use for the DB instanceâs server certificate.\n\nThis setting doesnât apply to RDS Custom DB instances.\n\nFor more information, see Using SSL/TLS to encrypt a connection to a DB instance in the Amazon RDS User Guide and Using SSL/TLS to encrypt a connection to a DB cluster in the Amazon Aurora User Guide .\n\n--manage-master-user-password | --no-manage-master-user-password (boolean)\n\nSpecifies whether to manage the master user password with Amazon Web Services Secrets Manager.\n\nFor more information, see Password management with Amazon Web Services Secrets Manager in the Amazon RDS User Guide.\n\n--master-user-secret-kms-key-id (string)\n\nThe Amazon Web Services KMS key identifier to encrypt a secret that is automatically generated and managed in Amazon Web Services Secrets Manager.\n\nThis setting is valid only if the master user password is managed by RDS in Amazon Web Services Secrets Manager for the DB instance.\n\nThe Amazon Web Services KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key. To use a KMS key in a different Amazon Web Services account, specify the key ARN or alias ARN.\n\nIf you donât specify MasterUserSecretKmsKeyId , then the aws/secretsmanager KMS key is used to encrypt the secret. If the secret is in a different Amazon Web Services account, then you canât use the aws/secretsmanager KMS key to encrypt the secret, and you must use a customer managed KMS key.\n\nThere is a default KMS key for your Amazon Web Services account. Your Amazon Web Services account has a different default KMS key for each Amazon Web Services Region.\n\n--multi-tenant | --no-multi-tenant (boolean)\n\nSpecifies whether to use the multi-tenant configuration or the single-tenant configuration (default). This parameter only applies to RDS for Oracle container database (CDB) engines.\n\nNote the following restrictions:\n\n--dedicated-log-volume | --no-dedicated-log-volume (boolean)\n\n--engine-lifecycle-support (string)\n\nThe life cycle type for this DB instance.\n\n• month * 31, where month is a number of months from 1-23. Examples: 93 (3 months * 31), 341 (11 months * 31), 589 (19 months * 31)\n\n• Amazon Aurora (CloudWatch Logs exports are managed by the DB cluster.)\n\n• RDS for Db2 - diag.log | notify.log | iam-db-auth-error\n• RDS for MariaDB - audit | error | general | slowquery | iam-db-auth-error\n• RDS for Microsoft SQL Server - agent | error\n• RDS for MySQL - audit | error | general | slowquery | iam-db-auth-error\n• RDS for Oracle - alert | audit | listener | trace | oemagent\n• RDS for PostgreSQL - postgresql | upgrade | iam-db-auth-error\n\n• CreateDBInstance\n• ModifyDBInstance\n• RestoreDBInstanceFromDBSnapshot\n• RestoreDBInstanceFromS3\n• RestoreDBInstanceToPointInTime\n\n• DescribeDBInstances\n• DescribeDBSnapshots\n• DescribeValidDBInstanceModifications\n\n• You are accessing an Oracle DB instance.\n• Your Oracle DB instance class supports configuring the number of CPU cores and threads per core.\n• The current number CPU cores and threads is set to a non-default value.\n\n• Amazon Aurora (Storage is managed by the DB cluster.)\n\n• local (Dedicated Local Zone)\n• outposts (Amazon Web Services Outposts)\n• region (Amazon Web Services Region)\n\n• The profile must exist in your account.\n• The profile must have an IAM role that Amazon EC2 has permissions to assume.\n• The instance profile name and the associated IAM role name must start with the prefix AWSRDSCustom .\n\n• Canât manage the master user password with Amazon Web Services Secrets Manager if MasterUserPassword is specified.\n\n• The DB engine that you specify in the request must support the multi-tenant configuration. If you attempt to enable the multi-tenant configuration on a DB engine that doesnât support it, the request fails.\n• If you specify the multi-tenant configuration when you create your DB instance, you canât later modify this DB instance to use the single-tenant configuration.\n\n[Note] NoteBy default, this value is set to open-source-rds-extended-support , which enrolls your DB instance into Amazon RDS Extended Support. At the end of standard support, you can avoid charges for Extended Support by setting the value to open-source-rds-extended-support-disabled . In this case, creating the DB instance will fail if the DB major version is past its end of standard support date.",
        "code_examples": [
          "```\n\"string\"\"string\"...\n```",
          "```\nName=string,Value=string...\n```",
          "```\n[{\"Name\":\"string\",\"Value\":\"string\"}...]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 70,
          "content_length": 9532
        }
      },
      {
        "header": "Note",
        "content": "This setting applies only to RDS for MySQL and RDS for PostgreSQL. For Amazon Aurora DB instances, the life cycle type is managed by the DB cluster.\n\nYou can use this setting to enroll your DB instance into Amazon RDS Extended Support. With RDS Extended Support, you can run the selected major engine version on your DB instance past the end of standard support for that engine version. For more information, see Amazon RDS Extended Support with Amazon RDS in the Amazon RDS User Guide .\n\nValid Values: open-source-rds-extended-support | open-source-rds-extended-support-disabled\n\nDefault: open-source-rds-extended-support\n\n--master-user-authentication-type (string)\n\nSpecifies the authentication type for the master user. With IAM master user authentication, you can configure the master DB user with IAM database authentication when you create a DB instance.\n\nYou can specify one of the following values:\n\nThis option is only valid for RDS for PostgreSQL and Aurora PostgreSQL engines.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command. The generated JSON skeleton is not stable between versions of the AWS CLI and there are no backwards compatibility guarantees in the JSON skeleton generated.\n\n• password - Use standard database authentication with a password.\n• iam-db-auth - Use IAM database authentication for the master user.\n\n• iam-db-auth",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2202
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nTo create a DB instance\n\nThe following create-db-instance example uses the required options to launch a new DB instance.\n\nFor more information, see Creating an Amazon RDS DB Instance in the Amazon RDS User Guide.",
        "code_examples": [
          "```\nawsrdscreate-db-instance\\--db-instance-identifiertest-mysql-instance\\--db-instance-classdb.t3.micro\\--enginemysql\\--master-usernameadmin\\--master-user-passwordsecret99\\--allocated-storage20\n```",
          "```\n{\"DBInstance\":{\"DBInstanceIdentifier\":\"test-mysql-instance\",\"DBInstanceClass\":\"db.t3.micro\",\"Engine\":\"mysql\",\"DBInstanceStatus\":\"creating\",\"MasterUsername\":\"admin\",\"AllocatedStorage\":20,\"PreferredBackupWindow\":\"12:55-13:25\",\"BackupRetentionPeriod\":1,\"DBSecurityGroups\":[],\"VpcSecurityGroups\":[{\"VpcSecurityGroupId\":\"sg-12345abc\",\"Status\":\"active\"}],\"DBParameterGroups\":[{\"DBParameterGroupName\":\"default.mysql5.7\",\"ParameterApplyStatus\":\"in-sync\"}],\"DBSubnetGroup\":{\"DBSubnetGroupName\":\"default\",\"DBSubnetGroupDescription\":\"default\",\"VpcId\":\"vpc-2ff2ff2f\",\"SubnetGroupStatus\":\"Complete\",\"Subnets\":[{\"SubnetIdentifier\":\"subnet-########\",\"SubnetAvailabilityZone\":{\"Name\":\"us-west-2c\"},\"SubnetStatus\":\"Active\"},{\"SubnetIdentifier\":\"subnet-########\",\"SubnetAvailabilityZone\":{\"Name\":\"us-west-2d\"},\"SubnetStatus\":\"Active\"},{\"SubnetIdentifier\":\"subnet-########\",\"SubnetAvailabilityZone\":{\"Name\":\"us-west-2a\"},\"SubnetStatus\":\"Active\"},{\"SubnetIdentifier\":\"subnet-########\",\"SubnetAvailabilityZone\":{\"Name\":\"us-west-2b\"},\"SubnetStatus\":\"Active\"}]},\"PreferredMaintenanceWindow\":\"sun:08:07-sun:08:37\",\"PendingModifiedValues\":{\"MasterUserPassword\":\"****\"},\"MultiAZ\":false,\"EngineVersion\":\"5.7.22\",\"AutoMinorVersionUpgrade\":true,\"ReadReplicaDBInstanceIdentifiers\":[],\"LicenseModel\":\"general-public-license\",\"OptionGroupMemberships\":[{\"OptionGroupName\":\"default:mysql-5-7\",\"Status\":\"in-sync\"}],\"PubliclyAccessible\":true,\"StorageType\":\"gp2\",\"DbInstancePort\":0,\"StorageEncrypted\":false,\"DbiResourceId\":\"db-5555EXAMPLE44444444EXAMPLE\",\"CACertificateIdentifier\":\"rds-ca-2019\",\"DomainMemberships\":[],\"CopyTagsToSnapshot\":false,\"MonitoringInterval\":0,\"DBInstanceArn\":\"arn:aws:rds:us-west-2:123456789012:db:test-mysql-instance\",\"IAMDatabaseAuthenticationEnabled\":false,\"PerformanceInsightsEnabled\":false,\"DeletionProtection\":false,\"AssociatedRoles\":[]}}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 587
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "DBInstance -> (structure)\n\nContains the details of an Amazon RDS DB instance.\n\nThis data type is used as a response element in the operations CreateDBInstance , CreateDBInstanceReadReplica , DeleteDBInstance , DescribeDBInstances , ModifyDBInstance , PromoteReadReplica , RebootDBInstance , RestoreDBInstanceFromDBSnapshot , RestoreDBInstanceFromS3 , RestoreDBInstanceToPointInTime , StartDBInstance , and StopDBInstance .\n\nDBInstanceIdentifier -> (string)\n\nDBInstanceClass -> (string)\n\nDBInstanceStatus -> (string)\n\nThe current state of this database.\n\nFor information about DB instance statuses, see Viewing DB instance status in the Amazon RDS User Guide.\n\nMasterUsername -> (string)\n\nEndpoint -> (structure)\n\nThe connection endpoint for the DB instance.\n\n[Note] NoteThe endpoint might not be shown for instances with the status of creating .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 845
        }
      },
      {
        "header": "Note",
        "content": "HostedZoneId -> (string)\n\nAllocatedStorage -> (integer)\n\nInstanceCreateTime -> (timestamp)\n\nPreferredBackupWindow -> (string)\n\nBackupRetentionPeriod -> (integer)\n\nDBSecurityGroups -> (list)\n\nA list of DB security group elements containing DBSecurityGroup.Name and DBSecurityGroup.Status subelements.\n\nThis data type is used as a response element in the following actions:\n\nDBSecurityGroupName -> (string)\n\nVpcSecurityGroups -> (list)\n\nThe list of Amazon EC2 VPC security groups that the DB instance belongs to.\n\nThis data type is used as a response element for queries on VPC security group membership.\n\nVpcSecurityGroupId -> (string)\n\nThe membership status of the VPC security group.\n\nCurrently, the only valid status is active .\n\nDBParameterGroups -> (list)\n\nThe list of DB parameter groups applied to this DB instance.\n\nThe status of the DB parameter group.\n\nThis data type is used as a response element in the following actions:\n\nDBParameterGroupName -> (string)\n\nParameterApplyStatus -> (string)\n\nThe status of parameter updates. Valid values are:\n\nAvailabilityZone -> (string)\n\nDBSubnetGroup -> (structure)\n\nInformation about the subnet group associated with the DB instance, including the name, description, and subnets in the subnet group.\n\nDBSubnetGroupName -> (string)\n\nDBSubnetGroupDescription -> (string)\n\nSubnetGroupStatus -> (string)\n\nContains a list of Subnet elements. The list of subnets shown here might not reflect the current state of your VPC. For the most up-to-date information, we recommend checking your VPC configuration directly.\n\nThis data type is used as a response element for the DescribeDBSubnetGroups operation.\n\nSubnetIdentifier -> (string)\n\nSubnetAvailabilityZone -> (structure)\n\nContains Availability Zone information.\n\nThis data type is used as an element in the OrderableDBInstanceOption data type.\n\nSubnetOutpost -> (structure)\n\nIf the subnet is associated with an Outpost, this value specifies the Outpost.\n\nFor more information about RDS on Outposts, see Amazon RDS on Amazon Web Services Outposts in the Amazon RDS User Guide.\n\nSubnetStatus -> (string)\n\nDBSubnetGroupArn -> (string)\n\nSupportedNetworkTypes -> (list)\n\nThe network type of the DB subnet group.\n\nA DBSubnetGroup can support only the IPv4 protocol or the IPv4 and the IPv6 protocols (DUAL ).\n\nFor more information, see Working with a DB instance in a VPC in the Amazon RDS User Guide.\n\nPreferredMaintenanceWindow -> (string)\n\nPendingModifiedValues -> (structure)\n\nInformation about pending changes to the DB instance. This information is returned only when there are pending changes. Specific changes are identified by subelements.\n\nDBInstanceClass -> (string)\n\nAllocatedStorage -> (integer)\n\nMasterUserPassword -> (string)\n\nBackupRetentionPeriod -> (integer)\n\nMultiAZ -> (boolean)\n\nEngineVersion -> (string)\n\nLicenseModel -> (string)\n\nThe license model for the DB instance.\n\nValid values: license-included | bring-your-own-license | general-public-license\n\nStorageThroughput -> (integer)\n\nDBInstanceIdentifier -> (string)\n\nStorageType -> (string)\n\nCACertificateIdentifier -> (string)\n\nThe identifier of the CA certificate for the DB instance.\n\nFor more information, see Using SSL/TLS to encrypt a connection to a DB instance in the Amazon RDS User Guide and Using SSL/TLS to encrypt a connection to a DB cluster in the Amazon Aurora User Guide .\n\nDBSubnetGroupName -> (string)\n\nPendingCloudwatchLogsExports -> (structure)\n\nA list of the log types whose configuration is still pending. In other words, these log types are in the process of being activated or deactivated.\n\nLogTypesToEnable -> (list)\n\nLog types that are in the process of being deactivated. After they are deactivated, these log types arenât exported to CloudWatch Logs.\n\nLogTypesToDisable -> (list)\n\nLog types that are in the process of being enabled. After they are enabled, these log types are exported to CloudWatch Logs.\n\nProcessorFeatures -> (list)\n\nThe number of CPU cores and the number of threads per core for the DB instance class of the DB instance.\n\nContains the processor features of a DB instance class.\n\nTo specify the number of CPU cores, use the coreCount feature name for the Name parameter. To specify the number of threads per core, use the threadsPerCore feature name for the Name parameter.\n\nYou can set the processor features of the DB instance class for a DB instance when you call one of the following actions:\n\nYou can view the valid processor values for a particular instance class by calling the DescribeOrderableDBInstanceOptions action and specifying the instance class for the DBInstanceClass parameter.\n\nIn addition, you can use the following actions for DB instance class processor information:\n\nIf you call DescribeDBInstances , ProcessorFeature returns non-null values only if the following conditions are met:\n\nFor more information, see Configuring the processor for a DB instance class in RDS for Oracle in the Amazon RDS User Guide.\n\nAutomationMode -> (string)\n\nThe automation mode of the RDS Custom DB instance: full or all-paused . If full , the DB instance automates monitoring and instance recovery. If all-paused , the instance pauses automation for the duration set by --resume-full-automation-mode-minutes .\n\nResumeFullAutomationModeTime -> (timestamp)\n\nMultiTenant -> (boolean)\n\nIAMDatabaseAuthenticationEnabled -> (boolean)\n\nDedicatedLogVolume -> (boolean)\n\nLatestRestorableTime -> (timestamp)\n\nMultiAZ -> (boolean)\n\nEngineVersion -> (string)\n\nAutoMinorVersionUpgrade -> (boolean)\n\nIndicates whether minor version patches are applied automatically.\n\nFor more information about automatic minor version upgrades, see Automatically upgrading the minor engine version .\n\nReadReplicaSourceDBInstanceIdentifier -> (string)\n\nReadReplicaDBInstanceIdentifiers -> (list)\n\nThe identifiers of the read replicas associated with this DB instance.\n\nReadReplicaDBClusterIdentifiers -> (list)\n\nThe identifiers of Aurora DB clusters to which the RDS DB instance is replicated as a read replica. For example, when you create an Aurora read replica of an RDS for MySQL DB instance, the Aurora MySQL DB cluster for the Aurora read replica is shown. This output doesnât contain information about cross-Region Aurora read replicas.\n\n• ModifyDBInstance\n• RebootDBInstance\n• RestoreDBInstanceFromDBSnapshot\n• RestoreDBInstanceToPointInTime\n\n• CreateDBInstance\n• CreateDBInstanceReadReplica\n• DeleteDBInstance\n• ModifyDBInstance\n• RebootDBInstance\n• RestoreDBInstanceFromDBSnapshot\n\n• applying : The parameter group change is being applied to the database.\n• failed-to-apply : The parameter group is in an invalid state.\n• in-sync : The parameter group change is synchronized with the database.\n• pending-database-upgrade : The parameter group change will be applied after the DB instance is upgraded.\n• pending-reboot : The parameter group change will be applied after the DB instance reboots.\n\n• CreateDBInstance\n• ModifyDBInstance\n• RestoreDBInstanceFromDBSnapshot\n• RestoreDBInstanceFromS3\n• RestoreDBInstanceToPointInTime\n\n• DescribeDBInstances\n• DescribeDBSnapshots\n• DescribeValidDBInstanceModifications\n\n• You are accessing an Oracle DB instance.\n• Your Oracle DB instance class supports configuring the number of CPU cores and threads per core.\n• The current number CPU cores and threads is set to a non-default value.\n\n[Note] NoteCurrently, each RDS DB instance can have only one Aurora read replica.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 94,
          "content_length": 7405
        }
      },
      {
        "header": "Note",
        "content": "ReplicaMode -> (string)\n\nThe open mode of a Db2 or an Oracle read replica. The default is open-read-only . For more information, see Working with replicas for Amazon RDS for Db2 and Working with read replicas for Amazon RDS for Oracle in the Amazon RDS User Guide .\n\n[Note] NoteThis attribute is only supported in RDS for Db2, RDS for Oracle, and RDS Custom for Oracle.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 369
        }
      },
      {
        "header": "Note",
        "content": "LicenseModel -> (string)\n\nStorageThroughput -> (integer)\n\nThe storage throughput for the DB instance.\n\nThis setting applies only to the gp3 storage type.\n\nOptionGroupMemberships -> (list)\n\nThe list of option group memberships for this DB instance.\n\nProvides information on the option groups the DB instance is a member of.\n\nOptionGroupName -> (string)\n\nCharacterSetName -> (string)\n\nNcharCharacterSetName -> (string)\n\nSecondaryAvailabilityZone -> (string)\n\nPubliclyAccessible -> (boolean)\n\nIndicates whether the DB instance is publicly accessible.\n\nWhen the DB instance is publicly accessible and you connect from outside of the DB instanceâs virtual private cloud (VPC), its Domain Name System (DNS) endpoint resolves to the public IP address. When you connect from within the same VPC as the DB instance, the endpoint resolves to the private IP address. Access to the DB cluster is ultimately controlled by the security group it uses. That public access isnât permitted if the security group assigned to the DB cluster doesnât permit it.\n\nWhen the DB instance isnât publicly accessible, it is an internal DB instance with a DNS name that resolves to a private IP address.\n\nFor more information, see CreateDBInstance .\n\nStatusInfos -> (list)\n\nThe status of a read replica. If the DB instance isnât a read replica, the value is blank.\n\nProvides a list of status information for a DB instance.\n\nStatusType -> (string)\n\nStorageType -> (string)\n\nTdeCredentialArn -> (string)\n\nDbInstancePort -> (integer)\n\nDBClusterIdentifier -> (string)\n\nStorageEncrypted -> (boolean)\n\nKmsKeyId -> (string)\n\nIf StorageEncrypted is enabled, the Amazon Web Services KMS key identifier for the encrypted DB instance.\n\nThe Amazon Web Services KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key.\n\nDbiResourceId -> (string)\n\nCACertificateIdentifier -> (string)\n\nThe identifier of the CA certificate for this DB instance.\n\nFor more information, see Using SSL/TLS to encrypt a connection to a DB instance in the Amazon RDS User Guide and Using SSL/TLS to encrypt a connection to a DB cluster in the Amazon Aurora User Guide .\n\nDomainMemberships -> (list)\n\nThe Active Directory Domain membership records associated with the DB instance.\n\nAn Active Directory Domain membership record associated with the DB instance or cluster.\n\nIAMRoleName -> (string)\n\nAuthSecretArn -> (string)\n\nThe IPv4 DNS IP addresses of the primary and secondary Active Directory domain controllers.\n\nCopyTagsToSnapshot -> (boolean)\n\nIndicates whether tags are copied from the DB instance to snapshots of the DB instance.\n\nThis setting doesnât apply to Amazon Aurora DB instances. Copying tags to snapshots is managed by the DB cluster. Setting this value for an Aurora DB instance has no effect on the DB cluster setting. For more information, see DBCluster .\n\nMonitoringInterval -> (integer)\n\nEnhancedMonitoringResourceArn -> (string)\n\nMonitoringRoleArn -> (string)\n\nPromotionTier -> (integer)\n\nDBInstanceArn -> (string)\n\nTimezone -> (string)\n\nIAMDatabaseAuthenticationEnabled -> (boolean)\n\nIndicates whether mapping of Amazon Web Services Identity and Access Management (IAM) accounts to database accounts is enabled for the DB instance.\n\nFor a list of engine versions that support IAM database authentication, see IAM database authentication in the Amazon RDS User Guide and IAM database authentication in Aurora in the Amazon Aurora User Guide .\n\nDatabaseInsightsMode -> (string)\n\nThe mode of Database Insights that is enabled for the instance.\n\nPerformanceInsightsEnabled -> (boolean)\n\nPerformanceInsightsKMSKeyId -> (string)\n\nThe Amazon Web Services KMS key identifier for encryption of Performance Insights data.\n\nThe Amazon Web Services KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key.\n\nPerformanceInsightsRetentionPeriod -> (integer)\n\nThe number of days to retain Performance Insights data.\n\nEnabledCloudwatchLogsExports -> (list)\n\nA list of log types that this DB instance is configured to export to CloudWatch Logs.\n\nLog types vary by DB engine. For information about the log types for each DB engine, see Monitoring Amazon RDS log files in the Amazon RDS User Guide.\n\nProcessorFeatures -> (list)\n\nThe number of CPU cores and the number of threads per core for the DB instance class of the DB instance.\n\nContains the processor features of a DB instance class.\n\nTo specify the number of CPU cores, use the coreCount feature name for the Name parameter. To specify the number of threads per core, use the threadsPerCore feature name for the Name parameter.\n\nYou can set the processor features of the DB instance class for a DB instance when you call one of the following actions:\n\nYou can view the valid processor values for a particular instance class by calling the DescribeOrderableDBInstanceOptions action and specifying the instance class for the DBInstanceClass parameter.\n\nIn addition, you can use the following actions for DB instance class processor information:\n\nIf you call DescribeDBInstances , ProcessorFeature returns non-null values only if the following conditions are met:\n\nFor more information, see Configuring the processor for a DB instance class in RDS for Oracle in the Amazon RDS User Guide.\n\nDeletionProtection -> (boolean)\n\nAssociatedRoles -> (list)\n\nThe Amazon Web Services Identity and Access Management (IAM) roles associated with the DB instance.\n\nInformation about an Amazon Web Services Identity and Access Management (IAM) role that is associated with a DB instance.\n\nFeatureName -> (string)\n\nInformation about the state of association between the IAM role and the DB instance. The Status property returns one of the following values:\n\nListenerEndpoint -> (structure)\n\nThe listener connection endpoint for SQL Server Always On.\n\nHostedZoneId -> (string)\n\nMaxAllocatedStorage -> (integer)\n\nFor more information, see Tagging Amazon RDS resources in the Amazon RDS User Guide or Tagging Amazon Aurora and Amazon RDS resources in the Amazon Aurora User Guide .\n\nMetadata assigned to an Amazon RDS resource consisting of a key-value pair.\n\nFor more information, see Tagging Amazon RDS resources in the Amazon RDS User Guide or Tagging Amazon Aurora and Amazon RDS resources in the Amazon Aurora User Guide .\n\nAutomationMode -> (string)\n\nThe automation mode of the RDS Custom DB instance: full or all paused . If full , the DB instance automates monitoring and instance recovery. If all paused , the instance pauses automation for the duration set by --resume-full-automation-mode-minutes .\n\nResumeFullAutomationModeTime -> (timestamp)\n\nCustomerOwnedIpEnabled -> (boolean)\n\nIndicates whether a customer-owned IP address (CoIP) is enabled for an RDS on Outposts DB instance.\n\nA CoIP provides local or external connectivity to resources in your Outpost subnets through your on-premises network. For some use cases, a CoIP can provide lower latency for connections to the DB instance from outside of its virtual private cloud (VPC) on your local network.\n\nFor more information about RDS on Outposts, see Working with Amazon RDS on Amazon Web Services Outposts in the Amazon RDS User Guide .\n\nFor more information about CoIPs, see Customer-owned IP addresses in the Amazon Web Services Outposts User Guide .\n\nNetworkType -> (string)\n\nThe network type of the DB instance.\n\nThe network type is determined by the DBSubnetGroup specified for the DB instance. A DBSubnetGroup can support only the IPv4 protocol or the IPv4 and the IPv6 protocols (DUAL ).\n\nFor more information, see Working with a DB instance in a VPC in the Amazon RDS User Guide and Working with a DB instance in a VPC in the Amazon Aurora User Guide.\n\nValid Values: IPV4 | DUAL\n\nActivityStreamStatus -> (string)\n\nThe status of the database activity stream.\n\nActivityStreamKmsKeyId -> (string)\n\nActivityStreamKinesisStreamName -> (string)\n\nActivityStreamMode -> (string)\n\nThe mode of the database activity stream. Database events such as a change or access generate an activity stream event. RDS for Oracle always handles these events asynchronously.\n\nActivityStreamEngineNativeAuditFieldsIncluded -> (boolean)\n\nAwsBackupRecoveryPointArn -> (string)\n\nDBInstanceAutomatedBackupsReplications -> (list)\n\nThe list of replicated automated backups associated with the DB instance.\n\nAutomated backups of a DB instance replicated to another Amazon Web Services Region. They consist of system backups, transaction logs, and database instance properties.\n\nDBInstanceAutomatedBackupsArn -> (string)\n\nBackupTarget -> (string)\n\nAutomaticRestartTime -> (timestamp)\n\nCustomIamInstanceProfile -> (string)\n\nThe instance profile associated with the underlying Amazon EC2 instance of an RDS Custom DB instance. The instance profile must meet the following requirements:\n\nFor the list of permissions required for the IAM role, see Configure IAM and your VPC in the Amazon RDS User Guide .\n\nActivityStreamPolicyStatus -> (string)\n\nThe status of the policy state of the activity stream.\n\nCertificateDetails -> (structure)\n\nThe details of the DB instanceâs server certificate.\n\nCAIdentifier -> (string)\n\nValidTill -> (timestamp)\n\nDBSystemId -> (string)\n\nMasterUserSecret -> (structure)\n\nThe secret managed by RDS in Amazon Web Services Secrets Manager for the master user password.\n\nFor more information, see Password management with Amazon Web Services Secrets Manager in the Amazon RDS User Guide.\n\nSecretArn -> (string)\n\nSecretStatus -> (string)\n\nThe status of the secret.\n\nThe possible status values include the following:\n\nKmsKeyId -> (string)\n\nReadReplicaSourceDBClusterIdentifier -> (string)\n\nPercentProgress -> (string)\n\nMultiTenant -> (boolean)\n\nDedicatedLogVolume -> (boolean)\n\nIsStorageConfigUpgradeAvailable -> (boolean)\n\nEngineLifecycleSupport -> (string)\n\nThe lifecycle type for the DB instance.\n\nFor more information, see CreateDBInstance.\n\n• open-read-only\n\n• month * 31, where month is a number of months from 1-23. Examples: 93 (3 months * 31), 341 (11 months * 31), 589 (19 months * 31)\n\n• CreateDBInstance\n• ModifyDBInstance\n• RestoreDBInstanceFromDBSnapshot\n• RestoreDBInstanceFromS3\n• RestoreDBInstanceToPointInTime\n\n• DescribeDBInstances\n• DescribeDBSnapshots\n• DescribeValidDBInstanceModifications\n\n• You are accessing an Oracle DB instance.\n• Your Oracle DB instance class supports configuring the number of CPU cores and threads per core.\n• The current number CPU cores and threads is set to a non-default value.\n\n• ACTIVE - the IAM role ARN is associated with the DB instance and can be used to access other Amazon Web Services services on your behalf.\n• PENDING - the IAM role ARN is being associated with the DB instance.\n• INVALID - the IAM role ARN is associated with the DB instance, but the DB instance is unable to assume the IAM role in order to access other Amazon Web Services services on your behalf.\n\n• The profile must exist in your account.\n• The profile must have an IAM role that Amazon EC2 has permissions to assume.\n• The instance profile name and the associated IAM role name must start with the prefix AWSRDSCustom .\n\n• locking-policy\n• unlocking-policy\n\n• creating - The secret is being created.\n• active - The secret is available for normal use and rotation.\n• rotating - The secret is being rotated.\n• impaired - The secret can be used to access database credentials, but it canât be rotated. A secret might have this status if, for example, permissions are changed so that RDS can no longer access either the secret or the KMS key for the secret. When a secret has this status, you can correct the condition that caused the status. Alternatively, modify the DB instance to turn off automatic management of database credentials, and then modify the DB instance again to turn on automatic management of database credentials.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 136,
          "content_length": 11773
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/rds/create-db-instance.html",
    "doc_type": "aws",
    "total_sections": 12
  },
  {
    "title": "iamÂ¶",
    "summary": "DescriptionÂ¶ Identity and Access Management (IAM) is a web service for securely controlling access to Amazon Web Services services. With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which Amazon Web Services resources users and applications can access. For more information about IAM, see Identity and Access Management (IAM) and the Identity and Access Management User Guide .\n\nIdentity and Access Management (IAM) is a web service for",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Identity and Access Management (IAM) is a web service for securely controlling access to Amazon Web Services services. With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control which Amazon Web Services resources users and applications can access. For more information about IAM, see Identity and Access Management (IAM) and the Identity and Access Management User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 427
        }
      },
      {
        "header": "Available CommandsÂ¶",
        "content": "• add-client-id-to-open-id-connect-provider\n• add-role-to-instance-profile\n• add-user-to-group\n• attach-group-policy\n• attach-role-policy\n• attach-user-policy\n• change-password\n• create-access-key\n• create-account-alias\n• create-delegation-request\n• create-group\n• create-instance-profile\n• create-login-profile\n• create-open-id-connect-provider\n• create-policy\n• create-policy-version\n• create-role\n• create-saml-provider\n• create-service-linked-role\n• create-service-specific-credential\n• create-user\n• create-virtual-mfa-device\n• deactivate-mfa-device\n• delete-access-key\n• delete-account-alias\n• delete-account-password-policy\n• delete-group\n• delete-group-policy\n• delete-instance-profile\n• delete-login-profile\n• delete-open-id-connect-provider\n• delete-policy\n• delete-policy-version\n• delete-role\n• delete-role-permissions-boundary\n• delete-role-policy\n• delete-saml-provider\n• delete-server-certificate\n• delete-service-linked-role\n• delete-service-specific-credential\n• delete-signing-certificate\n• delete-ssh-public-key\n• delete-user\n• delete-user-permissions-boundary\n• delete-user-policy\n• delete-virtual-mfa-device\n• detach-group-policy\n• detach-role-policy\n• detach-user-policy\n• disable-organizations-root-credentials-management\n• disable-organizations-root-sessions\n• enable-mfa-device\n• enable-organizations-root-credentials-management\n• enable-organizations-root-sessions\n• generate-credential-report\n• generate-organizations-access-report\n• generate-service-last-accessed-details\n• get-access-key-last-used\n• get-account-authorization-details\n• get-account-password-policy\n• get-account-summary\n• get-context-keys-for-custom-policy\n• get-context-keys-for-principal-policy\n• get-credential-report\n• get-group-policy\n• get-instance-profile\n• get-login-profile\n• get-mfa-device\n• get-open-id-connect-provider\n• get-organizations-access-report\n• get-policy-version\n• get-role-policy\n• get-saml-provider\n• get-server-certificate\n• get-service-last-accessed-details\n• get-service-last-accessed-details-with-entities\n• get-service-linked-role-deletion-status\n• get-ssh-public-key\n• get-user-policy\n• list-access-keys\n• list-account-aliases\n• list-attached-group-policies\n• list-attached-role-policies\n• list-attached-user-policies\n• list-entities-for-policy\n• list-group-policies\n• list-groups\n• list-groups-for-user\n• list-instance-profile-tags\n• list-instance-profiles\n• list-instance-profiles-for-role\n• list-mfa-device-tags\n• list-mfa-devices\n• list-open-id-connect-provider-tags\n• list-open-id-connect-providers\n• list-organizations-features\n• list-policies\n• list-policies-granting-service-access\n• list-policy-tags\n• list-policy-versions\n• list-role-policies\n• list-role-tags\n• list-saml-provider-tags\n• list-saml-providers\n• list-server-certificate-tags\n• list-server-certificates\n• list-service-specific-credentials\n• list-signing-certificates\n• list-ssh-public-keys\n• list-user-policies\n• list-user-tags\n• list-virtual-mfa-devices\n• put-group-policy\n• put-role-permissions-boundary\n• put-role-policy\n• put-user-permissions-boundary\n• put-user-policy\n• remove-client-id-from-open-id-connect-provider\n• remove-role-from-instance-profile\n• remove-user-from-group\n• reset-service-specific-credential\n• resync-mfa-device\n• set-default-policy-version\n• set-security-token-service-preferences\n• simulate-custom-policy\n• simulate-principal-policy\n• tag-instance-profile\n• tag-mfa-device\n• tag-open-id-connect-provider\n• tag-saml-provider\n• tag-server-certificate\n• untag-instance-profile\n• untag-mfa-device\n• untag-open-id-connect-provider\n• untag-policy\n• untag-saml-provider\n• untag-server-certificate\n• update-access-key\n• update-account-password-policy\n• update-assume-role-policy\n• update-group\n• update-login-profile\n• update-open-id-connect-provider-thumbprint\n• update-role\n• update-role-description\n• update-saml-provider\n• update-server-certificate\n• update-service-specific-credential\n• update-signing-certificate\n• update-ssh-public-key\n• update-user\n• upload-server-certificate\n• upload-signing-certificate\n• upload-ssh-public-key",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 4058
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/iam/index.html",
    "doc_type": "aws",
    "total_sections": 2
  },
  {
    "title": "create-userÂ¶",
    "summary": "DescriptionÂ¶ Creates a new IAM user for your Amazon Web Services account. For information about quotas for the number of IAM users you can create, see IAM and STS quotas in the IAM User Guide . See also: AWS API Documentation\n\nCreates a new IAM user for your Amazon Web Services account.",
    "sections": [
      {
        "header": "DescriptionÂ¶",
        "content": "Creates a new IAM user for your Amazon Web Services account.\n\nFor information about quotas for the number of IAM users you can create, see IAM and STS quotas in the IAM User Guide .\n\nSee also: AWS API Documentation",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 214
        }
      },
      {
        "header": "OptionsÂ¶",
        "content": "The path for the user name. For more information about paths, see IAM identifiers in the IAM User Guide .\n\nThis parameter is optional. If it is not included, it defaults to a slash (/).\n\nThis parameter allows (through its regex pattern ) a string of characters consisting of either a forward slash (/) by itself or a string that must begin and end with forward slashes. In addition, it can contain any ASCII character from the ! (\\u0021 ) through the DEL character (\\u007F ), including most punctuation characters, digits, and upper and lowercased letters.\n\n--user-name (string) [required]\n\nThe name of the user to create.\n\nIAM user, group, role, and policy names must be unique within the account. Names are not distinguished by case. For example, you cannot create resources named both âMyResourceâ and âmyresourceâ.\n\n--permissions-boundary (string)\n\nThe ARN of the managed policy that is used to set the permissions boundary for the user.\n\nA permissions boundary policy defines the maximum permissions that identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. To learn more, see Permissions boundaries for IAM entities in the IAM User Guide .\n\nFor more information about policy types, see Policy types in the IAM User Guide .\n\nA list of tags that you want to attach to the new user. Each tag consists of a key name and an associated value. For more information about tagging, see Tagging IAM resources in the IAM User Guide .\n\n• pattern: (\\u002F)|(\\u002F[\\u0021-\\u007E]+\\u002F)\n\n• pattern: [\\w+=,.@-]+\n\n[Note] NoteIf any one of the tags is invalid or if you exceed the allowed maximum number of tags, then the entire request fails and the resource is not created.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 1815
        }
      },
      {
        "header": "Note",
        "content": "A structure that represents user-provided metadata that can be associated with an IAM resource. For more information about tagging, see Tagging IAM resources in the IAM User Guide .\n\nKey -> (string) [required]\n\nThe key name that can be used to look up or retrieve the associated value. For example, Department or Cost Center are common choices.\n\nValue -> (string) [required]\n\nThe value associated with this tag. For example, tags with a key name of Department could have values such as Human Resources , Accounting , and Support . Tags with a key name of Cost Center might have values that consist of the number associated with the different cost centers in your company. Typically, many resources have tags with the same key name but with different values.\n\n--cli-input-json | --cli-input-yaml (string) Reads arguments from the JSON string provided. The JSON string follows the format provided by --generate-cli-skeleton. If other arguments are provided on the command line, those values will override the JSON-provided values. It is not possible to pass arbitrary binary values using a JSON-provided value as the string will be taken literally. This may not be specified along with --cli-input-yaml.\n\n--generate-cli-skeleton (string) Prints a JSON skeleton to standard output without sending an API request. If provided with no value or the value input, prints a sample input JSON that can be used as an argument for --cli-input-json. Similarly, if provided yaml-input it will print a sample input YAML that can be used with --cli-input-yaml. If provided with the value output, it validates the command inputs and returns a sample output JSON for that command. The generated JSON skeleton is not stable between versions of the AWS CLI and there are no backwards compatibility guarantees in the JSON skeleton generated.\n\n• pattern: [\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]+\n\n• pattern: [\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]*",
        "code_examples": [
          "```\nKey=string,Value=string...\n```",
          "```\n[{\"Key\":\"string\",\"Value\":\"string\"}...]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1900
        }
      },
      {
        "header": "Global OptionsÂ¶",
        "content": "Turn on debug logging.\n\n--endpoint-url (string)\n\nOverride commandâs default URL with the given URL.\n\n--no-verify-ssl (boolean)\n\nBy default, the AWS CLI uses SSL when communicating with AWS services. For each SSL connection, the AWS CLI will verify SSL certificates. This option overrides the default behavior of verifying SSL certificates.\n\n--no-paginate (boolean)\n\nDisable automatic pagination. If automatic pagination is disabled, the AWS CLI will only make one call, for the first page of results.\n\nThe formatting style for command output.\n\nA JMESPath query to use in filtering the response data.\n\nUse a specific profile from your credential file.\n\nThe region to use. Overrides config/env settings.\n\nDisplay the version of this tool.\n\nTurn on/off color output.\n\n--no-sign-request (boolean)\n\nDo not sign requests. Credentials will not be loaded if this argument is provided.\n\n--ca-bundle (string)\n\nThe CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.\n\n--cli-read-timeout (int)\n\nThe maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-connect-timeout (int)\n\nThe maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.\n\n--cli-binary-format (string)\n\nThe formatting style to be used for binary blobs. The default format is base64. The base64 format expects binary blobs to be provided as a base64 encoded string. The raw-in-base64-out format preserves compatibility with AWS CLI V1 behavior and binary values must be passed literally. When providing contents from a file that map to a binary blob fileb:// will always be treated as binary and use the file contents directly regardless of the cli-binary-format setting. When using file:// the file contents will need to properly formatted for the configured cli-binary-format.\n\n--no-cli-pager (boolean)\n\nDisable cli pager for output.\n\n--cli-auto-prompt (boolean)\n\nAutomatically prompt for CLI input parameters.\n\n--no-cli-auto-prompt (boolean)\n\nDisable automatically prompt for CLI input parameters.\n\n• yaml-stream\n\n• raw-in-base64-out",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 29,
          "content_length": 2225
        }
      },
      {
        "header": "ExamplesÂ¶",
        "content": "[Note] NoteTo use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information. Unless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 383
        }
      },
      {
        "header": "Note",
        "content": "To use the following examples, you must have the AWS CLI installed and configured. See the Getting started guide in the AWS CLI User Guide for more information.\n\nUnless otherwise stated, all examples have unix-like quotation rules. These examples will need to be adapted to your terminalâs quoting rules. See Using quotation marks with strings in the AWS CLI User Guide .\n\nExample 1: To create an IAM user\n\nThe following create-user command creates an IAM user named Bob in the current account.\n\nFor more information, see Creating an IAM user in your AWS account in the AWS IAM User Guide.\n\nExample 2: To create an IAM user at a specified path\n\nThe following create-user command creates an IAM user named Bob at the specified path.\n\nFor more information, see IAM identifiers in the AWS IAM User Guide.\n\nExample 3: To Create an IAM User with tags\n\nThe following create-user command creates an IAM user named Bob with tags. This example uses the --tags parameter flag with the following JSON-formatted tags: '{\"Key\": \"Department\", \"Value\": \"Accounting\"}' '{\"Key\": \"Location\", \"Value\": \"Seattle\"}'. Alternatively, the --tags flag can be used with tags in the shorthand format: 'Key=Department,Value=Accounting Key=Location,Value=Seattle'.\n\nFor more information, see Tagging IAM users in the AWS IAM User Guide.\n\nExample 3: To create an IAM user with a set permissions boundary\n\nThe following create-user command creates an IAM user named Bob with the permissions boundary of AmazonS3FullAccess.\n\nFor more information, see Permissions boundaries for IAM entities in the AWS IAM User Guide.",
        "code_examples": [
          "```\nawsiamcreate-user\\--user-nameBob\n```",
          "```\n{\"User\":{\"UserName\":\"Bob\",\"Path\":\"/\",\"CreateDate\":\"2023-06-08T03:20:41.270Z\",\"UserId\":\"AIDAIOSFODNN7EXAMPLE\",\"Arn\":\"arn:aws:iam::123456789012:user/Bob\"}}\n```",
          "```\nawsiamcreate-user\\--user-nameBob\\--path/division_abc/subdivision_xyz/\n```",
          "```\n{\"User\":{\"Path\":\"/division_abc/subdivision_xyz/\",\"UserName\":\"Bob\",\"UserId\":\"AIDAIOSFODNN7EXAMPLE\",\"Arn\":\"arn:aws:iam::12345678012:user/division_abc/subdivision_xyz/Bob\",\"CreateDate\":\"2023-05-24T18:20:17+00:00\"}}\n```",
          "```\nawsiamcreate-user\\--user-nameBob\\--tags'{\"Key\": \"Department\", \"Value\": \"Accounting\"}''{\"Key\": \"Location\", \"Value\": \"Seattle\"}'\n```",
          "```\n{\"User\":{\"Path\":\"/\",\"UserName\":\"Bob\",\"UserId\":\"AIDAIOSFODNN7EXAMPLE\",\"Arn\":\"arn:aws:iam::12345678012:user/Bob\",\"CreateDate\":\"2023-05-25T17:14:21+00:00\",\"Tags\":[{\"Key\":\"Department\",\"Value\":\"Accounting\"},{\"Key\":\"Location\",\"Value\":\"Seattle\"}]}}\n```",
          "```\nawsiamcreate-user\\--user-nameBob\\--permissions-boundaryarn:aws:iam::aws:policy/AmazonS3FullAccess\n```",
          "```\n{\"User\":{\"Path\":\"/\",\"UserName\":\"Bob\",\"UserId\":\"AIDAIOSFODNN7EXAMPLE\",\"Arn\":\"arn:aws:iam::12345678012:user/Bob\",\"CreateDate\":\"2023-05-24T17:50:53+00:00\",\"PermissionsBoundary\":{\"PermissionsBoundaryType\":\"Policy\",\"PermissionsBoundaryArn\":\"arn:aws:iam::aws:policy/AmazonS3FullAccess\"}}}\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 1587
        }
      },
      {
        "header": "OutputÂ¶",
        "content": "A structure with details about the new IAM user.\n\nPath -> (string) [required]\n\nThe path to the user. For more information about paths, see IAM identifiers in the IAM User Guide .\n\nThe ARN of the policy used to set the permissions boundary for the user.\n\nUserName -> (string) [required]\n\nThe friendly name identifying the user.\n\nUserId -> (string) [required]\n\nThe stable and unique string identifying the user. For more information about IDs, see IAM identifiers in the IAM User Guide .\n\nArn -> (string) [required]\n\nThe Amazon Resource Name (ARN) that identifies the user. For more information about ARNs and how to use ARNs in policies, see IAM Identifiers in the IAM User Guide .\n\nCreateDate -> (timestamp) [required]\n\nPasswordLastUsed -> (timestamp)\n\nThe date and time, in ISO 8601 date-time format , when the userâs password was last used to sign in to an Amazon Web Services website. For a list of Amazon Web Services websites that capture a userâs last sign-in time, see the Credential reports topic in the IAM User Guide . If a password is used more than once in a five-minute span, only the first use is returned in this field. If the field is null (no value), then it indicates that they never signed in with a password. This can be because:\n\nA null value does not mean that the user never had a password. Also, if the user does not currently have a password but had one in the past, then this field contains the date and time the most recent password was used.\n\nThis value is returned only in the GetUser and ListUsers operations.\n\nPermissionsBoundary -> (structure)\n\nFor more information about permissions boundaries, see Permissions boundaries for IAM identities in the IAM User Guide .\n\nPermissionsBoundaryType -> (string)\n\nThe permissions boundary usage type that indicates what type of IAM resource is used as the permissions boundary for an entity. This data type can only have a value of Policy .\n\nPermissionsBoundaryArn -> (string)\n\nThe ARN of the policy used to set the permissions boundary for the user or role.\n\nA list of tags that are associated with the user. For more information about tagging, see Tagging IAM resources in the IAM User Guide .\n\nA structure that represents user-provided metadata that can be associated with an IAM resource. For more information about tagging, see Tagging IAM resources in the IAM User Guide .\n\nKey -> (string) [required]\n\nThe key name that can be used to look up or retrieve the associated value. For example, Department or Cost Center are common choices.\n\nValue -> (string) [required]\n\nThe value associated with this tag. For example, tags with a key name of Department could have values such as Human Resources , Accounting , and Support . Tags with a key name of Cost Center might have values that consist of the number associated with the different cost centers in your company. Typically, many resources have tags with the same key name but with different values.\n\n• pattern: (\\u002F)|(\\u002F[\\u0021-\\u007E]+\\u002F)\n\n• pattern: [\\w+=,.@-]+\n\n• pattern: [\\w]+\n\n• The user never had a password.\n• A password exists but has not been used since IAM started tracking this information on October 20, 2014.\n\n• PermissionsBoundaryPolicy\n\n• pattern: [\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]+\n\n• pattern: [\\p{L}\\p{Z}\\p{N}_.:/=+\\-@]*",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 27,
          "content_length": 3274
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/cli/latest/reference/iam/create-user.html",
    "doc_type": "aws",
    "total_sections": 7
  },
  {
    "title": "Boto3 documentationÂ¶",
    "summary": "You use the AWS SDK for Python (Boto3) to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services.\n\nNoteDocumentation and developers tend to refer to the AWS SDK for Python as âBoto3,â and this documentation often does so as well.",
    "sections": [
      {
        "header": "",
        "content": "You use the AWS SDK for Python (Boto3) to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services.\n\n[Note] NoteDocumentation and developers tend to refer to the AWS SDK for Python as âBoto3,â and this documentation often does so as well.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 410
        }
      },
      {
        "header": "Note",
        "content": "Documentation and developers tend to refer to the AWS SDK for Python as âBoto3,â and this documentation often does so as well.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 130
        }
      },
      {
        "header": "QuickstartÂ¶",
        "content": "• Quickstart Installation Configuration Using Boto3\n• A Sample Tutorial SQS Creating a queue Using an existing queue Sending messages Processing messages\n• Code Examples Amazon CloudWatch examples Amazon DynamoDB Amazon EC2 examples AWS Identity and Access Management examples AWS Key Management Service (AWS KMS) examples Amazon S3 examples AWS Secrets Manager Amazon SES examples Amazon SQS examples\n\n• Installation\n• Configuration\n• Using Boto3\n\n• Creating a queue\n• Using an existing queue\n• Sending messages\n• Processing messages\n\n• Amazon CloudWatch examples\n• Amazon DynamoDB\n• Amazon EC2 examples\n• AWS Identity and Access Management examples\n• AWS Key Management Service (AWS KMS) examples\n• Amazon S3 examples\n• AWS Secrets Manager\n• Amazon SES examples\n• Amazon SQS examples",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 785
        }
      },
      {
        "header": "User guidesÂ¶",
        "content": "• Developer Guide SDK features Configuration Credentials Low-level clients Resources Session Collections Paginators Error handling Retries Extensibility guide Tools Cloud9 Migrations Whatâs new Migrating from Boto 2.x Migrating to Python 3 Upgrading notes\n\n• SDK features Configuration Credentials Low-level clients Resources Session Collections Paginators Error handling Retries Extensibility guide\n• Tools Cloud9\n• Migrations Whatâs new Migrating from Boto 2.x Migrating to Python 3 Upgrading notes\n\n• Configuration\n• Credentials\n• Low-level clients\n• Collections\n• Error handling\n• Extensibility guide\n\n• Whatâs new\n• Migrating from Boto 2.x\n• Migrating to Python 3\n• Upgrading notes",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 692
        }
      },
      {
        "header": "SecurityÂ¶",
        "content": "• Security Data protection Identity and access management Compliance validation Resilience Infrastructure security Enforcing TLS 1.2 Enforcing TLS 1.3\n\n• Data protection\n• Identity and access management\n• Compliance validation\n• Infrastructure security\n• Enforcing TLS 1.2\n• Enforcing TLS 1.3",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 292
        }
      },
      {
        "header": "ServicesÂ¶",
        "content": "• Available Services AccessAnalyzer Client Paginators Account Client Paginators ACM Client Paginators Waiters ACMPCA Client Paginators Waiters AIOps Client Paginators PrometheusService Client Paginators Waiters Amplify Client Paginators AmplifyBackend Client Paginators AmplifyUIBuilder Client Paginators APIGateway Client Paginators ApiGatewayManagementApi Client ApiGatewayV2 Client Paginators AppConfig Client Paginators Waiters AppConfigData Client AppFabric Client Paginators Appflow Client AppIntegrationsService Client Paginators ApplicationAutoScaling Client Paginators ApplicationInsights Client CloudWatchApplicationSignals Client Paginators ApplicationCostProfiler Client Paginators AppMesh Client Paginators AppRunner Client AppStream Client Paginators Waiters AppSync Client Paginators ARCRegionswitch Client Paginators Waiters ARCZonalShift Client Paginators Artifact Client Paginators Athena Client Paginators AuditManager Client AutoScaling Client Paginators AutoScalingPlans Client Paginators B2BI Client Paginators Waiters Backup Client Paginators BackupGateway Client Paginators BackupSearch Client Paginators Batch Client Paginators BillingandCostManagementDashboards Client Paginators BillingandCostManagementDataExports Client Paginators BillingandCostManagementPricingCalculator Client Paginators BillingandCostManagementRecommendedActions Client Paginators Bedrock Client Paginators AgentsforBedrock Client Paginators AgentsforBedrockRuntime Client Paginators BedrockAgentCore Client Paginators BedrockAgentCoreControl Client Paginators Waiters DataAutomationforBedrock Client Paginators RuntimeforBedrockDataAutomation Client BedrockRuntime Client Paginators Billing Client Paginators BillingConductor Client Paginators Braket Client Paginators Budgets Client Paginators CostExplorer Client Paginators Chatbot Client Paginators Chime Client Paginators ChimeSDKIdentity Client ChimeSDKMediaPipelines Client ChimeSDKMeetings Client ChimeSDKMessaging Client ChimeSDKVoice Client Paginators CleanRoomsService Client Paginators CleanRoomsML Client Paginators Cloud9 Client Paginators CloudControlApi Client Paginators Waiters CloudDirectory Client Paginators CloudFormation Client Paginators Waiters Resources CloudFront Client Paginators Waiters Examples CloudFrontKeyValueStore Client Paginators CloudHSM Client Paginators CloudHSMV2 Client Paginators CloudSearch Client CloudSearchDomain Client CloudTrail Client Paginators CloudTrailDataService Client CloudWatch Client Paginators Waiters Resources CodeArtifact Client Paginators CodeBuild Client Paginators CodeCatalyst Client Paginators CodeCommit Client Paginators CodeConnections Client CodeDeploy Client Paginators Waiters CodeGuruReviewer Client Paginators Waiters CodeGuruSecurity Client Paginators CodeGuruProfiler Client Paginators CodePipeline Client Paginators CodeStarconnections Client CodeStarNotifications Client Paginators CognitoIdentity Client Paginators CognitoIdentityProvider Client Paginators CognitoSync Client Comprehend Client Paginators ComprehendMedical Client ComputeOptimizer Client Paginators ConfigService Client Paginators Connect Client Paginators ConnectContactLens Client ConnectCampaignService Client Paginators ConnectCampaignServiceV2 Client Paginators ConnectCases Client Paginators ConnectParticipant Client ControlCatalog Client Paginators ControlTower Client Paginators CostOptimizationHub Client Paginators CostandUsageReportService Client Paginators CustomerProfiles Client Paginators GlueDataBrew Client Paginators DataExchange Client Paginators DataPipeline Client Paginators DataSync Client Paginators DataZone Client Paginators DAX Client Paginators DeadlineCloud Client Paginators Waiters Detective Client DeviceFarm Client Paginators DevOpsGuru Client Paginators DirectConnect Client Paginators ApplicationDiscoveryService Client Paginators DLM Client DatabaseMigrationService Client Paginators Waiters DocDB Client Paginators Waiters DocDBElastic Client Paginators drs Client Paginators DirectoryService Client Paginators Waiters DirectoryServiceData Client Paginators AuroraDSQL Client Paginators Waiters DynamoDB Client Paginators Waiters Resources DynamoDBStreams Client EBS Client EC2 Client Paginators Waiters Resources EC2InstanceConnect Client ECR Client Paginators Waiters ECRPublic Client Paginators ECS Client Paginators Waiters EFS Client Paginators EKS Client Paginators Waiters EKSAuth Client ElastiCache Client Paginators Waiters ElasticBeanstalk Client Paginators Waiters ElasticTranscoder Client Paginators Waiters ElasticLoadBalancing Client Paginators Waiters ElasticLoadBalancingv2 Client Paginators Waiters EMR Client Paginators Waiters EMRContainers Client Paginators EMRServerless Client Paginators EntityResolution Client Paginators ElasticsearchService Client Paginators EventBridge Client Paginators CloudWatchEvidently Client Paginators EVS Client Paginators finspace Client Paginators FinSpaceData Client Paginators Firehose Client FIS Client Paginators FMS Client Paginators ForecastService Client Paginators ForecastQueryService Client FraudDetector Client FreeTier Client Paginators FSx Client Paginators GameLift Client Paginators GameLiftStreams Client Paginators Waiters LocationServiceMapsV2 Client LocationServicePlacesV2 Client LocationServiceRoutesV2 Client Glacier Client Paginators Waiters Resources GlobalAccelerator Client Paginators Glue Client Paginators ManagedGrafana Client Paginators Greengrass Client Paginators GreengrassV2 Client Paginators GroundStation Client Paginators Waiters GuardDuty Client Paginators Health Client Paginators HealthLake Client Waiters IAM Client Paginators Waiters Resources IdentityStore Client Paginators imagebuilder Client Paginators ImportExport Client Paginators Inspector Client Paginators inspectorscan Client Inspector2 Client Paginators CloudWatchInternetMonitor Client Paginators Invoicing Client Paginators IoT Client Paginators IoTDataPlane Client Paginators IoTJobsDataPlane Client ManagedintegrationsforIoTDeviceManagement Client Paginators IoTAnalytics Client Paginators IoTDeviceAdvisor Client IoTEvents Client IoTEventsData Client IoTFleetWise Client Paginators IoTSecureTunneling Client IoTSiteWise Client Paginators Waiters IoTThingsGraph Client Paginators IoTTwinMaker Client IoTWireless Client IVS Client Paginators ivsrealtime Client Paginators ivschat Client Kafka Client Paginators KafkaConnect Client Paginators kendra Client KendraRanking Client Keyspaces Client Paginators KeyspacesStreams Client Paginators Kinesis Client Paginators Waiters KinesisVideoArchivedMedia Client Paginators KinesisVideoMedia Client KinesisVideoSignalingChannels Client KinesisVideoWebRTCStorage Client KinesisAnalytics Client KinesisAnalyticsV2 Client Paginators KinesisVideo Client Paginators KMS Client Paginators LakeFormation Client Paginators Lambda Client Paginators Waiters LaunchWizard Client Paginators LexModelBuildingService Client Paginators LexRuntimeService Client LexModelsV2 Client Waiters LexRuntimeV2 Client LicenseManager Client Paginators LicenseManagerLinuxSubscriptions Client Paginators LicenseManagerUserSubscriptions Client Paginators Lightsail Client Paginators LocationService Client Paginators CloudWatchLogs Client Paginators LookoutEquipment Client MainframeModernization Client Paginators MachineLearning Client Paginators Waiters Macie2 Client Paginators Waiters MailManager Client Paginators ManagedBlockchain Client Paginators ManagedBlockchainQuery Client Paginators AgreementService Client MarketplaceCatalog Client Paginators MarketplaceDeploymentService Client MarketplaceEntitlementService Client Paginators MarketplaceReportingService Client MarketplaceCommerceAnalytics Client MediaConnect Client Paginators Waiters MediaConvert Client Paginators MediaLive Client Paginators Waiters MediaPackage Client Paginators MediaPackageVod Client Paginators mediapackagev2 Client Paginators Waiters MediaStore Client Paginators MediaStoreData Client Paginators MediaTailor Client Paginators HealthImaging Client Paginators MemoryDB Client Paginators MarketplaceMetering Client MigrationHub Client Paginators mgn Client Paginators MigrationHubRefactorSpaces Client Paginators MigrationHubConfig Client MigrationHubOrchestrator Client Paginators MigrationHubStrategyRecommendations Client Paginators MultipartyApproval Client Paginators MQ Client Paginators MTurk Client Paginators MWAA Client Paginators Neptune Client Paginators Waiters NeptuneGraph Client Paginators Waiters NeptuneData Client NetworkFirewall Client Paginators NetworkFlowMonitor Client Paginators NetworkManager Client Paginators CloudWatchNetworkMonitor Client Paginators UserNotifications Client Paginators UserNotificationsContacts Client Paginators CloudWatchObservabilityAccessManager Client Paginators CloudWatchObservabilityAdminService Client Paginators odb Client Paginators Omics Client Paginators Waiters OpenSearchService Client Paginators OpenSearchServiceServerless Client Organizations Client Paginators OpenSearchIngestion Client Paginators Outposts Client Paginators Panorama Client PartnerCentralSellingAPI Client Paginators PaymentCryptographyControlPlane Client Paginators PaymentCryptographyDataPlane Client PcaConnectorAd Client Paginators PrivateCAConnectorforSCEP Client Paginators ParallelComputingService Client Paginators Personalize Client Paginators PersonalizeEvents Client PersonalizeRuntime Client PI Client Pinpoint Client PinpointEmail Client Paginators PinpointSMSVoice Client PinpointSMSVoiceV2 Client Paginators EventBridgePipes Client Paginators Polly Client Paginators Pricing Client Paginators Proton Client Paginators Waiters QApps Client Paginators QBusiness Client Paginators QConnect Client Paginators QuickSight Client Paginators RAM Client Paginators RecycleBin Client Paginators RDS Client Paginators Waiters RDSDataService Client Redshift Client Paginators Waiters RedshiftDataAPIService Client Paginators RedshiftServerless Client Paginators Rekognition Client Paginators Waiters rePostPrivate Client Paginators Waiters ResilienceHub Client Paginators ResourceExplorer Client Paginators ResourceGroups Client Paginators ResourceGroupsTaggingAPI Client Paginators IAMRolesAnywhere Client Paginators Route53 Client Paginators Waiters Route53RecoveryCluster Client Paginators Route53RecoveryControlConfig Client Paginators Waiters Route53RecoveryReadiness Client Paginators Route53Domains Client Paginators Route53Profiles Client Paginators Route53Resolver Client Paginators RTBFabric Client Paginators Waiters CloudWatchRUM Client Paginators S3 Client Paginators Waiters Resources Examples Client Context Parameters S3Control Client Paginators S3Outposts Client Paginators S3Tables Client Paginators S3Vectors Client Paginators SageMaker Client Paginators Waiters AugmentedAIRuntime Client Paginators SagemakerEdgeManager Client SageMakerFeatureStoreRuntime Client SageMakergeospatialcapabilities Client Paginators SageMakerMetrics Client SageMakerRuntime Client SavingsPlans Client EventBridgeScheduler Client Paginators Schemas Client Paginators Waiters SimpleDB Client Paginators SecretsManager Client Paginators SecurityIncidentResponse Client Paginators SecurityHub Client Paginators SecurityLake Client Paginators ServerlessApplicationRepository Client Paginators ServiceQuotas Client Paginators ServiceCatalog Client Paginators AppRegistry Client Paginators ServiceDiscovery Client Paginators SES Client Paginators Waiters SESV2 Client Paginators Shield Client Paginators signer Client Paginators Waiters SimSpaceWeaver Client SnowDeviceManagement Client Paginators Snowball Client Paginators SNS Client Paginators Resources EndUserMessagingSocial Client Paginators SQS Client Paginators Resources SSM Client Paginators Waiters SSMContacts Client Paginators SSMGUIConnect Client SSMIncidents Client Paginators Waiters SystemsManagerQuickSetup Client Paginators SsmSap Client Paginators SSO Client Paginators SSOAdmin Client Paginators SSOOIDC Client SFN Client Paginators StorageGateway Client Paginators STS Client SupplyChain Client Paginators Support Client Paginators SupportApp Client SWF Client Paginators Synthetics Client TaxSettings Client Paginators Textract Client Paginators TimestreamInfluxDB Client Paginators TimestreamQuery Client Paginators TimestreamWrite Client TelcoNetworkBuilder Client Paginators TranscribeService Client Transfer Client Paginators Waiters Translate Client Paginators TrustedAdvisorPublicAPI Client Paginators VerifiedPermissions Client Paginators VoiceID Client Paginators VPCLattice Client Paginators WAF Client Paginators WAFRegional Client WAFV2 Client WellArchitected Client ConnectWisdomService Client Paginators WorkDocs Client Paginators WorkMail Client Paginators WorkMailMessageFlow Client WorkSpaces Client Paginators WorkspacesInstances Client Paginators WorkSpacesThinClient Client Paginators WorkSpacesWeb Client Paginators XRay Client Paginators\n\n• AccessAnalyzer Client Paginators\n• Account Client Paginators\n• ACM Client Paginators Waiters\n• ACMPCA Client Paginators Waiters\n• AIOps Client Paginators\n• PrometheusService Client Paginators Waiters\n• Amplify Client Paginators\n• AmplifyBackend Client Paginators\n• AmplifyUIBuilder Client Paginators\n• APIGateway Client Paginators\n• ApiGatewayManagementApi Client\n• ApiGatewayV2 Client Paginators\n• AppConfig Client Paginators Waiters\n• AppConfigData Client\n• AppFabric Client Paginators\n• Appflow Client\n• AppIntegrationsService Client Paginators\n• ApplicationAutoScaling Client Paginators\n• ApplicationInsights Client\n• CloudWatchApplicationSignals Client Paginators\n• ApplicationCostProfiler Client Paginators\n• AppMesh Client Paginators\n• AppRunner Client\n• AppStream Client Paginators Waiters\n• AppSync Client Paginators\n• ARCRegionswitch Client Paginators Waiters\n• ARCZonalShift Client Paginators\n• Artifact Client Paginators\n• Athena Client Paginators\n• AuditManager Client\n• AutoScaling Client Paginators\n• AutoScalingPlans Client Paginators\n• B2BI Client Paginators Waiters\n• Backup Client Paginators\n• BackupGateway Client Paginators\n• BackupSearch Client Paginators\n• Batch Client Paginators\n• BillingandCostManagementDashboards Client Paginators\n• BillingandCostManagementDataExports Client Paginators\n• BillingandCostManagementPricingCalculator Client Paginators\n• BillingandCostManagementRecommendedActions Client Paginators\n• Bedrock Client Paginators\n• AgentsforBedrock Client Paginators\n• AgentsforBedrockRuntime Client Paginators\n• BedrockAgentCore Client Paginators\n• BedrockAgentCoreControl Client Paginators Waiters\n• DataAutomationforBedrock Client Paginators\n• RuntimeforBedrockDataAutomation Client\n• BedrockRuntime Client Paginators\n• Billing Client Paginators\n• BillingConductor Client Paginators\n• Braket Client Paginators\n• Budgets Client Paginators\n• CostExplorer Client Paginators\n• Chatbot Client Paginators\n• Chime Client Paginators\n• ChimeSDKIdentity Client\n• ChimeSDKMediaPipelines Client\n• ChimeSDKMeetings Client\n• ChimeSDKMessaging Client\n• ChimeSDKVoice Client Paginators\n• CleanRoomsService Client Paginators\n• CleanRoomsML Client Paginators\n• Cloud9 Client Paginators\n• CloudControlApi Client Paginators Waiters\n• CloudDirectory Client Paginators\n• CloudFormation Client Paginators Waiters Resources\n• CloudFront Client Paginators Waiters Examples\n• CloudFrontKeyValueStore Client Paginators\n• CloudHSM Client Paginators\n• CloudHSMV2 Client Paginators\n• CloudSearch Client\n• CloudSearchDomain Client\n• CloudTrail Client Paginators\n• CloudTrailDataService Client\n• CloudWatch Client Paginators Waiters Resources\n• CodeArtifact Client Paginators\n• CodeBuild Client Paginators\n• CodeCatalyst Client Paginators\n• CodeCommit Client Paginators\n• CodeConnections Client\n• CodeDeploy Client Paginators Waiters\n• CodeGuruReviewer Client Paginators Waiters\n• CodeGuruSecurity Client Paginators\n• CodeGuruProfiler Client Paginators\n• CodePipeline Client Paginators\n• CodeStarconnections Client\n• CodeStarNotifications Client Paginators\n• CognitoIdentity Client Paginators\n• CognitoIdentityProvider Client Paginators\n• CognitoSync Client\n• Comprehend Client Paginators\n• ComprehendMedical Client\n• ComputeOptimizer Client Paginators\n• ConfigService Client Paginators\n• Connect Client Paginators\n• ConnectContactLens Client\n• ConnectCampaignService Client Paginators\n• ConnectCampaignServiceV2 Client Paginators\n• ConnectCases Client Paginators\n• ConnectParticipant Client\n• ControlCatalog Client Paginators\n• ControlTower Client Paginators\n• CostOptimizationHub Client Paginators\n• CostandUsageReportService Client Paginators\n• CustomerProfiles Client Paginators\n• GlueDataBrew Client Paginators\n• DataExchange Client Paginators\n• DataPipeline Client Paginators\n• DataSync Client Paginators\n• DataZone Client Paginators\n• DAX Client Paginators\n• DeadlineCloud Client Paginators Waiters\n• Detective Client\n• DeviceFarm Client Paginators\n• DevOpsGuru Client Paginators\n• DirectConnect Client Paginators\n• ApplicationDiscoveryService Client Paginators\n• DatabaseMigrationService Client Paginators Waiters\n• DocDB Client Paginators Waiters\n• DocDBElastic Client Paginators\n• drs Client Paginators\n• DirectoryService Client Paginators Waiters\n• DirectoryServiceData Client Paginators\n• AuroraDSQL Client Paginators Waiters\n• DynamoDB Client Paginators Waiters Resources\n• DynamoDBStreams Client\n• EC2 Client Paginators Waiters Resources\n• EC2InstanceConnect Client\n• ECR Client Paginators Waiters\n• ECRPublic Client Paginators\n• ECS Client Paginators Waiters\n• EFS Client Paginators\n• EKS Client Paginators Waiters\n• EKSAuth Client\n• ElastiCache Client Paginators Waiters\n• ElasticBeanstalk Client Paginators Waiters\n• ElasticTranscoder Client Paginators Waiters\n• ElasticLoadBalancing Client Paginators Waiters\n• ElasticLoadBalancingv2 Client Paginators Waiters\n• EMR Client Paginators Waiters\n• EMRContainers Client Paginators\n• EMRServerless Client Paginators\n• EntityResolution Client Paginators\n• ElasticsearchService Client Paginators\n• EventBridge Client Paginators\n• CloudWatchEvidently Client Paginators\n• EVS Client Paginators\n• finspace Client Paginators\n• FinSpaceData Client Paginators\n• Firehose Client\n• FIS Client Paginators\n• FMS Client Paginators\n• ForecastService Client Paginators\n• ForecastQueryService Client\n• FraudDetector Client\n• FreeTier Client Paginators\n• FSx Client Paginators\n• GameLift Client Paginators\n• GameLiftStreams Client Paginators Waiters\n• LocationServiceMapsV2 Client\n• LocationServicePlacesV2 Client\n• LocationServiceRoutesV2 Client\n• Glacier Client Paginators Waiters Resources\n• GlobalAccelerator Client Paginators\n• Glue Client Paginators\n• ManagedGrafana Client Paginators\n• Greengrass Client Paginators\n• GreengrassV2 Client Paginators\n• GroundStation Client Paginators Waiters\n• GuardDuty Client Paginators\n• Health Client Paginators\n• HealthLake Client Waiters\n• IAM Client Paginators Waiters Resources\n• IdentityStore Client Paginators\n• imagebuilder Client Paginators\n• ImportExport Client Paginators\n• Inspector Client Paginators\n• inspectorscan Client\n• Inspector2 Client Paginators\n• CloudWatchInternetMonitor Client Paginators\n• Invoicing Client Paginators\n• IoT Client Paginators\n• IoTDataPlane Client Paginators\n• IoTJobsDataPlane Client\n• ManagedintegrationsforIoTDeviceManagement Client Paginators\n• IoTAnalytics Client Paginators\n• IoTDeviceAdvisor Client\n• IoTEvents Client\n• IoTEventsData Client\n• IoTFleetWise Client Paginators\n• IoTSecureTunneling Client\n• IoTSiteWise Client Paginators Waiters\n• IoTThingsGraph Client Paginators\n• IoTTwinMaker Client\n• IoTWireless Client\n• IVS Client Paginators\n• ivsrealtime Client Paginators\n• ivschat Client\n• Kafka Client Paginators\n• KafkaConnect Client Paginators\n• kendra Client\n• KendraRanking Client\n• Keyspaces Client Paginators\n• KeyspacesStreams Client Paginators\n• Kinesis Client Paginators Waiters\n• KinesisVideoArchivedMedia Client Paginators\n• KinesisVideoMedia Client\n• KinesisVideoSignalingChannels Client\n• KinesisVideoWebRTCStorage Client\n• KinesisAnalytics Client\n• KinesisAnalyticsV2 Client Paginators\n• KinesisVideo Client Paginators\n• KMS Client Paginators\n• LakeFormation Client Paginators\n• Lambda Client Paginators Waiters\n• LaunchWizard Client Paginators\n• LexModelBuildingService Client Paginators\n• LexRuntimeService Client\n• LexModelsV2 Client Waiters\n• LexRuntimeV2 Client\n• LicenseManager Client Paginators\n• LicenseManagerLinuxSubscriptions Client Paginators\n• LicenseManagerUserSubscriptions Client Paginators\n• Lightsail Client Paginators\n• LocationService Client Paginators\n• CloudWatchLogs Client Paginators\n• LookoutEquipment Client\n• MainframeModernization Client Paginators\n• MachineLearning Client Paginators Waiters\n• Macie2 Client Paginators Waiters\n• MailManager Client Paginators\n• ManagedBlockchain Client Paginators\n• ManagedBlockchainQuery Client Paginators\n• AgreementService Client\n• MarketplaceCatalog Client Paginators\n• MarketplaceDeploymentService Client\n• MarketplaceEntitlementService Client Paginators\n• MarketplaceReportingService Client\n• MarketplaceCommerceAnalytics Client\n• MediaConnect Client Paginators Waiters\n• MediaConvert Client Paginators\n• MediaLive Client Paginators Waiters\n• MediaPackage Client Paginators\n• MediaPackageVod Client Paginators\n• mediapackagev2 Client Paginators Waiters\n• MediaStore Client Paginators\n• MediaStoreData Client Paginators\n• MediaTailor Client Paginators\n• HealthImaging Client Paginators\n• MemoryDB Client Paginators\n• MarketplaceMetering Client\n• MigrationHub Client Paginators\n• mgn Client Paginators\n• MigrationHubRefactorSpaces Client Paginators\n• MigrationHubConfig Client\n• MigrationHubOrchestrator Client Paginators\n• MigrationHubStrategyRecommendations Client Paginators\n• MultipartyApproval Client Paginators\n• MQ Client Paginators\n• MTurk Client Paginators\n• MWAA Client Paginators\n• Neptune Client Paginators Waiters\n• NeptuneGraph Client Paginators Waiters\n• NeptuneData Client\n• NetworkFirewall Client Paginators\n• NetworkFlowMonitor Client Paginators\n• NetworkManager Client Paginators\n• CloudWatchNetworkMonitor Client Paginators\n• UserNotifications Client Paginators\n• UserNotificationsContacts Client Paginators\n• CloudWatchObservabilityAccessManager Client Paginators\n• CloudWatchObservabilityAdminService Client Paginators\n• odb Client Paginators\n• Omics Client Paginators Waiters\n• OpenSearchService Client Paginators\n• OpenSearchServiceServerless Client\n• Organizations Client Paginators\n• OpenSearchIngestion Client Paginators\n• Outposts Client Paginators\n• Panorama Client\n• PartnerCentralSellingAPI Client Paginators\n• PaymentCryptographyControlPlane Client Paginators\n• PaymentCryptographyDataPlane Client\n• PcaConnectorAd Client Paginators\n• PrivateCAConnectorforSCEP Client Paginators\n• ParallelComputingService Client Paginators\n• Personalize Client Paginators\n• PersonalizeEvents Client\n• PersonalizeRuntime Client\n• Pinpoint Client\n• PinpointEmail Client Paginators\n• PinpointSMSVoice Client\n• PinpointSMSVoiceV2 Client Paginators\n• EventBridgePipes Client Paginators\n• Polly Client Paginators\n• Pricing Client Paginators\n• Proton Client Paginators Waiters\n• QApps Client Paginators\n• QBusiness Client Paginators\n• QConnect Client Paginators\n• QuickSight Client Paginators\n• RAM Client Paginators\n• RecycleBin Client Paginators\n• RDS Client Paginators Waiters\n• RDSDataService Client\n• Redshift Client Paginators Waiters\n• RedshiftDataAPIService Client Paginators\n• RedshiftServerless Client Paginators\n• Rekognition Client Paginators Waiters\n• rePostPrivate Client Paginators Waiters\n• ResilienceHub Client Paginators\n• ResourceExplorer Client Paginators\n• ResourceGroups Client Paginators\n• ResourceGroupsTaggingAPI Client Paginators\n• IAMRolesAnywhere Client Paginators\n• Route53 Client Paginators Waiters\n• Route53RecoveryCluster Client Paginators\n• Route53RecoveryControlConfig Client Paginators Waiters\n• Route53RecoveryReadiness Client Paginators\n• Route53Domains Client Paginators\n• Route53Profiles Client Paginators\n• Route53Resolver Client Paginators\n• RTBFabric Client Paginators Waiters\n• CloudWatchRUM Client Paginators\n• S3 Client Paginators Waiters Resources Examples Client Context Parameters\n• S3Control Client Paginators\n• S3Outposts Client Paginators\n• S3Tables Client Paginators\n• S3Vectors Client Paginators\n• SageMaker Client Paginators Waiters\n• AugmentedAIRuntime Client Paginators\n• SagemakerEdgeManager Client\n• SageMakerFeatureStoreRuntime Client\n• SageMakergeospatialcapabilities Client Paginators\n• SageMakerMetrics Client\n• SageMakerRuntime Client\n• SavingsPlans Client\n• EventBridgeScheduler Client Paginators\n• Schemas Client Paginators Waiters\n• SimpleDB Client Paginators\n• SecretsManager Client Paginators\n• SecurityIncidentResponse Client Paginators\n• SecurityHub Client Paginators\n• SecurityLake Client Paginators\n• ServerlessApplicationRepository Client Paginators\n• ServiceQuotas Client Paginators\n• ServiceCatalog Client Paginators\n• AppRegistry Client Paginators\n• ServiceDiscovery Client Paginators\n• SES Client Paginators Waiters\n• SESV2 Client Paginators\n• Shield Client Paginators\n• signer Client Paginators Waiters\n• SimSpaceWeaver Client\n• SnowDeviceManagement Client Paginators\n• Snowball Client Paginators\n• SNS Client Paginators Resources\n• EndUserMessagingSocial Client Paginators\n• SQS Client Paginators Resources\n• SSM Client Paginators Waiters\n• SSMContacts Client Paginators\n• SSMGUIConnect Client\n• SSMIncidents Client Paginators Waiters\n• SystemsManagerQuickSetup Client Paginators\n• SsmSap Client Paginators\n• SSO Client Paginators\n• SSOAdmin Client Paginators\n• SSOOIDC Client\n• SFN Client Paginators\n• StorageGateway Client Paginators\n• SupplyChain Client Paginators\n• Support Client Paginators\n• SupportApp Client\n• SWF Client Paginators\n• Synthetics Client\n• TaxSettings Client Paginators\n• Textract Client Paginators\n• TimestreamInfluxDB Client Paginators\n• TimestreamQuery Client Paginators\n• TimestreamWrite Client\n• TelcoNetworkBuilder Client Paginators\n• TranscribeService Client\n• Transfer Client Paginators Waiters\n• Translate Client Paginators\n• TrustedAdvisorPublicAPI Client Paginators\n• VerifiedPermissions Client Paginators\n• VoiceID Client Paginators\n• VPCLattice Client Paginators\n• WAF Client Paginators\n• WAFRegional Client\n• WAFV2 Client\n• WellArchitected Client\n• ConnectWisdomService Client Paginators\n• WorkDocs Client Paginators\n• WorkMail Client Paginators\n• WorkMailMessageFlow Client\n• WorkSpaces Client Paginators\n• WorkspacesInstances Client Paginators\n• WorkSpacesThinClient Client Paginators\n• WorkSpacesWeb Client Paginators\n• XRay Client Paginators\n\n• Client Context Parameters",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 26691
        }
      },
      {
        "header": "CoreÂ¶",
        "content": "• Core References Boto3 reference client() resource() set_stream_logger() setup_default_session() Collections reference CollectionFactory CollectionManager ResourceCollection Resources reference Resource model Request parameters Response handlers Resource actions Resource base Resource factory Session reference Session\n\n• Boto3 reference client() resource() set_stream_logger() setup_default_session()\n• Collections reference CollectionFactory CollectionManager ResourceCollection\n• Resources reference Resource model Request parameters Response handlers Resource actions Resource base Resource factory\n• Session reference Session\n\n• set_stream_logger()\n• setup_default_session()\n\n• CollectionFactory\n• CollectionManager\n• ResourceCollection\n\n• Resource model\n• Request parameters\n• Response handlers\n• Resource actions\n• Resource base\n• Resource factory",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 856
        }
      },
      {
        "header": "CustomizationsÂ¶",
        "content": "• Customization References DynamoDB customization reference Valid DynamoDB types Custom Boto3 types DynamoDB conditions S3 customization reference S3 transfers\n\n• DynamoDB customization reference Valid DynamoDB types Custom Boto3 types DynamoDB conditions\n• S3 customization reference S3 transfers\n\n• Valid DynamoDB types\n• Custom Boto3 types\n• DynamoDB conditions\n\n• S3 transfers\n\n• Module Index\n• Search Page",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 410
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/index.html",
    "doc_type": "aws",
    "total_sections": 8
  },
  {
    "title": "QuickstartÂ¶",
    "summary": "This guide details the steps needed to install or update the AWS SDK for Python.\n\nThe SDK is composed of two key Python packages: Botocore (the library providing the low-level functionality shared between the Python SDK and the AWS CLI) and Boto3 (the package implementing the Python SDK itself).",
    "sections": [
      {
        "header": "",
        "content": "This guide details the steps needed to install or update the AWS SDK for Python.\n\nThe SDK is composed of two key Python packages: Botocore (the library providing the low-level functionality shared between the Python SDK and the AWS CLI) and Boto3 (the package implementing the Python SDK itself).\n\n[Note] NoteDocumentation and developers tend to refer to the AWS SDK for Python as âBoto3,â and this documentation often does so as well.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 439
        }
      },
      {
        "header": "Note",
        "content": "Documentation and developers tend to refer to the AWS SDK for Python as âBoto3,â and this documentation often does so as well.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 130
        }
      },
      {
        "header": "InstallationÂ¶",
        "content": "To use Boto3, you first need to install it and its dependencies.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 64
        }
      },
      {
        "header": "Install or update PythonÂ¶",
        "content": "Before installing Boto3, ensure youâre using Python 3.10 or later. For details on how to update your project from older versions of Python, see Migrating to Python 3.\n\nFor information about how to get the latest version of Python, see the official Python documentation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 271
        }
      },
      {
        "header": "Setup a virtual environmentÂ¶",
        "content": "Once you have a supported version of Python installed, you should set up your workspace by creating a virtual environment and activate it:\n\nThis provides an isolated space for your installation that will avoid unexpected interactions with packages installed at the system level. Skipping this step may result in unexpected dependency conflicts or failures with other tools installed on your system.",
        "code_examples": [],
        "usage_examples": [
          "```\n$ python -m venv .venv\n...\n$ source .venv/bin/activate\n```"
        ],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 398
        }
      },
      {
        "header": "Install Boto3Â¶",
        "content": "Install the latest Boto3 release via pip:\n\nIf your project requires a specific version of Boto3, or has compatibility concerns with certain versions, you may provide constraints when installing:\n\n[Note] NoteThe latest development version of Boto3 is on GitHub.",
        "code_examples": [
          "```\npipinstallboto3\n```",
          "```\n# Install Boto3 version 1.0 specificallypipinstallboto3==1.0.0# Make sure Boto3 is no older than version 1.15.0pipinstallboto3>=1.15.0# Avoid versions of Boto3 newer than version 1.15.3pipinstallboto3<=1.15.3\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 260
        }
      },
      {
        "header": "Note",
        "content": "The latest development version of Boto3 is on GitHub.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 53
        }
      },
      {
        "header": "Using the AWS Common Runtime (CRT)Â¶",
        "content": "In addition to the default install of Boto3, you can choose to include the new AWS Common Runtime (CRT). The AWS CRT is a collection of modular packages that serve as a new foundation for AWS SDKs. Each library provides better performance and minimal footprint for the functional area it implements. Using the CRT, SDKs can share the same base code when possible, improving consistency and throughput optimizations across AWS SDKs.\n\nWhen the AWS CRT is included, Boto3 uses it to incorporate features not otherwise available in the AWS SDK for Python.\n\nYouâll find it used in features like:\n\nAmazon S3 Multi-Region Access Points\n\nAmazon S3 Object Integrity\n\nAmazon EventBridge Global Endpoints\n\nHowever, Boto3 doesnât use the AWS CRT by default but you can opt into using it by specifying the crt extra feature when installing Boto3:\n\nTo revert to the non-CRT version of Boto3, use this command:\n\nIf you need to re-enable CRT, reinstall boto3[crt] to ensure you get a compatible version of awscrt:\n\n• Amazon S3 Multi-Region Access Points\n• Amazon S3 Object Integrity\n• Amazon EventBridge Global Endpoints",
        "code_examples": [
          "```\npipinstallboto3[crt]\n```",
          "```\npipuninstallawscrt\n```",
          "```\npipinstallboto3[crt]\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1108
        }
      },
      {
        "header": "ConfigurationÂ¶",
        "content": "Before using Boto3, you need to set up authentication credentials for your AWS account using either the IAM Console or the AWS CLI. You can either choose an existing user or create a new one.\n\nFor instructions about how to create a user using the IAM Console, see Creating IAM users. Once the user has been created, see Managing access keys to learn how to create and retrieve the keys used to authenticate the user.\n\nIf you have the AWS CLI installed, then you can use the aws configure command to configure your credentials file:\n\nAlternatively, you can create the credentials file yourself. By default, its location is ~/.aws/credentials. At a minimum, the credentials file should specify the access key and secret access key. In this example, the key and secret key for the account are specified in the default profile:\n\nYou may also want to add a default region to the AWS configuration file, which is located by default at ~/.aws/config:\n\nAlternatively, you can pass a region_name when creating clients and resources.\n\nYou have now configured credentials for the default profile as well as a default region to use when creating connections. See Configuration for in-depth configuration sources and options.",
        "code_examples": [
          "```\nawsconfigure\n```",
          "```\n[default]aws_access_key_id=YOUR_ACCESS_KEYaws_secret_access_key=YOUR_SECRET_KEY\n```",
          "```\n[default]region=us-east-1\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1212
        }
      },
      {
        "header": "Using Boto3Â¶",
        "content": "To use Boto3, you must first import it and indicate which service or services youâre going to use:\n\nNow that you have an s3 resource, you can make send requests to the service. The following code uses the buckets collection to print out all bucket names:\n\nYou can also upload and download binary data. For example, the following uploads a new file to S3, assuming that the bucket amzn-s3-demo-bucket already exists:\n\nResources and Collections are covered in more detail in the following sections.",
        "code_examples": [
          "```\nimportboto3# Let's use Amazon S3s3=boto3.resource('s3')\n```",
          "```\n# Print out bucket namesforbucketins3.buckets.all():print(bucket.name)\n```",
          "```\n# Upload a new filewithopen('test.jpg','rb')asdata:s3.Bucket('amzn-s3-demo-bucket').put_object(Key='test.jpg',Body=data)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 498
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html",
    "doc_type": "aws",
    "total_sections": 10
  },
  {
    "title": "ConfigurationÂ¶",
    "summary": "Boto3 looks at various configuration locations until it finds configuration values. Boto3 adheres to the following lookup order when searching through sources for configuration values:\n\nA Config object thatâs created and passed as the config parameter when creating a client",
    "sections": [
      {
        "header": "OverviewÂ¶",
        "content": "Boto3 looks at various configuration locations until it finds configuration values. Boto3 adheres to the following lookup order when searching through sources for configuration values:\n\nA Config object thatâs created and passed as the config parameter when creating a client\n\nEnvironment variables\n\nThe ~/.aws/config file\n\n• A Config object thatâs created and passed as the config parameter when creating a client\n• Environment variables\n• The ~/.aws/config file\n\n[Note] NoteConfigurations are not wholly atomic. This means configuration values set in your AWS config file can be singularly overwritten by setting a specific environment variable or through the use of a Config object.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 688
        }
      },
      {
        "header": "Note",
        "content": "Configurations are not wholly atomic. This means configuration values set in your AWS config file can be singularly overwritten by setting a specific environment variable or through the use of a Config object.\n\nFor details about credential configuration, see the Credentials guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 281
        }
      },
      {
        "header": "Using the Config objectÂ¶",
        "content": "This option is for configuring client-specific configurations that affect the behavior of your specific client object only. As described earlier, there are options used here that will supersede those found in other configuration locations:\n\nregion_name (string) - The AWS Region used in instantiating the client. If used, this takes precedence over environment variable and configuration file values. But it doesnât overwrite a region_name value explicitly passed to individual service methods.\n\nsignature_version (string) - The signature version used when signing requests. Note that the default version is Signature Version 4. If youâre using a presigned URL with an expiry of greater than 7 days, you should specify Signature Version 2.\n\ns3 (related configurations; dictionary) - Amazon S3 service-specific configurations. For more information, see the Botocore config reference.\n\nproxies (dictionary) - Each entry maps a protocol name to the proxy server Boto3 should use to communicate using that protocol. See Specifying proxy servers for more information.\n\nproxies_config (dictionary) - Additional proxy configuration settings. For more information, see Configuring proxies.\n\nretries (dictionary) - Client retry behavior configuration options that include retry mode and maximum retry attempts. For more information, see the Retries guide.\n\nFor more information about additional options, or for a complete list of options, see the Config reference.\n\nTo set these configuration options, create a Config object with the options you want, and then pass them into your client.\n\n• region_name (string) - The AWS Region used in instantiating the client. If used, this takes precedence over environment variable and configuration file values. But it doesnât overwrite a region_name value explicitly passed to individual service methods.\n• signature_version (string) - The signature version used when signing requests. Note that the default version is Signature Version 4. If youâre using a presigned URL with an expiry of greater than 7 days, you should specify Signature Version 2.\n• s3 (related configurations; dictionary) - Amazon S3 service-specific configurations. For more information, see the Botocore config reference.\n• proxies (dictionary) - Each entry maps a protocol name to the proxy server Boto3 should use to communicate using that protocol. See Specifying proxy servers for more information.\n• proxies_config (dictionary) - Additional proxy configuration settings. For more information, see Configuring proxies.\n• retries (dictionary) - Client retry behavior configuration options that include retry mode and maximum retry attempts. For more information, see the Retries guide.",
        "code_examples": [
          "```\nimportboto3frombotocore.configimportConfigmy_config=Config(region_name='us-west-2',signature_version='v4',retries={'max_attempts':10,'mode':'standard'})client=boto3.client('kinesis',config=my_config)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 2701
        }
      },
      {
        "header": "Using proxiesÂ¶",
        "content": "With Boto3, you can use proxies as intermediaries between your code and AWS. Proxies can provide functions such as filtering, security, firewalls, and privacy assurance.\n\nYou can specify proxy servers to be used for connections when using specific protocols. The proxies option in the Config object is a dictionary in which each entry maps a protocol to the address and port number of the proxy server for that protocol.\n\nIn the following example, a proxy list is set up to use proxy.amazon.com, port 6502 as the proxy for all HTTP requests by default. HTTPS requests use port 2010 on proxy.amazon.org instead.\n\nAlternatively, you can use the HTTP_PROXY and HTTPS_PROXY environment variables to specify proxy servers. Proxy servers specified using the proxies option in the Config object will override proxy servers specified using environment variables.\n\nYou can configure how Boto3 uses proxies by specifying the proxies_config option, which is a dictionary that specifies the values of several proxy options by name. There are three keys in this dictionary: proxy_ca_bundle, proxy_client_cert, and proxy_use_forwarding_for_https. For more information about these keys, see the Botocore config reference.\n\nWith the addition of the proxies_config option shown here, the proxy will use the specified certificate file for authentication when using the HTTPS proxy.",
        "code_examples": [
          "```\nimportboto3frombotocore.configimportConfigproxy_definitions={'http':'http://proxy.amazon.com:6502','https':'https://proxy.amazon.org:2010'}my_config=Config(region_name='us-east-2',signature_version='v4',proxies=proxy_definitions)client=boto3.client('kinesis',config=my_config)\n```",
          "```\nimportboto3frombotocore.configimportConfigproxy_definitions={'http':'http://proxy.amazon.com:6502','https':'https://proxy.amazon.org:2010'}my_config=Config(region_name='us-east-2',signature_version='v4',proxies=proxy_definitions,proxies_config={'proxy_client_cert':'/path/of/certificate'})client=boto3.client('kinesis',config=my_config)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1363
        }
      },
      {
        "header": "Using client context parametersÂ¶",
        "content": "Some services have configuration settings that are specific to their clients. These settings are called client context parameters. Please refer to the Client Context Parameters section of a service clientâs documentation for a list of available parameters and information on how to use them.\n\nYou can configure client context parameters by passing a dictionary of key-value pairs to the client_context_params parameter in your Config. Invalid parameter values or parameters that are not modeled by the service will be ignored.\n\nBoto3 does not support setting client_context_params per request. Differing configurations will require creation of a new client.",
        "code_examples": [
          "```\nimportboto3frombotocore.configimportConfigmy_config=Config(region_name='us-east-2',client_context_params={'my_great_context_param':'foo'})client=boto3.client('kinesis',config=my_config)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 659
        }
      },
      {
        "header": "Using environment variablesÂ¶",
        "content": "You can set configuration settings using system-wide environment variables. These configurations are global and will affect all clients created unless you override them with a Config object.\n\n[Note] NoteOnly the configuration settings listed below can be set using environment variables.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 287
        }
      },
      {
        "header": "Note",
        "content": "Only the configuration settings listed below can be set using environment variables.\n\nThe access key for your AWS account.\n\nThe secret key for your AWS account.\n\nThe session key for your AWS account. This is only needed when you are using temporary credentials. The AWS_SECURITY_TOKEN environment variable can also be used, but is only supported for backward-compatibility purposes. AWS_SESSION_TOKEN is supported by multiple AWS SDKs in addition to Boto3.\n\nThe default AWS Region to use, for example, us-west-1 or us-west-2.\n\nThe default profile to use, if any. If no value is specified, Boto3 attempts to search the shared credentials file and the config file for the default profile.\n\nThe location of the config file used by Boto3. By default this value is ~/.aws/config. You only need to set this variable if you want to change this location.\n\nThe location of the shared credentials file. By default this value is ~/.aws/credentials. You only need to set this variable if you want to change this location.\n\nThe location of the Boto2 credentials file. This is not set by default. You only need to set this variable if you want to use credentials stored in Boto2 format in a location other than /etc/boto.cfg or ~/.boto.\n\nThe path to a custom certificate bundle to use when establishing SSL/TLS connections. Boto3 includes a CA bundle that it uses by default, but you can set this environment variable to use a different CA bundle.\n\nThe number of seconds before a connection to the instance metadata service should time out. When attempting to retrieve credentials on an Amazon EC2 instance that is configured with an IAM role, a connection to the instance metadata service will time out after 1 second by default. If you know youâre running on an EC2 instance with an IAM role configured, you can increase this value if needed.\n\nWhen attempting to retrieve credentials on an Amazon EC2 instance that has been configured with an IAM role, Boto3 will make only one attempt to retrieve credentials from the instance metadata service before giving up. If you know your code will be running on an EC2 instance, you can increase this value to make Boto3 retry multiple times before giving up.\n\nA list of additional directories to check when loading botocore data. You typically donât need to set this value. There are two built-in search paths: <botocoreroot>/data/ and ~/.aws/models. Setting this environment variable indicates additional directories to check first before falling back to the built-in search paths. Multiple entries should be separated with the os.pathsep character, which is : on Linux and ; on Windows.\n\nSets AWS STS endpoint resolution logic. See the sts_regional_endpoints configuration file section for more information on how to use this.\n\nThe total number of attempts made for a single request. For more information, see the max_attempts configuration file section.\n\nSpecifies the types of retries the SDK will use. For more information, see the retry_mode configuration file section.\n\nAppId is an optional application specific identifier that can be set. When set it will be appended to the User-Agent header of every request in the form of App/{AppId}.\n\nA comma-delimited list of regions to sign when signing with SigV4a. For more information, see the sigv4a_signing_region_set configuration file section.\n\nDetermines when a checksum will be calculated for request payloads. For more information, see the request_checksum_calculation configuration file section.\n\nDetermines when checksum validation will be performed on response payloads. For more information, see the response_checksum_validation configuration file section.\n\n**AWS_ACCESS_KEY_ID**: The access key for your AWS account.\n**AWS_SECRET_ACCESS_KEY**: The secret key for your AWS account.\n**AWS_SESSION_TOKEN**: The session key for your AWS account. This is only needed when you are using temporary credentials. The AWS_SECURITY_TOKEN environment variable can also be used, but is only supported for backward-compatibility purposes. AWS_SESSION_TOKEN is supported by multiple AWS SDKs in addition to Boto3.\n**AWS_DEFAULT_REGION**: The default AWS Region to use, for example, us-west-1 or us-west-2.\n**AWS_PROFILE**: The default profile to use, if any. If no value is specified, Boto3 attempts to search the shared credentials file and the config file for the default profile.\n**AWS_CONFIG_FILE**: The location of the config file used by Boto3. By default this value is ~/.aws/config. You only need to set this variable if you want to change this location.\n**AWS_SHARED_CREDENTIALS_FILE**: The location of the shared credentials file. By default this value is ~/.aws/credentials. You only need to set this variable if you want to change this location.\n**BOTO_CONFIG**: The location of the Boto2 credentials file. This is not set by default. You only need to set this variable if you want to use credentials stored in Boto2 format in a location other than /etc/boto.cfg or ~/.boto.\n**AWS_CA_BUNDLE**: The path to a custom certificate bundle to use when establishing SSL/TLS connections. Boto3 includes a CA bundle that it uses by default, but you can set this environment variable to use a different CA bundle.\n**AWS_METADATA_SERVICE_TIMEOUT**: The number of seconds before a connection to the instance metadata service should time out. When attempting to retrieve credentials on an Amazon EC2 instance that is configured with an IAM role, a connection to the instance metadata service will time out after 1 second by default. If you know youâre running on an EC2 instance with an IAM role configured, you can increase this value if needed.\n**AWS_METADATA_SERVICE_NUM_ATTEMPTS**: When attempting to retrieve credentials on an Amazon EC2 instance that has been configured with an IAM role, Boto3 will make only one attempt to retrieve credentials from the instance metadata service before giving up. If you know your code will be running on an EC2 instance, you can increase this value to make Boto3 retry multiple times before giving up.\n**AWS_DATA_PATH**: A list of additional directories to check when loading botocore data. You typically donât need to set this value. There are two built-in search paths: <botocoreroot>/data/ and ~/.aws/models. Setting this environment variable indicates additional directories to check first before falling back to the built-in search paths. Multiple entries should be separated with the os.pathsep character, which is : on Linux and ; on Windows.\n**AWS_STS_REGIONAL_ENDPOINTS**: Sets AWS STS endpoint resolution logic. See the sts_regional_endpoints configuration file section for more information on how to use this.\n**AWS_MAX_ATTEMPTS**: The total number of attempts made for a single request. For more information, see the max_attempts configuration file section.\n**AWS_RETRY_MODE**: Specifies the types of retries the SDK will use. For more information, see the retry_mode configuration file section.\n**AWS_SDK_UA_APP_ID**: AppId is an optional application specific identifier that can be set. When set it will be appended to the User-Agent header of every request in the form of App/{AppId}.\n**AWS_SIGV4A_SIGNING_REGION_SET**: A comma-delimited list of regions to sign when signing with SigV4a. For more information, see the sigv4a_signing_region_set configuration file section.\n**AWS_REQUEST_CHECKSUM_CALCULATION**: Determines when a checksum will be calculated for request payloads. For more information, see the request_checksum_calculation configuration file section.\n**AWS_RESPONSE_CHECKSUM_VALIDATION**: Determines when checksum validation will be performed on response payloads. For more information, see the response_checksum_validation configuration file section.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 20,
          "content_length": 7708
        }
      },
      {
        "header": "Using a configuration fileÂ¶",
        "content": "Boto3 will also search the ~/.aws/config file when looking for configuration values. You can change the location of this file by setting the AWS_CONFIG_FILE environment variable.\n\nThis file is an INI-formatted file that contains at least one section: [default]. You can create multiple profiles (logical groups of configuration) by creating sections named [profile profile-name]. If your profile name has spaces, you need to surround this value with quotation marks: [profile \"my profile name\"]. The following are all the config variables supported in the ~/.aws/config file.\n\nSpecifies the API version to use for a particular AWS service.\n\nThe api_versions settings are nested configuration values that require special formatting in the AWS configuration file. If the values are set by the AWS CLI or programmatically by an SDK, the formatting is handled automatically. If you set them by manually editing the AWS configuration file, the following is the required format. Notice the indentation of each value.\n\nThe access key to use.\n\nThe secret access key to use.\n\nThe session token to use. This is typically needed only when using temporary credentials. Note aws_security_token is supported for backward compatibility.\n\nThe CA bundle to use. For more information, see the previous description of the AWS_CA_BUNDLE environment variable.\n\nSpecifies an external command to run to generate or retrieve authentication credentials. For more information, see Sourcing credentials with an external process.\n\nTo invoke an AWS service from an Amazon EC2 instance, you can use an IAM role attached to either an EC2 instance profile or an Amazon ECS container. In such a scenario, use the credential_source setting to specify where to find the credentials.\n\nThe credential_source and source_profile settings are mutually exclusive.\n\nThe following values are supported.\n\nUse the IAM role attached to the Amazon EC2 instance profile.\n\nUse the IAM role attached to the Amazon ECS container.\n\nRetrieve the credentials from environment variables.\n\nThe length of time in seconds of the role session. The value can range from 900 seconds (15 minutes) to the maximum session duration setting for the role. The default value is 3600 seconds (one hour).\n\nUnique identifier to pass when making AssumeRole calls.\n\nThe number of seconds before timing out when retrieving data from the instance metadata service. For more information, see the previous documentation on AWS_METADATA_SERVICE_TIMEOUT.\n\nThe number of attempts to make before giving up when retrieving data from the instance metadata service. For more information, see the previous documentation on AWS_METADATA_SERVICE_NUM_ATTEMPTS.\n\nSerial number of the Amazon Resource Name (ARN) of a multi-factor authentication (MFA) device to use when assuming a role.\n\nDisable parameter validation (default is true, parameters are validated). This is a Boolean value that is either true or false. Whenever you make an API call using a client, the parameters you provide are run through a set of validation checks, including (but not limited to) required parameters provided, type checking, no unknown parameters, minimum length checks, and so on. Typically, you should leave parameter validation enabled.\n\nThe default AWS Region to use, for example, us-west-1 or us-west-2. When specifying a Region inline during client initialization, this property is named region_name.\n\nThe ARN of the role you want to assume.\n\nThe role name to use when assuming a role. If this value is not provided, a session name will be automatically generated.\n\nThe path to a file that contains an OAuth 2.0 access token or OpenID Connect ID token that is provided by the identity provider. The contents of this file will be loaded and passed as the WebIdentityToken argument to the AssumeRoleWithWebIdentity operation.\n\nSet Amazon S3-specific configuration data. Typically, these values do not need to be set.\n\nThe s3 settings are nested configuration values that require special formatting in the AWS configuration file. If the values are set by the AWS CLI or programmatically by an SDK, the formatting is handled automatically. If you set them manually by editing the AWS configuration file, the following is the required format. Notice the indentation of each value.\n\naddressing_style: The S3 addressing style. When necessary, Boto automatically switches the addressing style to an appropriate value. The following values are supported.\n\n(Default) Attempts to use virtual, but falls back to path if necessary.\n\nBucket name is included in the URI path.\n\nBucket name is included in the hostname.\n\npayload_signing_enabled: Specifies whether to include an SHA-256 checksum with Amazon Signature Version 4 payloads. Valid settings are true or false.\n\nFor streaming uploads (UploadPart and PutObject) that use HTTPS and include a content-md5 header, this setting is disabled by default.\n\nsignature_version: The AWS signature version to use when signing requests. When necessary, Boto automatically switches the signature version to an appropriate value. The following values are recognized.\n\n(Default) Signature Version 4\n\n(Deprecated) Signature Version 2\n\nuse_accelerate_endpoint: Specifies whether to use the Amazon S3 Accelerate endpoint. The bucket must be enabled to use S3 Accelerate. Valid settings are true or false. Default: false\n\nEither use_accelerate_endpoint or use_dualstack_endpoint can be enabled, but not both.\n\nuse_dualstack_endpoint: Specifies whether to direct all Amazon S3 requests to the dual IPv4/IPv6 endpoint for the configured Region. Valid settings are true or false. Default: false\n\nEither use_accelerate_endpoint or use_dualstack_endpoint can be enabled, but not both.\n\nThe profile name that contains credentials to use for the initial AssumeRole call.\n\nThe credential_source and source_profile settings are mutually exclusive.\n\nSets AWS STS endpoint resolution logic. This configuration can also be set using the environment variable AWS_STS_REGIONAL_ENDPOINTS. By default, this configuration option is set to regional. Valid values are the following:\n\nUses the STS endpoint that corresponds to the configured Region. For example, if the client is configured to use us-west-2, all calls to STS will be made to the sts.us-west-2.amazonaws.com regional endpoint instead of the global sts.amazonaws.com endpoint.\n\nUses the global STS endpoint, sts.amazonaws.com, for the following configured Regions:\n\nAll other Regions will use their respective regional endpoint.\n\nToggles the TCP Keep-Alive socket option used when creating connections. By default this value is false; TCP Keepalive will not be used when creating connections. To enable TCP Keepalive with the system default configurations, set this value to true.\n\nAn integer representing the maximum number of attempts that will be made for a single request, including the initial attempt. For example, setting this value to 5 will result in a request being retried up to 4 times. If not provided, the number of retries will default to whatever is modeled, which is typically 5 total attempts in the legacy retry mode, and 3 in the standard and adaptive retry modes.\n\nA string representing the type of retries Boto3 will perform. Valid values are the following:\n\nlegacy - The preexisting retry behavior. This is the default value if no retry mode is provided.\n\nstandard - A standardized set of retry rules across the AWS SDKs. This includes a standard set of errors that are retried and support for retry quotas, which limit the number of unsuccessful retries an SDK can make. This mode will default the maximum number of attempts to 3 unless a max_attempts is explicitly provided.\n\nadaptive - An experimental retry mode that includes all the functionality of standard mode with automatic client-side throttling. This is a provisional mode whose behavior might change.\n\nA comma-delimited list of regions use when signing with SigV4a. If this is not set, the SDK will check if the service has modeled a default; if none is found, this will default to *.\n\nDetermines when a checksum will be calculated for request payloads. Valid values are:\n\nwhen_supported â When set, a checksum will be calculated for all request payloads of operations modeled with the httpChecksum trait where requestChecksumRequired is true or a requestAlgorithmMember is modeled.\n\nwhen_required â When set, a checksum will only be calculated for request payloads of operations modeled with the httpChecksum trait where requestChecksumRequired is true or where a requestAlgorithmMember is modeled and supplied.\n\nDetermines when checksum validation will be performed on response payloads. Valid values are:\n\nwhen_supported â When set, checksum validation is performed on all response payloads of operations modeled with the httpChecksum trait where responseAlgorithms is modeled, except when no modeled checksum algorithms are supported.\n\nwhen_required â When set, checksum validation is not performed on response payloads of operations unless the checksum algorithm is supported and the requestValidationModeMember member is set to ENABLED.\n\nWhen true, dualstack endpoint resolution is enabled. Valid values are true or false. Default : false.\n\n• addressing_style: The S3 addressing style. When necessary, Boto automatically switches the addressing style to an appropriate value. The following values are supported. auto(Default) Attempts to use virtual, but falls back to path if necessary. pathBucket name is included in the URI path. virtualBucket name is included in the hostname.\n• payload_signing_enabled: Specifies whether to include an SHA-256 checksum with Amazon Signature Version 4 payloads. Valid settings are true or false. For streaming uploads (UploadPart and PutObject) that use HTTPS and include a content-md5 header, this setting is disabled by default.\n• signature_version: The AWS signature version to use when signing requests. When necessary, Boto automatically switches the signature version to an appropriate value. The following values are recognized. s3v4(Default) Signature Version 4 s3(Deprecated) Signature Version 2\n• use_accelerate_endpoint: Specifies whether to use the Amazon S3 Accelerate endpoint. The bucket must be enabled to use S3 Accelerate. Valid settings are true or false. Default: false Either use_accelerate_endpoint or use_dualstack_endpoint can be enabled, but not both.\n• use_dualstack_endpoint: Specifies whether to direct all Amazon S3 requests to the dual IPv4/IPv6 endpoint for the configured Region. Valid settings are true or false. Default: false Either use_accelerate_endpoint or use_dualstack_endpoint can be enabled, but not both.\n\n• regionalUses the STS endpoint that corresponds to the configured Region. For example, if the client is configured to use us-west-2, all calls to STS will be made to the sts.us-west-2.amazonaws.com regional endpoint instead of the global sts.amazonaws.com endpoint.\n• legacyUses the global STS endpoint, sts.amazonaws.com, for the following configured Regions: ap-northeast-1 ap-south-1 ap-southeast-1 ap-southeast-2 aws-global ca-central-1 eu-central-1 eu-north-1 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-east-2 us-west-1 us-west-2 All other Regions will use their respective regional endpoint.\n\n• ap-northeast-1\n• ap-southeast-1\n• ap-southeast-2\n• ca-central-1\n• eu-central-1\n\n• legacy - The preexisting retry behavior. This is the default value if no retry mode is provided.\n• standard - A standardized set of retry rules across the AWS SDKs. This includes a standard set of errors that are retried and support for retry quotas, which limit the number of unsuccessful retries an SDK can make. This mode will default the maximum number of attempts to 3 unless a max_attempts is explicitly provided.\n• adaptive - An experimental retry mode that includes all the functionality of standard mode with automatic client-side throttling. This is a provisional mode whose behavior might change.\n\n• when_supported â When set, a checksum will be calculated for all request payloads of operations modeled with the httpChecksum trait where requestChecksumRequired is true or a requestAlgorithmMember is modeled.\n• when_required â When set, a checksum will only be calculated for request payloads of operations modeled with the httpChecksum trait where requestChecksumRequired is true or where a requestAlgorithmMember is modeled and supplied.\n\n• when_supported â When set, checksum validation is performed on all response payloads of operations modeled with the httpChecksum trait where responseAlgorithms is modeled, except when no modeled checksum algorithms are supported.\n• when_required â When set, checksum validation is not performed on response payloads of operations unless the checksum algorithm is supported and the requestValidationModeMember member is set to ENABLED.\n\n**api_versions**: Specifies the API version to use for a particular AWS service. The api_versions settings are nested configuration values that require special formatting in the AWS configuration file. If the values are set by the AWS CLI or programmatically by an SDK, the formatting is handled automatically. If you set them by manually editing the AWS configuration file, the following is the required format. Notice the indentation of each value. [default] region = us-east-1 api_versions = ec2 = 2015-03-01 cloudfront = 2015-09-17\n**aws_access_key_id**: The access key to use.\n**aws_secret_access_key**: The secret access key to use.\n**aws_session_token**: The session token to use. This is typically needed only when using temporary credentials. Note aws_security_token is supported for backward compatibility.\n**ca_bundle**: The CA bundle to use. For more information, see the previous description of the AWS_CA_BUNDLE environment variable.\n**credential_process**: Specifies an external command to run to generate or retrieve authentication credentials. For more information, see Sourcing credentials with an external process.\n**credential_source**: To invoke an AWS service from an Amazon EC2 instance, you can use an IAM role attached to either an EC2 instance profile or an Amazon ECS container. In such a scenario, use the credential_source setting to specify where to find the credentials. The credential_source and source_profile settings are mutually exclusive. The following values are supported. Ec2InstanceMetadataUse the IAM role attached to the Amazon EC2 instance profile. EcsContainerUse the IAM role attached to the Amazon ECS container. EnvironmentRetrieve the credentials from environment variables.\n**Ec2InstanceMetadata**: Use the IAM role attached to the Amazon EC2 instance profile.\n**EcsContainer**: Use the IAM role attached to the Amazon ECS container.\n**Environment**: Retrieve the credentials from environment variables.\n**duration_seconds**: The length of time in seconds of the role session. The value can range from 900 seconds (15 minutes) to the maximum session duration setting for the role. The default value is 3600 seconds (one hour).\n**external_id**: Unique identifier to pass when making AssumeRole calls.\n**metadata_service_timeout**: The number of seconds before timing out when retrieving data from the instance metadata service. For more information, see the previous documentation on AWS_METADATA_SERVICE_TIMEOUT.\n**metadata_service_num_attempts**: The number of attempts to make before giving up when retrieving data from the instance metadata service. For more information, see the previous documentation on AWS_METADATA_SERVICE_NUM_ATTEMPTS.\n**mfa_serial**: Serial number of the Amazon Resource Name (ARN) of a multi-factor authentication (MFA) device to use when assuming a role.\n**parameter_validation**: Disable parameter validation (default is true, parameters are validated). This is a Boolean value that is either true or false. Whenever you make an API call using a client, the parameters you provide are run through a set of validation checks, including (but not limited to) required parameters provided, type checking, no unknown parameters, minimum length checks, and so on. Typically, you should leave parameter validation enabled.\n**region**: The default AWS Region to use, for example, us-west-1 or us-west-2. When specifying a Region inline during client initialization, this property is named region_name.\n**role_arn**: The ARN of the role you want to assume.\n**role_session_name**: The role name to use when assuming a role. If this value is not provided, a session name will be automatically generated.\n**web_identity_token_file**: The path to a file that contains an OAuth 2.0 access token or OpenID Connect ID token that is provided by the identity provider. The contents of this file will be loaded and passed as the WebIdentityToken argument to the AssumeRoleWithWebIdentity operation.\n**s3**: Set Amazon S3-specific configuration data. Typically, these values do not need to be set. The s3 settings are nested configuration values that require special formatting in the AWS configuration file. If the values are set by the AWS CLI or programmatically by an SDK, the formatting is handled automatically. If you set them manually by editing the AWS configuration file, the following is the required format. Notice the indentation of each value. [default] region = us-east-1 s3 = addressing_style = path signature_version = s3v4 addressing_style: The S3 addressing style. When necessary, Boto automatically switches the addressing style to an appropriate value. The following values are supported. auto(Default) Attempts to use virtual, but falls back to path if necessary. pathBucket name is included in the URI path. virtualBucket name is included in the hostname. payload_signing_enabled: Specifies whether to include an SHA-256 checksum with Amazon Signature Version 4 payloads. Valid settings are true or false. For streaming uploads (UploadPart and PutObject) that use HTTPS and include a content-md5 header, this setting is disabled by default. signature_version: The AWS signature version to use when signing requests. When necessary, Boto automatically switches the signature version to an appropriate value. The following values are recognized. s3v4(Default) Signature Version 4 s3(Deprecated) Signature Version 2 use_accelerate_endpoint: Specifies whether to use the Amazon S3 Accelerate endpoint. The bucket must be enabled to use S3 Accelerate. Valid settings are true or false. Default: false Either use_accelerate_endpoint or use_dualstack_endpoint can be enabled, but not both. use_dualstack_endpoint: Specifies whether to direct all Amazon S3 requests to the dual IPv4/IPv6 endpoint for the configured Region. Valid settings are true or false. Default: false Either use_accelerate_endpoint or use_dualstack_endpoint can be enabled, but not both.\n**auto**: (Default) Attempts to use virtual, but falls back to path if necessary.\n**path**: Bucket name is included in the URI path.\n**virtual**: Bucket name is included in the hostname.\n**s3v4**: (Default) Signature Version 4\n**s3**: (Deprecated) Signature Version 2\n**source_profile**: The profile name that contains credentials to use for the initial AssumeRole call. The credential_source and source_profile settings are mutually exclusive.\n**sts_regional_endpoints**: Sets AWS STS endpoint resolution logic. This configuration can also be set using the environment variable AWS_STS_REGIONAL_ENDPOINTS. By default, this configuration option is set to regional. Valid values are the following: regionalUses the STS endpoint that corresponds to the configured Region. For example, if the client is configured to use us-west-2, all calls to STS will be made to the sts.us-west-2.amazonaws.com regional endpoint instead of the global sts.amazonaws.com endpoint. legacyUses the global STS endpoint, sts.amazonaws.com, for the following configured Regions: ap-northeast-1 ap-south-1 ap-southeast-1 ap-southeast-2 aws-global ca-central-1 eu-central-1 eu-north-1 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-east-2 us-west-1 us-west-2 All other Regions will use their respective regional endpoint.\n**regional**: Uses the STS endpoint that corresponds to the configured Region. For example, if the client is configured to use us-west-2, all calls to STS will be made to the sts.us-west-2.amazonaws.com regional endpoint instead of the global sts.amazonaws.com endpoint.\n**legacy**: Uses the global STS endpoint, sts.amazonaws.com, for the following configured Regions: ap-northeast-1 ap-south-1 ap-southeast-1 ap-southeast-2 aws-global ca-central-1 eu-central-1 eu-north-1 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-east-2 us-west-1 us-west-2 All other Regions will use their respective regional endpoint.\n**tcp_keepalive**: Toggles the TCP Keep-Alive socket option used when creating connections. By default this value is false; TCP Keepalive will not be used when creating connections. To enable TCP Keepalive with the system default configurations, set this value to true.\n**max_attempts**: An integer representing the maximum number of attempts that will be made for a single request, including the initial attempt. For example, setting this value to 5 will result in a request being retried up to 4 times. If not provided, the number of retries will default to whatever is modeled, which is typically 5 total attempts in the legacy retry mode, and 3 in the standard and adaptive retry modes.\n**retry_mode**: A string representing the type of retries Boto3 will perform. Valid values are the following: legacy - The preexisting retry behavior. This is the default value if no retry mode is provided. standard - A standardized set of retry rules across the AWS SDKs. This includes a standard set of errors that are retried and support for retry quotas, which limit the number of unsuccessful retries an SDK can make. This mode will default the maximum number of attempts to 3 unless a max_attempts is explicitly provided. adaptive - An experimental retry mode that includes all the functionality of standard mode with automatic client-side throttling. This is a provisional mode whose behavior might change.\n**sigv4a_signing_region_set**: A comma-delimited list of regions use when signing with SigV4a. If this is not set, the SDK will check if the service has modeled a default; if none is found, this will default to *.\n**request_checksum_calculation**: Determines when a checksum will be calculated for request payloads. Valid values are: when_supported â When set, a checksum will be calculated for all request payloads of operations modeled with the httpChecksum trait where requestChecksumRequired is true or a requestAlgorithmMember is modeled. when_required â When set, a checksum will only be calculated for request payloads of operations modeled with the httpChecksum trait where requestChecksumRequired is true or where a requestAlgorithmMember is modeled and supplied.\n**response_checksum_validation**: Determines when checksum validation will be performed on response payloads. Valid values are: when_supported â When set, checksum validation is performed on all response payloads of operations modeled with the httpChecksum trait where responseAlgorithms is modeled, except when no modeled checksum algorithms are supported. when_required â When set, checksum validation is not performed on response payloads of operations unless the checksum algorithm is supported and the requestValidationModeMember member is set to ENABLED.\n**use_dualstack_endpoint**: When true, dualstack endpoint resolution is enabled. Valid values are true or false. Default : false.\n**Ec2InstanceMetadata**: Use the IAM role attached to the Amazon EC2 instance profile.\n**EcsContainer**: Use the IAM role attached to the Amazon ECS container.\n**Environment**: Retrieve the credentials from environment variables.\n**auto**: (Default) Attempts to use virtual, but falls back to path if necessary.\n**path**: Bucket name is included in the URI path.\n**virtual**: Bucket name is included in the hostname.\n**s3v4**: (Default) Signature Version 4\n**s3**: (Deprecated) Signature Version 2\n**regional**: Uses the STS endpoint that corresponds to the configured Region. For example, if the client is configured to use us-west-2, all calls to STS will be made to the sts.us-west-2.amazonaws.com regional endpoint instead of the global sts.amazonaws.com endpoint.\n**legacy**: Uses the global STS endpoint, sts.amazonaws.com, for the following configured Regions: ap-northeast-1 ap-south-1 ap-southeast-1 ap-southeast-2 aws-global ca-central-1 eu-central-1 eu-north-1 eu-west-1 eu-west-2 eu-west-3 sa-east-1 us-east-1 us-east-2 us-west-1 us-west-2 All other Regions will use their respective regional endpoint.",
        "code_examples": [
          "```\n[default]region=us-east-1api_versions=ec2=2015-03-01cloudfront=2015-09-17\n```",
          "```\n[default]region=us-east-1s3=addressing_style=pathsignature_version=s3v4\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 60,
          "content_length": 24690
        }
      },
      {
        "header": "Using Account ID-Based EndpointsÂ¶",
        "content": "Boto3 supports account ID-based endpoints, which improve performance and scalability by using your AWS account ID to streamline request routing for services that support this feature. When Boto3 resolves credentials containing an account ID, it automatically constructs an account ID-based endpoint instead of a regional endpoint.\n\nAccount ID-based endpoints follow this format:\n\n<account-id> is the AWS account ID sourced from your credentials.\n\n<region> is the AWS region where the request is being made.\n\n• <account-id> is the AWS account ID sourced from your credentials.\n• <region> is the AWS region where the request is being made.",
        "code_examples": [
          "```\nhttps://<account-id>.myservice.<region>.amazonaws.com\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 637
        }
      },
      {
        "header": "Supported Credential ProvidersÂ¶",
        "content": "Boto3 can automatically construct account ID-based endpoints by sourcing the AWS account ID from the following places:\n\nCredentials set using the boto3.client() method\n\nCredentials set when creating a Session object\n\nEnvironment variables\n\nAssume role provider\n\nAssume role with web identity provider\n\nAWS IAM Identity Center credential provider\n\nShared credential file (~/.aws/credentials)\n\nAWS config file (~/.aws/config)\n\nContainer credential provider\n\nYou can read more about these locations in the Credentials guide.\n\n• Credentials set using the boto3.client() method\n• Credentials set when creating a Session object\n• Environment variables\n• Assume role provider\n• Assume role with web identity provider\n• AWS IAM Identity Center credential provider\n• Shared credential file (~/.aws/credentials)\n• AWS config file (~/.aws/config)\n• Container credential provider",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 867
        }
      },
      {
        "header": "Configuring Account IDÂ¶",
        "content": "You can provide an account ID along with your AWS credentials using one of the following:\n\nPassing it as a parameter when creating clients:\n\nPassing it as a parameter when creating a Session object:\n\nSetting an environment variable:\n\nSetting it in the shared credentials or config file:",
        "code_examples": [
          "```\nimportboto3client=boto3.client('dynamodb',aws_access_key_id=ACCESS_KEY,aws_secret_access_key=SECRET_KEY,aws_account_id=ACCOUNT_ID)\n```",
          "```\nimportboto3session=boto3.Session(aws_access_key_id=ACCESS_KEY,aws_secret_access_key=SECRET_KEY,aws_account_id=ACCOUNT_ID)\n```",
          "```\nexportAWS_ACCESS_KEY_ID=<ACCESS_KEY>exportAWS_SECRET_ACCESS_KEY=<SECRET_KEY>exportAWS_ACCOUNT_ID=<ACCOUNT_ID>\n```",
          "```\n[default]aws_access_key_id=fooaws_secret_access_key=baraws_account_id=baz\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 286
        }
      },
      {
        "header": "Configuring Endpoint Routing BehaviorÂ¶",
        "content": "The account ID endpoint mode is a setting that can be used to turn off account ID-based endpoint routing if necessary.\n\npreferred â The endpoint should include account ID if available.\n\ndisabled â A resolved endpoint doesnât include account ID.\n\nrequired â The endpoint must include account ID. If the account ID isnât available, the SDK throws an error.\n\n• preferred â The endpoint should include account ID if available.\n• disabled â A resolved endpoint doesnât include account ID.\n• required â The endpoint must include account ID. If the account ID isnât available, the SDK throws an error.\n\n[Note] NoteThe default behavior in Boto3 is preferred.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 670
        }
      },
      {
        "header": "Note",
        "content": "The default behavior in Boto3 is preferred.\n\nYou can configure the setting using one of the following:\n\nSetting it in the Config object when creating clients:\n\nSetting an environment variable:\n\nSetting it in the shared credentials or config file:",
        "code_examples": [
          "```\nimportboto3frombotocore.configimportConfigmy_config=Config(account_id_endpoint_mode='disabled')client=boto3.client('dynamodb',config=my_config)\n```",
          "```\nexportAWS_ACCOUNT_ID_ENDPOINT_MODE=disabled\n```",
          "```\n[default]account_id_endpoint_mode=disabled\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 246
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html",
    "doc_type": "aws",
    "total_sections": 13
  },
  {
    "title": "CredentialsÂ¶",
    "summary": "Boto3 credentials can be configured in multiple ways. Regardless of the source or sources that you choose, you must have both AWS credentials and an AWS Region set in order to make requests.\n\nIf you have the AWS CLI, then you can use its interactive configure command to set up your credentials and default region:",
    "sections": [
      {
        "header": "OverviewÂ¶",
        "content": "Boto3 credentials can be configured in multiple ways. Regardless of the source or sources that you choose, you must have both AWS credentials and an AWS Region set in order to make requests.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 190
        }
      },
      {
        "header": "Interactive configurationÂ¶",
        "content": "If you have the AWS CLI, then you can use its interactive configure command to set up your credentials and default region:\n\nFollow the prompts and it will generate configuration files in the correct locations for you.",
        "code_examples": [
          "```\nawsconfigure\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 217
        }
      },
      {
        "header": "Configuring credentialsÂ¶",
        "content": "There are two types of configuration data in Boto3: credentials and non-credentials. Credentials include items such as aws_access_key_id, aws_secret_access_key, and aws_session_token. Non-credential configuration includes items such as which region to use or which addressing style to use for Amazon S3. For more information on how to configure non-credential configurations, see the Configuration guide.\n\nBoto3 will look in several locations when searching for credentials. The mechanism in which Boto3 looks for credentials is to search through a list of possible locations and stop as soon as it finds credentials. The order in which Boto3 searches for credentials is:\n\nPassing credentials as parameters in the boto3.client() method\n\nPassing credentials as parameters when creating a Session object\n\nEnvironment variables\n\nAssume role provider\n\nAssume role with web identity provider\n\nAWS IAM Identity Center credential provider\n\nShared credential file (~/.aws/credentials)\n\nAWS config file (~/.aws/config)\n\nBoto2 config file (/etc/boto.cfg and ~/.boto)\n\nContainer credential provider\n\nInstance metadata service on an Amazon EC2 instance that has an IAM role configured.\n\nEach of those locations is discussed in more detail below.\n\n• Passing credentials as parameters in the boto3.client() method\n• Passing credentials as parameters when creating a Session object\n• Environment variables\n• Assume role provider\n• Assume role with web identity provider\n• AWS IAM Identity Center credential provider\n• Shared credential file (~/.aws/credentials)\n• AWS config file (~/.aws/config)\n• Boto2 config file (/etc/boto.cfg and ~/.boto)\n• Container credential provider\n• Instance metadata service on an Amazon EC2 instance that has an IAM role configured.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 14,
          "content_length": 1747
        }
      },
      {
        "header": "Passing credentials as parametersÂ¶",
        "content": "There are valid use cases for providing credentials to the client() method and Session object, these include:\n\nRetrieving temporary credentials using AWS STS (such as sts.get_session_token()).\n\nLoading credentials from some external location, e.g the OS keychain.\n\nThe first option for providing credentials to Boto3 is passing them as parameters when creating clients:\n\nThe second option for providing credentials to Boto3 is passing them as parameters when creating a Session object:\n\n• Retrieving temporary credentials using AWS STS (such as sts.get_session_token()).\n• Loading credentials from some external location, e.g the OS keychain.\n\n[Warning] WarningACCESS_KEY, SECRET_KEY, and SESSION_TOKEN are variables that contain your access key, secret key, and optional session token. Note that the examples above do not have hard coded credentials. We do not recommend hard coding credentials in your source code.",
        "code_examples": [
          "```\nimportboto3client=boto3.client('s3',aws_access_key_id=ACCESS_KEY,aws_secret_access_key=SECRET_KEY,aws_session_token=SESSION_TOKEN)\n```",
          "```\nimportboto3session=boto3.Session(aws_access_key_id=ACCESS_KEY,aws_secret_access_key=SECRET_KEY,aws_session_token=SESSION_TOKEN)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 916
        }
      },
      {
        "header": "Warning",
        "content": "ACCESS_KEY, SECRET_KEY, and SESSION_TOKEN are variables that contain your access key, secret key, and optional session token. Note that the examples above do not have hard coded credentials. We do not recommend hard coding credentials in your source code.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 255
        }
      },
      {
        "header": "Environment variablesÂ¶",
        "content": "Boto3 will check these environment variables for credentials:\n\nAWS_ACCESS_KEY_ID - The access key for your AWS account.\n\nAWS_SECRET_ACCESS_KEY - The secret key for your AWS account.\n\nAWS_SESSION_TOKEN - The session key for your AWS account. This is only needed when you are using temporary credentials. The AWS_SECURITY_TOKEN environment variable can also be used, but is only supported for backwards compatibility purposes. AWS_SESSION_TOKEN is supported by multiple AWS SDKs besides python.\n\n• AWS_ACCESS_KEY_ID - The access key for your AWS account.\n• AWS_SECRET_ACCESS_KEY - The secret key for your AWS account.\n• AWS_SESSION_TOKEN - The session key for your AWS account. This is only needed when you are using temporary credentials. The AWS_SECURITY_TOKEN environment variable can also be used, but is only supported for backwards compatibility purposes. AWS_SESSION_TOKEN is supported by multiple AWS SDKs besides python.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 927
        }
      },
      {
        "header": "Assume role providerÂ¶",
        "content": "[Note] NoteThis is a different set of credentials configuration than using IAM roles for EC2 instances, which is discussed in a section below.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 142
        }
      },
      {
        "header": "Note",
        "content": "This is a different set of credentials configuration than using IAM roles for EC2 instances, which is discussed in a section below.\n\nWithin the ~/.aws/config file, you can also configure a profile to indicate that Boto3 should assume a role. When you do this, Boto3 will automatically make the corresponding AssumeRole calls to AWS STS on your behalf. It will handle in-memory caching as well as refreshing credentials as needed.\n\nYou can specify the following configuration values for configuring an IAM role in Boto3. For more information about a particular setting, see the Configuration section.\n\nrole_arn - The ARN of the role you want to assume.\n\nsource_profile - The boto3 profile that contains credentials we should use for the initial AssumeRole call.\n\ncredential_source - The resource (Amazon EC2 instance profile, Amazon ECS container role, or environment variable) that contains the credentials to use for the initial AssumeRole call.\n\nexternal_id - A unique identifier that is used by third parties to assume a role in their customersâ accounts. This maps to the ExternalId parameter in the AssumeRole operation. This is an optional parameter.\n\nmfa_serial - The identification number of the MFA device to use when assuming a role. This is an optional parameter. Specify this value if the trust policy of the role being assumed includes a condition that requires MFA authentication. The value is either the serial number for a hardware device (such as GAHT12345678) or an Amazon Resource Name (ARN) for a virtual device (such as arn:aws:iam::123456789012:mfa/user).\n\nrole_session_name - The name applied to this assume-role session. This value affects the assumed role user ARN (such as arn:aws:sts::123456789012:assumed-role/role_name/role_session_name). This maps to the RoleSessionName parameter in the AssumeRole operation. This is an optional parameter. If you do not provide this value, a session name will be automatically generated.\n\nduration_seconds - The length of time in seconds of the role session.\n\nIf MFA authentication is not enabled then you only need to specify a role_arn and a source_profile.\n\nWhen you specify a profile that has an IAM role configuration, Boto3 will make an AssumeRole call to retrieve temporary credentials. Subsequent Boto3 API calls will use the cached temporary credentials until they expire, in which case Boto3 will then automatically refresh the credentials.\n\nPlease note that Boto3 does not write these temporary credentials to disk. This means that temporary credentials from the AssumeRole calls are only cached in-memory within a single session. All clients created from that session will share the same temporary credentials.\n\nIf you specify mfa_serial, then the first time an AssumeRole call is made, you will be prompted to enter the MFA code. Program execution will block until you enter the MFA code. Youâll need to keep this in mind if you have an mfa_serial device configured, but would like to use Boto3 in an automated script.\n\nBelow is an example configuration for the minimal amount of configuration needed to configure an assume role profile:\n\nSee Using IAM Roles for general information on IAM roles.\n\n• role_arn - The ARN of the role you want to assume.\n• source_profile - The boto3 profile that contains credentials we should use for the initial AssumeRole call.\n• credential_source - The resource (Amazon EC2 instance profile, Amazon ECS container role, or environment variable) that contains the credentials to use for the initial AssumeRole call.\n• external_id - A unique identifier that is used by third parties to assume a role in their customersâ accounts. This maps to the ExternalId parameter in the AssumeRole operation. This is an optional parameter.\n• mfa_serial - The identification number of the MFA device to use when assuming a role. This is an optional parameter. Specify this value if the trust policy of the role being assumed includes a condition that requires MFA authentication. The value is either the serial number for a hardware device (such as GAHT12345678) or an Amazon Resource Name (ARN) for a virtual device (such as arn:aws:iam::123456789012:mfa/user).\n• role_session_name - The name applied to this assume-role session. This value affects the assumed role user ARN (such as arn:aws:sts::123456789012:assumed-role/role_name/role_session_name). This maps to the RoleSessionName parameter in the AssumeRole operation. This is an optional parameter. If you do not provide this value, a session name will be automatically generated.\n• duration_seconds - The length of time in seconds of the role session.",
        "code_examples": [
          "```\n# In ~/.aws/credentials:[development]aws_access_key_id=fooaws_access_key_id=bar# In ~/.aws/config[profile crossaccount]role_arn=arn:aws:iam:...source_profile=development\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 16,
          "content_length": 4612
        }
      },
      {
        "header": "Assume Role With Web Identity ProviderÂ¶",
        "content": "Within the ~/.aws/config file, you can also configure a profile to indicate that Boto3 should assume a role. When you do this, Boto3 will automatically make the corresponding AssumeRoleWithWebIdentity calls to AWS STS on your behalf. It will handle in-memory caching as well as refreshing credentials, as needed.\n\nYou can specify the following configuration values for configuring an IAM role in Boto3:\n\nrole_arn - The ARN of the role you want to assume.\n\nweb_identity_token_file - The path to a file which contains an OAuth 2.0 access token or OpenID Connect ID token that is provided by the identity provider. The contents of this file will be loaded and passed as the WebIdentityToken argument to the AssumeRoleWithWebIdentity operation.\n\nrole_session_name - The name applied to this assume-role session. This value affects the assumed role user ARN (such as arn:aws:sts::123456789012:assumed-role/role_name/role_session_name). This maps to the RoleSessionName parameter in the AssumeRoleWithWebIdentity operation. This is an optional parameter. If you do not provide this value, a session name will be automatically generated.\n\nBelow is an example configuration for the minimal amount of configuration needed to configure an assume role with web identity profile:\n\nThis provider can also be configured via environment variables:\n\nAWS_ROLE_ARN - The ARN of the role you want to assume.\n\nAWS_WEB_IDENTITY_TOKEN_FILE - The path to the web identity token file.\n\nAWS_ROLE_SESSION_NAME - The name applied to this assume-role session.\n\n• role_arn - The ARN of the role you want to assume.\n• web_identity_token_file - The path to a file which contains an OAuth 2.0 access token or OpenID Connect ID token that is provided by the identity provider. The contents of this file will be loaded and passed as the WebIdentityToken argument to the AssumeRoleWithWebIdentity operation.\n• role_session_name - The name applied to this assume-role session. This value affects the assumed role user ARN (such as arn:aws:sts::123456789012:assumed-role/role_name/role_session_name). This maps to the RoleSessionName parameter in the AssumeRoleWithWebIdentity operation. This is an optional parameter. If you do not provide this value, a session name will be automatically generated.\n\n• AWS_ROLE_ARN - The ARN of the role you want to assume.\n• AWS_WEB_IDENTITY_TOKEN_FILE - The path to the web identity token file.\n• AWS_ROLE_SESSION_NAME - The name applied to this assume-role session.\n\n[Note] NoteThese environment variables currently only apply to the assume role with web identity provider and do not apply to the general assume role provider configuration.",
        "code_examples": [
          "```\n# In ~/.aws/config[profile web-identity]role_arn=arn:aws:iam:...web_identity_token_file=/path/to/a/token\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 2641
        }
      },
      {
        "header": "Note",
        "content": "These environment variables currently only apply to the assume role with web identity provider and do not apply to the general assume role provider configuration.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 162
        }
      },
      {
        "header": "AWS IAM Identity CenterÂ¶",
        "content": "Support for the AWS IAM Identity Center (successor to AWS Single Sign-On) credential provider was added in 1.14.0. The IAM Identity Center provides support for single sign-on (SSO) credentials.\n\nTo begin using the IAM Identity Center credential provider, start by using the AWS CLI (v2) to configure and manage your SSO profiles and login sessions. For detailed instructions on the configuration and login process see the AWS CLI User Guide for SSO. Once completed you will have one or many profiles in the shared configuration file with the following settings:\n\nsso_start_url - The URL that points to the organizationâs IAM Identity Center user portal.\n\nsso_region - The AWS Region that contains the IAM Identity Center portal host. This is separate from the default AWS CLI Region parameter, and can also be a different Region.\n\nsso_account_id - The AWS account ID that contains the IAM role that you want to use with this profile.\n\nsso_role_name - The name of the IAM role that defines the userâs permissions when using this profile.\n\nYou can then specify the profile name via the AWS_PROFILE environment variable or the profile_name argument when creating a Session. For example, we can create a Session using the my-sso-profile profile and any clients created from this session will use the my-sso-profile credentials:\n\n• sso_start_url - The URL that points to the organizationâs IAM Identity Center user portal.\n• sso_region - The AWS Region that contains the IAM Identity Center portal host. This is separate from the default AWS CLI Region parameter, and can also be a different Region.\n• sso_account_id - The AWS account ID that contains the IAM role that you want to use with this profile.\n• sso_role_name - The name of the IAM role that defines the userâs permissions when using this profile.",
        "code_examples": [
          "```\n# In ~/.aws/config[profile my-sso-profile]sso_start_url=https://my-sso-portal.awsapps.com/startsso_region=us-east-1sso_account_id=123456789011sso_role_name=readOnly\n```",
          "```\nimportboto3session=boto3.Session(profile_name='my-sso-profile')s3_client=session.client('s3')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1811
        }
      },
      {
        "header": "Shared credentials fileÂ¶",
        "content": "The shared credentials file has a default location of ~/.aws/credentials. You can change the location of the shared credentials file by setting the AWS_SHARED_CREDENTIALS_FILE environment variable.\n\nThis file is an INI formatted file with section names corresponding to profiles. With each section, the three configuration variables shown above can be specified: aws_access_key_id, aws_secret_access_key, aws_session_token. These are the only supported values in the shared credential file.\n\nBelow is a minimal example of the shared credentials file:\n\nThe shared credentials file also supports the concept of profiles. Profiles represent logical groups of configuration. The shared credential file can have multiple profiles:\n\nYou can then specify a profile name via the AWS_PROFILE environment variable or the profile_name argument when creating a Session. For example, we can create a Session using the âdevâ profile and any clients created from this session will use the âdevâ credentials:",
        "code_examples": [
          "```\n[default]aws_access_key_id=fooaws_secret_access_key=baraws_session_token=baz\n```",
          "```\n[default]aws_access_key_id=fooaws_secret_access_key=bar[dev]aws_access_key_id=foo2aws_secret_access_key=bar2[prod]aws_access_key_id=foo3aws_secret_access_key=bar3\n```",
          "```\nimportboto3session=boto3.Session(profile_name='dev')dev_s3_client=session.client('s3')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1000
        }
      },
      {
        "header": "AWS config fileÂ¶",
        "content": "Boto3 can also load credentials from ~/.aws/config. You can change this default location by setting the AWS_CONFIG_FILE environment variable. The config file is an INI format, with the same keys supported by the shared credentials file. The only difference is that profile sections must have the format of [profile profile-name], except for the default profile:\n\nThe reason that section names must start with profile in the ~/.aws/config file is because there are other sections in this file that are permitted that arenât profile configurations.",
        "code_examples": [
          "```\n[default]aws_access_key_id=fooaws_secret_access_key=bar[profile dev]aws_access_key_id=foo2aws_secret_access_key=bar2[profile prod]aws_access_key_id=foo3aws_secret_access_key=bar3\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 548
        }
      },
      {
        "header": "Boto2 configuration file supportÂ¶",
        "content": "Boto3 will attempt to load credentials from the Boto2 config file. It first checks the file pointed to by BOTO_CONFIG if set, otherwise it will check /etc/boto.cfg and ~/.boto. Note that only the [Credentials] section of the boto config file is used. All other configuration data in the boto config file is ignored.\n\n[Note] NoteThis credential provider is primarily for backwards compatibility purposes with Boto2.",
        "code_examples": [],
        "usage_examples": [
          "```\n# Example ~/.boto file[Credentials]aws_access_key_id=fooaws_secret_access_key=bar\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 414
        }
      },
      {
        "header": "Note",
        "content": "This credential provider is primarily for backwards compatibility purposes with Boto2.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 86
        }
      },
      {
        "header": "Container credential providerÂ¶",
        "content": "If you are using Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), you can obtain credentials by specifying an HTTP endpoint as an environment variable. The SDK will request credentials from the specified endpoint. For more information, see Container credential provider in the Amazon SDKs and Tools Reference Guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 363
        }
      },
      {
        "header": "IAM rolesÂ¶",
        "content": "If you are running on Amazon EC2 and no credentials have been found by any of the providers above, Boto3 will try to load credentials from the instance metadata service. In order to take advantage of this feature, you must have specified an IAM role to use when you launched your EC2 instance.\n\nFor more information on how to configure IAM roles on EC2 instances, see the IAM Roles for Amazon EC2 guide.\n\nNote that if youâve launched an EC2 instance with an IAM role configured, thereâs no explicit configuration you need to set in Boto3 to use these credentials. Boto3 will automatically use IAM role credentials if it does not find credentials in any of the other places listed previously.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 695
        }
      },
      {
        "header": "Best practices for configuring credentialsÂ¶",
        "content": "If youâre running on an EC2 instance, use AWS IAM roles. See the IAM Roles for Amazon EC2 guide for more information on how to set this up.\n\nIf you want to interoperate with multiple AWS SDKs (e.g Java, JavaScript, Ruby, PHP, .NET, AWS CLI, Go, C++), use the shared credentials file (~/.aws/credentials). By using the shared credentials file, you can use a single file for credentials that will work in all AWS SDKs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 418
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html",
    "doc_type": "aws",
    "total_sections": 18
  },
  {
    "title": "ResourcesÂ¶",
    "summary": "NoteThe AWS Python SDK team does not intend to add new features to the resources interface in boto3. Existing interfaces will continue to operate during boto3âs lifecycle. Customers can find access to newer service features through the client interface.\n\nThe AWS Python SDK team does not intend to add new features to the resources interface in boto3. Existing interfaces will continue to operate during boto3âs lifecycle. Customers can find access to newer service features through the client in",
    "sections": [
      {
        "header": "OverviewÂ¶",
        "content": "[Note] NoteThe AWS Python SDK team does not intend to add new features to the resources interface in boto3. Existing interfaces will continue to operate during boto3âs lifecycle. Customers can find access to newer service features through the client interface.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 262
        }
      },
      {
        "header": "Note",
        "content": "The AWS Python SDK team does not intend to add new features to the resources interface in boto3. Existing interfaces will continue to operate during boto3âs lifecycle. Customers can find access to newer service features through the client interface.\n\nResources represent an object-oriented interface to Amazon Web Services (AWS). They provide a higher-level abstraction than the raw, low-level calls made by service clients. To use resources, you invoke the resource() method of a Session and pass in a service name:\n\nEvery resource instance has a number of attributes and methods. These can conceptually be split up into identifiers, attributes, actions, references, sub-resources, and collections. Each of these is described in further detail below and in the following section.\n\nResources themselves can also be conceptually split into service resources (like sqs, s3, ec2, etc) and individual resources (like sqs.Queue or s3.Bucket). Service resources do not have identifiers or attributes. The two share the same components otherwise.",
        "code_examples": [
          "```\n# Get resources from the default sessionsqs=boto3.resource('sqs')s3=boto3.resource('s3')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1041
        }
      },
      {
        "header": "Identifiers and attributesÂ¶",
        "content": "An identifier is a unique value that is used to call actions on the resource. Resources must have at least one identifier, except for the top-level service resources (e.g. sqs or s3). An identifier is set at instance creation-time, and failing to provide all necessary identifiers during instantiation will result in an exception. Examples of identifiers:\n\nIdentifiers may also be passed as positional arguments:\n\nIdentifiers also play a role in resource instance equality. For two instances of a resource to be considered equal, their identifiers must be equal:\n\n[Note] NoteOnly identifiers are taken into account for instance equality. Region, account ID and other data members are not considered. When using temporary credentials or multiple regions in your code please keep this in mind.",
        "code_examples": [
          "```\n# SQS Queue (url is an identifier)queue=sqs.Queue(url='http://...')print(queue.url)# S3 Object (bucket_name and key are identifiers)obj=s3.Object(bucket_name='amzn-s3-demo-bucket',key='test.py')print(obj.bucket_name)print(obj.key)# Raises exception, missing identifier: key!obj=s3.Object(bucket_name='amzn-s3-demo-bucket')\n```",
          "```\n# SQS Queuequeue=sqs.Queue('http://...')# S3 Objectobj=s3.Object('boto3','test.py')# Raises exception, missing key!obj=s3.Object('boto3')\n```"
        ],
        "usage_examples": [
          "```\n>>>bucket1=s3.Bucket('amzn-s3-demo-bucket1')>>>bucket2=s3.Bucket('amzn-s3-demo-bucket1')>>>bucket3=s3.Bucket('amzn-s3-demo-bucket3')>>>bucket1==bucket2True>>>bucket1==bucket3False\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 791
        }
      },
      {
        "header": "Note",
        "content": "Only identifiers are taken into account for instance equality. Region, account ID and other data members are not considered. When using temporary credentials or multiple regions in your code please keep this in mind.\n\nResources may also have attributes, which are lazy-loaded properties on the instance. They may be set at creation time from the response of an action on another resource, or they may be set when accessed or via an explicit call to the load or reload action. Examples of attributes:\n\n[Warning] WarningAttributes may incur a load action when first accessed. If latency is a concern, then manually calling load will allow you to control exactly when the load action (and thus latency) is invoked. The documentation for each resource explicitly lists its attributes. Additionally, attributes may be reloaded after an action has been performed on the resource. For example, if the last_modified attribute of an S3 object is loaded and then a put action is called, then the next time you access last_modified it will reload the objectâs metadata.",
        "code_examples": [
          "```\n# SQS Messagemessage.body# S3 Objectobj.last_modifiedobj.e_tag\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1060
        }
      },
      {
        "header": "Warning",
        "content": "Attributes may incur a load action when first accessed. If latency is a concern, then manually calling load will allow you to control exactly when the load action (and thus latency) is invoked. The documentation for each resource explicitly lists its attributes.\n\nAdditionally, attributes may be reloaded after an action has been performed on the resource. For example, if the last_modified attribute of an S3 object is loaded and then a put action is called, then the next time you access last_modified it will reload the objectâs metadata.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 543
        }
      },
      {
        "header": "ActionsÂ¶",
        "content": "An action is a method which makes a call to the service. Actions may return a low-level response, a new resource instance or a list of new resource instances. Actions automatically set the resource identifiers as parameters, but allow you to pass additional parameters via keyword arguments. Examples of actions:\n\nExamples of sending additional parameters:\n\n[Note] NoteParameters must be passed as keyword arguments. They will not work as positional arguments.",
        "code_examples": [
          "```\n# SQS Queuemessages=queue.receive_messages()# SQS Messageformessageinmessages:message.delete()# S3 Objectobj=s3.Object(bucket_name='amzn-s3-demo-bucket',key='test.py')response=obj.get()data=response['Body'].read()\n```",
          "```\n# SQS Servicequeue=sqs.get_queue_by_name(QueueName='test')# SQS Queuequeue.send_message(MessageBody='hello')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 460
        }
      },
      {
        "header": "Note",
        "content": "Parameters must be passed as keyword arguments. They will not work as positional arguments.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 91
        }
      },
      {
        "header": "ReferencesÂ¶",
        "content": "A reference is an attribute which may be None or a related resource instance. The resource instance does not share identifiers with its reference resource, that is, it is not a strict parent to child relationship. In relational terms, these can be considered many-to-one or one-to-one. Examples of references:\n\nIn the above example, an EC2 instance may have exactly one associated subnet, and may have exactly one associated VPC. The subnet does not require the instance ID to exist, hence it is not a parent to child relationship.",
        "code_examples": [
          "```\n# EC2 Instanceinstance.subnetinstance.vpc\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 531
        }
      },
      {
        "header": "Sub-resourcesÂ¶",
        "content": "A sub-resource is similar to a reference, but is a related class rather than an instance. Sub-resources, when instantiated, share identifiers with their parent. It is a strict parent-child relationship. In relational terms, these can be considered one-to-many. Examples of sub-resources:\n\nBecause an SQS message cannot exist without a queue, and an S3 object cannot exist without a bucket, these are parent to child relationships.",
        "code_examples": [
          "```\n# SQSqueue=sqs.Queue(url='...')message=queue.Message(receipt_handle='...')print(queue.url==message.queue_url)print(message.receipt_handle)# S3obj=bucket.Object(key='new_file.txt')print(obj.bucket_name)print(obj.key)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 430
        }
      },
      {
        "header": "WaitersÂ¶",
        "content": "A waiter is similar to an action. A waiter will poll the status of a resource and suspend execution until the resource reaches the state that is being polled for or a failure occurs while polling. Waiters automatically set the resource identifiers as parameters, but allow you to pass additional parameters via keyword arguments. Examples of waiters include:",
        "code_examples": [
          "```\n# S3: Wait for a bucket to exist.bucket.wait_until_exists()# EC2: Wait for an instance to reach the running state.instance.wait_until_running()\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 358
        }
      },
      {
        "header": "Multithreading or multiprocessing with resourcesÂ¶",
        "content": "Resource instances are not thread safe and should not be shared across threads or processes. These special classes contain additional meta data that cannot be shared. Itâs recommended to create a new Resource for each thread or process:\n\nIn the example above, each thread would have its own Boto3 session and its own instance of the S3 resource. This is a good idea because resources contain shared data when loaded and calling actions, accessing properties, or manually loading or reloading the resource can modify this data.",
        "code_examples": [
          "```\nimportboto3importboto3.sessionimportthreadingclassMyTask(threading.Thread):defrun(self):# Here we create a new session per threadsession=boto3.session.Session()# Next, we create a resource client using our thread's session objects3=session.resource('s3')# Put your thread-safe code here\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 528
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/resources.html",
    "doc_type": "aws",
    "total_sections": 11
  },
  {
    "title": "CollectionsÂ¶",
    "summary": "A collection provides an iterable interface to a group of resources. Collections behave similarly to Django QuerySets and expose a similar API. A collection seamlessly handles pagination for you, making it possible to easily iterate over all items from all pages of data. Example of a collection:\n\n# SQS list all queues sqs = boto3.resource('sqs') for queue in sqs.queues.all(): print(queue.url)",
    "sections": [
      {
        "header": "OverviewÂ¶",
        "content": "A collection provides an iterable interface to a group of resources. Collections behave similarly to Django QuerySets and expose a similar API. A collection seamlessly handles pagination for you, making it possible to easily iterate over all items from all pages of data. Example of a collection:",
        "code_examples": [
          "```\n# SQS list all queuessqs=boto3.resource('sqs')forqueueinsqs.queues.all():print(queue.url)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 296
        }
      },
      {
        "header": "When collections make requestsÂ¶",
        "content": "Collections can be created and manipulated without any request being made to the underlying service. A collection makes a remote service request under the following conditions:\n\nConversion to list():\n\nBatch actions (see below):\n\n• Iteration: for bucket in s3.buckets.all(): print(bucket.name)\n• Conversion to list(): buckets = list(s3.buckets.all())\n• Batch actions (see below): s3.Bucket('amzn-s3-demo-bucket').objects.delete()",
        "code_examples": [
          "```\nforbucketins3.buckets.all():print(bucket.name)\n```",
          "```\nbuckets=list(s3.buckets.all())\n```",
          "```\ns3.Bucket('amzn-s3-demo-bucket').objects.delete()\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 428
        }
      },
      {
        "header": "FilteringÂ¶",
        "content": "Some collections support extra arguments to filter the returned data set, which are passed into the underlying service operation. Use the filter() method to filter the results:\n\n[Warning] WarningBehind the scenes, the above example will call ListBuckets, ListObjects, and HeadObject many times. If you have a large number of S3 objects then this could incur a significant cost.",
        "code_examples": [
          "```\n# S3 list all keys with the prefix 'photos/'s3=boto3.resource('s3')forbucketins3.buckets.all():forobjinbucket.objects.filter(Prefix='photos/'):print('{0}:{1}'.format(bucket.name,obj.key))\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 377
        }
      },
      {
        "header": "Warning",
        "content": "Behind the scenes, the above example will call ListBuckets, ListObjects, and HeadObject many times. If you have a large number of S3 objects then this could incur a significant cost.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 182
        }
      },
      {
        "header": "ChainabilityÂ¶",
        "content": "Collection methods are chainable. They return copies of the collection rather than modifying the collection, including a deep copy of any associated operation parameters. For example, this allows you to build up multiple collections from a base which they all have in common:",
        "code_examples": [
          "```\n# EC2 find instancesec2=boto3.resource('ec2')base=ec2.instances.filter(InstanceIds=['id1','id2','id3'])filters=[{'Name':'tenancy','Values':['dedicated']}]filtered1=base.filter(Filters=filters)# Note, this does NOT modify the filters in ``filtered1``!filters.append({'name':'instance-type','value':'t1.micro'})filtered2=base.filter(Filters=filters)print('All instances:')forinstanceinbase:print(instance.id)print('Dedicated instances:')forinstanceinfiltered1:print(instance.id)print('Dedicated micro instances:')forinstanceinfiltered2:print(instance.id)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 275
        }
      },
      {
        "header": "Limiting resultsÂ¶",
        "content": "It is possible to limit the number of items returned from a collection by using either the limit() method:\n\nIn both cases, up to 10 items total will be returned. If you do not have 10 buckets, then all of your buckets will be returned.",
        "code_examples": [
          "```\n# S3 iterate over first ten bucketsforbucketins3.buckets.limit(10):print(bucket.name)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 235
        }
      },
      {
        "header": "Controlling page sizeÂ¶",
        "content": "Collections automatically handle paging through results, but you may want to control the number of items returned from a single service operation call. You can do so using the page_size() method:\n\nBy default, S3 will return 1000 objects at a time, so the above code would let you process the items in smaller batches, which could be beneficial for slow or unreliable internet connections.",
        "code_examples": [
          "```\n# S3 iterate over all objects 100 at a timeforobjinbucket.objects.page_size(100):print(obj.key)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 388
        }
      },
      {
        "header": "Batch actionsÂ¶",
        "content": "Some collections support batch actions, which are actions that operate on an entire page of results at a time. They will automatically handle pagination:\n\n[Admonition] DangerThe above example will completely erase all data in the amzn-s3-demo-bucket bucket! Please be careful with batch actions.",
        "code_examples": [
          "```\n# S3 delete everything in `amzn-s3-demo-bucket`s3=boto3.resource('s3')s3.Bucket('amzn-s3-demo-bucket').objects.delete()\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 295
        }
      },
      {
        "header": "Danger",
        "content": "The above example will completely erase all data in the amzn-s3-demo-bucket bucket! Please be careful with batch actions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 121
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/collections.html",
    "doc_type": "aws",
    "total_sections": 9
  },
  {
    "title": "PaginatorsÂ¶",
    "summary": "Some AWS operations return results that are incomplete and require subsequent requests in order to attain the entire result set. The process of sending subsequent requests to continue where a previous request left off is called pagination. For example, the list_objects operation of Amazon S3 returns up to 1000 objects at a time, and you must send subsequent requests with the appropriate Marker in order to retrieve the next page of results.\n\nPaginators are a feature of boto3 that act as an abstra",
    "sections": [
      {
        "header": "",
        "content": "Some AWS operations return results that are incomplete and require subsequent requests in order to attain the entire result set. The process of sending subsequent requests to continue where a previous request left off is called pagination. For example, the list_objects operation of Amazon S3 returns up to 1000 objects at a time, and you must send subsequent requests with the appropriate Marker in order to retrieve the next page of results.\n\nPaginators are a feature of boto3 that act as an abstraction over the process of iterating over an entire result set of a truncated API operation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 591
        }
      },
      {
        "header": "Creating paginatorsÂ¶",
        "content": "Paginators are created via the get_paginator() method of a boto3 client. The get_paginator() method accepts an operation name and returns a reusable Paginator object. You then call the paginate method of the Paginator, passing in any relevant operation parameters to apply to the underlying API operation. The paginate method then returns an iterable PageIterator:",
        "code_examples": [
          "```\nimportboto3# Create a clientclient=boto3.client('s3',region_name='us-west-2')# Create a reusable Paginatorpaginator=client.get_paginator('list_objects_v2')# Create a PageIterator from the Paginatorpage_iterator=paginator.paginate(Bucket='amzn-s3-demo-bucket')forpageinpage_iterator:print(page['Contents'])\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 364
        }
      },
      {
        "header": "Customizing page iteratorsÂ¶",
        "content": "You must call the paginate method of a Paginator in order to iterate over the pages of API operation results. The paginate method accepts a PaginationConfig named argument that can be used to customize the pagination:\n\nLimits the maximum number of total returned items returned while paginating.\n\nCan be used to modify the starting marker or token of a paginator. This argument if useful for resuming pagination from a previous token or starting pagination at a known position.\n\nControls the number of items returned per page of each result.\n\n**MaxItems**: Limits the maximum number of total returned items returned while paginating.\n**StartingToken**: Can be used to modify the starting marker or token of a paginator. This argument if useful for resuming pagination from a previous token or starting pagination at a known position.\n**PageSize**: Controls the number of items returned per page of each result. NoteServices may choose to return more or fewer items than specified in the PageSize argument depending on the service, the operation, or the resource you are paginating.\n\n[Note] NoteServices may choose to return more or fewer items than specified in the PageSize argument depending on the service, the operation, or the resource you are paginating.",
        "code_examples": [
          "```\npaginator=client.get_paginator('list_objects_v2')page_iterator=paginator.paginate(Bucket='amzn-s3-demo-bucket',PaginationConfig={'MaxItems':10})\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1260
        }
      },
      {
        "header": "Note",
        "content": "Services may choose to return more or fewer items than specified in the PageSize argument depending on the service, the operation, or the resource you are paginating.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 166
        }
      },
      {
        "header": "Filtering resultsÂ¶",
        "content": "Many Paginators can be filtered server-side with options that are passed through to each underlying API call. For example, S3.Paginator.list_objects.paginate() accepts a Prefix parameter used to filter the paginated results by prefix server-side before sending them to the client:",
        "code_examples": [
          "```\nimportboto3client=boto3.client('s3',region_name='us-west-2')paginator=client.get_paginator('list_objects_v2')operation_parameters={'Bucket':'amzn-s3-demo-bucket','Prefix':'foo/baz'}page_iterator=paginator.paginate(**operation_parameters)forpageinpage_iterator:print(page['Contents'])\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 280
        }
      },
      {
        "header": "Filtering results with JMESPathÂ¶",
        "content": "JMESPath is a query language for JSON that can be used directly on paginated results. You can filter results client-side using JMESPath expressions that are applied to each page of results through the search method of a PageIterator.\n\nWhen filtering with JMESPath expressions, each page of results that is yielded by the paginator is mapped through the JMESPath expression. If a JMESPath expression returns a single value that is not an array, that value is yielded directly. If the result of applying the JMESPath expression to a page of results is a list, then each value of the list is yielded individually (essentially implementing a flat map). For example, in the above expression, each key that has a Size greater than 100 is yielded by the filtered_iterator.",
        "code_examples": [
          "```\nimportboto3client=boto3.client('s3',region_name='us-west-2')paginator=client.get_paginator('list_objects_v2')page_iterator=paginator.paginate(Bucket='amzn-s3-demo-bucket')filtered_iterator=page_iterator.search(\"Contents[?Size > `100`][]\")forkey_datainfiltered_iterator:print(key_data)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 765
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/paginators.html",
    "doc_type": "aws",
    "total_sections": 6
  },
  {
    "title": "Amazon S3Â¶",
    "summary": "Boto 2.x contains a number of customizations to make working with Amazon S3 buckets and keys easy. Boto3 exposes these same objects through its resources interface in a unified and consistent way.\n\nBoto3 has both low-level clients and higher-level resources. For Amazon S3, the higher-level resources are the most similar to Boto 2.xâs s3 module:",
    "sections": [
      {
        "header": "",
        "content": "Boto 2.x contains a number of customizations to make working with Amazon S3 buckets and keys easy. Boto3 exposes these same objects through its resources interface in a unified and consistent way.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 196
        }
      },
      {
        "header": "Creating the connectionÂ¶",
        "content": "Boto3 has both low-level clients and higher-level resources. For Amazon S3, the higher-level resources are the most similar to Boto 2.xâs s3 module:",
        "code_examples": [
          "```\n# Boto 2.ximportbotos3_connection=boto.connect_s3()# Boto3importboto3s3=boto3.resource('s3')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 150
        }
      },
      {
        "header": "Creating a bucketÂ¶",
        "content": "Creating a bucket in Boto 2 and Boto3 is very similar, except that in Boto3 all action parameters must be passed via keyword arguments and a bucket configuration must be specified manually:",
        "code_examples": [
          "```\n# Boto 2.xs3_connection.create_bucket('amzn-s3-demo-bucket')s3_connection.create_bucket('amzn-s3-demo-bucket',location=Location.USWest)# Boto3s3.create_bucket(Bucket='amzn-s3-demo-bucket')s3.create_bucket(Bucket='amzn-s3-demo-bucket',CreateBucketConfiguration={'LocationConstraint':'us-west-1'})\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 189
        }
      },
      {
        "header": "Storing dataÂ¶",
        "content": "Storing data from a file, stream, or string is easy:",
        "code_examples": [
          "```\n# Boto 2.xfromboto.s3.keyimportKeykey=Key('hello.txt')key.set_contents_from_file('/tmp/hello.txt')# Boto3s3.Object('amzn-s3-demo-bucket','hello.txt').put(Body=open('/tmp/hello.txt','rb'))\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 52
        }
      },
      {
        "header": "Accessing a bucketÂ¶",
        "content": "Getting a bucket is easy with Boto3âs resources, however these do not automatically validate whether a bucket exists:",
        "code_examples": [
          "```\n# Boto 2.xbucket=s3_connection.get_bucket('amzn-s3-demo-bucket',validate=False)exists=s3_connection.lookup('amzn-s3-demo-bucket')# Boto3importbotocorebucket=s3.Bucket('amzn-s3-demo-bucket')exists=Truetry:s3.meta.client.head_bucket(Bucket='amzn-s3-demo-bucket')exceptbotocore.exceptions.ClientErrorase:# If a client error is thrown, then check that it was a 404 error.# If it was a 404 error, then the bucket does not exist.error_code=e.response['Error']['Code']iferror_code=='404':exists=False\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 119
        }
      },
      {
        "header": "Deleting a bucketÂ¶",
        "content": "All of the keys in a bucket must be deleted before the bucket itself can be deleted:",
        "code_examples": [
          "```\n# Boto 2.xforkeyinbucket:key.delete()bucket.delete()# Boto3forkeyinbucket.objects.all():key.delete()bucket.delete()\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 84
        }
      },
      {
        "header": "Iteration of buckets and keysÂ¶",
        "content": "Bucket and key objects are no longer iterable, but now provide collection attributes which can be iterated:",
        "code_examples": [
          "```\n# Boto 2.xforbucketins3_connection:forkeyinbucket:print(key.name)# Boto3forbucketins3.buckets.all():forkeyinbucket.objects.all():print(key.key)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 107
        }
      },
      {
        "header": "Access controlsÂ¶",
        "content": "Getting and setting canned access control values in Boto3 operates on an ACL resource object:\n\nItâs also possible to retrieve the policy grant information:\n\nBoto3 lacks the grant shortcut methods present in Boto 2.x, but it is still fairly simple to add grantees:",
        "code_examples": [
          "```\n# Boto 2.xbucket.set_acl('public-read')key.set_acl('public-read')# Boto3bucket.Acl().put(ACL='public-read')obj.Acl().put(ACL='public-read')\n```",
          "```\n# Boto 2.xacp=bucket.get_acl()forgrantinacp.acl.grants:print(grant.display_name,grant.permission)# Boto3acl=bucket.Acl()forgrantinacl.grants:print(grant['Grantee']['DisplayName'],grant['Permission'])\n```",
          "```\n# Boto 2.xbucket.add_email_grant('READ','user@domain.tld')# Boto3bucket.Acl.put(GrantRead='emailAddress=user@domain.tld')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 265
        }
      },
      {
        "header": "Key metadataÂ¶",
        "content": "Itâs possible to set arbitrary metadata on keys:",
        "code_examples": [
          "```\n# Boto 2.xkey.set_metadata('meta1','This is my metadata value')print(key.get_metadata('meta1'))# Boto3key.put(Metadata={'meta1':'This is my metadata value'})print(key.metadata['meta1'])\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 50
        }
      },
      {
        "header": "Managing CORS configurationsÂ¶",
        "content": "Allows you to manage the cross-origin resource sharing configuration for S3 buckets:",
        "code_examples": [
          "```\n# Boto 2.xcors=bucket.get_cors()config=CORSConfiguration()config.add_rule('GET','*')bucket.set_cors(config)bucket.delete_cors()# Boto3cors=bucket.Cors()config={'CORSRules':[{'AllowedMethods':['GET'],'AllowedOrigins':['*']}]}cors.put(CORSConfiguration=config)cors.delete()\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 84
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/guide/migrations3.html",
    "doc_type": "aws",
    "total_sections": 10
  },
  {
    "title": "S3Â¶",
    "summary": "A low-level client representing Amazon Simple Storage Service (S3)\n\nabort_multipart_upload can_paginate close complete_multipart_upload copy copy_object create_bucket create_bucket_metadata_configuration create_bucket_metadata_table_configuration create_multipart_upload create_session delete_bucket delete_bucket_analytics_configuration delete_bucket_cors delete_bucket_encryption delete_bucket_intelligent_tiering_configuration delete_bucket_inventory_configuration delete_bucket_lifecycle delete_b",
    "sections": [
      {
        "header": "ClientÂ¶",
        "content": "A low-level client representing Amazon Simple Storage Service (S3)\n\nThese are the available methods:\n\n• abort_multipart_upload\n• can_paginate\n• complete_multipart_upload\n• copy_object\n• create_bucket\n• create_bucket_metadata_configuration\n• create_bucket_metadata_table_configuration\n• create_multipart_upload\n• create_session\n• delete_bucket\n• delete_bucket_analytics_configuration\n• delete_bucket_cors\n• delete_bucket_encryption\n• delete_bucket_intelligent_tiering_configuration\n• delete_bucket_inventory_configuration\n• delete_bucket_lifecycle\n• delete_bucket_metadata_configuration\n• delete_bucket_metadata_table_configuration\n• delete_bucket_metrics_configuration\n• delete_bucket_ownership_controls\n• delete_bucket_policy\n• delete_bucket_replication\n• delete_bucket_tagging\n• delete_bucket_website\n• delete_object\n• delete_object_tagging\n• delete_objects\n• delete_public_access_block\n• download_file\n• download_fileobj\n• generate_presigned_post\n• generate_presigned_url\n• get_bucket_accelerate_configuration\n• get_bucket_acl\n• get_bucket_analytics_configuration\n• get_bucket_cors\n• get_bucket_encryption\n• get_bucket_intelligent_tiering_configuration\n• get_bucket_inventory_configuration\n• get_bucket_lifecycle\n• get_bucket_lifecycle_configuration\n• get_bucket_location\n• get_bucket_logging\n• get_bucket_metadata_configuration\n• get_bucket_metadata_table_configuration\n• get_bucket_metrics_configuration\n• get_bucket_notification\n• get_bucket_notification_configuration\n• get_bucket_ownership_controls\n• get_bucket_policy\n• get_bucket_policy_status\n• get_bucket_replication\n• get_bucket_request_payment\n• get_bucket_tagging\n• get_bucket_versioning\n• get_bucket_website\n• get_object_acl\n• get_object_attributes\n• get_object_legal_hold\n• get_object_lock_configuration\n• get_object_retention\n• get_object_tagging\n• get_object_torrent\n• get_paginator\n• get_public_access_block\n• head_bucket\n• head_object\n• list_bucket_analytics_configurations\n• list_bucket_intelligent_tiering_configurations\n• list_bucket_inventory_configurations\n• list_bucket_metrics_configurations\n• list_buckets\n• list_directory_buckets\n• list_multipart_uploads\n• list_object_versions\n• list_objects\n• list_objects_v2\n• put_bucket_accelerate_configuration\n• put_bucket_acl\n• put_bucket_analytics_configuration\n• put_bucket_cors\n• put_bucket_encryption\n• put_bucket_intelligent_tiering_configuration\n• put_bucket_inventory_configuration\n• put_bucket_lifecycle\n• put_bucket_lifecycle_configuration\n• put_bucket_logging\n• put_bucket_metrics_configuration\n• put_bucket_notification\n• put_bucket_notification_configuration\n• put_bucket_ownership_controls\n• put_bucket_policy\n• put_bucket_replication\n• put_bucket_request_payment\n• put_bucket_tagging\n• put_bucket_versioning\n• put_bucket_website\n• put_object_acl\n• put_object_legal_hold\n• put_object_lock_configuration\n• put_object_retention\n• put_object_tagging\n• put_public_access_block\n• rename_object\n• restore_object\n• select_object_content\n• update_bucket_metadata_inventory_table_configuration\n• update_bucket_metadata_journal_table_configuration\n• upload_file\n• upload_fileobj\n• upload_part\n• upload_part_copy\n• write_get_object_response\n\n**classS3.ClientÂ¶**: A low-level client representing Amazon Simple Storage Service (S3) importboto3 client = boto3.client('s3')",
        "code_examples": [
          "```\nimportboto3client=boto3.client('s3')\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 3292
        }
      },
      {
        "header": "PaginatorsÂ¶",
        "content": "Paginators are available on a client instance via the get_paginator method. For more detailed instructions and examples on the usage of paginators, see the paginators user guide.\n\nThe available paginators are:\n\n• ListBuckets\n• ListDirectoryBuckets\n• ListMultipartUploads\n• ListObjectVersions\n• ListObjects\n• ListObjectsV2",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 321
        }
      },
      {
        "header": "WaitersÂ¶",
        "content": "Waiters are available on a client instance via the get_waiter method. For more detailed instructions and examples on the usage or waiters, see the waiters user guide.\n\nThe available waiters are:\n\n• BucketExists\n• BucketNotExists\n• ObjectExists\n• ObjectNotExists",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 261
        }
      },
      {
        "header": "ResourcesÂ¶",
        "content": "Resources are available in boto3 via the resource method. For more detailed instructions and examples on the usage of resources, see the resources user guide.\n\nThe available resources are:\n\n• Service Resource\n• BucketLifecycle\n• BucketLifecycleConfiguration\n• BucketLogging\n• BucketNotification\n• BucketPolicy\n• BucketRequestPayment\n• BucketTagging\n• BucketVersioning\n• BucketWebsite\n• MultipartUpload\n• MultipartUploadPart\n• ObjectSummary\n• ObjectVersion",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 455
        }
      },
      {
        "header": "List objects in an Amazon S3 bucketÂ¶",
        "content": "The following example shows how to use an Amazon S3 bucket resource to list the objects in the bucket.",
        "code_examples": [
          "```\nimportboto3s3=boto3.resource('s3')bucket=s3.Bucket('amzn-s3-demo-bucket')forobjinbucket.objects.all():print(obj.key)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 102
        }
      },
      {
        "header": "List top-level common prefixes in Amazon S3 bucketÂ¶",
        "content": "This example shows how to list all of the top-level common prefixes in an Amazon S3 bucket:",
        "code_examples": [
          "```\nimportboto3client=boto3.client('s3')paginator=client.get_paginator('list_objects')result=paginator.paginate(Bucket='amzn-s3-demo-bucket',Delimiter='/')forprefixinresult.search('CommonPrefixes'):print(prefix.get('Prefix'))\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 91
        }
      },
      {
        "header": "Restore Glacier objects in an Amazon S3 bucketÂ¶",
        "content": "The following example shows how to initiate restoration of glacier objects in an Amazon S3 bucket, determine if a restoration is on-going, and determine if a restoration is finished.",
        "code_examples": [
          "```\nimportboto3s3=boto3.resource('s3')bucket=s3.Bucket('amzn-s3-demo-bucket')forobj_suminbucket.objects.all():obj=s3.Object(obj_sum.bucket_name,obj_sum.key)ifobj.storage_class=='GLACIER':# Try to restore the object if the storage class is glacier and# the object does not have a completed or ongoing restoration# request.ifobj.restoreisNone:print('Submitting restoration request:%s'%obj.key)obj.restore_object(RestoreRequest={'Days':1})# Print out objects whose restoration is on-goingelif'ongoing-request=\"true\"'inobj.restore:print('Restoration in-progress:%s'%obj.key)# Print out objects whose restoration is completeelif'ongoing-request=\"false\"'inobj.restore:print('Restoration complete:%s'%obj.key)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 182
        }
      },
      {
        "header": "Uploading/downloading files using SSE KMSÂ¶",
        "content": "This example shows how to use SSE-KMS to upload objects using server side encryption with a key managed by KMS.\n\nWe can either use the default KMS master key, or create a custom key in AWS and use it to encrypt the object by passing in its key id.\n\nWith KMS, nothing else needs to be provided for getting the object; S3 already knows how to decrypt the object.",
        "code_examples": [
          "```\nimportboto3importosBUCKET='amzn-s3-demo-bucket's3=boto3.client('s3')keyid='<the key id>'print(\"Uploading S3 object with SSE-KMS\")s3.put_object(Bucket=BUCKET,Key='encrypt-key',Body=b'foobar',ServerSideEncryption='aws:kms',# Optional: SSEKMSKeyIdSSEKMSKeyId=keyid)print(\"Done\")# Getting the object:print(\"Getting S3 object...\")response=s3.get_object(Bucket=BUCKET,Key='encrypt-key')print(\"Done, response body:\")print(response['Body'].read())\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 360
        }
      },
      {
        "header": "Uploading/downloading files using SSE Customer KeysÂ¶",
        "content": "This example shows how to use SSE-C to upload objects using server side encryption with a customer provided key.\n\nFirst, weâll need a 32 byte key. For this example, weâll randomly generate a key but you can use any 32 byte key you want. Remember, you must the same key to download the object. If you lose the encryption key, you lose the object.\n\nAlso note how we donât have to provide the SSECustomerKeyMD5. Boto3 will automatically compute this value for us.",
        "code_examples": [
          "```\nimportboto3importosBUCKET='amzn-s3-demo-bucket'KEY=os.urandom(32)s3=boto3.client('s3')print(\"Uploading S3 object with SSE-C\")s3.put_object(Bucket=BUCKET,Key='encrypt-key',Body=b'foobar',SSECustomerKey=KEY,SSECustomerAlgorithm='AES256')print(\"Done\")# Getting the object:print(\"Getting S3 object...\")# Note how we're using the same ``KEY`` we# created earlier.response=s3.get_object(Bucket=BUCKET,Key='encrypt-key',SSECustomerKey=KEY,SSECustomerAlgorithm='AES256')print(\"Done, response body:\")print(response['Body'].read())\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 466
        }
      },
      {
        "header": "Downloading a specific version of an S3 objectÂ¶",
        "content": "This example shows how to download a specific version of an S3 object.",
        "code_examples": [
          "```\nimportboto3s3=boto3.client('s3')s3.download_file(\"amzn-s3-demo-bucket\",\"key-name\",\"tmp.txt\",ExtraArgs={\"VersionId\":\"my-version-id\"})\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 70
        }
      },
      {
        "header": "Filter objects by last modified time using JMESPathÂ¶",
        "content": "This example shows how to filter objects by last modified time using JMESPath.",
        "code_examples": [
          "```\nimportboto3s3=boto3.client(\"s3\")s3_paginator=s3.get_paginator('list_objects_v2')s3_iterator=s3_paginator.paginate(Bucket='amzn-s3-demo-bucket')filtered_iterator=s3_iterator.search(\"Contents[?to_string(LastModified)>='\\\"2022-01-05 08:05:37+00:00\\\"'].Key\")forkey_datainfiltered_iterator:print(key_data)\n```"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 78
        }
      },
      {
        "header": "Client Context ParametersÂ¶",
        "content": "Client context parameters are configurable on a client instance via the client_context_params parameter in the Config object. For more detailed instructions and examples on the exact usage of context params see the configuration guide.\n\nThe available s3 client context params are:\n\nbuckets and reverts to using conventional SigV4 for those.\n\n• disable_s3_express_session_auth (boolean) - Disables this clientâs usage of Session Auth for S3Expressbuckets and reverts to using conventional SigV4 for those.\n\n**disable_s3_express_session_auth (boolean) - Disables this clientâs usage of Session Auth for S3Express**: buckets and reverts to using conventional SigV4 for those.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 676
        }
      }
    ],
    "url": "https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html",
    "doc_type": "aws",
    "total_sections": 12
  },
  {
    "title": "Launch a Linux Virtual Machine with Amazon Lightsail",
    "summary": "Launch a Linux Virtual Machine with Amazon Lightsail AWS experience Beginner Time to complete 10 minutes Cost to complete Free Tier eligible Requires AWS account NoteAccounts created within the past 24 hours might not yet have access to the services required for this tutorial. Recommended browser: The latest version of Chrome or Firefox Last updated Aug 23, 2022 Overview LightsailÂ is one of the easiest ways to get started on AWS. It offers virtual servers, storage, databases, and networking, pl",
    "sections": [
      {
        "header": "",
        "content": "Accounts created within the past 24 hours might not yet have access to the services required for this tutorial.\n\nRecommended browser: The latest version of Chrome or Firefox\n\n• AWS account NoteAccounts created within the past 24 hours might not yet have access to the services required for this tutorial.\n• Recommended browser: The latest version of Chrome or Firefox\n\nAWS experience | Beginner\nTime to complete | 10 minutes\nCost to complete | Free Tier eligible\nRequires | AWS account NoteAccounts created within the past 24 hours might not yet have access to the services required for this tutorial. Recommended browser: The latest version of Chrome or Firefox\nLast updated | Aug 23, 2022\n\n[Note] NoteAccounts created within the past 24 hours might not yet have access to the services required for this tutorial.\n\n[Note] Accounts created within the past 24 hours might not yet have access to the services required for this tutorial.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 934
        }
      },
      {
        "header": "Overview",
        "content": "LightsailÂ is one of the easiest ways to get started on AWS. It offers virtual servers, storage, databases, and networking, plus a cost-effective, monthly plan. Itâs designed to help you start small, and then scale as you grow.\n\nIn this tutorial, you create an Amazon Linux instance in Amazon Lightsail in seconds. After the instance is up and running, you connect to it via SSH within the Lightsail console using the browser-based SSH terminal.\n\nGet started with Lightsail for free",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 484
        }
      },
      {
        "header": "Implementation",
        "content": "There are no charges for using Amazon Lightsail for this tutorial.\n\nSign up for an AWS account\n\nAlready have an account? Sign-in\n\nChoose Create instance in the Instances tab of the Lightsail home page.\n\nAn AWS Region and Availability Zone is selected for you. Choose Change AWS Region and Availability Zone to create your instance in another location.\n\nStatic IP addresses can only be attached to instances in the same region.\n\nChoose an instance image\n\nChoose the Linux/Unix platform option, and choose OS Only to view the operating system-only instance images available in Lightsail.\n\nTo learn more about Lightsail instance images, see Choose an Amazon Lightsail instance image.\n\nChoose the blueprint\n\nChoose the Amazon Linux 2 blueprint option.\n\n(Optional) Configure launch script\n\nChoose Add launch script to add a shell script that will run on your instance when it launches.\n\n(Optional) Choose an SSH key pair\n\nChoose Change SSH key pair to select, create, or upload the key pair you would like to use to SSH into your instance.\n\n(Optional) Configure automatic snapshots\n\nChoose Enable Automatic Snapshots to automatically create a backup image of your instance and attached disks on a daily schedule.\n\nSelect a pricing plan\n\nChoose your instance plan. You can try the $3.50 USD Lightsail plan free for the first three months (up to 750 hours). We'll credit the first three months to your account.\n\nLearn more on our Lightsail pricing page.\n\nSpecify the instance name\n\nEnter a name for your instance.\n\n(Optional) Add instance tags\n\nChoose one of the following options to add tags to your instance:\n\n(Optional) Add key-only tags â Enter your new tag into the tag key text box, and press Enter. Choose Save when youâre done entering your tags to add them, or choose Cancel to not add them.\n\n(Optional) Create a key-value tag â Enter a key into the Key text box, and a value into the Value text box. Choose Save when youâre done entering your tags, or choose Cancel to not add them.\n\nKey-value tags can only be added one at a time before saving. To add more than one key-value tag, repeat the previous steps.\n\nFor more information about key-only and key-value tags, see Tags in Amazon Lightsail.\n\nChoose Create instance.\n\nConnect to your instance using the browser-based SSH terminal in Lightsail.\n\nConnect to the instance terminal\n\nIn the Instances tab of the Lightsail home page, choose the terminal icon, or the ellipsis (â®) icon next to the Amazon Linux instance you just created.\n\nThe browser-based SSH terminal window appears. You can type Linux commands into the browser terminal, and manage your instance without configuring an SSH client.\n\n• There are no charges for using Amazon Lightsail for this tutorial. Sign up for an AWS account Already have an account? Sign-in\n\n• Choose Create instance in the Instances tab of the Lightsail home page.\n\n• Choose a Region An AWS Region and Availability Zone is selected for you. Choose Change AWS Region and Availability Zone to create your instance in another location. NoteStatic IP addresses can only be attached to instances in the same region.\n• Choose an instance image Choose the Linux/Unix platform option, and choose OS Only to view the operating system-only instance images available in Lightsail. To learn more about Lightsail instance images, see Choose an Amazon Lightsail instance image.\n• Choose the blueprint Choose the Amazon Linux 2 blueprint option.\n• (Optional) Configure launch script Choose Add launch script to add a shell script that will run on your instance when it launches.\n• (Optional) Choose an SSH key pair Choose Change SSH key pair to select, create, or upload the key pair you would like to use to SSH into your instance.\n• (Optional) Configure automatic snapshots Choose Enable Automatic Snapshots to automatically create a backup image of your instance and attached disks on a daily schedule.\n• Select a pricing plan Choose your instance plan. You can try the $3.50 USD Lightsail plan free for the first three months (up to 750 hours). We'll credit the first three months to your account. Learn more on our Lightsail pricing page.\n• Specify the instance name Enter a name for your instance.\n• (Optional) Add instance tags Choose one of the following options to add tags to your instance: (Optional) Add key-only tags â Enter your new tag into the tag key text box, and press Enter. Choose Save when youâre done entering your tags to add them, or choose Cancel to not add them. (Optional) Create a key-value tag â Enter a key into the Key text box, and a value into the Value text box. Choose Save when youâre done entering your tags, or choose Cancel to not add them. Key-value tags can only be added one at a time before saving. To add more than one key-value tag, repeat the previous steps. For more information about key-only and key-value tags, see Tags in Amazon Lightsail.\n• Launch the instance Choose Create instance.\n\n• Connect to the instance terminal In the Instances tab of the Lightsail home page, choose the terminal icon, or the ellipsis (â®) icon next to the Amazon Linux instance you just created. The browser-based SSH terminal window appears. You can type Linux commands into the browser terminal, and manage your instance without configuring an SSH client.\n\n[Note] NoteStatic IP addresses can only be attached to instances in the same region.\n\n[Note] Static IP addresses can only be attached to instances in the same region.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 33,
          "content_length": 5451
        }
      },
      {
        "header": "Congratulations",
        "content": "Congratulations! You used Amazon Lightsail to easily spin up and configure a Linux instance.\n\nAmazon Lightsail is great for developers, web pros, and anyone looking to get started on AWS in a quick and cheap way. You can launch instances, databases, and SSD-based storage; transfer data; and monitor your resources, and so much more in a managed way.\n\nWhether you're an individual developer creating a project or a blogger creating a personal website, Amazon Lightsail is a great way for you to get started.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 803
        }
      }
    ],
    "url": "https://aws.amazon.com/getting-started/hands-on/launch-a-virtual-machine/",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Host a Static Website",
    "summary": "OverviewWhat you will accomplishPrerequisitesImplementationCongratulations\n\nHost a Static Website AWS experience Beginner Time to complete 10 minutes Cost to complete Total cost of hosting your static website on AWS is dependent on your usage Outside of AWS Free Tier Limits: typically $1-3/mo. Within AWS Free Tier Limits: typically $0.50/mo. To see a breakdown of the services used and their associated costs, see pricing for AWS Amplify and Amazon RouteÂ 53. Get help Troubleshooting Amplify Last ",
    "sections": [
      {
        "header": "",
        "content": "Total cost of hosting your static website on AWS is dependent on your usage\n\nOutside of AWS Free Tier Limits: typically $1-3/mo.\n\nWithin AWS Free Tier Limits: typically $0.50/mo.\n\nTo see a breakdown of the services used and their associated costs, see pricing for AWS Amplify and Amazon RouteÂ 53.\n\nTroubleshooting Amplify\n\n• Outside of AWS Free Tier Limits: typically $1-3/mo.\n• Within AWS Free Tier Limits: typically $0.50/mo.\n\nAWS experience | Beginner\nTime to complete | 10 minutes\nCost to complete | Total cost of hosting your static website on AWS is dependent on your usage Outside of AWS Free Tier Limits: typically $1-3/mo. Within AWS Free Tier Limits: typically $0.50/mo. To see a breakdown of the services used and their associated costs, see pricing for AWS Amplify and Amazon RouteÂ 53.\nGet help | Troubleshooting Amplify\nLast updated | July 16, 2024",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 5,
          "content_length": 863
        }
      },
      {
        "header": "Overview",
        "content": "In this tutorial, you will learn how to deploy a static website with AWS Amplify. Amplify offers a Git-based CI/CD workflow for building, deploying, and hosting websites. Static websites deliver HTML, JavaScript, images, video and other files to your website visitors. Static websites are very low cost, provide high-levels of reliability, require almost no IT administration, and scale to handle enterprise-level traffic with no additional work.Â\n\nFor more information, see the FAQs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 484
        }
      },
      {
        "header": "What you will accomplish",
        "content": "In this tutorial, you will:\n\nHost a static website using AWS AmplifyÂ in the AWS Management Console. AWS Amplify provides fully managed hosting for static websites and web apps. Amplifyâs hosting solution leverages Amazon CloudFront and Amazon S3 to deliver your site assets via the AWS content delivery network (CDN).\n\nSet up continuous deployment: Amplify offers a Git-based workflow with continuous deployment, allowing you to automatically deploy updates to your site on every code commit.\n\n• Host a static website using AWS AmplifyÂ in the AWS Management Console. AWS Amplify provides fully managed hosting for static websites and web apps. Amplifyâs hosting solution leverages Amazon CloudFront and Amazon S3 to deliver your site assets via the AWS content delivery network (CDN).\n• Set up continuous deployment: Amplify offers a Git-based workflow with continuous deployment, allowing you to automatically deploy updates to your site on every code commit.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 966
        }
      },
      {
        "header": "Prerequisites",
        "content": "Before starting this tutorial, you will need:\n\nAn AWS account: if you don't already have one follow theÂ Setup Your Environment tutorial.\n\nYour AWS profile configured for local development.\n\nInstalled on your environment: Nodejs andÂ npm.\n\nFamiliarity with git and a Github account.\n\n• An AWS account: if you don't already have one follow theÂ Setup Your Environment tutorial.\n• Your AWS profile configured for local development.\n• Installed on your environment: Nodejs andÂ npm.\n• Familiarity with git and a Github account.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 524
        }
      },
      {
        "header": "Implementation",
        "content": "Already have a repository to connect? Skip to Initialize GitHub Repository\n\nWant to deploy without connecting to a Git provider? Follow these instructions\n\nCreate the application\n\nIn a new terminal window or command line, run the following command to use Vite to create a React application:\n\nView the application\n\nIn the terminal window, select and open the Local link to view the Vite + React application.\n\nIn this step, you will create a GitHub repository and commit your code to the repository. You will need a GitHub account to complete this step, if you do not have an account, sign up here.\n\nIf you have never used GitHub on your computer, follow the steps to generate and add an SSH key to your account before continuing to allow connection to your account.\n\nSign in to GitHub at https://github.com/.\n\nIn the Start a new repository section, make the following selections:\n\nFor Repository name, enter staticwebsite, and choose the Public radio button.\n\nThen select, Create a new repository.\n\nPush the application to GitHub\n\nOpen a new terminalÂ window, navigate to your projects root folder (staticwebsite), and run the following commands to initialize a git and push the application to the new GitHub repo:Â\n\nReplace the SSH GitHub URL in the command with your SSH GitHub URL.\n\nIn this step, you will connect the GitHub repository you just created to AWS Amplify. This will enable you to build, deploy, and host your app on AWS.\n\nCreate an Amplify app\n\nSign in to the AWS Management Console in a new browser window, and open the AWS Amplify console at https://console.aws.amazon.com/amplify/apps.\n\nChoose Create new app.\n\nChoose the GitHub repository\n\nOn the Start building with Amplify page, for Deploy your app, select GitHub, and select Next.\n\nIf you are using an existing repository, connect your GitHub, Bitbucket, GitLab, or AWS CodeCommit repositories.\n\nYou also have the option of manually uploading your build artifacts without connecting a Git repository (see Manual Deploys).\n\nAfter you authorize the Amplify console, Amplify fetches an access token from the repository provider, but it doesnât store the token on the AWS servers. Amplify accesses your repository using deploy keys installed in a specific repository only.\n\nSelect repository branch\n\nWhen prompted, authenticate with GitHub. You will be automatically redirected back to the Amplify console. Choose the repository and main branch you created earlier. Then select Next.\n\nReview build settings\n\nLeave the default build settings and select Next.\n\nAmplify inspects your repository to automatically detect the sequence of build commands to be invoked.\n\nReview the inputs selected, and choose Save and deploy to deploy your web app to a global content delivery network (CDN).Â\n\nView your deployed app\n\nAWS Amplify will now build your source code and deploy your app at https://...amplifyapp.com, andÂ on every git push your deployment instance will update.Â It may take 2-5 minutes to deploy your app based on the size.\n\nOnce the build completes, select the Visit deployed URL button to see your web app up and running live.Â\n\nIt is recommended that you delete the app and the backend resources that you created during this tutorial to prevent unexpected costs.\n\nIn the Amplify console, in the left-hand navigation for the staticwebsite app, choose App settings, and select General settings.\n\nIn the General settings section, choose Delete app.\n\n• Create the application In a new terminal window or command line, run the following command to use Vite to create a React application: npm create vite@latest staticwebsite -- --template react cd staticwebsite npm install npm run dev\n• View the application In the terminal window, select and open the Local link to view the Vite + React application. npm create vite@latest staticwebsite -- --template react cd staticwebsite npm install npm run dev\n\n• Open GitHub Sign in to GitHub at https://github.com/.\n• Create a repository In the Start a new repository section, make the following selections: For Repository name, enter staticwebsite, and choose the Public radio button. Then select, Create a new repository.\n• Push the application to GitHub Open a new terminalÂ window, navigate to your projects root folder (staticwebsite), and run the following commands to initialize a git and push the application to the new GitHub repo:Â NoteReplace the SSH GitHub URL in the command with your SSH GitHub URL. git init git add . git commit -m \"first commit\" git remote add origin git@github.com:<your-username>/staticwebsite.git git branch -M main git push -u origin main\n\n• For Repository name, enter staticwebsite, and choose the Public radio button.\n• Then select, Create a new repository.\n\n• Create an Amplify app Sign in to the AWS Management Console in a new browser window, and open the AWS Amplify console at https://console.aws.amazon.com/amplify/apps. Choose Create new app.\n• Choose the GitHub repository On the Start building with Amplify page, for Deploy your app, select GitHub, and select Next. Notes If you are using an existing repository, connect your GitHub, Bitbucket, GitLab, or AWS CodeCommit repositories. You also have the option of manually uploading your build artifacts without connecting a Git repository (see Manual Deploys). After you authorize the Amplify console, Amplify fetches an access token from the repository provider, but it doesnât store the token on the AWS servers. Amplify accesses your repository using deploy keys installed in a specific repository only.\n• Select repository branch When prompted, authenticate with GitHub. You will be automatically redirected back to the Amplify console. Choose the repository and main branch you created earlier. Then select Next.\n• Review build settings Leave the default build settings and select Next. Amplify inspects your repository to automatically detect the sequence of build commands to be invoked.\n• Deploy the app Review the inputs selected, and choose Save and deploy to deploy your web app to a global content delivery network (CDN).Â\n• View your deployed app AWS Amplify will now build your source code and deploy your app at https://...amplifyapp.com, andÂ on every git push your deployment instance will update.Â It may take 2-5 minutes to deploy your app based on the size. Once the build completes, select the Visit deployed URL button to see your web app up and running live.Â\n\n• If you are using an existing repository, connect your GitHub, Bitbucket, GitLab, or AWS CodeCommit repositories.\n• You also have the option of manually uploading your build artifacts without connecting a Git repository (see Manual Deploys).\n• After you authorize the Amplify console, Amplify fetches an access token from the repository provider, but it doesnât store the token on the AWS servers. Amplify accesses your repository using deploy keys installed in a specific repository only.\n\n• Amplify inspects your repository to automatically detect the sequence of build commands to be invoked.\n\n• Delete the app In the Amplify console, in the left-hand navigation for the staticwebsite app, choose App settings, and select General settings. In the General settings section, choose Delete app.\n\n[Note] NoteIf you have never used GitHub on your computer, follow the steps to generate and add an SSH key to your account before continuing to allow connection to your account.\n\n[Note] If you have never used GitHub on your computer, follow the steps to generate and add an SSH key to your account before continuing to allow connection to your account.\n\n[Note] NoteReplace the SSH GitHub URL in the command with your SSH GitHub URL.\n\n[Note] Replace the SSH GitHub URL in the command with your SSH GitHub URL.\n\n[Note] Notes If you are using an existing repository, connect your GitHub, Bitbucket, GitLab, or AWS CodeCommit repositories. You also have the option of manually uploading your build artifacts without connecting a Git repository (see Manual Deploys). After you authorize the Amplify console, Amplify fetches an access token from the repository provider, but it doesnât store the token on the AWS servers. Amplify accesses your repository using deploy keys installed in a specific repository only.\n\n[Note] If you are using an existing repository, connect your GitHub, Bitbucket, GitLab, or AWS CodeCommit repositories. You also have the option of manually uploading your build artifacts without connecting a Git repository (see Manual Deploys). After you authorize the Amplify console, Amplify fetches an access token from the repository provider, but it doesnât store the token on the AWS servers. Amplify accesses your repository using deploy keys installed in a specific repository only.",
        "code_examples": [
          "```\nnpm create vite@latest staticwebsite -- --template react\ncd staticwebsite\nnpm install\nnpm run dev\n```",
          "```\nnpm create vite@latest staticwebsite -- --template react\ncd staticwebsite\nnpm install\nnpm run dev\n```"
        ],
        "usage_examples": [
          "```\ngit init\ngit add .\ngit commit -m \"first commit\"\ngit remote add origin git@github.com:<your-username>/staticwebsite.git\ngit branch -M main\ngit push -u origin main\n```"
        ],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": true,
          "has_table": false,
          "paragraph_count": 36,
          "content_length": 8701
        }
      },
      {
        "header": "Congratulations",
        "content": "You have finished the Host a Static Website on AWS tutorial!\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 356
        }
      }
    ],
    "url": "https://aws.amazon.com/getting-started/hands-on/host-static-website/",
    "doc_type": "aws",
    "total_sections": 6
  },
  {
    "title": "Deploy Docker Containers on Amazon ECS",
    "summary": "Deploy Docker Containers on Amazon ECS AWS experience Beginner Time to complete 10 minutes Cost to complete Cost will vary by region, and will be around $0.004 / hour of running the container Services used Amazon ECS AWS Fargate Elastic Load Balancing Last updated August 11, 2022 Overview Amazon Elastic Container Service (Amazon ECS) is the AWS service you use to run Docker applications on a scalable cluster. In this how-to guide, you will learn how to run a Docker-enabled sample application on ",
    "sections": [
      {
        "header": "",
        "content": "Cost will vary by region, and will be around $0.004 / hour of running the container\n\nElastic Load Balancing\n\nAWS experience | Beginner\nTime to complete | 10 minutes\nCost to complete | Cost will vary by region, and will be around $0.004 / hour of running the container\nServices used | Amazon ECS AWS Fargate Elastic Load Balancing\nLast updated | August 11, 2022",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": true,
          "paragraph_count": 2,
          "content_length": 360
        }
      },
      {
        "header": "Overview",
        "content": "Amazon Elastic Container Service (Amazon ECS) is the AWS service you use to run Docker applications on a scalable cluster. In this how-to guide, you will learn how to run a Docker-enabled sample application on an Amazon ECS cluster behind a load balancer, test the sample application, and delete your resources to avoid charges. This guide uses AWS Fargate, which has a ~$0.004 (less than half of a US cent) cost per hour when using the 0.25 vCPU / 0.5 GB configuration.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 470
        }
      },
      {
        "header": "Implementation",
        "content": "The Amazon ECS first-run wizard will guide you through creating a cluster and launching a sample web application. In this step, you will enter the Amazon ECS console and launch the wizard.\n\nLaunch the first-run wizard\n\nTo launch the Amazon ECS first-run wizard, choose the Get started button. (If your layout looks different, disable the New ECS Experience toggle button at the top left of the console).\n\nA task definition is like a blueprint for your application. In this step, you will specify a task definition so Amazon ECS knows which Docker image to use for containers, how many containers to use in the task, and the resource allocation for each container.\n\nSelect a task definition\n\nIn the Container definition field, select sample-app.\n\nReview the default values\n\nThe task definition comes preloaded with default configuration values.\n\nReview the default values and choose Next.\n\nIf you prefer to modify the configurations or would like to learn more, see Task definition parameters.\n\nNow that you have created a task definition, you will configure the Amazon ECS service. A service launches and maintains copies of the task definition in your cluster. For example, by running an application as a service, Amazon ECS will auto-recover any stopped tasks and maintain the number of copies you specify.\n\nReview service options\n\nService options come preloaded with default configuration values.\n\nService name:Â The default sample-app-service is a web-based \"Hello World\"Â application provided by AWS. It is meant to run indefinitely; because it is running as a service, it will restart if the task becomes unhealthy or unexpectedlyÂ stops.\n\nNumber of desired tasks: Leave the default value of 1. This willÂ create one copy of your task.\n\nReview load balancing settings\n\nLoad balancing: You have the option to use a load balancer with your service. Amazon ECS can create an Elastic Load Balancing (ELB) load balancer to distribute the traffic across the container instances your task is launched on.\n\nSelect theÂ Application Load Balancer option.\n\nThe default values for Load balancer listener port and Load balancer listener protocol are set up for the sample application. For more information on load balancing configuration, see Service load balancing.\n\nReview your settings and choose Next.Â\n\nYour Amazon ECS tasks run on a cluster, which uses AWS Fargate to provide the compute engine so that you do not need to manage servers. In this step, you will configure the cluster.\n\nIn the Cluster name field, enter sample-cluster and choose Next.\n\nIn the previous steps, you configured your task definition (which is like an application blueprint), the Amazon ECS service (which launches and maintains copies of your task definitions), and your cluster. In this step, you will review, launch, and view the resources you create.\n\nReview task definition\n\nYou have a final chance to review your task definition, task configuration, and cluster configuration before launching. Choose Create.Â\n\nYou are on a Launch Status page that shows the status of your launch and describes each step of the process. After the launch is complete, choose View service.Â\n\nIn this step, you will verify that the sample application is up and running by pointing your browser to the load balancer DNS name.\n\nView details about the application\n\nOn the sample-app-service page, select the Details tab and select the entry under Target Group Name.Â\n\nView target group details\n\nOn the Target groups page, select the target group name.Â\n\nSelect the load balancer\n\nIn the Details section, choose the Load balancer link.Â\n\nCopy the DNS name of the application\n\nIn the Description tab, select the two page icon next to the load balancer DNS to copy the DNS name to your clipboard.Â\n\nView the sample application\n\nPaste the name into a new browser window, and press Enter to view the sample application (in this case, a static webpage).Â\n\nThroughout this guide, you've launched three resources: an Amazon ECS cluster, AWS Fargate to run your container, and a load balancer. In this step, you can clean up all your resources to avoid unwanted charges.\n\nNavigate back to the Amazon ECS console page and select the cluster name (sample-cluster).\n\nChoose Delete Cluster\n\nChoose Delete Cluster to delete the cluster.\n\nConfirm cluster deletion\n\nEnter delete me in the dialog box and choose Delete.\n\nMonitor cluster deletion\n\nYou will now see the progress as all the resources created are deleted.\n\nCluster deletion complete\n\nOnce everything has been deleted, you will see the Deleted cluster sample-cluster successfully message in green. You have now completed this guide.\n\n• Launch the first-run wizard To launch the Amazon ECS first-run wizard, choose the Get started button. (If your layout looks different, disable the New ECS Experience toggle button at the top left of the console).\n\n• Select a task definition In the Container definition field, select sample-app.\n• Review the default values The task definition comes preloaded with default configuration values. Review the default values and choose Next. If you prefer to modify the configurations or would like to learn more, see Task definition parameters.\n\n• Review service options Service options come preloaded with default configuration values. Service name:Â The default sample-app-service is a web-based \"Hello World\"Â application provided by AWS. It is meant to run indefinitely; because it is running as a service, it will restart if the task becomes unhealthy or unexpectedlyÂ stops. Number of desired tasks: Leave the default value of 1. This willÂ create one copy of your task.\n• Review load balancing settings Load balancing: You have the option to use a load balancer with your service. Amazon ECS can create an Elastic Load Balancing (ELB) load balancer to distribute the traffic across the container instances your task is launched on. Select theÂ Application Load Balancer option. The default values for Load balancer listener port and Load balancer listener protocol are set up for the sample application. For more information on load balancing configuration, see Service load balancing. Review your settings and choose Next.Â\n\n• Service name:Â The default sample-app-service is a web-based \"Hello World\"Â application provided by AWS. It is meant to run indefinitely; because it is running as a service, it will restart if the task becomes unhealthy or unexpectedlyÂ stops.\n• Number of desired tasks: Leave the default value of 1. This willÂ create one copy of your task.\n\n• Set cluster name In the Cluster name field, enter sample-cluster and choose Next.\n\n• Review task definition You have a final chance to review your task definition, task configuration, and cluster configuration before launching. Choose Create.Â\n• View service status You are on a Launch Status page that shows the status of your launch and describes each step of the process. After the launch is complete, choose View service.Â\n\n• View details about the application On the sample-app-service page, select the Details tab and select the entry under Target Group Name.Â\n• View target group details On the Target groups page, select the target group name.Â\n• Select the load balancer In the Details section, choose the Load balancer link.Â\n• Copy the DNS name of the application In the Description tab, select the two page icon next to the load balancer DNS to copy the DNS name to your clipboard.Â\n• View the sample application Paste the name into a new browser window, and press Enter to view the sample application (in this case, a static webpage).Â\n\n• Select the cluster Navigate back to the Amazon ECS console page and select the cluster name (sample-cluster).\n• Choose Delete Cluster Choose Delete Cluster to delete the cluster.\n• Confirm cluster deletion Enter delete me in the dialog box and choose Delete.\n• Monitor cluster deletion You will now see the progress as all the resources created are deleted.\n• Cluster deletion complete Once everything has been deleted, you will see the Deleted cluster sample-cluster successfully message in green. You have now completed this guide.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 47,
          "content_length": 8100
        }
      },
      {
        "header": "Congratulations",
        "content": "Congratulations! You have learned how to configure and deploy your Docker-enabled application to Amazon ECS, and how to delete resources that are no longer needed. Amazon ECS is a highly scalable, high performance container management service that supports Docker containers and allows you to easily run applications on a managed cluster of Amazon EC2 instances.\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it.\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation better.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 658
        }
      }
    ],
    "url": "https://aws.amazon.com/getting-started/hands-on/deploy-docker-containers/",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Run a Serverless \"Hello, World!\" with AWS Lambda",
    "summary": "AWS Tutorials Directory Getting Started Resource Center Developer Center IT Pro Center Architecture Center Tools & SDKs More AWS› Products› Run Serverless Code TUTORIAL Run a Serverless \"Hello, World!\" with AWS Lambda Overview In this tutorial, you will learn the basics of running code on AWS Lambda without provisioning or managing servers. We will walk through how to create a Hello World Lambda function using the AWS Lambda console. We will then show you how to manually invoke the Lambda functi",
    "sections": [
      {
        "header": "",
        "content": "AWS Tutorials Directory\n\n• Getting Started Resource Center\n• Developer Center\n• IT Pro Center\n• Architecture Center\n• Tools & SDKs\n\n• Run Serverless Code",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 153
        }
      },
      {
        "header": "Overview",
        "content": "In this tutorial, you will learn the basics of running code on AWS Lambda without provisioning or managing servers. We will walk through how to create a Hello World Lambda function using the AWS Lambda console. We will then show you how to manually invoke the Lambda function using sample event data and review your output metrics.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 331
        }
      },
      {
        "header": "Requires",
        "content": "[**]Accounts created within the past 24 hours might not yet have access to the services required for this tutorial.\n\n• AWS account\n• Recommended browser: The latest version of Chrome or Firefox",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 193
        }
      },
      {
        "header": "Select a Lambda blueprint and configure it",
        "content": "Blueprints provide example code to do some minimal processing. Most blueprints process events from specific event sources, such as Amazon S3, Amazon DynamoDB, or a custom application.\n\na. In the AWS Lambda console, choose Create function.\n\nNote: The console shows this page only if you do not have any Lambda functions created. If you have created functions already, you will see the Lambda > Functions page. On the list page, choose Create a function to go to the Create function page.\n\nA Lambda function consists of code you provide, associated dependencies, and configuration. The configuration information you provide includes the compute resources you want to allocate (for example, memory), execution timeout, and an IAM role that AWS Lambda can assume to execute your Lambda function on your behalf.\n\nName: You can name your Lambda function here. For this tutorial, we will use hello-world-python\n\nRole: You will create an IAM role (referred to as the execution role) with the necessary permissions that AWS Lambda can assume to invoke your Lambda function on your behalf.\n\nRole name: lambda_basic_execution.\n\nLambda function code: In this section, you can review the example code authored in Python.To continue building your function:\n\nb. Select use a blueprint. c. In the Blueprint name box, ensure Hello world function with python 3.10 blueprint is selected. d. In the Fuction name box, enter hello-world-python. e. For Execution role, select Create a new role from AWS policy templates. f. In the Role name box, enter lambda_basic_execution. g. Press the Create Function button.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 1589
        }
      },
      {
        "header": "Review Lambda settings",
        "content": "After creating your function, review other settings.\n\nIn this example, Lambda identifies this from the code sample and this should be pre-populated with lambda_function.lambda_handler.\n\n• Runtime: Currently, you can author your Lambda function code in Java, Node.js, C#, Go, or Python. For this tutorial, we are using Python 3.10 as the runtime.\n• Handler: You can specify a handler (a method/function in your code) where AWS Lambda can begin executing your code. AWS Lambda provides event data as input to this handler, which processes the event.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 547
        }
      },
      {
        "header": "Invoke Lambda function and verify results",
        "content": "The console shows the hello-world-python Lambda function. You can now test the function, verify results, and review the logs.\n\na. Select Configure Test Event from the drop-down menu called Test.\n\nb. The editor pops up so you can enter an event to test your function.\n\nSelect Create new event.\n\nType in an event name like HelloWorldEvent. Retain default setting of Private for Event sharing settings.\n\nChoose hello-world from the template list.\n\nYou can change the values in the sample JSON, but don’t change the event structure. For this tutorial, replace value1 with hello, world!.\n\nd. Upon successful execution, view the results in the console:\n\nThe Execution results tab verifies that the execution succeeded.\n\nThe Function Logs section will show the logs generated by the Lambda function execution as well as key information reported in the Log output.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 856
        }
      },
      {
        "header": "Monitor your metrics",
        "content": "AWS Lambda automatically monitors Lambda functions and reports metrics through Amazon CloudWatch. To help you monitor your code as it executes, Lambda automatically tracks the number of requests, the latency per request, and the number of requests resulting in an error and publishes the associated metrics.\n\na. Invoke the Lambda function a few more times by repeatedly choosing the Test button. This will generate the metrics that can be viewed in the next step.\n\nb. Select the Monitor tab to view the results.\n\nc. Scroll down to view the metrics for your Lambda\n\nfunction. Lambda metrics are reported through Amazon CloudWatch. You can leverage these metrics to set custom alarms. For more information about CloudWatch, see the Amazon CloudWatch Developer Guide.\n\nThe Monitoring tab will show seven CloudWatch metrics: Invocations, Duration, Error count and success rate (%), Throttles, Async delivery failures, IteratorAge, and Concurrent executions.\n\nWith AWS Lambda, you pay for what you use. After you hit your AWS Lambda free tier limit, you are charged based on the number of requests for your functions (invocation count) and the time your code executes (invocation duration). For more information, see AWS Lambda Pricing.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1231
        }
      },
      {
        "header": "Delete the Lambda function",
        "content": "While you will not get charged for keeping your Lambda function, you can easily delete it from the AWS Lambda console.\n\na. Select the Actions button and select Delete function.\n\nb. You will be asked to confirm your termination - select Delete.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 243
        }
      },
      {
        "header": "Conclusion",
        "content": "Congratulations! You have created your first AWS Lambda function. This is your first step in learning how to run applications without needing to provision or manage servers. Lambda automatically scales your applications by running your code in response to each trigger, scaling precisely with the size of your workloads.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 320
        }
      },
      {
        "header": "Was this page helpful?",
        "content": "Let us know so we can improve the quality of the content on our pages",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 69
        }
      }
    ],
    "url": "https://aws.amazon.com/getting-started/hands-on/run-serverless-code/",
    "doc_type": "aws",
    "total_sections": 10
  },
  {
    "title": "What is Amazon Elastic Container Service?",
    "summary": "Amazon Elastic Container Service (Amazon ECS) is a fully managed container orchestration service that helps you easily deploy, manage, and scale containerized applications. As a fully managed service, Amazon ECS comes with AWS configuration and operational best practices built-in. It's integrated with both AWS tools, such as Amazon Elastic Container Registry, and third-party tools, such as Docker. This integration makes it easier for teams to focus on building the applications, not the environment. You can run and scale your container workloads across AWS Regions in the cloud, and on-premises, without the complexity of managing a control plane. There are three layers in Amazon ECS:",
    "sections": [
      {
        "header": "Terminology and components",
        "content": "There are three layers in Amazon ECS:\n\nCapacity - The infrastructure where your containers run Controller - Deploy and manage your applications that run on the containers Provisioning - The tools that you can use to interface with the scheduler to deploy and manage your applications and containers\n\nThe following diagram shows the Amazon ECS layers.\n\nThe capacity is the infrastructure where your containers run. The following is an overview of the capacity options:\n\nAmazon ECS Managed Instances is a compute option for Amazon ECS that enables you to run containerized workloads on a range of Amazon EC2 instance types while offloading infrastructure management to AWS. With Amazon ECS Managed Instances, you can access specific compute capabilities such as GPU acceleration, specific CPU architectures, high network performance, and specialized instances types, while AWS handles provisioning, patching, scaling, and maintenance of the underlying infrastructure. Amazon EC2 instances in the AWS cloud You choose the instance type, the number of instances, and manage the capacity. Serverless in the AWS cloud Fargate is a serverless, pay-as-you-go compute engine. With Fargate you don't need to manage servers, handle capacity planning, or isolate container workloads for security. On-premises virtual machines (VM) or servers Amazon ECS Anywhere provides support for registering an external instance such as an on-premises server or virtual machine (VM), to your Amazon ECS cluster.\n\nThe Amazon ECS scheduler is the software that manages your applications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1560
        }
      },
      {
        "header": "Features",
        "content": "Amazon ECS provides the following high-level features:\n\nTask definition The blueprint for the application. Cluster The infrastructure your application runs on. Task An application such as a batch job that performs work, and then stops. Service A long running stateless application. Account Setting Allows access to features. Cluster Auto Scaling Amazon ECS manages the scaling of Amazon EC2 instances that are registered to your cluster. Service Auto Scaling Amazon ECS increases or decreases the desired number of tasks in your service automatically.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 551
        }
      },
      {
        "header": "Provisioning",
        "content": "There are multiple options for provisioning Amazon ECS:\n\nAWS Management Console â Provides a web interface that you can use to access your Amazon ECS resources. AWS Command Line Interface (AWS CLI) â Provides commands for a broad set of AWS services, including Amazon ECS. It's supported on Windows, Mac, and Linux. For more information, see AWS Command Line Interface . AWS SDKs â Provides language-specific APIs and takes care of many of the connection details. These include calculating signatures, handling request retries, and error handling. For more information, see AWS SDKs . AWS CDK â Provides an open-source software development framework that you can use to model and provision your cloud application resources using familiar programming languages. The AWS CDK provisions your resources in a safe, repeatable manner through AWS CloudFormation.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 863
        }
      },
      {
        "header": "Pricing",
        "content": "Amazon ECS pricing depends on the capacity option you choose for your containers.\n\nAmazon ECS pricing â Pricing information for Amazon ECS. AWS Fargate pricing â Pricing information for Fargate.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 198
        }
      },
      {
        "header": "Related services",
        "content": "Services to use with Amazon ECS You can use other AWS services to help you deploy yours tasks and services on Amazon ECS. Amazon EC2 Auto Scaling Helps ensure you have the correct number of Amazon EC2 instances available to handle the load for your application. Amazon CloudWatch Monitor your services and tasks. Amazon Elastic Container Registry Store and manage container images. Elastic Load Balancing Automatically distribute incoming service traffic. Amazon GuardDuty Detect potentially unauthorized or malicious use of your container instances and workloads.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 564
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "Learn how to create and use Amazon ECS resources",
    "summary": "The following guides provide an introduction to the tools available to access Amazon ECS and introductory procedures to run containers. Docker basics takes you through the basic steps to create a Docker container image and upload it to an Amazon ECR private repository. The getting started guides walk you through using the AWS Copilot command line interface and the AWS Management Console to complete the common tasks to run your containers on Amazon ECS and AWS Fargate. Set up",
    "sections": [
      {
        "header": "",
        "content": "Learn how to create and use Amazon ECS resources The following guides provide an introduction to the tools available to access Amazon ECS and introductory procedures to run containers. Docker basics takes you through the basic steps to create a Docker container image and upload it to an Amazon ECR private repository. The getting started guides walk you through using the AWS Copilot command line interface and the AWS Management Console to complete the common tasks to run your containers on Amazon ECS and AWS Fargate. Contents Set up Creating a container image Learn how to create a task for Amazon ECS Managed Instances Learn how to create a task for Amazon ECS Managed Instances with the AWS CLI Learn how to create a Linux task for Fargate Learn how to create a Windows task for Fargate Learn how to create a Windows task for EC2 Using the AWS CDK Creating resources using the AWS Copilot CLI Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions What is Amazon ECS? Set up",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1139
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/getting-started.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "Amazon ECS task definitions",
    "summary": "A task definition is a blueprint for your application. It is a text file in JSON format that describes the parameters and one or more containers that form your application. The following are some of the parameters that you can specify in a task definition:",
    "sections": [
      {
        "header": "",
        "content": "Amazon ECS task definitions A task definition is a blueprint for your application. It is a text file in JSON format that describes the parameters and one or more containers that form your application. The following are some of the parameters that you can specify in a task definition: The capacity to use, which determines the infrastructure that your tasks are hosted on The Docker image to use with each container in your task How much CPU and memory to use with each task or each container within a task The memory and CPU requirements The operating system of the container that the task runs on The Docker networking mode to use for the containers in your task The logging configuration to use for your tasks Whether the task continues to run if the container finishes or fails The command that the container runs when it's started Any data volumes that are used with the containers in the task The IAM role that your tasks use For a complete list of task definition parameters, see Amazon ECS task definition parameters for Fargate . After you create a task definition, you can run the task definition as a task or a service. A task is the instantiation of a task definition within a cluster. After you create a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster. An Amazon ECS service runs and maintains your desired number of tasks simultaneously in an Amazon ECS cluster. How it works is that, if any of your tasks fail or stop for any reason, the Amazon ECS service scheduler launches another instance based on your task definition. It does this to replace it and thereby maintain your desired number of tasks in the service. Topics Amazon ECS task definition states Architect your application for Amazon ECS Creating an Amazon ECS task definition using the console Using Amazon Q Developer to provide task definition recommendations in the Amazon ECS console Updating an Amazon ECS task definition using the console Deregistering an Amazon ECS task definition revision using the console Deleting an Amazon ECS task definition revision using the console Amazon ECS task definition use cases Amazon ECS task definition parameters for Amazon ECS Managed Instances Amazon ECS task definition parameters for Fargate Amazon ECS task definition parameters for Amazon EC2 Amazon ECS task definition template Example Amazon ECS task definitions Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Amazon ECS Anywhere Task definition states",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 12,
          "content_length": 2665
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "Amazon ECS services",
    "summary": "You can use an Amazon ECS service to run and maintain a specified number of instances of a task definition simultaneously in an Amazon ECS cluster. If one of your tasks fails or stops, the Amazon ECS service scheduler launches another instance of your task definition to replace it. This helps maintain your desired number of tasks in the service. You can also optionally run your service behind a load balancer. The load balancer distributes traffic across the tasks that are associated with the service.",
    "sections": [
      {
        "header": "Infrastructure compute option",
        "content": "There are two compute options that distribute your tasks.\n\nA capacity provider strategy causes Amazon ECS to distribute your tasks in one or across multiple capacity providers. If you want to run your workloads on Amazon ECS Managed Instances, you must use the Capacity provider strategy option. For the capacity provider strategy , the console selects a compute option by default. The following describes the order that the console uses to select a default: If your cluster has a default capacity provider strategy defined, it is selected. If your cluster doesn't have a default capacity provider strategy defined but you have the Fargate capacity providers added to the cluster, a custom capacity provider strategy that uses the FARGATE capacity provider is selected. If your cluster doesn't have a default capacity provider strategy defined but you have one or more Auto Scaling group capacity providers added to the cluster, the Use custom (Advanced) option is selected and you need to manually define the strategy. If your cluster doesn't have a default capacity provider strategy defined and no capacity providers added to the cluster, the Fargate launch type is selected. A launch type causes Amazon ECS to launch our tasks directly on either Fargate or on the EC2 instances registered to your clusters. If you want to run your workloads on Amazon ECS Managed Instances, you must use the Capacity provider strategy option. By default the service starts in the subnets in your cluster VPC.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1495
        }
      },
      {
        "header": "Service auto scaling",
        "content": "Service auto scaling is the ability to increase or decrease the desired number of tasks in your Amazon ECS service automatically. Amazon ECS leverages the Application Auto Scaling service to provide this functionality.\n\nFor more information, see Automatically scale your Amazon ECS service .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 291
        }
      },
      {
        "header": "Service load balancing",
        "content": "Amazon ECS services hosted on AWS Fargate support the Application Load Balancers, Network Load Balancers, and Gateway Load Balancers. Use the following table to learn about what type of load balancer to use.\n\nLoad Balancer type Use in these cases Application Load Balancer Route HTTP/HTTPS (or layer 7) traffic. Application Load Balancers offer several features that make them attractive for use with Amazon ECS services: Each service can serve traffic from multiple load balancers and expose multiple load balanced ports by specifying multiple target groups. They are supported by tasks hosted on both Fargate and EC2 instances. Application Load Balancers allow containers to use dynamic host port mapping (so that multiple tasks from the same service are allowed per container instance). Application Load Balancers support path-based routing and priority rules (so that multiple services can use the same listener port on a single Application Load Balancer). Network Load Balancer Route TCP or UDP (or layer 4) traffic. Gateway Load Balancer Route TCP or UDP (or layer 4) traffic. Use virtual appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems.\n\nFor more information, see Use load balancing to distribute Amazon ECS service traffic .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1294
        }
      },
      {
        "header": "Interconecting services",
        "content": "If you need an application to connect to other applications that run as Amazon ECS services, Amazon ECS provides the following ways to do this without a load balancer:\n\nService Connect - Allows for service-to-service communications with automatic discovery using short names and standard ports. Service discovery - Service discovery uses RouteÂ 53 to create a namespace for your service, which allows it to be discoverable through DNS. Amazon VPC Lattice - VPC Lattice is a fully managed application networking service to connect, secure, and monitor your services across multiple accounts and VPCs. There is a cost associated with it.\n\nFor more information, see Interconnect Amazon ECS services .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 697
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Amazon ECS clusters",
    "summary": "An Amazon ECS cluster is a logical grouping of tasks or services that provides the infrastructure capacity for your containerized applications. When creating a cluster, you choose from the three primary infrastructure types, each optimized for different use cases and operational requirements. Amazon ECS offers three infrastructure types for your clusters. Choose the type that best matches your workload requirements, operational preferences, and cost optimization goals:",
    "sections": [
      {
        "header": "Choosing the right cluster type",
        "content": "Amazon ECS offers three infrastructure types for your clusters. Choose the type that best matches your workload requirements, operational preferences, and cost optimization goals:\n\nAmazon ECS Managed Instances (Recommended) Best for most workloads - AWS fully manages the underlying Amazon EC2 instances, including provisioning, patching, and scaling. This option provides the optimal balance of performance, cost-effectiveness, and operational simplicity. Use when: You want AWS to handle infrastructure management You need cost-effective compute with automatic optimization You want to focus on your applications rather than infrastructure You need predictable performance with flexible scaling Fargate Serverless compute - Pay only for the resources your tasks use without managing any infrastructure. Ideal for variable workloads and getting started quickly. Use when: You want completely serverless operations You have unpredictable or variable workloads You want to minimize operational overhead You need rapid deployment and scaling Amazon EC2 instances Full control - You manage the underlying Amazon EC2 instances directly, including instance selection, configuration, and maintenance. Use when: You need specific instance types or configurations You have existing Amazon EC2 infrastructure to leverage You require custom AMIs or specialized software You need maximum control over the underlying infrastructure\n\nNote Amazon ECS Managed Instances is the recommended choice for most new workloads as it provides the best combination of performance, cost optimization, and operational simplicity while allowing AWS to handle infrastructure management tasks.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1663
        }
      },
      {
        "header": "Cluster components",
        "content": "In addition to the infrastructure capacity, a cluster consists of the following resources:\n\nThe network (VPC and subnet) where your tasks and services run When you use Amazon ECS Managed Instances or Amazon EC2 instances for the capacity, the subnet can be in Availability Zones, Local Zones, Wavelength Zones, or AWS Outposts. An optional namespace A namespace is used for service-to-service communication with Service Connect. A monitoring option CloudWatch Container Insights comes at an additional cost and is a fully managed service. It automatically collects, aggregates, and summarizes Amazon ECS metrics and logs.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 621
        }
      },
      {
        "header": "Cluster concepts",
        "content": "The following are general concepts about Amazon ECS clusters.\n\nYou create clusters to separate your resources. Clusters are AWS Region specific. Clusters can be in any of the following states. ACTIVE The cluster is ready to accept tasks and, if applicable, you can register container instances with the cluster. PROVISIONING The cluster has capacity providers associated with it and the resources needed for the capacity provider are being created. DEPROVISIONING The cluster has capacity providers associated with it and the resources needed for the capacity provider are being deleted. FAILED The cluster has capacity providers associated with it and the resources needed for the capacity provider have failed to create. INACTIVE The cluster has been deleted. Clusters with an INACTIVE status may remain discoverable in your account for a period of time. This behavior is subject to change in the future, so make sure you do not rely on INACTIVE clusters persisting. A cluster can contain a mix of tasks that are hosted on Amazon ECS Managed Instances, AWS Fargate, Amazon EC2 instances, or external instances. Tasks can run on Amazon ECS Managed Instances, Fargate or EC2 infrastructure as a launch type or a capacity provider strategy. If you use EC2 capacity providers, Amazon ECS doesn't track and scale the capacity of Amazon EC2 Auto Scaling groups. A cluster can contain a mix of Amazon ECS Managed Instances capacity providers, Auto Scaling group capacity providers, and Fargate capacity providers. A capacity provider strategy can only contain Amazon ECS Managed Instances capacity providers, Auto Scaling group capacity providers, or Fargate capacity providers. You can use different instance types for the Amazon ECS Managed Instances and EC2, or Auto Scaling group capacity providers. An instance can only be registered to one cluster at a time. You can restrict access to clusters by creating custom IAM policies. For information, see Amazon ECS cluster examples section in Identity-based policy examples for Amazon Elastic Container Service . You can use Service Auto Scaling to scale Fargate tasks. For more information, see Automatically scale your Amazon ECS service . You can configure a default Service Connect namespace for a cluster. After you set a default Service Connect namespace, any new services created in the cluster can be added as client services in the namespace by turning on Service Connect. No additional configuration is required. For more information, see Use Service Connect to connect Amazon ECS services with short names .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 2564
        }
      },
      {
        "header": "Capacity providers",
        "content": "Amazon ECS capacity providers manage the scaling of infrastructure for tasks in your clusters. Each cluster can have one or more capacity providers and an optional capacity provider strategy. You can assign a default capacity provider strategy to the cluster. The capacity provider strategy determines how the tasks are spread across the cluster's capacity providers. When you run a standalone task or create a service, you either use the cluster's default capacity provider strategy or a capacity provider strategy that overrides the default one. The cluster's default capacity provider strategy only applies when you don't specify a launch type, or capacity provider strategy for your task or service. If you provide either of these parameters, the default strategy isn't used.\n\nAmazon ECS offers three types of capacity providers for your clusters:\n\nAmazon ECS Managed Instances capacity providers AWS fully manages the underlying Amazon EC2 instances, including provisioning, patching, scaling, and lifecycle management. This provides the optimal balance of performance, cost-effectiveness, and operational simplicity. Amazon ECS Managed Instances capacity providers automatically optimize instance selection and scaling based on your workload requirements. With Amazon ECS Managed Instances, you benefit from: Automatic instance provisioning and scaling Managed patching and security updates Cost optimization through intelligent instance selection Reduced operational overhead Fargate capacity providers Serverless compute where you pay only for the resources your tasks use without managing any infrastructure. You just need to associate the pre-defined capacity providers (Fargate and Fargate Spot) with the cluster. Auto Scaling group capacity providers When you use Amazon EC2 instances for your capacity, you use Auto Scaling group to manage the Amazon EC2 instances. Auto Scaling helps ensure that you have the correct number of Amazon EC2 instances available to handle the application load. You have full control over the underlying infrastructure.\n\nA cluster can contain a mix of tasks that are hosted on Amazon ECS Managed Instances, AWS Fargate, Amazon EC2 instances, or external instances. Tasks can run on Amazon ECS Managed Instances, Fargate or EC2 infrastructure as a launch type or a capacity provider strategy. If you use EC2 as a launch type, Amazon ECS doesn't track and scale the capacity of Amazon EC2 Auto Scaling groups.\n\nA cluster can contain a mix of Amazon ECS Managed Instances capacity providers, Auto Scaling group capacity providers, and Fargate capacity providers. A capacity provider strategy can only contain Amazon ECS Managed Instances capacity providers, Auto Scaling group capacity providers, or Fargate capacity providers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 2766
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusters.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "What is Amazon Elastic Container Registry?",
    "summary": "Amazon Elastic Container Registry (Amazon ECR) is an AWS managed container image registry service that is secure, scalable, and reliable. Amazon ECR supports private repositories with resource-based permissions using AWS IAM. This is so that specified users or Amazon EC2 instances can access your container repositories and images. You can use your preferred CLI to push, pull, and manage Docker images, Open Container Initiative (OCI) images, and OCI compatible artifacts. Amazon ECR supports public container image repositories as well. For more information, see What is Amazon ECR Public in the Amazon ECR Public User Guide .",
    "sections": [
      {
        "header": "Features of Amazon ECR",
        "content": "Amazon ECR provides the following features:\n\nLifecycle policies help with managing the lifecycle of the images in your repositories. You define rules that result in the cleaning up of unused images. You can test rules before applying them to your repository. For more information, see Automate the cleanup of images by using lifecycle policies in Amazon ECR . Image scanning helps in identifying software vulnerabilities in your container images. Each repository can be configured to scan on push . This ensures that each new image pushed to the repository is scanned. You can then retrieve the results of the image scan. For more information, see Scan images for software vulnerabilities in Amazon ECR . Cross-Region and cross-account replication makes it easier for you to have your images where you need them. This is configured as a registry setting and is on a per-Region basis. For more information, see Private registry settings in Amazon ECR . Pull through cache rules provide a way to cache repositories in an upstream registry in your private Amazon ECR registry. Using a pull through cache rule, Amazon ECR will periodically reach out to the upstream registry to ensure the cached image in your Amazon ECR private registry is up to date. For more information, see Sync an upstream registry with an Amazon ECR private registry . Repository creation templates allow you to define the settings for repositories created by Amazon ECR on your behalf during pull through cache or replication actions. You can specify tag immutability, encryption configuration, repository policies, lifecycle policies, and resource tags for automatically created repositories. For more information, see Templates to control repositories created during a pull through cache or replication action .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1784
        }
      },
      {
        "header": "How to get started with Amazon ECR",
        "content": "If you are using Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS), note that the setup for those two services is similar to the setup for Amazon ECR because Amazon ECR is an extension of both services.\n\nWhen using the AWS Command Line Interface with Amazon ECR, use a version of the AWS CLI that supports the latest Amazon ECR features. If you don't see support for an Amazon ECR feature in the AWS CLI, upgrade to the latest version of the AWS CLI. For information about installing the latest version of the AWS CLI, see Install or update to the latest version of the AWS CLI in the AWS Command Line Interface User Guide .\n\nTo learn how to push a container image to a private Amazon ECR repository using the AWS CLI and Docker, see Moving an image through its lifecycle in Amazon ECR .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 833
        }
      },
      {
        "header": "Pricing for Amazon ECR",
        "content": "With Amazon ECR, you only pay for the amount of data you store in your repositories and for the data transfer from your image pushes and pulls. For more information, see Amazon ECR pricing .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 190
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html",
    "doc_type": "aws",
    "total_sections": 3
  },
  {
    "title": "Moving an image through its lifecycle in Amazon ECR",
    "summary": "If you are using Amazon ECR for the first time, use the following steps with the Docker CLI and the AWS CLI to create a sample image, authenticate to the default registry, and create a private repository. Then push an image to and pull an image from the private repository. When you are finished with the sample image, delete the sample image and the repository. To use the AWS Management Console instead of the AWS CLI, see Creating an Amazon ECR private repository to store images .",
    "sections": [
      {
        "header": "Prerequisites",
        "content": "If you do not have the latest AWS CLI and Docker installed and ready to use, use the following steps to install both of these tools.\n\nInstall the AWS CLI\n\nTo use the AWS CLI with Amazon ECR, install the latest AWS CLI version. For information, see Installing the AWS Command Line Interface in the AWS Command Line Interface User Guide .\n\nInstall Docker\n\nDocker is available on many different operating systems, including most modern Linux distributions, like Ubuntu, and even macOS and Windows. For more information about how to install Docker on your particular operating system, go to the Docker installation guide .\n\nYou do not need a local development system to use Docker. If you are using Amazon EC2 already, you can launch an Amazon Linux 2023 instance and install Docker to get started.\n\nIf you already have Docker installed, skip to Step 1: Create a Docker image .\n\nTo install Docker on an Amazon EC2 instance using an Amazon Linux 2023 AMI Launch an instance with the latest Amazon Linux 2023 AMI. For more information, see Launching an instance in the Amazon EC2 User Guide . Connect to your instance. For more information, see Connect to Your Linux Instance in the Amazon EC2 User Guide . Update the installed packages and package cache on your instance. sudo yum update -y Install the most recent Docker Community Edition package. sudo yum install docker Start the Docker service. sudo service docker start Add the ec2-user to the docker group so you can execute Docker commands without using sudo . sudo usermod -a -G docker ec2-user Log out and log back in again to pick up the new docker group permissions. You can accomplish this by closing your current SSH terminal window and reconnecting to your instance in a new one. Your new SSH session will have the appropriate docker group permissions. Verify that the ec2-user can run Docker commands without sudo . docker info Note In some cases, you may need to reboot your instance to provide permissions for the ec2-user to access the Docker daemon. Try rebooting your instance if you see the following error: Cannot connect to the Docker daemon. Is the docker daemon running on this host?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2153
        }
      },
      {
        "header": "Install the AWS CLI",
        "content": "To use the AWS CLI with Amazon ECR, install the latest AWS CLI version. For information, see Installing the AWS Command Line Interface in the AWS Command Line Interface User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 181
        }
      },
      {
        "header": "Install Docker",
        "content": "Docker is available on many different operating systems, including most modern Linux distributions, like Ubuntu, and even macOS and Windows. For more information about how to install Docker on your particular operating system, go to the Docker installation guide .\n\nYou do not need a local development system to use Docker. If you are using Amazon EC2 already, you can launch an Amazon Linux 2023 instance and install Docker to get started.\n\nIf you already have Docker installed, skip to Step 1: Create a Docker image .\n\nTo install Docker on an Amazon EC2 instance using an Amazon Linux 2023 AMI Launch an instance with the latest Amazon Linux 2023 AMI. For more information, see Launching an instance in the Amazon EC2 User Guide . Connect to your instance. For more information, see Connect to Your Linux Instance in the Amazon EC2 User Guide . Update the installed packages and package cache on your instance. sudo yum update -y Install the most recent Docker Community Edition package. sudo yum install docker Start the Docker service. sudo service docker start Add the ec2-user to the docker group so you can execute Docker commands without using sudo . sudo usermod -a -G docker ec2-user Log out and log back in again to pick up the new docker group permissions. You can accomplish this by closing your current SSH terminal window and reconnecting to your instance in a new one. Your new SSH session will have the appropriate docker group permissions. Verify that the ec2-user can run Docker commands without sudo . docker info Note In some cases, you may need to reboot your instance to provide permissions for the ec2-user to access the Docker daemon. Try rebooting your instance if you see the following error: Cannot connect to the Docker daemon. Is the docker daemon running on this host?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1799
        }
      },
      {
        "header": "Step 1: Create a Docker image",
        "content": "In this step, you create a Docker image of a simple web application, and test it on your local system or Amazon EC2 instance.\n\nTo create a Docker image of a simple web application Create a file called Dockerfile . A Dockerfile is a manifest that describes the base image to use for your Docker image and what you want installed and running on it. For more information about Dockerfiles, go to the Dockerfile Reference . touch Dockerfile Edit the Dockerfile you just created and add the following content. FROM public.ecr.aws/amazonlinux/amazonlinux:latest # Install dependencies RUN yum update -y && \\ yum install -y httpd # Install apache and write hello world message RUN echo 'Hello World!' > /var/www/html/index.html # Configure apache RUN echo 'mkdir -p /var/run/httpd' >> /root/run_apache.sh && \\ echo 'mkdir -p /var/lock/httpd' >> /root/run_apache.sh && \\ echo '/usr/sbin/httpd -D FOREGROUND' >> /root/run_apache.sh && \\ chmod 755 /root/run_apache.sh EXPOSE 80 CMD /root/run_apache.sh This Dockerfile uses the public Amazon Linux 2 image hosted on Amazon ECR Public. The RUN instructions update the package caches, installs some software packages for the web server, and then write the \"Hello World!\" content to the web servers document root. The EXPOSE instruction exposes port 80 on the container, and the CMD instruction starts the web server. Build the Docker image from your Dockerfile. Note Some versions of Docker may require the full path to your Dockerfile in the following command, instead of the relative path shown below. docker build -t hello-world . List your container image. docker images --filter reference=hello-world Output: REPOSITORY TAG IMAGE ID CREATED SIZE hello-world latest e9ffedc8c286 4 minutes ago 194MB Run the newly built image. The -p 80:80 option maps the exposed port 80 on the container to port 80 on the host system. For more information about docker run , go to the Docker run reference . docker run -t -i -p 80:80 hello-world Note Output from the Apache web server is displayed in the terminal window. You can ignore the \" Could not reliably determine the fully qualified domain name \" message. Open a browser and point to the server that is running Docker and hosting your container. If you are using an EC2 instance, this is the Public DNS value for the server, which is the same address you use to connect to the instance with SSH. Make sure that the security group for your instance allows inbound traffic on port 80. If you are running Docker locally, point your browser to http://localhost/ . If you are using docker-machine on a Windows or Mac computer, find the IP address of the VirtualBox VM that is hosting Docker with the docker-machine ip command, substituting machine-name with the name of the docker machine you are using. docker-machine ip machine-name You should see a web page with your \"Hello World!\" statement. Stop the Docker container by typing Ctrl + c .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 2922
        }
      },
      {
        "header": "Step 2: Create a repository",
        "content": "Now that you have an image to push to Amazon ECR, you must create a repository to hold it. In this example, you create a repository called hello-repository to which you later push the hello-world:latest image. To create a repository, run the following command:",
        "code_examples": [
          "aws ecr create-repository \\\n    --repository-name\nhello-repository\n\\\n    --region\nregion"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 260
        }
      },
      {
        "header": "Step 3: Authenticate to your default registry",
        "content": "After you have installed and configured the AWS CLI, authenticate the Docker CLI to your default registry. That way, the docker command can push and pull images with Amazon ECR. The AWS CLI provides a get-login-password command to simplify the authentication process.\n\nTo authenticate Docker to an Amazon ECR registry with get-login-password, run the aws ecr get-login-password command. When passing the authentication token to the docker login command, use the value AWS for the username and specify the Amazon ECR registry URI you want to authenticate to. If authenticating to multiple registries, you must repeat the command for each registry. Important If you receive an error, install or upgrade to the latest version of the AWS CLI. For more information, see Installing the AWS Command Line Interface in the AWS Command Line Interface User Guide . get-login-password (AWS CLI) aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id .dkr.ecr. region .amazonaws.com Get-ECRLoginCommand (AWS Tools for Windows PowerShell) (Get-ECRLoginCommand).Password | docker login --username AWS --password-stdin aws_account_id .dkr.ecr. region .amazonaws.com",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1199
        }
      },
      {
        "header": "Step 4: Push an image to Amazon ECR",
        "content": "Now you can push your image to the Amazon ECR repository you created in the previous section. Use the docker CLI to push images after the following prerequisites are met:\n\nThe minimum version of docker is installed: 1.7. The Amazon ECR authorization token was configured with docker login . The Amazon ECR repository exists and the user has access to push to the repository.\n\nAfter those prerequisites are met, you can push your image to your newly created repository in the default registry for your account.\n\nTo tag and push an image to Amazon ECR List the images you have stored locally to identify the image to tag and push. docker images Output: REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE hello-world latest e9ffedc8c286 4 minutes ago 241MB Tag the image to push to your repository. docker tag hello-world:latest aws_account_id .dkr.ecr. region .amazonaws.com/hello-repository Push the image. docker push aws_account_id .dkr.ecr. region .amazonaws.com/hello-repository:latest Output: The push refers to a repository [ aws_account_id .dkr.ecr. region .amazonaws.com/hello-repository] (len: 1) e9ae3c220b23: Pushed a6785352b25c: Pushed 0998bf8fb9e9: Pushed 0a85502c06c9: Pushed latest: digest: sha256:215d7e4121b30157d8839e81c4e0912606fca105775bb0636EXAMPLE size: 6774",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1273
        }
      },
      {
        "header": "Step 5: Pull an image from Amazon ECR",
        "content": "After your image is pushed to your Amazon ECR repository, you can pull it from other locations. Use the docker CLI to pull images after the following prerequisites are met:\n\nThe minimum version of docker is installed: 1.7. The Amazon ECR authorization token was configured with docker login . The Amazon ECR repository exists and the user has access to pull from the repository.\n\nAfter those prerequisites are met, you can pull your image. To pull your example image from Amazon ECR, run the following command:\n\nOutput:",
        "code_examples": [
          "docker pull\naws_account_id\n.dkr.ecr.\nregion\n.amazonaws.com/hello-repository:latest",
          "latest: Pulling from hello-repository\n0a85502c06c9: Pull complete\n0998bf8fb9e9: Pull complete\na6785352b25c: Pull complete\ne9ae3c220b23: Pull complete\nDigest: sha256:215d7e4121b30157d8839e81c4e0912606fca105775bb0636EXAMPLE\nStatus: Downloaded newer image for\naws_account_id\n.dkr.\nregion\n.amazonaws.com/hello-repository:latest"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 519
        }
      },
      {
        "header": "Step 6: Delete an image",
        "content": "If you no longer need an image in one of your repositories, you can delete the image. To delete an image, specify the repository that it's in and either an imageTag or imageDigest value for the image. The following example deletes an image in the hello-repository repository with the image tag latest . To delete your example image from the repository, run the following command:",
        "code_examples": [
          "aws ecr batch-delete-image \\\n      --repository-name hello-repository \\\n      --image-ids imageTag=latest \\\n      --region\nregion"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 379
        }
      },
      {
        "header": "Step 7: Delete a repository",
        "content": "If you no longer need an entire repository of images, you can delete the repository. The following example uses the --force flag to delete a repository that contains images. To delete a repository that contains images (and all the images within it), run the following command:",
        "code_examples": [
          "aws ecr delete-repository \\\n      --repository-name hello-repository \\\n      --force \\\n      --region\nregion"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 276
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/getting-started-cli.html",
    "doc_type": "aws",
    "total_sections": 10
  },
  {
    "title": "Pushing a Docker image to an Amazon ECR private repository",
    "summary": "You can push your container images to an Amazon ECR repository with the docker push command. Amazon ECR also supports creating and pushing Docker manifest lists that are used for multi-architecture images. For information, see Pushing a multi-architecture image to an Amazon ECR private repository .",
    "sections": [
      {
        "header": "",
        "content": "Pushing a Docker image to an Amazon ECR private repository You can push your container images to an Amazon ECR repository with the docker push command. Amazon ECR also supports creating and pushing Docker manifest lists that are used for multi-architecture images. For information, see Pushing a multi-architecture image to an Amazon ECR private repository . To push a Docker image to an Amazon ECR repository The Amazon ECR repository must exist before you push the image. For more information, see Creating an Amazon ECR private repository to store images . Authenticate your Docker client to the Amazon ECR registry to which you intend to push your image. Authentication tokens must be obtained for each registry used, and the tokens are valid for 12 hours. For more information, see Private registry authentication in Amazon ECR . To authenticate Docker to an Amazon ECR registry, run the aws ecr get-login-password command. When passing the authentication token to the docker login command, use the value AWS for the username and specify the Amazon ECR registry URI you want to authenticate to. If authenticating to multiple registries, you must repeat the command for each registry. Important If you receive an error, install or upgrade to the latest version of the AWS CLI. For more information, see Installing the AWS Command Line Interface in the AWS Command Line Interface User Guide . aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin < aws_account_id > .dkr.ecr. <region> .amazonaws.com If your image repository doesn't exist in the registry you intend to push to yet, create it. For more information, see Creating an Amazon ECR private repository to store images . Identify the local image to push. Run the docker images command to list the container images on your system. docker images You can identify an image with the repository:tag value or the image ID in the resulting command output. Tag your image with the Amazon ECR registry, repository, and optional image tag name combination to use. The registry format is aws_account_id .dkr.ecr. region .amazonaws.com . The repository name should match the repository that you created for your image. If you omit the image tag, we assume that the tag is latest . The following example tags a local image with the ID e9ae3c220b23 as aws_account_id .dkr.ecr. region .amazonaws.com /my-repository:tag . docker tag e9ae3c220b23 aws_account_id .dkr.ecr. region .amazonaws.com / my-repository:tag Push the image using the docker push command: docker push aws_account_id .dkr.ecr. region .amazonaws.com / my-repository:tag (Optional) Apply any additional tags to your image and push those tags to Amazon ECR by repeating StepÂ 4 and StepÂ 5 . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Required IAM permissions Pushing a multi-architecture image",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 32,
          "content_length": 3013
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "AWS Lambda",
    "summary": "",
    "sections": [],
    "url": "https://docs.aws.amazon.com/lambda/latest/dg/",
    "doc_type": "aws",
    "total_sections": 0
  },
  {
    "title": "Managing permissions in AWS Lambda",
    "summary": "You can use AWS Identity and Access Management (IAM) to manage permissions in AWS Lambda. There are two main categories of permissions that you need to consider when working with Lambda functions: Permissions that your Lambda functions need to perform API actions and access other AWS resources",
    "sections": [
      {
        "header": "",
        "content": "Managing permissions in AWS Lambda You can use AWS Identity and Access Management (IAM) to manage permissions in AWS Lambda. There are two main categories of permissions that you need to consider when working with Lambda functions: Permissions that your Lambda functions need to perform API actions and access other AWS resources Permissions that other AWS users and entities need to access your Lambda functions Lambda functions often need to access other AWS resources, and perform various API operations on those resources. For example, you might have a Lambda function that responds to an event by updating entries in an Amazon DynamoDB database. In this case, your function needs permissions to access the database, as well as permissions to put or update items in that database. You define the permissions that your Lambda function needs in a special IAM role called an execution role . In this role, you can attach a policy that defines every permission your function needs to access other AWS resources, and read from event sources. Every Lambda function must have an execution role. At a minimum, your execution role must have access to Amazon CloudWatch because Lambda functions log to CloudWatch Logs by default. You can attach the AWSLambdaBasicExecutionRole managed policy to your execution role to satisfy this requirement. To give other AWS accounts, organizations, and services permissions to access your Lambda resources, you have a few options: You can use identity-based policies to grant other users access to your Lambda resources. Identity-based policies can apply to users directly, or to groups and roles that are associated with a user. You can use resource-based policies to give other accounts and AWS services permissions to access your Lambda resources. When a user tries to access a Lambda resource, Lambda considers both the user's identity-based policies and the resource's resource-based policy. When an AWS service such as Amazon Simple Storage Service (Amazon S3) calls your Lambda function, Lambda considers only the resource-based policy. You can use an attribute-based access control (ABAC) model to control access to your Lambda functions. With ABAC, you can attach tags to a Lambda function, pass them in certain API requests, or attach them to the IAM principal making the request. Specify the same tags in the condition element of an IAM policy to control function access. In AWS, it's a best practice to grant only the permissions required to perform a task ( least-privilege permissions ). To implement this in Lambda, we recommend starting with an AWS managed policy . You can use these managed policies as-is, or as a starting point for writing your own more restrictive policies. To help you fine-tune your permissions for least-privilege access, Lambda provides some additional conditions you can include in your policies. For more information, see Fine-tuning the Resources and Conditions sections of policies . For more information about IAM, see the IAM User Guide . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Tutorial Execution role (permissions for functions to access other resources)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 26,
          "content_length": 3308
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/lambda/latest/dg/lambda-permissions.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "Amazon EC2 instance types",
    "summary": "When you launch an instance, the instance type that you specify determines the hardware of the host computer used for your instance. Each instance type offers different compute, memory, and storage capabilities, and is grouped in an instance family based on these capabilities. Select an instance type based on the requirements of the application or software that you plan to run on your instance. For more information about features and use cases, see Amazon EC2 Instance Types . Amazon EC2 dedicates some resources of the host computer, such as CPU, memory, and instance storage, to a particular instance. Amazon EC2 shares other resources of the host computer, such as the network and the disk subsystem, among instances. If each instance on a host computer tries to use as much of one of these shared resources as possible, each receives an equal share of that resource. However, when a resource is underused, an instance can consume a higher share of that resource while it's available.",
    "sections": [
      {
        "header": "Available instance types",
        "content": "Amazon EC2 provides a wide selection of instance types optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.\n\nInstance type naming conventions\n\nNames are based on instance family, generation, processor family, capabilities, and size. For more information, see Naming conventions in the Amazon EC2 Instance Types Guide .\n\nFind an instance type\n\nTo determine which instance types meet your requirements, such as supported Regions, compute resources, or storage resources, see Find an Amazon EC2 instance type and Amazon EC2 instance type specifications in the Amazon EC2 Instance Types Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 902
        }
      },
      {
        "header": "Hardware specifications",
        "content": "For detailed instance type specifications, see Specifications in the Amazon EC2 Instance Types Guide . For pricing information, see Amazon EC2 On-Demand Pricing .\n\nTo determine which instance type best meets your needs, we recommend that you launch an instance and use your own benchmark application. Because you pay by the instance second, it's convenient and inexpensive to test multiple instance types before making a decision. If your needs change, even after you make a decision, you can change the instance type later. For more information, see Amazon EC2 instance type changes .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 585
        }
      },
      {
        "header": "Hypervisor type",
        "content": "Amazon EC2 supports the following hypervisors: Xen and Nitro.\n\nNitro-based instances General purpose: M5 | M5a | M5ad | M5d | M5dn | M5n | M5zn | M6a | M6g | M6gd | M6i | M6id | M6idn | M6in | M7a | M7g | M7gd | M7i | M7i-flex | M8a | M8g | M8gd | M8i | M8i-flex | T3 | T3a | T4g Compute optimized: C5 | C5a | C5ad | C5d | C5n | C6a | C6g | C6gd | C6gn | C6i | C6id | C6in | C7a | C7g | C7gd | C7gn | C7i | C7i-flex | C8g | C8gd | C8gn | C8i | C8i-flex Memory optimized: R5 | R5a | R5ad | R5b | R5d | R5dn | R5n | R6a | R6g | R6gd | R6i | R6id | R6idn | R6in | R7a | R7g | R7gd | R7i | R7iz | R8a | R8g | R8gb | R8gd | R8gn | R8i | R8i-flex | U-3tb1 | U-6tb1 | U-9tb1 | U-12tb1 | U-18tb1 | U-24tb1 | U7i-6tb | U7i-8tb | U7i-12tb | U7in-16tb | U7in-24tb | U7in-32tb | U7inh-32tb | X2gd | X2idn | X2iedn | X2iezn | X8g | z1d Storage optimized: D3 | D3en | I3en | I4g | I4i | I7i | I7ie | I8g | I8ge | Im4gn | Is4gen Accelerated computing: DL1 | DL2q | F2 | G4ad | G4dn | G5 | G5g | G6 | G6e | G6f | Gr6 | Gr6f | Inf1 | Inf2 | P4d | P4de | P5 | P5e | P5en | P6-B200 | P6-B300 | P6e-GB200 | Trn1 | Trn1n | Trn2 | Trn2u | VT1 High-performance computing: Hpc6a | Hpc6id | Hpc7a | Hpc7g Previous generation: A1 | P3dn\n\nFor more information about the supported versions of Nitro hypervisor, see Network feature support in the Amazon EC2 Instance Types Guide .\n\nXen-based instances General purpose : M1 | M2 | M3 | M4 | T1 | T2 Compute optimized : C1 | C3 | C4 Memory optimized : R3 | R4 | X1 | X1e Storage optimized : D2 | H1 | I2 | I3 Accelerated computing : F1 | G3 | P2 | P3",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1569
        }
      },
      {
        "header": "AMI virtualization types",
        "content": "The virtualization type of your instance is determined by the AMI that you use to launch it. Current generation instance types support hardware virtual machine (HVM) only. Some previous generation instance types support paravirtual (PV) and some AWS Regions support PV instances. For more information, see Virtualization types .\n\nFor best performance, we recommend that you use an HVM AMI. In addition, HVM AMIs are required to take advantage of enhanced networking. HVM virtualization uses hardware-assist technology provided by the AWS platform. With HVM virtualization, the guest VM runs as if it were on a native hardware platform, except that it still uses PV network and storage drivers for improved performance.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 718
        }
      },
      {
        "header": "Processors",
        "content": "EC2 instances support a variety of processors.\n\nProcessors Intel processors AMD processors AWS Graviton processors AWS Trainium AWS Inferentia\n\nIntel processors\n\nAmazon EC2 instances that run on Intel processors might include the following processor features. Not all instances that run on Intel processors support all of these processor features. For information about which features are available for each instance type, see Amazon EC2 Instance types .\n\nIntel AES New Instructions (AES-NI) â Intel AES-NI encryption instruction set improves upon the original Advanced Encryption Standard (AES) algorithm to provide faster data protection and greater security. All current generation EC2 instances support this processor feature. Intel Advanced Vector Extensions (Intel AVX, Intel AVX2, and Intel AVX-512) â Intel AVX and Intel AVX2 are 256-bit, and Intel AVX-512 is a 512-bit instruction set extension designed for applications that are Floating Point (FP) intensive. Intel AVX instructions improve performance for applications like image and audio/video processing, scientific simulations, financial analytics, and 3D modeling and analysis. These features are only available on instances launched with HVM AMIs. Intel Turbo Boost Technology â Intel Turbo Boost Technology processors automatically run cores faster than the base operating frequency. Intel Deep Learning Boost (Intel DL Boost) â Accelerates AI deep learning use cases. The 2nd Gen Intel Xeon Scalable processors extend Intel AVX-512 with a new Vector Neural Network Instruction (VNNI/INT8) that significantly increases deep learning inference performance over previous generation Intel Xeon Scalable processors (with FP32) for image recognition/segmentation, object detection, speech recognition, language translation, recommendation systems, reinforcement learning, and more. VNNI may not be compatible with all Linux distributions. The following instances support VNNI: M5n , R5n , M5dn , M5zn , R5b , R5dn , D3 , D3en , and C6i . C5 and C5d instances support VNNI for only 12xlarge , 24xlarge , and metal instances.\n\nConfusion can result from industry naming conventions for 64-bit CPUs. Chip manufacturer Advanced Micro Devices (AMD) introduced the first commercially successful 64-bit architecture based on the Intel x86 instruction set. Consequently, the architecture is widely referred to as AMD64 regardless of the chip manufacturer. Windows and several Linux distributions follow this practice. This explains why the internal system information on an instance running Ubuntu or Windows displays the CPU architecture as AMD64 even though the instances are running on Intel hardware.\n\nAMD processors\n\nAmazon EC2 instances that run on AMD EPYC processors can help you optimize both cost and performance for your workloads. These instances might support the following processor features. Not all instances that run on AMD processors support all of these processor features. For information about which features are available for each instance type, see Amazon EC2 Instance types .\n\nAMD Secure Memory Encryption (SME) AMD Transparent Single Key Memory Encryption (TSME) AMD Advanced Vector Extensions (AVX) AMD Secure Encrypted Virtualization-Secure Nested Paging ( SEV-SNP ) Vector Neural Network Instructions (VNNI) BFloat16\n\nAWS Graviton processors\n\nAWS Graviton is a family of processors designed to deliver the best price performance for your workloads running on Amazon EC2 instances.\n\nFor more information, see Getting started with Graviton .\n\nAWS Trainium\n\nInstances powered by AWS Trainium are purpose built for high-performance, cost-effective deep learning training. You can use these instances to train natural language processing, computer vision, and recommender models used across a broad set of applications, such as speech recognition, recommendation, fraud detection, and image and video classification. Use your existing workflows in popular ML frameworks, such as PyTorch and TensorFlow.\n\nAWS Inferentia\n\nInstances powered by AWS Inferentia are designed to accelerate machine learning. They provide high performance and low latency machine learning inference. These instances are optimized for deploying deep learning (DL) models for applications, such as natural language processing, object detection and classification, content personalization and filtering, and speech recognition.\n\nThere are a variety of ways that you can get started: Use SageMaker AI, a fully-managed service that is the easiest way to get started with machine learning models. For more information, see Get Started with SageMaker AI in the Amazon SageMaker AI Developer Guide . Launch an Inf1 or Inf2 instance using the Deep Learning AMI. For more information, see AWS Inferentia with DLAMI in the AWS Deep Learning AMIs Developer Guide . Launch an Inf1 or Inf2 instance using your own AMI and install the AWS Neuron SDK , which enables you to compile, run, and profile deep learning models for AWS Inferentia. Launch a container instance using an Inf1 or Inf2 instance and an Amazon ECS-optimized AMI. For more information, see Amazon Linux 2 (Inferentia) AMIs in the Amazon Elastic Container Service Developer Guide . Create an Amazon EKS cluster with nodes running Inf1 instances. For more information, see Inferentia support in the Amazon EKS User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 5333
        }
      },
      {
        "header": "Intel processors",
        "content": "Amazon EC2 instances that run on Intel processors might include the following processor features. Not all instances that run on Intel processors support all of these processor features. For information about which features are available for each instance type, see Amazon EC2 Instance types .\n\nIntel AES New Instructions (AES-NI) â Intel AES-NI encryption instruction set improves upon the original Advanced Encryption Standard (AES) algorithm to provide faster data protection and greater security. All current generation EC2 instances support this processor feature. Intel Advanced Vector Extensions (Intel AVX, Intel AVX2, and Intel AVX-512) â Intel AVX and Intel AVX2 are 256-bit, and Intel AVX-512 is a 512-bit instruction set extension designed for applications that are Floating Point (FP) intensive. Intel AVX instructions improve performance for applications like image and audio/video processing, scientific simulations, financial analytics, and 3D modeling and analysis. These features are only available on instances launched with HVM AMIs. Intel Turbo Boost Technology â Intel Turbo Boost Technology processors automatically run cores faster than the base operating frequency. Intel Deep Learning Boost (Intel DL Boost) â Accelerates AI deep learning use cases. The 2nd Gen Intel Xeon Scalable processors extend Intel AVX-512 with a new Vector Neural Network Instruction (VNNI/INT8) that significantly increases deep learning inference performance over previous generation Intel Xeon Scalable processors (with FP32) for image recognition/segmentation, object detection, speech recognition, language translation, recommendation systems, reinforcement learning, and more. VNNI may not be compatible with all Linux distributions. The following instances support VNNI: M5n , R5n , M5dn , M5zn , R5b , R5dn , D3 , D3en , and C6i . C5 and C5d instances support VNNI for only 12xlarge , 24xlarge , and metal instances.\n\nConfusion can result from industry naming conventions for 64-bit CPUs. Chip manufacturer Advanced Micro Devices (AMD) introduced the first commercially successful 64-bit architecture based on the Intel x86 instruction set. Consequently, the architecture is widely referred to as AMD64 regardless of the chip manufacturer. Windows and several Linux distributions follow this practice. This explains why the internal system information on an instance running Ubuntu or Windows displays the CPU architecture as AMD64 even though the instances are running on Intel hardware.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 2504
        }
      },
      {
        "header": "AMD processors",
        "content": "Amazon EC2 instances that run on AMD EPYC processors can help you optimize both cost and performance for your workloads. These instances might support the following processor features. Not all instances that run on AMD processors support all of these processor features. For information about which features are available for each instance type, see Amazon EC2 Instance types .\n\nAMD Secure Memory Encryption (SME) AMD Transparent Single Key Memory Encryption (TSME) AMD Advanced Vector Extensions (AVX) AMD Secure Encrypted Virtualization-Secure Nested Paging ( SEV-SNP ) Vector Neural Network Instructions (VNNI) BFloat16",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 622
        }
      },
      {
        "header": "AWS Graviton processors",
        "content": "AWS Graviton is a family of processors designed to deliver the best price performance for your workloads running on Amazon EC2 instances.\n\nFor more information, see Getting started with Graviton .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 196
        }
      },
      {
        "header": "AWS Trainium",
        "content": "Instances powered by AWS Trainium are purpose built for high-performance, cost-effective deep learning training. You can use these instances to train natural language processing, computer vision, and recommender models used across a broad set of applications, such as speech recognition, recommendation, fraud detection, and image and video classification. Use your existing workflows in popular ML frameworks, such as PyTorch and TensorFlow.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 442
        }
      },
      {
        "header": "AWS Inferentia",
        "content": "Instances powered by AWS Inferentia are designed to accelerate machine learning. They provide high performance and low latency machine learning inference. These instances are optimized for deploying deep learning (DL) models for applications, such as natural language processing, object detection and classification, content personalization and filtering, and speech recognition.\n\nThere are a variety of ways that you can get started: Use SageMaker AI, a fully-managed service that is the easiest way to get started with machine learning models. For more information, see Get Started with SageMaker AI in the Amazon SageMaker AI Developer Guide . Launch an Inf1 or Inf2 instance using the Deep Learning AMI. For more information, see AWS Inferentia with DLAMI in the AWS Deep Learning AMIs Developer Guide . Launch an Inf1 or Inf2 instance using your own AMI and install the AWS Neuron SDK , which enables you to compile, run, and profile deep learning models for AWS Inferentia. Launch a container instance using an Inf1 or Inf2 instance and an Amazon ECS-optimized AMI. For more information, see Amazon Linux 2 (Inferentia) AMIs in the Amazon Elastic Container Service Developer Guide . Create an Amazon EKS cluster with nodes running Inf1 instances. For more information, see Inferentia support in the Amazon EKS User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1328
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html",
    "doc_type": "aws",
    "total_sections": 10
  },
  {
    "title": "Bucket policies for Amazon S3",
    "summary": "A bucket policy is a resource-based policy that you can use to grant access permissions to your Amazon S3 bucket and the objects in it. Only the bucket owner can associate a policy with a bucket. The permissions attached to the bucket apply to all of the objects in the bucket that are owned by the bucket owner. These permissions don't apply to objects that are owned by other AWS accounts. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to control ownership of objects uploaded to your bucket and to disable or enable access control lists (ACLs). By default, Object Ownership is set to the Bucket owner enforced setting and all ACLs are disabled. The bucket owner owns all the objects in the bucket and manages access to data exclusively using policies.",
    "sections": [
      {
        "header": "",
        "content": "Bucket policies for Amazon S3 A bucket policy is a resource-based policy that you can use to grant access permissions to your Amazon S3 bucket and the objects in it. Only the bucket owner can associate a policy with a bucket. The permissions attached to the bucket apply to all of the objects in the bucket that are owned by the bucket owner. These permissions don't apply to objects that are owned by other AWS accounts. S3 Object Ownership is an Amazon S3 bucket-level setting that you can use to control ownership of objects uploaded to your bucket and to disable or enable access control lists (ACLs). By default, Object Ownership is set to the Bucket owner enforced setting and all ACLs are disabled. The bucket owner owns all the objects in the bucket and manages access to data exclusively using policies. Bucket policies use JSON-based AWS Identity and Access Management (IAM) policy language. You can use bucket policies to add or deny permissions for the objects in a bucket. Bucket policies can allow or deny requests based on the elements in the policy. These elements include the requester, S3 actions, resources, and aspects or conditions of the request (such as the IP address that's used to make the request). For example, you can create a bucket policy that does the following: Grants other accounts cross-account permissions to upload objects to your S3 bucket Makes sure that you, the bucket owner, has full control of the uploaded objects For more information, see Examples of Amazon S3 bucket policies . Important You can't use a bucket policy to prevent deletions or transitions by an S3 Lifecycle rule. For example, even if your bucket policy denies all actions for all principals, your S3 Lifecycle configuration still functions as normal. The topics in this section provide examples and show you how to add a bucket policy in the S3 console. For information about identity-based policies, see Identity-based policies for Amazon S3 . For information about bucket policy language, see Policies and permissions in Amazon S3 . For more information about the permissions to S3 API operations by S3 resource types, see Required permissions for Amazon S3 API operations . Topics Adding a bucket policy by using the Amazon S3 console Controlling access from VPC endpoints with bucket policies Examples of Amazon S3 bucket policies Bucket policy examples using condition keys Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Policies and permissions Adding a bucket policy",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 21,
          "content_length": 2652
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is CloudFormation?",
    "summary": "CloudFormation is a service that helps you model and set up your AWS resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and CloudFormation takes care of provisioning and configuring those resources for you. You don't need to individually create and configure AWS resources and figure out what's dependent on what; CloudFormation handles that. The following scenarios demonstrate how CloudFormation can help. For a scalable web application that also includes a backend database, you might use an Auto Scaling group, an Elastic Load Balancing load balancer, and an Amazon Relational Database Service database instance. You might use each individual service to provision these resources and after you create the resources, you would have to configure them to work together. All these tasks can add complexity and time before you even get your application up and running.",
    "sections": [
      {
        "header": "Simplify infrastructure management",
        "content": "For a scalable web application that also includes a backend database, you might use an Auto Scaling group, an Elastic Load Balancing load balancer, and an Amazon Relational Database Service database instance. You might use each individual service to provision these resources and after you create the resources, you would have to configure them to work together. All these tasks can add complexity and time before you even get your application up and running.\n\nInstead, you can create a CloudFormation template or modify an existing one. A template describes all your resources and their properties. When you use that template to create a CloudFormation stack, CloudFormation provisions the Auto Scaling group, load balancer, and database for you. After the stack has been successfully created, your AWS resources are up and running. You can delete the stack just as easily, which deletes all the resources in the stack. By using CloudFormation, you easily manage a collection of resources as a single unit.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1007
        }
      },
      {
        "header": "Quickly replicate your infrastructure",
        "content": "If your application requires additional availability, you might replicate it in multiple regions so that if one region becomes unavailable, your users can still use your application in other regions. The challenge in replicating your application is that it also requires you to replicate your resources. Not only do you need to record all the resources that your application requires, but you must also provision and configure those resources in each region.\n\nReuse your CloudFormation template to create your resources in a consistent and repeatable manner. To reuse your template, describe your resources once and then provision the same resources over and over in multiple regions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 684
        }
      },
      {
        "header": "Easily control and track changes to your infrastructure",
        "content": "In some cases, you might have underlying resources that you want to upgrade incrementally. For example, you might change to a higher performing instance type in your Auto Scaling launch configuration so that you can reduce the maximum number of instances in your Auto Scaling group. If problems occur after you complete the update, you might need to roll back your infrastructure to the original settings. To do this manually, you not only have to remember which resources were changed, you also have to know what the original settings were.\n\nWhen you provision your infrastructure with CloudFormation, the CloudFormation template describes exactly what resources are provisioned and their settings. Because these templates are text files, you simply track differences in your templates to track changes to your infrastructure, similar to the way developers control revisions to source code. For example, you can use a version control system with your templates so that you know exactly what changes were made, who made them, and when. If at any point you need to reverse changes to your infrastructure, you can use a previous version of your template.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1152
        }
      },
      {
        "header": "Getting started with CloudFormation",
        "content": "CloudFormation is available through the CloudFormation console , API , AWS CLI , AWS SDKs , and through several integrations.\n\nFor an introduction to CloudFormation, see How CloudFormation works .\n\nTo start using CloudFormation, see Creating your first stack .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 260
        }
      },
      {
        "header": "Related information",
        "content": "You can learn more about CloudFormation in this user guide, as well as the following resources:\n\nFor product details and FAQs, see the AWS CloudFormation product page . For pricing information, see AWS CloudFormation pricing .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 226
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "Getting started with CloudFormation",
    "summary": "You can begin using CloudFormation through the AWS Management Console by creating a stack from an example template, which will help you learn the basics of stack creation. A template is a text file that defines all the resources within a stack. A stack is the deployment of a CloudFormation template. From a single template, you can create multiple stacks. Each stack contains a collection of AWS resources that can be managed as a single unit. CloudFormation is a free service; however, you are charged for the AWS resources you include in your stacks at the current rates for each. For more information about AWS pricing, go to the detail page for each product on http://aws.amazon.com .",
    "sections": [
      {
        "header": "",
        "content": "Getting started with CloudFormation You can begin using CloudFormation through the AWS Management Console by creating a stack from an example template, which will help you learn the basics of stack creation. A template is a text file that defines all the resources within a stack. A stack is the deployment of a CloudFormation template. From a single template, you can create multiple stacks. Each stack contains a collection of AWS resources that can be managed as a single unit. CloudFormation is a free service; however, you are charged for the AWS resources you include in your stacks at the current rates for each. For more information about AWS pricing, go to the detail page for each product on http://aws.amazon.com . Video: Getting started with CloudFormation The following video is an introduction to creating CloudFormation stacks from the AWS Management Console. Topics How CloudFormation works Signing up for an AWS account Creating your first stack Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions What is CloudFormation? How CloudFormation works",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 1224
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/GettingStarted.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "CloudFormation best practices",
    "summary": "Best practices are recommendations that can help you use CloudFormation more effectively and adopt safe practices throughout its entire workflow. Learn how to plan and organize your stacks, create templates that describe your resources and the software applications that run on them, and manage your stacks and their resources. The following best practices are based on real-world experience from current CloudFormation customers. Shorten the feedback loop to improve development velocity",
    "sections": [
      {
        "header": "Shorten the feedback loop to improve development velocity",
        "content": "Adopt practices and tools that help you shorten the feedback loop for your infrastructure you describe with CloudFormation templates. This includes performing early linting and testing of your templates in your workstation; when you do, you have the opportunity to discover potential syntax and configuration issues even before you submit your contributions to a source code repository. Early discovery of such issues helps with preventing them from reaching formal lifecycle environments, such as development, quality assurance, and production. This early-testing, fail-fast approach gives you the benefits of reducing rework wait time, reducing potential areas of impact, and increasing your level of confidence in having successful provisioning operations.\n\nTooling choices that help you achieve fail-fast practices include the CloudFormation Linter ( cfn-lint ) and TaskCat command line tools. The cfn-lint tool gives you the ability to validate your CloudFormation templates against the CloudFormation Resource Specification . This includes checking valid values for resource properties, as well as best practices. Plugins for cfn-lint are available for a number of code editors ; this gives you the ability to visualize issues within your editor and to get direct linter feedback. You can also choose to integrate cfn-lint in your source code repositoryâs configuration, so that you can perform template validation when you commit your contributions. For more information, see Git pre-commit validation of CloudFormation templates with cfn-lint . Once you have performed your initial lintingâand fixed any issues cfn-lint might have raisedâyou can use TaskCat to test your templates by programmatically creating stacks in the AWS Regions you choose. TaskCat also generates a report with a pass/fail grades for each Region you chose.\n\nFor a step-by-step, hands-on walkthrough on how to use both tools to shorten the feedback loop, follow the Linting and Testing lab of the CloudFormation Workshop .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 2009
        }
      },
      {
        "header": "Organize your stacks by lifecycle and ownership",
        "content": "Use the lifecycle and ownership of your AWS resources to help you decide what resources should go in each stack. Initially, you might put all your resources in one stack, but as your stack grows in scale and broadens in scope, managing a single stack can be cumbersome and time consuming. By grouping resources with common lifecycles and ownership, owners can make changes to their set of resources by using their own process and schedule without affecting other resources.\n\nFor example, imagine a team of developers and engineers who own a website that's hosted on Amazon EC2 Auto Scaling instances behind a load balancer. Because the website has its own lifecycle and is maintained by the website team, you can create a stack for the website and its resources. Now imagine that the website also uses back-end databases, where the databases are in a separate stack that are owned and maintained by database administrators. Whenever the website team or database team needs to update their resources, they can do so without affecting each other's stack. If all resources were in a single stack, coordinating, and communicating updates can be difficult.\n\nFor additional guidance about organizing your stacks, you can use two common frameworks: a multi-layered architecture and service-oriented architecture (SOA).\n\nA layered architecture organizes stacks into multiple horizontal layers that build on top of one another, where each layer has a dependency on the layer directly below it. You can have one or more stacks in each layer, but within each layer, your stacks should have AWS resources with similar lifecycles and ownership.\n\nWith a service-oriented architecture, you can organize big business problems into manageable parts. Each of these parts is a service that has a clearly defined purpose and represents a self-contained unit of functionality. You can map these services to a stack, where each stack has its own lifecycle and owners. These services (stacks) can be wired together so that they can interact with one another.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2035
        }
      },
      {
        "header": "Use cross-stack references to return the value of an output exported by another stack",
        "content": "When you organize your AWS resources based on lifecycle and ownership, you might want to build a stack that uses resources that are in another stack. You can hardcode values or use input parameters to pass resource names and IDs. However, these methods can make templates difficult to reuse or can increase the overhead to get a stack running. Instead, use cross-stack references to return the value of an output exported by another stack so that other stacks can use them. Stacks can use the exported resources by calling them using the Fn::ImportValue function.\n\nFor example, you might have a network stack that includes a VPC, a security group, and a subnet. You want all public web applications to use these resources. By exporting the resources, you allow all stacks with public web applications to use them. For more information, see Get exported outputs from a deployed CloudFormation stack .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 899
        }
      },
      {
        "header": "Use CloudFormation StackSets for multi-account and multi-region deployments",
        "content": "CloudFormation StackSets extend the capability of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Use StackSets for deploying common infrastructure components, compliance controls, or shared services across your organization.\n\nWhen using StackSets, implement service-managed permissions with AWS Organizations for simplified permission management. This approach allows you to deploy StackSets to accounts within your organization without the need to manually configure IAM roles in each account.\n\nFor more information on StackSets see StackSets concepts .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 628
        }
      },
      {
        "header": "Verify quotas for all resource types",
        "content": "Before launching a stack, ensure that you can create all the resources that you want without hitting your AWS account limits. If you hit a limit, CloudFormation won't create your stack successfully until you increase your quota or delete extra resources. Each service can have various limits that you should be aware of before launching a stack. For example, by default, you can only launch 2000 CloudFormation stacks per Region in your AWS account. For more information about limits and how to increase the default limits, see AWS service quotas in the AWS General Reference .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 577
        }
      },
      {
        "header": "Reuse templates to replicate stacks in multiple environments",
        "content": "After you have your stacks and resources set up, you can reuse your templates to replicate your infrastructure in multiple environments. For example, you can create environments for development, testing, and production so that you can test changes before implementing them into production. To make templates reusable, use the parameters, mappings, and conditions sections so that you can customize your stacks when you create them. For example, for your development environments, you can specify a lower-cost instance type compared to your production environment, but all other configurations and settings remain the same. For more information about parameters, mappings, and conditions, see CloudFormation template sections .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 726
        }
      },
      {
        "header": "Use modules to reuse resource configurations",
        "content": "As your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates. Modules are a way for you to package resource configurations for inclusion across stack templates, in a transparent, manageable, and repeatable way. Modules can encapsulate common service configurations and best practices as modular, customizable building blocks for you to include in your stack templates.\n\nThese building blocks can be for a single resource, like best practices for defining an Amazon Elastic Compute Cloud (Amazon EC2) instance, or they can be for multiple resources, to define common patterns of application architecture. These building blocks can be nested into other modules, so you can stack your best practices into higher-level building blocks. CloudFormation modules are available in the CloudFormation registry , so you can use them just like a native resource. When you use a CloudFormation module, the module template is expanded into the consuming template, which makes it possible for you to access the resources inside the module using a Ref or Fn::GetAtt . For more information, see Create reusable resource configurations that can be included across templates with CloudFormation modules .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1248
        }
      },
      {
        "header": "Adopt infrastructure as code practices",
        "content": "Treat your CloudFormation templates as code by implementing infrastructure as code (IaC) practices. Store your templates in version control systems, implement code reviews, and use automated testing to validate changes. This approach ensures consistency, improves collaboration, and provides an audit trail for infrastructure changes.\n\nConsider implementing CI/CD pipelines for your infrastructure code to automate the testing and deployment of your CloudFormation templates. Tools like AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy can be used to create automated workflows for your infrastructure deployments.\n\nFor more information on implementing IaC best practices, see Using AWS CloudFormation as an IaC tool .\n\nFor more information on using continuous delivery with CloudFormation, see Continuous delivery with CodePipeline .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 837
        }
      },
      {
        "header": "Don't embed credentials in your templates",
        "content": "Rather than embedding sensitive information in your CloudFormation templates, we recommend you use dynamic references in your stack template.\n\nDynamic references provide a compact, powerful way for you to reference external values that are stored and managed in other services, such as the AWS Systems Manager Parameter Store or AWS Secrets Manager. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations, and passes the value to the appropriate resource. However, CloudFormation never stores the actual reference value. For more information, see Using dynamic references to specify template values .\n\nAWS Secrets Manager helps you to securely encrypt, store, and retrieve credentials for your databases and other services. The AWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management.\n\nFor more information on defining template parameters, see CloudFormation template Parameters syntax .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1034
        }
      },
      {
        "header": "Use AWS-specific parameter types",
        "content": "If your template requires inputs for existing AWS-specific values, such as existing Amazon Virtual Private Cloud IDs or an Amazon EC2 key pair name, use AWS-specific parameter types. For example, you can specify a parameter as type AWS::EC2::KeyPair::KeyName , which takes an existing key pair name that's in your AWS account and in the Region where you are creating the stack. CloudFormation can quickly validate values for AWS-specific parameter types before creating your stack. Also, if you use the CloudFormation console, CloudFormation shows a drop down list of valid values, so you don't have to look up or memorize the correct VPC IDs or key pair names. For more information, see Specify existing resources at runtime with CloudFormation-supplied parameter types .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 772
        }
      },
      {
        "header": "Use parameter constraints",
        "content": "With constraints, you can describe allowed input values so that CloudFormation catches any not valid values before creating a stack. You can set constraints such as a minimum length, maximum length, and allowed patterns. For example, you can set constraints on a database user name value so that it must be a minimum length of eight character and contain only alphanumeric characters. For more information, see CloudFormation template Parameters syntax .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 454
        }
      },
      {
        "header": "Use pseudo parameters to promote portability",
        "content": "You can use pseudo parameters in your templates as arguments for intrinsic functions , such as Ref and Fn::Sub . Pseudo parameters are parameters that are predefined by CloudFormation. You don't declare them in your template. Using pseudo parameters in intrinsic functions increases the portability of your stack templates across Regions and accounts.\n\nFor example, imagine you wanted to create a template where, for a given resource property, you need to specify the Amazon Resource Name (ARN) of another existing resource. In this case, the existing resource is an AWS Systems Manager Parameter Store resource with the following ARN: arn:aws:ssm:us-east-1:123456789012:parameter/MySampleParameter . You will need to adapt the ARN format to your target AWS partition, Region, and account ID. Instead of hard-coding these values, you can use AWS::Partition , AWS::Region , and AWS::AccountId pseudo parameters to make your template more portable. In this case, the following example shows you how to concatenate elements in an ARN with CloudFormation: !Sub 'arn:$ { AWS::Partition}:ssm:$ { AWS::Region}:$ { AWS::AccountId}:parameter/MySampleParameter .\n\nFor another example, assume you want to share resources or configurations across multiple stacks. In this example, assume you have created a subnet for your VPC, and then exported its ID for use with other stacks in the same AWS account and Region. In another stack, you reference the exported value of the subnet ID when describing an Amazon EC2 instance. For a detailed example of using the Export output field and Fn::ImportValue intrinsic function, see Refer to resource outputs in another CloudFormation stack .\n\nStack exports must be unique per account and Region. So, in this case, you can use the AWS::StackName pseudo parameter to create a prefix for your export. Since stack names must also be unique per account and Region, the usage of this pseudo parameter as a prefix increases the possibility of having a unique export name while also promoting a reusable approach across stacks from where you export values. Alternatively, you can use a prefix of your own choice.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 2133
        }
      },
      {
        "header": "Use AWS::CloudFormation::Init to deploy software applications on Amazon EC2 instances",
        "content": "When you launch stacks, you can install and configure software applications on Amazon EC2 instances by using the cfn-init helper script and the AWS::CloudFormation::Init resource. By using AWS::CloudFormation::Init , you can describe the configurations that you want rather than scripting procedural steps. You can also update configurations without recreating instances. And if anything goes wrong with your configuration, CloudFormation generates logs that you can use to investigate issues.\n\nIn your template, specify installation and configuration states in the AWS::CloudFormation::Init resource. For a walkthrough that shows how to use cfn-init and AWS::CloudFormation::Init , see Deploy applications on Amazon EC2 .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 722
        }
      },
      {
        "header": "Use the latest helper scripts",
        "content": "The CloudFormation helper scripts are updated periodically. Be sure you include the following command in the UserData property of your template before you call the helper scripts to ensure that your launched instances get the latest helper scripts:\n\nFor more information about getting the latest helper scripts, see the CloudFormation helper scripts reference in the CloudFormation Template Reference Guide .",
        "code_examples": [
          "yum install -y aws-cfn-bootstrap"
        ],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": true,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 408
        }
      },
      {
        "header": "Validate templates before using them",
        "content": "Before you use a template to create or update a stack, you can use CloudFormation to validate it. Validating a template can help you catch syntax and some semantic errors, such as circular dependencies, before CloudFormation creates any resources. If you use the CloudFormation console, the console automatically validates the template after you specify input parameters. For the AWS CLI or CloudFormation API, use the validate-template CLI command or ValidateTemplate API operation.\n\nDuring validation, CloudFormation first checks if the template is valid JSON. If it isn't, CloudFormation checks if the template is valid YAML. If both checks fail, CloudFormation returns a template validation error.\n\nValidate templates for organization policy compliance\n\nYou can also validate your template for compliance to organization policy guidelines. AWS CloudFormation Guard ( cfn-guard ) is an open-source command line interface (CLI) tool that provides a policy-as-code language to define rules that can check for both required and prohibited resource configurations. It then enables you to validate your templates against those rules. For example, administrators can create rules to ensure that users always create encrypted Amazon S3 buckets.\n\nYou can use cfn-guard either locally, while editing templates, or automatically as part of a CI/CD pipeline to stop deployment of non-compliant resources.\n\nAdditionally, cfn-guard includes a feature, rulegen , that enables you to extract rules from existing compliant CloudFormation templates.\n\nFor more information, see the cfn-guard repository on GitHub.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 1598
        }
      },
      {
        "header": "Validate templates for organization policy compliance",
        "content": "You can also validate your template for compliance to organization policy guidelines. AWS CloudFormation Guard ( cfn-guard ) is an open-source command line interface (CLI) tool that provides a policy-as-code language to define rules that can check for both required and prohibited resource configurations. It then enables you to validate your templates against those rules. For example, administrators can create rules to ensure that users always create encrypted Amazon S3 buckets.\n\nYou can use cfn-guard either locally, while editing templates, or automatically as part of a CI/CD pipeline to stop deployment of non-compliant resources.\n\nAdditionally, cfn-guard includes a feature, rulegen , that enables you to extract rules from existing compliant CloudFormation templates.\n\nFor more information, see the cfn-guard repository on GitHub.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 840
        }
      },
      {
        "header": "Using YAML or JSON for template authoring",
        "content": "CloudFormation supports both YAML and JSON formats for templates. Each has its advantages, and the choice depends on your specific needs:\n\nUse YAML when\n\nYou prioritize human readability and maintainability You want to include comments to document your template You're working on complex templates with nested structures You want to use YAML-specific features like anchors and aliases to reduce repetition\n\nUse JSON when:\n\nYou need to integrate with tools or systems that prefer JSON You're working with programmatic template generation or manipulation You require strict data validation\n\nYAML is generally recommended for manual template authoring due to its readability and comment support. It's particularly useful for complex templates where the indentation-based structure helps visualize resource hierarchies. JSON can be advantageous in automated workflows or when working with APIs that expect JSON input. It's also beneficial when you need to ensure strict adherence to a specific structure. Regardless of the format you choose, focus on creating well-structured, documented, and maintainable templates. If using YAML, leverage its features like anchors and aliases to reduce repetition and improve maintainability.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1224
        }
      },
      {
        "header": "Implement a comprehensive tagging strategy",
        "content": "Implement a consistent tagging strategy for all resources created by your CloudFormation templates. Tags help with resource organization, cost allocation, access control, and automation. Consider including tags for environment, owner, cost center, application, and purpose.\n\nUse the AWS::CloudFormation::Stack resource's Tags property to apply tags to all supported resources in a stack. You can also use the TagSpecifications property available on many resource types to apply tags during resource creation.\n\nFor more information on tagging, see Resource tag .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 561
        }
      },
      {
        "header": "Leverage template macros for advanced transformations",
        "content": "CloudFormation macros enable you to perform custom processing on templates, from simple actions like find-and-replace operations to complex transformations that generate additional resources. Use macros to extend the capabilities of CloudFormation templates and implement reusable patterns across your organization.\n\nThe AWS Serverless Application Model is an example of a macro that simplifies the development of serverless applications. Consider creating custom macros for organization-specific patterns and requirements.\n\nFor more information on using macros in your templates, see Overview of CloudFormation macros .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 620
        }
      },
      {
        "header": "Manage all stack resources through CloudFormation",
        "content": "After you launch a stack, use the CloudFormation console , API , or AWS CLI to update resources in your stack. Don't make changes to stack resources outside of CloudFormation. Doing so can create a mismatch between your stack's template and the current state of your stack resources, which can cause errors if you update or delete the stack. This is known as drift. If a change is made to a resource outside of the CloudFormation template and you update the stack, the changes made directly to the resource will be discarded, and the resource configuration will revert to the configuration in the template.\n\nFor more information on drift, see What is drift? .\n\nFor more information on updating stacks, see Updating a stack .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 724
        }
      },
      {
        "header": "Create change sets before updating your stacks",
        "content": "Change sets allow you to see how proposed changes to a stack might impact your running resources before you implement them. CloudFormation doesn't make any changes to your stack until you run the change set, allowing you to decide whether to proceed with your proposed changes or create another change set.\n\nUse change sets to check how your changes might impact your running resources, especially for critical resources. For example, if you change the name of an Amazon RDS database instance, CloudFormation will create a new database and delete the old one; you will lose the data in the old database unless you've already backed it up. If you generate a change set, you will see that your change will replace your database. This can help you plan before you update your stack. For more information, see Update CloudFormation stacks using change sets .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 854
        }
      },
      {
        "header": "Use stack policies to protect resources",
        "content": "Stack policies help protect critical stack resources from unintentional updates that could cause resources to be interrupted or even replaced. A stack policy is a JSON document that describes what update actions can be performed on designated resources. Specify a stack policy whenever you create a stack that has critical resources.\n\nDuring a stack update, you must explicitly specify the protected resources that you want to update; otherwise, no changes are made to protected resources. For more information, see Prevent updates to stack resources .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 552
        }
      },
      {
        "header": "Use AWS CloudTrail to log CloudFormation calls",
        "content": "AWS CloudTrail tracks anyone making CloudFormation API calls in your AWS account. API calls are logged whenever anyone uses the CloudFormation API, the CloudFormation console, a back-end console, or CloudFormation AWS CLI commands. Enable logging and specify an Amazon S3 bucket to store the logs. That way, if you ever need to, you can audit who made what CloudFormation call in your account.\n\nFor more information, see Logging CloudFormation API calls with AWS CloudTrail .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 475
        }
      },
      {
        "header": "Use code reviews and revision controls to manage your templates",
        "content": "Your stack templates describe the configuration of your AWS resources, such as their property values. To review changes and to keep an exact history of your resources, use code reviews and revision controls. These methods can help you track changes between different versions of your templates, which can help you track changes to your stack resources. Also, by maintaining a history, you can always revert your stack to a certain version of your template.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 456
        }
      },
      {
        "header": "Update your Amazon EC2 instances regularly",
        "content": "On all your Amazon EC2 Windows instances and Amazon EC2 Linux instances created with CloudFormation, regularly run the yum update command to update the RPM package. This ensures that you get the latest fixes and security updates.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 229
        }
      },
      {
        "header": "Use drift detection regularly",
        "content": "Regularly use the CloudFormation drift detection feature to identify resources that have been modified outside of CloudFormation management. Detecting and resolving drift helps maintain the integrity of your infrastructure as code approach and ensures that your templates accurately reflect the state of your deployed resources.\n\nConsider implementing automated drift detection as part of your operational procedures. You can use AWS Lambda functions triggered by Amazon EventBridge rules to periodically check for drift and notify your team when discrepancies are detected.\n\nFor more information on drift, see Detect unmanaged configuration changes to stacks and resources with drift detection .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 696
        }
      },
      {
        "header": "Configure rollback triggers for automatic recovery",
        "content": "Use rollback triggers to specify Amazon CloudWatch alarms that CloudFormation should monitor during stack creation and update operations. If any of the specified alarms go into the ALARM state, CloudFormation automatically rolls back the entire stack operation, helping to ensure that your infrastructure remains in a stable state.\n\nConfigure rollback triggers for critical metrics such as application error rates, system resource utilization, or custom business metrics that indicate the health of your application and infrastructure.\n\nFor more information on rollback triggers, see Roll back your stack on alarm breach .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 622
        }
      },
      {
        "header": "Implement effective stack refactoring strategies",
        "content": "As your infrastructure evolves, you may need to refactor your CloudFormation stacks to improve maintainability, reduce complexity, or adapt to changing requirements. Stack refactoring involves restructuring your templates and resources while preserving their external behavior and functionality. Stack refactoring is beneficial to use with CloudFormation in the following ways:\n\nSplitting monolithic stacks : Breaking down large, complex stacks into smaller, more manageable stacks organized by lifecycle or ownership Consolidating related resources : Combining related resources from multiple stacks into a single, cohesive stack to simplify management Extracting reusable components : Moving common patterns into modules or nested stacks to promote reuse and consistency Improving resource organization : Restructuring resources within a stack to better reflect their relationships and dependencies\n\nFor more information on refactoring your CloudFormation stacks, see Stack refactoring .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 989
        }
      },
      {
        "header": "Use CloudFormation Hooks for lifecycle management",
        "content": "CloudFormation Hooks provide code that proactively inspects the configuration of your AWS resources before provisioning, and perform complex validation checks. Hooks check if your resources, stacks, and changes sets are compliant with your organization's security, operational, and cost optimization needs. They providing warnings before a resource provisioning, or failing the operation and stopping it altogether, depending on how it has been configured. Violations and warnings are logged in Amazon CloudWatch to provide visibility into non-compliant deployments.\n\nFor more information on these best practices for Hooks see, AWS CloudFormation Hooks concepts .\n\nFor more information on what Hooks can do for your CloudFormation resources see, What are AWS CloudFormation Hooks?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 780
        }
      },
      {
        "header": "Use IaC Generator to create templates from existing resources",
        "content": "The CloudFormation IaC (infrastructure as code) Generator helps you create CloudFormation templates from your existing AWS resources. This capability is particularly useful when you need to replicate existing infrastructure, document manually created resources, or bring previously unmanaged resources under CloudFormation management. IaC generator is useful for creating your CloudFormation templates in the following ways:\n\nAccelerated template creation : Generate templates from existing resources instead of writing them from scratch Consistent infrastructure : Ensure new environments match existing ones by using generated templates as a starting point Migration to infrastructure as code : Gradually bring manually created resources under CloudFormation management Documentation : Create a record of your existing infrastructure in template form\n\nFor more information on IaC Generator, see Generate templates from existing resources with IaC generator .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 960
        }
      },
      {
        "header": "Use AWS Infrastructure Composer for visual template design",
        "content": "AWS Infrastructure Composer is a visual design tool that helps you create, visualize, and modify CloudFormation templates using a drag-and-drop interface. It can be particularly beneficial when using CloudFormation in the following ways:\n\nArchitecture planning : Design and validate infrastructure architectures before implementation Template modernization : Visualize existing templates to understand their structure and identify opportunities for improvement Training and onboarding : Help new team members understand CloudFormation concepts and AWS service relationships through visual learning Stakeholder communication : Present infrastructure designs to non-technical stakeholders using clear visual representations Compliance reviews : Use visual diagrams to facilitate security and compliance reviews of your infrastructure designs Compliance reviews : Use visual diagrams to facilitate security and compliance reviews of your infrastructure designs\n\nFor more information on Infrastructure Composer, see What is AWS Infrastructure Composer? .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1050
        }
      },
      {
        "header": "Consider using AWS Cloud Development Kit (AWS CDK) for complex infrastructure",
        "content": "For complex infrastructure requirements, consider using the CDK to define your cloud resources using familiar programming languages like TypeScript, Python, Java, and .NET. AWS CDK generates CloudFormation templates from your code, allowing you to leverage the full capabilities of CloudFormation while using the abstractions and programming constructs of your preferred language.\n\nThe AWS CDK provides high-level constructs that encapsulate best practices and simplify the definition of common infrastructure patterns. This can significantly reduce the amount of code needed to define your infrastructure while ensuring adherence to best practices.\n\nFor more information on the CDK, see AWS Cloud Development Kit (AWS CDK) .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 725
        }
      },
      {
        "header": "Use IAM to control access",
        "content": "IAM is an AWS service that you can use to manage users and their permissions in AWS. You can use IAM with CloudFormation to specify what CloudFormation actions users can perform, such as viewing stack templates, creating stacks, or deleting stacks. Furthermore, anyone managing CloudFormation stacks will require permissions to resources within those stacks. For example, if users want to use CloudFormation to launch, update, or terminate Amazon EC2 instances, they must have permission to call the relevant Amazon EC2 actions.\n\nIn most cases, users require full access to manage all of the resources in a template. CloudFormation makes calls to create, modify, and delete those resources on their behalf. To separate permissions between a user and the CloudFormation service, use a service role. CloudFormation uses the service role's policy to make calls instead of the user's policy. For more information, see CloudFormation service role .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 943
        }
      },
      {
        "header": "Apply the principle of least privilege",
        "content": "When configuring IAM roles for CloudFormation service roles or for resources created by your templates, always apply the principle of least privilege. Grant only the permissions necessary for the intended functionality, and avoid using wildcard permissions whenever possible.\n\nUse IAM Access Analyzer to review the permissions granted to your CloudFormation service roles and identify unused permissions that can be removed. Regularly review and update IAM policies to ensure they remain aligned with your security requirements.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 528
        }
      },
      {
        "header": "Secure sensitive parameters",
        "content": "For sensitive information such as passwords, API keys, and other secrets, use AWS Systems Manager Parameter Store or AWS Secrets Manager instead of embedding them directly in your templates. Use dynamic references in your templates to securely retrieve these values during stack operations.\n\nWhen using parameters in your templates, set the NoEcho property to true for sensitive parameters to prevent their values from being displayed in the console, API responses, or CLI output. Be aware that NoEcho doesn't prevent the value from being logged if it's passed to other services or resources that might log the value.\n\nFor more information on using AWS Systems Manager Parameter Store with CloudFormation see Get a plaintext value from AWS Systems Manager Parameter Store .\n\nFor more information on using the NoEcho property, see CloudFormation template Parameters syntax .\n\nFor more information on using AWS Secrets Manager with CloudFormation see Create AWS Secrets Manager secrets in AWS CloudFormation .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1007
        }
      },
      {
        "header": "Implement policy as code with AWS CloudFormation Guard",
        "content": "AWS CloudFormation Guard ( cfn-guard ) is an open-source policy-as-code tool that allows you to define and enforce rules for your CloudFormation templates. Use cfn-guard to ensure that your templates comply with organizational policies, security best practices, and governance requirements.\n\nIntegrate cfn-guard into your CI/CD pipelines to automatically validate templates against your policy rules before deployment. This helps prevent non-compliant resources from being deployed to your environment and provides early feedback to developers about policy violations.\n\nFor more information on Guard see What is AWS CloudFormation Guard?",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 637
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html",
    "doc_type": "aws",
    "total_sections": 36
  },
  {
    "title": "What is Amazon VPC?",
    "summary": "With Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS. The following diagram shows an example VPC. The VPC has one subnet in each of the Availability Zones in the Region, EC2 instances in each subnet, and an internet gateway to allow communication between the resources in your VPC and the internet.",
    "sections": [
      {
        "header": "Features",
        "content": "The following features help you configure a VPC to provide the connectivity that your applications need:\n\nVirtual private clouds (VPC) A VPC is a virtual network that closely resembles a traditional network that you'd operate in your own data center. After you create a VPC, you can add subnets. Subnets A subnet is a range of IP addresses in your VPC. A subnet must reside in a single Availability Zone. After you add subnets, you can deploy AWS resources in your VPC. IP addressing You can assign IP addresses , both IPv4 and IPv6, to your VPCs and subnets. You can also bring your public IPv4 addresses and IPv6 GUA addresses to AWS and allocate them to resources in your VPC, such as EC2 instances, NAT gateways, and Network Load Balancers. Routing Use route tables to determine where network traffic from your subnet or gateway is directed. Gateways and endpoints A gateway connects your VPC to another network. For example, use an internet gateway to connect your VPC to the internet. Use a VPC endpoint to connect to AWS services privately, without the use of an internet gateway or NAT device. Peering connections Use a VPC peering connection to route traffic between the resources in two VPCs. Traffic Mirroring Copy network traffic from network interfaces and send it to security and monitoring appliances for deep packet inspection. Transit gateways Use a transit gateway , which acts as a central hub, to route traffic between your VPCs, VPN connections, and Direct Connect connections. VPC Flow Logs A flow log captures information about the IP traffic going to and from network interfaces in your VPC. VPN connections Connect your VPCs to your on-premises networks using AWS Virtual Private Network (AWS VPN) .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1724
        }
      },
      {
        "header": "Getting started with Amazon VPC",
        "content": "Your AWS account includes a default VPC in each AWS Region. Your default VPCs are configured such that you can immediately start launching and connecting to EC2 instances. For more information, see Plan your VPC .\n\nYou can choose to create additional VPCs with the subnets, IP addresses, gateways and routing that you need. For more information, see Create a VPC .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 364
        }
      },
      {
        "header": "Working with Amazon VPC",
        "content": "You can create and manage your VPCs using any of the following interfaces:\n\nAWS Management Console â Provides a web interface that you can use to access your VPCs. AWS Command Line Interface (AWS CLI) â Provides commands for a broad set of AWS services, including Amazon VPC, and is supported on Windows, Mac, and Linux. For more information, see AWS Command Line Interface . AWS SDKs â Provides language-specific APIs and takes care of many of the connection details, such as calculating signatures, handling request retries, and error handling. For more information, see AWS SDKs . Query API â Provides low-level API actions that you call using HTTPS requests. Using the Query API is the most direct way to access Amazon VPC, but it requires that your application handle low-level details such as generating the hash to sign the request, and error handling. For more information, see Amazon VPC actions in the Amazon EC2 API Reference .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 946
        }
      },
      {
        "header": "Pricing for Amazon VPC",
        "content": "There's no additional charge for using a VPC. There are, however, charges for some VPC components, such as NAT gateways, IP Address Manager, traffic mirroring, Reachability Analyzer, and Network Access Analyzer. For more information, see Amazon VPC Pricing .\n\nNearly all resources that you launch in your virtual private cloud (VPC) provide you with an IP address for connectivity. The vast majority of resources in your VPC use private IPv4 addresses. Resources that require direct access to the internet over IPv4, however, use public IPv4 addresses.\n\nAmazon VPC enables you to launch managed services, such as Elastic Load Balancing, Amazon RDS, and Amazon EMR, without having a VPC set up beforehand. It does this by using the default VPC in your account if you have one. Any public IPv4 addresses provisioned to your account by the managed service will be charged. These charges will be associated with Amazon VPC service in your AWS Cost and Usage Report.\n\nPricing for public IPv4 addresses\n\nA public IPv4 address is an IPv4 address that is routable from the internet. A public IPv4 address is necessary for a resource to be directly reachable from the internet over IPv4.\n\nIf you are an existing or new AWS Free Tier customer, you get 750 hours of public IPv4 address usage with the EC2 service at no charge. If you are not using the EC2 service in the AWS Free Tier, Public IPv4 addresses are charged. For specific pricing information, see the Public IPv4 address tab in Amazon VPC Pricing .\n\nPrivate IPv4 addresses ( RFC 1918 ) are not charged. For more information about how public IPv4 addresses are charged for shared VPCs, see Billing and metering for the owner and participants .\n\nPublic IPv4 addresses have the following types:\n\nElastic IP addresses (EIPs) : Static, public IPv4 addresses provided by Amazon that you can associate with an EC2 instance, elastic network interface, or AWS resource. EC2 public IPv4 addresses : Public IPv4 addresses assigned to an EC2 instance by Amazon (if the EC2 instance is launched into a default subnet or if the instance is launched into a subnet thatâs been configured to automatically assign a public IPv4 address). BYOIPv4 addresses : Public IPv4 addresses in the IPv4 address range that youâve brought to AWS using Bring your own IP addresses (BYOIP) . Service-managed IPv4 addresses : Public IPv4 addresses automatically provisioned on AWS resources and managed by an AWS service. For example, public IPv4 addresses on Amazon ECS, Amazon RDS, or Amazon WorkSpaces.\n\nThe following list shows the most common AWS services that can use public IPv4 addresses.\n\nAmazon WorkSpaces Applications AWS Client VPN AWS Database Migration Service Amazon EC2 Amazon Elastic Container Service Amazon EKS Amazon EMR Amazon GameLift Servers AWS Global Accelerator AWS Mainframe Modernization Amazon Managed Streaming for Apache Kafka Amazon MQ Amazon RDS Amazon Redshift AWS Site-to-Site VPN Amazon VPC NAT gateway Amazon WorkSpaces Elastic Load Balancing",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 9,
          "content_length": 3000
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Plan your VPC",
    "summary": "Complete the following tasks to prepare to create and connect your VPCs. When you are finished, you will be ready to deploy your application on AWS. Sign up for an AWS account",
    "sections": [
      {
        "header": "Sign up for an AWS account",
        "content": "If you do not have an AWS account, complete the following steps to create one.\n\nTo sign up for an AWS account Open https://portal.aws.amazon.com/billing/signup . Follow the online instructions. Part of the sign-up procedure involves receiving a phone call or text message and entering a verification code on the phone keypad. When you sign up for an AWS account, an AWS account root user is created. The root user has access to all AWS services and resources in the account. As a security best practice, assign administrative access to a user, and use only the root user to perform tasks that require root user access .\n\nAWS sends you a confirmation email after the sign-up process is complete. At any time, you can view your current account activity and manage your account by going to https://aws.amazon.com/ and choosing My Account .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 836
        }
      },
      {
        "header": "Verify permissions",
        "content": "Before you can use Amazon VPC, you must have the required permissions. For more information, see Identity and access management for Amazon VPC and Amazon VPC policy examples .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 175
        }
      },
      {
        "header": "Determine your IP address ranges",
        "content": "The resources in your VPC communicate with each other and with resources over the internet using IP addresses. When you create VPCs and subnets, you can select their IP address ranges. When you deploy resources in a subnet, such as EC2 instances, they receive IP addresses from the IP address range of the subnet. For more information, see IP addressing for your VPCs and subnets .\n\nAs you choose a size for your VPC, consider how many IP addresses you'll need across your AWS accounts and VPCs. Ensure that the IP address ranges for your VPCs don't overlap with the IP address ranges for your own network. If you need connectivity between multiple VPCs, you must ensure that they have no overlapping IP addresses.\n\nIP Address Manager (IPAM) makes it easier to plan, track, and monitor the IP addresses for your application. For more information, see the IP Address Manager Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 881
        }
      },
      {
        "header": "Select your Availability Zones",
        "content": "An AWS Region is a physical location where we cluster data centers, known as Availability Zones. Each Availability Zone has independent power, cooling, and physical security, with redundant power, networking, and connectivity. The Availability Zones in a Region are physically separated by a meaningful distance, and interconnected through high-bandwidth, low-latency networking. You can design your application to run in multiple Availability Zones to achieve even greater fault tolerance.\n\nProduction environment\n\nFor a production environment, we recommend that you select at least two Availability Zones and deploy your AWS resources evenly in each active Availability Zone.\n\nDevelopment or test environment\n\nFor a development or test environment, you might choose to save money by deploying your resources in only one Availability Zone.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 840
        }
      },
      {
        "header": "Plan your internet connectivity",
        "content": "Plan to divide each VPC into subnets based on your connectivity requirements. For example:\n\nIf you have web servers that will receive traffic from clients on the internet, create a subnet for these servers in each Availability Zone. If you also have servers that will receive traffic only from other servers in the VPC, create a separate subnet for these servers in each Availability Zone. If you have servers that will receive traffic only through a VPN connection to your network, create a separate subnet for these servers in each Availability Zone.\n\nIf your application will receive traffic from the internet, the VPC must have an internet gateway. Attaching an internet gateway to a VPC does not automatically make your instances accessible from the internet. In addition to attaching the internet gateway, you must update the subnet route table with a route to the internet gateway. You must also ensure that the instances have public IP addresses and an associated security group that allows traffic from the internet over specific ports and protocols required by your application.\n\nAlternatively, register your instances with an internet-facing load balancer. The load balancer receives traffic from the clients and distributes it across the registered instances in one or more Availability Zones. For more information, see Elastic Load Balancing . To allow instances in a private subnet to access the internet (for example, to download updates) without allowing unsolicited inbound connections from the internet, add a public NAT gateway in each active Availability Zone and update the route table to send internet traffic to the NAT gateway. For more information, see Access the internet from a private subnet .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1721
        }
      },
      {
        "header": "Create your VPC",
        "content": "After you've determined the number of VPCs and subnets that you need, what CIDR blocks to assign to your VPCs and subnets, and how to connect your VPC to the internet, you are ready to create your VPC. If you create your VPC using the AWS Management Console and include public subnets in your configuration, we create a route table for the subnet and add the routes required for direct access to the internet. For more information, see Create a VPC .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 450
        }
      },
      {
        "header": "Deploy your application",
        "content": "After you've created your VPC, you can deploy your application.\n\nProduction environment For a production environment, you can use one of the following services to deploy servers in multiple Availability Zones, to configure scaling so that you maintain the minimum number of servers required by your application, and to register your servers with a load balancer to distribute traffic evenly across your servers. Amazon EC2 Auto Scaling EC2 Fleet Amazon Elastic Container Service (Amazon ECS)\n\nDevelopment or test environment\n\nFor a development or test environment, you might choose to launch a single EC2 instance. For more information, see Get started with Amazon EC2 in the Amazon EC2 User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 699
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-getting-started.html",
    "doc_type": "aws",
    "total_sections": 7
  },
  {
    "title": "Subnets for your VPC",
    "summary": "A subnet is a range of IP addresses in your VPC. You can create AWS resources, such as EC2 instances, in specific subnets. Subnet basics",
    "sections": [
      {
        "header": "Subnet basics",
        "content": "Each subnet must reside entirely within one Availability Zone and cannot span zones. By launching AWS resources in separate Availability Zones, you can protect your applications from the failure of a single Availability Zone.\n\nContents Subnet IP address range Subnet types Subnet diagram Subnet routing Subnet settings\n\nSubnet IP address range\n\nWhen you create a subnet, you specify its IP addresses, depending on the configuration of the VPC:\n\nIPv4 only â The subnet has an IPv4 CIDR block but does not have an IPv6 CIDR block. Resources in an IPv4-only subnet must communicate over IPv4. Dual stack â The subnet has both an IPv4 CIDR block and an IPv6 CIDR block. The VPC must have both an IPv4 CIDR block and an IPv6 CIDR block. Resources in a dual-stack subnet can communicate over IPv4 and IPv6. IPv6 only â The subnet has an IPv6 CIDR block but does not have an IPv4 CIDR block. The VPC must have an IPv6 CIDR block. Resources in an IPv6-only subnet must communicate over IPv6. Note Resources in IPv6-only subnets are assigned IPv4 link-local addresses from CIDR block 169.254.0.0/16. These addresses are used to communicate with services that are available only in the VPC. For examples, see Link-local addresses in the Amazon EC2 User Guide .\n\nFor more information, see IP addressing for your VPCs and subnets .\n\nSubnet types\n\nThe subnet type is determined by how you configure routing for your subnets. For example:\n\nPublic subnet â The subnet has a direct route to an internet gateway . Resources in a public subnet can access the public internet. Private subnet â The subnet does not have a direct route to an internet gateway. Resources in a private subnet require a NAT device to access the public internet. VPN-only subnet â The subnet has a route to a Site-to-Site VPN connection through a virtual private gateway. The subnet does not have a route to an internet gateway. Isolated subnet â The subnet has no routes to destinations outside its VPC. Resources in an isolated subnet can only access or be accessed by other resources in the same VPC. EVS subnet â This type of subnet is created using Amazon EVS. For more information, see VLAN subnet in the Amazon EVS User Guide .\n\nSubnet diagram\n\nThe following diagram shows a VPC with subnets in two Availability Zones and an internet gateway. Each Availability Zone has a public subnet and a private subnet.\n\nFor diagrams that show subnets in Local Zones and Wavelength Zones, see How AWS Local Zones work and How AWS Wavelength works .\n\nSubnet routing\n\nEach subnet must be associated with a route table, which specifies the allowed routes for outbound traffic leaving the subnet. Every subnet that you create is automatically associated with the main route table for the VPC. You can change the association, and you can change the contents of the main route table. For more information, see Configure route tables .\n\nSubnet settings\n\nAll subnets have a modifiable attribute that determines whether a network interface created in that subnet is assigned a public IPv4 address and, if applicable, an IPv6 address. This includes the primary network interface (for example, eth0) that's created for an instance when you launch an instance in that subnet. Regardless of the subnet attribute, you can still override this setting for a specific instance during launch.\n\nAfter you create a subnet, you can modify the following settings for the subnet: Auto-assign IP settings : Enables you to configure the auto-assign IP settings to automatically request a public IPv4 or IPv6 address for a new network interface in this subnet. Resource-based Name (RBN) settings : Enables you to specify the hostname type for EC2 instances in this subnet and configure how DNS A and AAAA record queries are handled. For more information, see Amazon EC2 instance hostname types in the Amazon EC2 User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3867
        }
      },
      {
        "header": "Subnet IP address range",
        "content": "When you create a subnet, you specify its IP addresses, depending on the configuration of the VPC:\n\nIPv4 only â The subnet has an IPv4 CIDR block but does not have an IPv6 CIDR block. Resources in an IPv4-only subnet must communicate over IPv4. Dual stack â The subnet has both an IPv4 CIDR block and an IPv6 CIDR block. The VPC must have both an IPv4 CIDR block and an IPv6 CIDR block. Resources in a dual-stack subnet can communicate over IPv4 and IPv6. IPv6 only â The subnet has an IPv6 CIDR block but does not have an IPv4 CIDR block. The VPC must have an IPv6 CIDR block. Resources in an IPv6-only subnet must communicate over IPv6. Note Resources in IPv6-only subnets are assigned IPv4 link-local addresses from CIDR block 169.254.0.0/16. These addresses are used to communicate with services that are available only in the VPC. For examples, see Link-local addresses in the Amazon EC2 User Guide .\n\nFor more information, see IP addressing for your VPCs and subnets .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 980
        }
      },
      {
        "header": "Subnet types",
        "content": "The subnet type is determined by how you configure routing for your subnets. For example:\n\nPublic subnet â The subnet has a direct route to an internet gateway . Resources in a public subnet can access the public internet. Private subnet â The subnet does not have a direct route to an internet gateway. Resources in a private subnet require a NAT device to access the public internet. VPN-only subnet â The subnet has a route to a Site-to-Site VPN connection through a virtual private gateway. The subnet does not have a route to an internet gateway. Isolated subnet â The subnet has no routes to destinations outside its VPC. Resources in an isolated subnet can only access or be accessed by other resources in the same VPC. EVS subnet â This type of subnet is created using Amazon EVS. For more information, see VLAN subnet in the Amazon EVS User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 867
        }
      },
      {
        "header": "Subnet diagram",
        "content": "The following diagram shows a VPC with subnets in two Availability Zones and an internet gateway. Each Availability Zone has a public subnet and a private subnet.\n\nFor diagrams that show subnets in Local Zones and Wavelength Zones, see How AWS Local Zones work and How AWS Wavelength works .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 291
        }
      },
      {
        "header": "Subnet routing",
        "content": "Each subnet must be associated with a route table, which specifies the allowed routes for outbound traffic leaving the subnet. Every subnet that you create is automatically associated with the main route table for the VPC. You can change the association, and you can change the contents of the main route table. For more information, see Configure route tables .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 362
        }
      },
      {
        "header": "Subnet settings",
        "content": "All subnets have a modifiable attribute that determines whether a network interface created in that subnet is assigned a public IPv4 address and, if applicable, an IPv6 address. This includes the primary network interface (for example, eth0) that's created for an instance when you launch an instance in that subnet. Regardless of the subnet attribute, you can still override this setting for a specific instance during launch.\n\nAfter you create a subnet, you can modify the following settings for the subnet: Auto-assign IP settings : Enables you to configure the auto-assign IP settings to automatically request a public IPv4 or IPv6 address for a new network interface in this subnet. Resource-based Name (RBN) settings : Enables you to specify the hostname type for EC2 instances in this subnet and configure how DNS A and AAAA record queries are handled. For more information, see Amazon EC2 instance hostname types in the Amazon EC2 User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 951
        }
      },
      {
        "header": "Subnet security",
        "content": "To protect your AWS resources, we recommend that you use private subnets. Use a bastion host or NAT device to provide internet access to resources, such as EC2 instances, in a private subnet.\n\nAWS provides features that you can use to increase security for the resources in your VPC. Security groups allow inbound and outbound traffic for associated resources, such as EC2 instances. Network ACLs allow or deny inbound and outbound traffic at the subnet level. In most cases, security groups can meet your needs. However, you can use network ACLs if you want an additional layer of security. For more information, see Compare security groups and network ACLs .\n\nBy design, each subnet must be associated with a network ACL. Every subnet that you create is automatically associated with the default network ACL for the VPC. The default network ACL allows all inbound and outbound traffic. You can update the default network ACL, or create custom network ACLs and associate them with your subnets. For more information, see Control subnet traffic with network access control lists .\n\nYou can create a flow log on your VPC or subnet to capture the traffic that flows to and from the network interfaces in your VPC or subnet. You can also create a flow log on an individual network interface. For more information, see Logging IP traffic using VPC Flow Logs .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1355
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html",
    "doc_type": "aws",
    "total_sections": 7
  },
  {
    "title": "What is Amazon DynamoDB?",
    "summary": "Amazon DynamoDB is a serverless, fully managed, distributed NoSQL database with single-digit millisecond performance at any scale. DynamoDB addresses your needs to overcome scaling and operational complexities of relational databases. DynamoDB is purpose-built and optimized for operational workloads that require consistent performance at any scale. For example, DynamoDB delivers consistent single-digit millisecond performance for a shopping cart use case, whether you've 10 or 100 million users. Launched in 2012 , DynamoDB continues to help you move away from relational databases while reducing cost and improving performance at scale.",
    "sections": [
      {
        "header": "Characteristics of DynamoDB",
        "content": "Serverless\n\nWith DynamoDB, you don't need to provision any servers, or patch, manage, install, maintain, or operate any software. DynamoDB provides zero downtime maintenance. It has no versions (major, minor, or patch), and there are no maintenance windows.\n\nDynamoDB's on-demand capacity mode offers pay-as-you-go pricing for read and write requests so you only pay for what you use. With on-demand, DynamoDB instantly scales up or down your tables to adjust for capacity and maintains performance with zero administration. It also scales down to zero so you don't pay for throughput when your table doesn't have traffic and there are no cold starts.\n\nNoSQL\n\nAs a NoSQL database, DynamoDB is purpose-built to deliver improved performance, scalability, manageability, and flexibility compared to traditional relational databases. To support a wide variety of use cases, DynamoDB supports both key-value and document data models.\n\nUnlike relational databases, DynamoDB doesn't support a JOIN operator. We recommend that you denormalize your data model to reduce database round trips and processing power needed to answer queries. As a NoSQL database, DynamoDB provides strong read consistency and ACID transactions to build enterprise-grade applications.\n\nFully managed\n\nAs a fully managed database service, DynamoDB handles the undifferentiated heavy lifting of managing a database so that you can focus on building value for your customers. It handles setup, configurations, maintenance, high availability, hardware provisioning, security, backups, monitoring, and more. This ensures that when you create a DynamoDB table, it's instantly ready for production workloads. DynamoDB constantly improves its availability, reliability, performance, security, and functionality without requiring upgrades or downtime.\n\nSingle-digit millisecond performance at any scale\n\nDynamoDB was purpose-built to improve upon the performance and scalability of relational databases to deliver single-digit millisecond performance at any scale. To achieve this scale and performance, DynamoDB is optimized for high-performance workloads and provides APIs that encourage efficient database usage. It omits features that are inefficient and non-performing at scale, for example, JOIN operations. DynamoDB delivers consistent single-digit millisecond performance for your application, whether you've 100 or 100 million users.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2402
        }
      },
      {
        "header": "Serverless",
        "content": "With DynamoDB, you don't need to provision any servers, or patch, manage, install, maintain, or operate any software. DynamoDB provides zero downtime maintenance. It has no versions (major, minor, or patch), and there are no maintenance windows.\n\nDynamoDB's on-demand capacity mode offers pay-as-you-go pricing for read and write requests so you only pay for what you use. With on-demand, DynamoDB instantly scales up or down your tables to adjust for capacity and maintains performance with zero administration. It also scales down to zero so you don't pay for throughput when your table doesn't have traffic and there are no cold starts.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 639
        }
      },
      {
        "header": "NoSQL",
        "content": "As a NoSQL database, DynamoDB is purpose-built to deliver improved performance, scalability, manageability, and flexibility compared to traditional relational databases. To support a wide variety of use cases, DynamoDB supports both key-value and document data models.\n\nUnlike relational databases, DynamoDB doesn't support a JOIN operator. We recommend that you denormalize your data model to reduce database round trips and processing power needed to answer queries. As a NoSQL database, DynamoDB provides strong read consistency and ACID transactions to build enterprise-grade applications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 593
        }
      },
      {
        "header": "Fully managed",
        "content": "As a fully managed database service, DynamoDB handles the undifferentiated heavy lifting of managing a database so that you can focus on building value for your customers. It handles setup, configurations, maintenance, high availability, hardware provisioning, security, backups, monitoring, and more. This ensures that when you create a DynamoDB table, it's instantly ready for production workloads. DynamoDB constantly improves its availability, reliability, performance, security, and functionality without requiring upgrades or downtime.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 541
        }
      },
      {
        "header": "Single-digit millisecond performance at any scale",
        "content": "DynamoDB was purpose-built to improve upon the performance and scalability of relational databases to deliver single-digit millisecond performance at any scale. To achieve this scale and performance, DynamoDB is optimized for high-performance workloads and provides APIs that encourage efficient database usage. It omits features that are inefficient and non-performing at scale, for example, JOIN operations. DynamoDB delivers consistent single-digit millisecond performance for your application, whether you've 100 or 100 million users.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 538
        }
      },
      {
        "header": "DynamoDB use cases",
        "content": "Customers across all sizes, industries, and geographies use DynamoDB to build modern, serverless applications that can start small and scale globally. DynamoDB is ideal for use cases that require consistent performance at any scale with little to zero operational overhead. The following list presents some use cases where you can use DynamoDB:\n\nFinancial service applications â Suppose you're a financial services company building applications, such as live trading and routing, loan management, token generation, and transaction ledgers. With DynamoDB global tables , your applications can respond to events and serve traffic from your chosen AWS Regions with fast, local read and write performance. DynamoDB is suitable for applications with the most stringent availability requirements. It removes the operational burden of manually scaling instances for increased storage or throughput, versioning, and licensing. You can use DynamoDB transactions to achieve atomicity, consistency, isolation, and durability (ACID) across one or more tables with a single request. (ACID) transactions suit workloads that include processing financial transactions or fulfilling orders. DynamoDB instantly accommodates your workloads as they ramp up or down, enabling you to efficiently scale your database for market conditions, such as trading hours. Gaming applications â As a gaming company, you can use DynamoDB for all parts of game platforms, for example, game state, player data, session history, and leaderboards. Choose DynamoDB for its scale, consistent performance, and the ease of operations provided by its serverless architecture. DynamoDB is well suited for scale-out architectures needed to support successful games. It quickly scales your gameâs throughput both in and out (scale to zero with no cold start). This scalability optimizes your architecture's efficiency whether youâre scaling out for peak traffic or scaling back when gameplay usage is low. Streaming applications â Media and entertainment companies use DynamoDB as a metadata index for content, content management service, or to serve near real-time sports statistics. They also use DynamoDB to run user watchlist and bookmarking services and process billions of daily customer events for generating recommendations. These customers benefit from DynamoDB's scalability, performance, and resiliency. DynamoDB scales to workload changes as they ramp up or down, enabling streaming media use cases that can support any levels of demand.\n\nTo learn more about how customers from different industries use DynamoDB, see Amazon DynamoDB Customers and This is My Architecture .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 2648
        }
      },
      {
        "header": "Capabilities of DynamoDB",
        "content": "Multi-active replication with global tables\n\nGlobal tables provide multi-active replication of your data across your chosen AWS Regions with 99.999% availability . Global tables deliver a fully managed solution for deploying a multi-Region, multi-active database, without building and maintaining your own replication solution. With global tables, you can specify the AWS Regions where you want the tables to be available. DynamoDB replicates ongoing data changes to all of these tables.\n\nYour globally distributed applications can access data locally in your selected Regions to achieve single-digit millisecond read and write performance. Because global tables are multi-active, you don't need a primary table. This means there are no complicated or delayed fail-overs, or database downtime when failing over an application between Regions.\n\nACID transactions\n\nDynamoDB is built for mission-critical workloads. It includes (ACID) transactions support for applications that require complex business logic. DynamoDB provides native, server-side support for transactions, simplifying the developer experience of making coordinated, all-or-nothing changes to multiple items within and across tables.\n\nChange data capture for event-driven architectures\n\nDynamoDB supports streaming of item-level change data capture (CDC) records in near-real time. It offers two streaming models for CDC: DynamoDB Streams and Kinesis Data Streams for DynamoDB . Whenever an application creates, updates, or deletes items in a table, streams records a time-ordered sequence of every item-level change in near-real time. This makes DynamoDB Streams ideal for applications with event-driven architecture to consume and act upon the changes.\n\nSecondary indexes\n\nDynamoDB offers the option to create both global and local secondary indexes , which let you query the table data using an alternate key. With these secondary indexes, you can access data with attributes other than the primary key, giving you maximum flexibility in accessing your data.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2025
        }
      },
      {
        "header": "Multi-active replication with global tables",
        "content": "Global tables provide multi-active replication of your data across your chosen AWS Regions with 99.999% availability . Global tables deliver a fully managed solution for deploying a multi-Region, multi-active database, without building and maintaining your own replication solution. With global tables, you can specify the AWS Regions where you want the tables to be available. DynamoDB replicates ongoing data changes to all of these tables.\n\nYour globally distributed applications can access data locally in your selected Regions to achieve single-digit millisecond read and write performance. Because global tables are multi-active, you don't need a primary table. This means there are no complicated or delayed fail-overs, or database downtime when failing over an application between Regions.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 797
        }
      },
      {
        "header": "ACID transactions",
        "content": "DynamoDB is built for mission-critical workloads. It includes (ACID) transactions support for applications that require complex business logic. DynamoDB provides native, server-side support for transactions, simplifying the developer experience of making coordinated, all-or-nothing changes to multiple items within and across tables.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 334
        }
      },
      {
        "header": "Change data capture for event-driven architectures",
        "content": "DynamoDB supports streaming of item-level change data capture (CDC) records in near-real time. It offers two streaming models for CDC: DynamoDB Streams and Kinesis Data Streams for DynamoDB . Whenever an application creates, updates, or deletes items in a table, streams records a time-ordered sequence of every item-level change in near-real time. This makes DynamoDB Streams ideal for applications with event-driven architecture to consume and act upon the changes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 467
        }
      },
      {
        "header": "Secondary indexes",
        "content": "DynamoDB offers the option to create both global and local secondary indexes , which let you query the table data using an alternate key. With these secondary indexes, you can access data with attributes other than the primary key, giving you maximum flexibility in accessing your data.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 286
        }
      },
      {
        "header": "Service integrations",
        "content": "DynamoDB broadly integrates with several AWS services to help you get more value from your data, eliminate undifferentiated heavy lifting, and operate your workloads at scale. Some examples are: AWS CloudFormation, Amazon CloudWatch, Amazon S3, AWS Identity and Access Management (IAM), and AWS Auto Scaling. The following sections describe some of the service integrations that you can perform using DynamoDB:\n\nServerless integrations\n\nTo build end-to-end serverless applications, DynamoDB integrates natively with a number of serverless AWS services. For example, you can integrate DynamoDB with AWS Lambda to create triggers , which are pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build event-driven applications that react to data modifications in DynamoDB tables. For cost optimization, you can filter events that Lambda processes from a DynamoDB stream.\n\nThe following list presents some examples of serverless integrations with DynamoDB:\n\nAWS AppSync for creating GraphQL APIs Amazon API Gateway for creating REST APIs Lambda for serverless compute Amazon Kinesis Data Streams for change data capture (CDC)\n\nImporting and exporting data to Amazon S3\n\nIntegrating DynamoDB with Amazon S3 enables you to easily export data to an Amazon S3 bucket for analytics and machine learning. DynamoDB supports full table exports and incremental exports to export changed, updated, or deleted data between a specified time period. You can also import data from Amazon S3 into a new DynamoDB table.\n\nZero-ETL integration\n\nDynamoDB supports zero-ETL integration with Amazon Redshift and Using an OpenSearch Ingestion pipeline with Amazon DynamoDB . These integrations enable you to run complex analytics and use advanced search capabilities on your DynamoDB table data. For example, you can perform full-text and vector search, and semantic search on your DynamoDB data. Zero-ETL integrations have no impact on production workloads running on DynamoDB.\n\nCaching\n\nDynamoDB Accelerator (DAX) is a fully managed, highly available caching service built for DynamoDB. DAX delivers up to 10 times performance improvement â from milliseconds to microseconds â even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring you to manage cache invalidation, data population, or cluster management.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2428
        }
      },
      {
        "header": "Serverless integrations",
        "content": "To build end-to-end serverless applications, DynamoDB integrates natively with a number of serverless AWS services. For example, you can integrate DynamoDB with AWS Lambda to create triggers , which are pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build event-driven applications that react to data modifications in DynamoDB tables. For cost optimization, you can filter events that Lambda processes from a DynamoDB stream.\n\nThe following list presents some examples of serverless integrations with DynamoDB:\n\nAWS AppSync for creating GraphQL APIs Amazon API Gateway for creating REST APIs Lambda for serverless compute Amazon Kinesis Data Streams for change data capture (CDC)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 728
        }
      },
      {
        "header": "Importing and exporting data to Amazon S3",
        "content": "Integrating DynamoDB with Amazon S3 enables you to easily export data to an Amazon S3 bucket for analytics and machine learning. DynamoDB supports full table exports and incremental exports to export changed, updated, or deleted data between a specified time period. You can also import data from Amazon S3 into a new DynamoDB table.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 333
        }
      },
      {
        "header": "Zero-ETL integration",
        "content": "DynamoDB supports zero-ETL integration with Amazon Redshift and Using an OpenSearch Ingestion pipeline with Amazon DynamoDB . These integrations enable you to run complex analytics and use advanced search capabilities on your DynamoDB table data. For example, you can perform full-text and vector search, and semantic search on your DynamoDB data. Zero-ETL integrations have no impact on production workloads running on DynamoDB.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 429
        }
      },
      {
        "header": "Caching",
        "content": "DynamoDB Accelerator (DAX) is a fully managed, highly available caching service built for DynamoDB. DAX delivers up to 10 times performance improvement â from milliseconds to microseconds â even at millions of requests per second. DAX does all the heavy lifting required to add in-memory acceleration to your DynamoDB tables, without requiring you to manage cache invalidation, data population, or cluster management.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 421
        }
      },
      {
        "header": "Security",
        "content": "DynamoDB utilizes IAM to help you securely control access to your DynamoDB resources. With IAM, you can centrally manage permissions that control which DynamoDB users can access resources. You use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources. Because DynamoDB utilizes IAM, there are no user names or passwords for accessing DynamoDB. Because you don't have any complicated password rotation policies to manage, it simplifies your security posture. With IAM, you can also enable fine-grained access control to provide authorization at the attribute level. You can also define resource-based policies with support for IAM Access Analyzer and Block Public Access (BPA) to simplify policy management.\n\nBy default, DynamoDB encrypts all customer data at rest. Encryption at rest enhances the security of your data by using encryption keys stored in AWS Key Management Service (AWS KMS). With encryption at rest, you can build security-sensitive applications that meet strict encryption compliance and regulatory requirements. When you access an encrypted table, DynamoDB decrypts the table data transparently. You don't have to change any code or applications to use or manage encrypted tables. DynamoDB continues to deliver the same single-digit millisecond latency that you've come to expect, and all DynamoDB queries work seamlessly on your encrypted data.\n\nYou can specify whether DynamoDB should use an AWS owned key (default encryption type), AWS managed key, or a Customer managed key to encrypt user data. The default encryption using AWS-owned KMS keys is available at no additional charge. For client-side encryption, you can use the AWS Database Encryption SDK .\n\nDynamoDB also adheres to several compliance standards , including HIPAA, PCI DSS, and GDPR, which enables you to meet regulatory requirements.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1866
        }
      },
      {
        "header": "Resilience",
        "content": "By default, DynamoDB automatically replicates your data across three Availability Zones to provide high durability and a 99.99% availability SLA. DynamoDB also provides additional capabilities to help you achieve your business continuity and disaster recovery objectives.\n\nDynamoDB includes the following features to help support your data resiliency and backup needs:\n\nFeatures Global tables Continuous backups and point-in-time recovery On-demand backup and restore\n\nGlobal tables\n\nDynamoDB global tables enable a 99.999% availability SLA and multi-Region resilience. This helps you build resilient applications and optimize them for the lowest recovery time objective (RTO) and recovery point objective (RPO). Global tables also integrates with AWS Fault Injection Service (AWS FIS) to perform fault injection experiments on your global table workloads. For example, pausing global table replication to any replica table.\n\nContinuous backups and point-in-time recovery\n\nContinuous backups provide you per-second granularity and the ability to initiate a point-in-time recovery. With point-in-time recovery, you can restore a table to any point in time up to the second during the last 35 days. You can set the recovery period to any value between 1 and 35 days.\n\nContinuous backups and initiating a point-in-time restore doesn't use provisioned capacity. They also don't have any impact on the performance or availability of your applications.\n\nOn-demand backup and restore\n\nOn-demand backup and restore let you create full backups of a table for long-term retention and archival for regulatory compliance needs. Backups don't impact the performance of your table and you can back up tables of any size. With AWS Backup integration , you can use AWS Backup to schedule, copy, tag, and manage the life cycle of your DynamoDB on-demand backups automatically. Using AWS Backup, you can copy on-demand backups across accounts and Regions, and transition older backups to cold storage for cost-optimization.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2005
        }
      },
      {
        "header": "Global tables",
        "content": "DynamoDB global tables enable a 99.999% availability SLA and multi-Region resilience. This helps you build resilient applications and optimize them for the lowest recovery time objective (RTO) and recovery point objective (RPO). Global tables also integrates with AWS Fault Injection Service (AWS FIS) to perform fault injection experiments on your global table workloads. For example, pausing global table replication to any replica table.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 440
        }
      },
      {
        "header": "Continuous backups and point-in-time recovery",
        "content": "Continuous backups provide you per-second granularity and the ability to initiate a point-in-time recovery. With point-in-time recovery, you can restore a table to any point in time up to the second during the last 35 days. You can set the recovery period to any value between 1 and 35 days.\n\nContinuous backups and initiating a point-in-time restore doesn't use provisioned capacity. They also don't have any impact on the performance or availability of your applications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 473
        }
      },
      {
        "header": "On-demand backup and restore",
        "content": "On-demand backup and restore let you create full backups of a table for long-term retention and archival for regulatory compliance needs. Backups don't impact the performance of your table and you can back up tables of any size. With AWS Backup integration , you can use AWS Backup to schedule, copy, tag, and manage the life cycle of your DynamoDB on-demand backups automatically. Using AWS Backup, you can copy on-demand backups across accounts and Regions, and transition older backups to cold storage for cost-optimization.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 527
        }
      },
      {
        "header": "Accessing DynamoDB",
        "content": "You can work with DynamoDB using the AWS Management Console , the AWS Command Line Interface , NoSQL Workbench for DynamoDB , or DynamoDB APIs .\n\nFor more information, see Accessing DynamoDB .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 192
        }
      },
      {
        "header": "DynamoDB pricing",
        "content": "DynamoDB charges for reading, writing, and storing data in your tables, along with any optional features you choose to enable. DynamoDB has two capacity modes with their respective billing options for processing reads and writes on your tables: on-demand and provisioned .\n\nDynamoDB is also included in the always free tier , providing 25 GB of storage. The Always free tier also includes 25 provisioned Write and 25 provisioned Read Capacity Units (WCU, RCU) which is enough to handle 200 M requests per month.\n\nFor more information, see Amazon DynamoDB pricing .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 564
        }
      },
      {
        "header": "Getting started with DynamoDB",
        "content": "If you're a first-time user of DynamoDB, we recommend that you begin by reading the following topics:\n\nGetting started with DynamoDB â Walks you through the process of setting up DynamoDB, creating sample tables, and uploading data. This topic also provides information about performing some basic database operations using the AWS Management Console, AWS CLI, NoSQL Workbench, and DynamoDB APIs. DynamoDB core components â Describes the basic DynamoDB concepts. Best practices for designing and architecting with DynamoDB â Provides recommendations about NoSQL design, DynamoDB Well-Architected Lens, table design and several other DynamoDB features. These best practices help you maximize performance and minimize throughput costs when working with DynamoDB.\n\nWe also recommend that you review the following tutorials that present complete end-to-end procedures to familiarize yourself with DynamoDB. You can complete these tutorials using the always free tier feature.\n\nCreate and Query a NoSQL Table with Amazon DynamoDB Build an Application Using a NoSQL Key-Value Data Store\n\nFor information about resources, tools, and strategies to migrate to DynamoDB, see Migrating to DynamoDB . To read the latest blogs and whitepapers, see Amazon DynamoDB resources .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1268
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html",
    "doc_type": "aws",
    "total_sections": 24
  },
  {
    "title": "Getting started with DynamoDB",
    "summary": "Youâll learn how to connect to, create, and manage DynamoDB tables in the following sections. Before you begin, you should familiarize yourself with the basic concepts in Amazon DynamoDB. You can get a quick overview in What is Amazon DynamoDB? and a more in-depth look in Core components of Amazon DynamoDB . Then, continue on to Prerequisites .",
    "sections": [
      {
        "header": "Prerequisites",
        "content": "Before starting the Amazon DynamoDB tutorial, learn about the ways you can access DynamoDB in Accessing DynamoDB . Then, set up DynamoDB through either the web service or the locally downloaded version in Setting up DynamoDB . After that, continue on to Step 1: Create a table in DynamoDB .\n\nNote If you plan to interact with DynamoDB only through the AWS Management Console, you don't need an AWS access key. Complete the steps in Signing up for AWS , and then continue on to Step 1: Create a table in DynamoDB . If you don't want to sign up for a free tier account, you can set up DynamoDB local (downloadable version) . Then continue on to Step 1: Create a table in DynamoDB . There are differences when working with CLI commands in terminals on Linux and Windows. The following guide presents commands formatted for Linux terminals (this includes macOS), and commands formatted for Windows CMD. Choose the command that best fits the terminal application you are using.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 972
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStartedDynamoDB.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is Amazon CloudWatch?",
    "summary": "Amazon CloudWatch monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real time, and offers many tools to give you system-wide observability of your application performance, operational health, and resource utilization. Operational visibility with metrics, alarms, and dashboards",
    "sections": [
      {
        "header": "Operational visibility with metrics, alarms, and dashboards",
        "content": "Metrics collect and track key performance data at user-defined intervals. Many AWS services automatically report metrics into CloudWatch, and you can also publish custom metrics in CloudWatch from your applications.\n\nDashboards offer a unified view of your resources and applications with visualizations of your metrics and logs in a single location. You can also share dashboards across accounts and Regions for enhanced operational awareness. CloudWatch provides curated automatic dashboards for many AWS services, so that you don't have to build them yourself.\n\nYou can set up alarms that continuously monitor CloudWatch metrics against user-defined thresholds. They can automatically alert you to breaches of the thresholds, and can also automatically respond to changes in your resources' behavior by triggering automated actions .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 836
        }
      },
      {
        "header": "Application performance monitoring (APM)",
        "content": "With Application Signals you can automatically detect and monitor your applications' key performance indicators like latency, error rates, and request rates without manual instrumentation or code changes. Application Signals also provides curated dashboards so you can begin monitoring with a minimum of setup.\n\nCloudWatch Synthetics complements this by enabling you to proactively monitor your endpoints and APIs through configurable scripts called canaries that simulate user behavior and alert you to availability issues or performance degradation before they impact real users. You can also use CloudWatch RUM to gather performance data from real user sessions.\n\nUse Service Level Objectives (SLOs) in CloudWatch to define, track, and alert on specific reliability targets for your applications, helping you maintain service quality commitments by setting error budgets and monitoring SLO compliance over time.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 914
        }
      },
      {
        "header": "Infrastructure monitoring",
        "content": "Many AWS services automatically send basic metrics to CloudWatch for free. Services that send metrics are listed here . Additionally, CloudWatch provides additional monitoring capabilities for several key pieces of AWS infrastructure:\n\nDatabase Insights allows you to monitor database performance metrics in real time, analyze SQL query performance, and troubleshoot database load issues for AWS database services. Lambda Insights provides system-level metrics for Lambda functions, including memory and CPU utilization tracking, and cold start detection and analysis. Container Insights allows you to collect and analyze metrics from containerized applications, on Amazon ECS clusters, Amazon EKS clusters, and self-managed Kubernetes clusters on Amazon EC2.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 759
        }
      },
      {
        "header": "Collect, store, and query logs",
        "content": "CloudWatch Logs offers a suite of powerful features for comprehensive log management and analysis. Logs ingested from AWS services and custom applications are stored in log groups and streams for easy organization. Use CloudWatch Logs Insights to perform interactive, fast queries on your log data, with a choice of three query languages including SQL and PPL. Use log outlier detection to find unusual patterns in log events in a log group, which can indicate issues. Create metric filters to extract numerical values from logs and generate CloudWatch metrics, which you can use for alerting and dashboards. Set up subscription filters to process and analyze logs in real-time or route them to other services like Amazon S3 or Firehose.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 737
        }
      },
      {
        "header": "Use the CloudWatch agent to gather metrics, logs, and traces from Amazon EC2 fleets",
        "content": "Use the CloudWatch agent to collect detailed system metrics about processes, CPU, memory, disk usage, and network performance from your fleets of Amazon EC2 instances and on-premises servers. You can also collect and monitor custom metrics from your applications, aggregate logs from multiple sources, and configure alarms based on the collected data. You can also use the agent to gather GPU metrics . The agent supports both Windows and Linux operating systems and can integrate with Systems Manager for centralized configuration management.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 543
        }
      },
      {
        "header": "Cross-account monitoring",
        "content": "CloudWatch cross-account observability lets you set up a central monitoring account to monitor and troubleshoot applications that span multiple accounts. From the central account, you can view metrics, logs, and traces from source accounts across your organization. This centralized approach enables you to create cross-account dashboards, set up alarms that watch metrics from multiple accounts, and perform root-cause analysis across account boundaries. With CloudWatch cross-account observability, you can link source accounts either individually or link them automatically through AWS Organizations.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 603
        }
      },
      {
        "header": "Solutions catalog",
        "content": "CloudWatch offers a catalog of readily available configurations to help you quickly implement monitoring for various AWS services and common workloads, such as Java Virtual Machines (JVM) , NVIDIA GPU , Apache Kafka , Apache Tomcat , and NGINX . These solutions provide focused guidance, including instructions for installing and configuring the CloudWatch agent, deploying pre-defined custom dashboards, and setting up related alarms.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 435
        }
      },
      {
        "header": "Network and internet monitoring",
        "content": "CloudWatch provides comprehensive network and internet monitoring capabilities through CloudWatch Network Monitoring.\n\nInternet Monitor uses AWS global networking data to analyze internet performance and availability between your applications and end users. With an internet monitor, you can identify or get notifications for increased latency or regional disruptions that impact your customers. Internet monitors work by analyzing your VPC flow logs to provide automated insights about network traffic patterns and performance. You can also get suggestions for how to optimize application performance for your clients.\n\nNetwork Flow Monitor displays network performance information gathered by lightweight software agents that you install on your instances. Using a flow monitor, you can quickly visualize packet loss and latency of your network connections over a time frame that you specify. Each monitor also generates a network health indicator (NHI), which tells you whether there were AWS network issues for the network flows tracked by your monitor during the time period that you're evaluating.\n\nWhen you connect by using Direct Connect, you can use synthetic monitors in Network Synthetic Monitor to proactively monitor network connectivity by running synthetic tests between a VPC and on-premises endpoints. When you create a synthetic monitor, you specify probes by providing a VPC subnet and on-premises IP addresses. AWS creates and manages the infrastructure in the background that is required to perform round-trip time and packet loss measurements with the probes. These tests detect issues with connectivity, DNS, and latency before they impact your applications, so that you can take action to improve your end usersâ experience.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1750
        }
      },
      {
        "header": "Billing and costs",
        "content": "For complete information about CloudWatch pricing, see Amazon CloudWatch Pricing .\n\nFor information that can help you analyze your bill and possibly optimize and reduce costs, see Analyzing, optimizing, and reducing CloudWatch costs .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 234
        }
      },
      {
        "header": "Amazon CloudWatch resources",
        "content": "The following related resources can help you as you work with this service.\n\nResource Description Amazon CloudWatch FAQs The FAQ covers the top questions developers have asked about this product. AWS Developer Center A central starting point to find documentation, code examples, release notes, and other information to help you build innovative applications with AWS. AWS Management Console The console allows you to perform most of the functions of Amazon CloudWatch and various other AWS offerings without programming. Amazon CloudWatch Discussion Forums Community-based forum for developers to discuss technical questions related to Amazon CloudWatch. AWS Support The hub for creating and managing your AWS Support cases. Also includes links to other helpful resources, such as forums, technical FAQs, service health status, and AWS Trusted Advisor. Amazon CloudWatch product information The primary web page for information about Amazon CloudWatch. Contact Us A central contact point for inquiries concerning AWS billing, account, events, abuse, etc.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1055
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html",
    "doc_type": "aws",
    "total_sections": 10
  },
  {
    "title": "Getting started with CloudWatch automatic dashboards",
    "summary": "The CloudWatch home page automatically displays metrics about every AWS service you use. You can additionally create custom dashboards to display metrics about your custom applications, and display custom collections of metrics that you choose. Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ .",
    "sections": [
      {
        "header": "",
        "content": "Getting started with CloudWatch automatic dashboards The CloudWatch home page automatically displays metrics about every AWS service you use. You can additionally create custom dashboards to display metrics about your custom applications, and display custom collections of metrics that you choose. Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ . The CloudWatch overview home page appears. The overview displays the following items, refreshed automatically. Alarms by AWS service displays a list of AWS services you use in your account, along with the state of alarms in those services. Next to that, two or four alarms in your account are displayed. The number depends on how many AWS services you use. The alarms shown are those in the ALARM state or those that most recently changed state. These upper areas help you quickly assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues. Below these areas is the default dashboard , if one exists. The default dashboard is a custom dashboard that you have created and named CloudWatch-Default . This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. Note The automatic dashboards on the CloudWatch home page display only information from the current account, even if the account is a monitoring account set up for CloudWatch cross-account observability. For information about creating custom cross-account dashboards, see Creating a CloudWatch cross-account cross-Region dashboard with the AWS Management Console . From this overview, you can see a cross-service dashboard of metrics from multiple AWS service, or focus your view to a specific resource group or a specific AWS service. This enables you to narrow your view to a subset of resources in which you are interested. Topics Viewing the cross-service CloudWatch dashboard Removing a service from appearing in the CloudWatch cross-service dashboard Viewing a CloudWatch dashboard for a single AWS service Viewing a CloudWatch dashboard for a resource group Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Dashboards Viewing the cross-service dashboard",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 21,
          "content_length": 2530
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/GettingStarted.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is Elastic Load Balancing?",
    "summary": "Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing scales your load balancer capacity automatically in response to changes in incoming traffic. A load balancer distributes workloads across multiple compute resources, such as virtual servers. Using a load balancer increases the availability and fault tolerance of your applications.",
    "sections": [
      {
        "header": "Load balancer benefits",
        "content": "A load balancer distributes workloads across multiple compute resources, such as virtual servers. Using a load balancer increases the availability and fault tolerance of your applications.\n\nYou can add and remove compute resources from your load balancer as your needs change, without disrupting the overall flow of requests to your applications.\n\nYou can configure health checks, which monitor the health of the compute resources, so that the load balancer sends requests only to the healthy ones. You can also offload the work of encryption and decryption to your load balancer so that your compute resources can focus on their main work.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 640
        }
      },
      {
        "header": "Features of Elastic Load Balancing",
        "content": "Elastic Load Balancing supports multiple load balancer types. You can select the type of load balancer that best suits your needs. For more information, see Elastic Load Balancing features .\n\nFor more information about the current generation load balancers, see the following documentation:\n\nUser Guide for Application Load Balancers User Guide for Network Load Balancers User Guide for Gateway Load Balancers\n\nClassic Load Balancers are the previous generation of load balancers from Elastic Load Balancing. We recommend that you migrate to a current generation load balancer. For more information, see Migrate your Classic Load Balancer .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 640
        }
      },
      {
        "header": "Accessing Elastic Load Balancing",
        "content": "You can create, access, and manage your load balancers using any of the following interfaces:\n\nAWS Management Console â Provides a web interface that you can use to access Elastic Load Balancing. AWS Command Line Interface (AWS CLI) â Provides commands for a broad set of AWS services, including Elastic Load Balancing. The AWS CLI is supported on Windows, macOS, and Linux. For more information, see AWS Command Line Interface . AWS SDKs â Provide language-specific APIs and take care of many of the connection details, such as calculating signatures, handling request retries, and error handling. For more information, see AWS SDKs . Query API â Provides low-level API actions that you call using HTTPS requests. Using the Query API is the most direct way to access Elastic Load Balancing. However, the Query API requires that your application handle low-level details such as generating the hash to sign the request, and error handling. For more information, see the following: Application Load Balancers, Network Load Balancers, and Gateway Load Balancers â API version 2015-12-01 Classic Load Balancers â API version 2012-06-01",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1144
        }
      },
      {
        "header": "Related services",
        "content": "Elastic Load Balancing works with the following services to improve the availability and scalability of your applications.\n\nAmazon EC2 â Virtual servers that run your applications in the cloud. You can configure your load balancer to route traffic to your EC2 instances. For more information, see the Amazon EC2 User Guide . Amazon EC2 Auto Scaling â Ensures that you are running your desired number of instances, even if an instance fails. Amazon EC2 Auto Scaling also enables you to automatically increase or decrease the number of instances as the demand on your instances changes. If you enable Auto Scaling with Elastic Load Balancing, instances that are launched by Auto Scaling are automatically registered with the load balancer. Likewise, instances that are terminated by Auto Scaling are automatically de-registered from the load balancer. For more information, see the Amazon EC2 Auto Scaling User Guide . AWS Certificate Manager â When you create an HTTPS listener, you can specify certificates provided by ACM. The load balancer uses certificates to terminate connections and decrypt requests from clients. Amazon CloudWatch â Enables you to monitor your load balancer and to take action as needed. For more information, see the Amazon CloudWatch User Guide . Amazon ECS â Enables you to run, stop, and manage Docker containers on a cluster of EC2 instances. You can configure your load balancer to route traffic to your containers. For more information, see the Amazon Elastic Container Service Developer Guide . AWS Global Accelerator â Improves the availability and performance of your application. Use an accelerator to distribute traffic across multiple load balancers in one or more AWS Regions. For more information, see the AWS Global Accelerator Developer Guide . RouteÂ 53 â Provides a reliable and cost-effective way to route visitors to websites by translating domain names into the numeric IP addresses that computers use to connect to each other. For example, it would translate www.example.com into the numeric IP address 192.0.2.1 . AWS assigns URLs to your resources, such as load balancers. However, you might want a URL that is easy for users to remember. For example, you can map your domain name to a load balancer. For more information, see the Amazon RouteÂ 53 Developer Guide . AWS WAF â You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). For more information, see the AWS WAF Developer Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 2538
        }
      },
      {
        "header": "Pricing",
        "content": "With your load balancer, you pay only for what you use. For more information, see Elastic Load Balancing pricing .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 114
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "What is an Application Load Balancer?",
    "summary": "Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It monitors the health of its registered targets, and routes traffic only to the healthy targets. Elastic Load Balancing scales your load balancer as your incoming traffic changes over time. It can automatically scale to the vast majority of workloads. Elastic Load Balancing supports the following load balancers: Application Load Balancers, Network Load Balancers, Gateway Load Balancers, and Classic Load Balancers. You can select the type of load balancer that best suits your needs. This guide discusses Application Load Balancers. For more information about the other load balancers, see the User Guide for Network Load Balancers , the User Guide for Gateway Load Balancers , and the User Guide for Classic Load Balancers .",
    "sections": [
      {
        "header": "Application Load Balancer components",
        "content": "A load balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer.\n\nA listener checks for connection requests from clients, using the protocol and port that you configure. The rules that you define for a listener determine how the load balancer routes requests to its registered targets. Each rule consists of a priority, one or more actions, and one or more conditions. When the conditions for a rule are met, then its actions are performed. You must define a default rule for each listener, and you can optionally define additional rules.\n\nEach target group routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. You can register a target with multiple target groups. You can configure health checks on a per target group basis. Health checks are performed on all targets registered to a target group that is specified in a listener rule for your load balancer.\n\nThe following diagram illustrates the basic components. Notice that each listener contains a default rule, and one listener contains another rule that routes requests to a different target group. One target is registered with two target groups.\n\nFor more information, see the following documentation:\n\nLoad balancers Listeners Target groups",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1522
        }
      },
      {
        "header": "Application Load Balancer overview",
        "content": "An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply, and then selects a target from the target group for the rule action. You can configure listener rules to route requests to different target groups based on the content of the application traffic. Routing is performed independently for each target group, even when a target is registered with multiple target groups. You can configure the routing algorithm used at the target group level. The default routing algorithm is round robin; alternatively, you can specify the least outstanding requests routing algorithm.\n\nYou can add and remove targets from your load balancer as your needs change, without disrupting the overall flow of requests to your application. Elastic Load Balancing scales your load balancer as traffic to your application changes over time. Elastic Load Balancing can scale to the vast majority of workloads automatically.\n\nYou can configure health checks, which are used to monitor the health of the registered targets so that the load balancer can send requests only to the healthy targets.\n\nFor more information, see How Elastic Load Balancing works in the Elastic Load Balancing User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1375
        }
      },
      {
        "header": "Benefits of migrating from a Classic Load Balancer",
        "content": "Using an Application Load Balancer instead of a Classic Load Balancer has the following benefits:\n\nSupport for Path conditions . You can configure rules for your listener that forward requests based on the URL in the request. This enables you to structure your application as smaller services, and route requests to the correct service based on the content of the URL. Support for Host conditions . You can configure rules for your listener that forward requests based on the host field in the HTTP header. This enables you to route requests to multiple domains using a single load balancer. Support for routing based on fields in the request, such as HTTP header conditions and methods, query parameters, and source IP addresses. Support for routing requests to multiple applications on a single EC2 instance. You can register an instance or IP address with multiple target groups, each on a different port. Support for redirecting requests from one URL to another. Support for returning a custom HTTP response. Support for registering targets by IP address, including targets outside the VPC for the load balancer. Support for registering Lambda functions as targets. Support for the load balancer to authenticate users of your applications through their corporate or social identities before routing requests. Support for containerized applications. Amazon Elastic Container Service (Amazon ECS) can select an unused port when scheduling a task and register the task with a target group using this port. This enables you to make efficient use of your clusters. Support for monitoring the health of each service independently, as health checks are defined at the target group level and many CloudWatch metrics are reported at the target group level. Attaching a target group to an Auto Scaling group enables you to scale each service dynamically based on demand. Access logs contain additional information and are stored in compressed format. Improved load balancer performance.\n\nFor more information about the features supported by each load balancer type, see Elastic Load Balancing features .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 2097
        }
      },
      {
        "header": "Related services",
        "content": "Elastic Load Balancing works with the following services to improve the availability and scalability of your applications.\n\nAmazon EC2 â Virtual servers that run your applications in the cloud. You can configure your load balancer to route traffic to your EC2 instances. Amazon EC2 Auto Scaling â Ensures that you are running your desired number of instances, even if an instance fails, and enables you to automatically increase or decrease the number of instances as the demand on your instances changes. If you enable Auto Scaling with Elastic Load Balancing, instances that are launched by Auto Scaling are automatically registered with the target group, and instances that are terminated by Auto Scaling are automatically de-registered from the target group. AWS Certificate Manager â When you create an HTTPS listener, you can specify certificates provided by ACM. The load balancer uses certificates to terminate connections and decrypt requests from clients. For more information, see SSL certificates for your Application Load Balancer . Amazon CloudWatch â Enables you to monitor your load balancer and take action as needed. For more information, see CloudWatch metrics for your Application Load Balancer . Amazon ECS â Enables you to run, stop, and manage Docker containers on a cluster of EC2 instances. You can configure your load balancer to route traffic to your containers. For more information, see Service load balancing in the Amazon Elastic Container Service Developer Guide . AWS Global Accelerator â Improves the availability and performance of your application. Use an accelerator to distribute traffic across multiple load balancers in one or more AWS Regions. For more information, see the AWS Global Accelerator Developer Guide . RouteÂ 53 â Provides a reliable and cost-effective way to route visitors to websites by translating domain names (such as www.example.com ) into the numeric IP addresses (such as 192.0.2.1 ) that computers use to connect to each other. AWS assigns URLs to your resources, such as load balancers. However, you might want a URL that is easy for users to remember. For example, you can map your domain name to a load balancer. For more information, see Routing traffic to an ELB load balancer in the Amazon RouteÂ 53 Developer Guide . AWS WAF â You can use AWS WAF with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL). For more information, see AWS WAF .\n\nTo view information about services that are integrated with your load balancer, select your load balancer in the AWS Management Console and choose the Integrated services tab.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 2668
        }
      },
      {
        "header": "Pricing",
        "content": "With your load balancer, you pay only for what you use. For more information, see Elastic Load Balancing pricing .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 114
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "What is Amazon EC2 Auto Scaling?",
    "summary": "Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups . You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size. You can specify the maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes above this size. If you specify the desired capacity, either when you create the group or at any time thereafter, Amazon EC2 Auto Scaling ensures that your group has this many instances. If you specify scaling policies, then Amazon EC2 Auto Scaling can launch or terminate instances as demand on your application increases or decreases. For example, the following Auto Scaling group has a minimum size of four instances, a desired capacity of six instances, and a maximum size of twelve instances. The scaling policies that you define adjust the number of instances, within your minimum and maximum number of instances, based on the criteria that you specify.",
    "sections": [
      {
        "header": "Features of Amazon EC2 Auto Scaling",
        "content": "With Amazon EC2 Auto Scaling, your EC2 instances are organized into Auto Scaling groups so that they can be treated as a logical unit for the purposes of scaling and management. Auto Scaling groups use launch templates (or launch configurations) as configuration templates for their EC2 instances.\n\nThe following are key features of Amazon EC2 Auto Scaling:\n\nMonitoring the health of running instances Amazon EC2 Auto Scaling automatically monitors the health and availability of your instances using EC2 health checks and replaces terminated or impaired instances to maintain your desired capacity. Custom health checks In addition to the built-in health checks, you can define custom health checks that are specific to your application to verify that it's responding as expected. If an instance fails your custom health check, it's automatically replaced to maintain your desired capacity. Balancing capacity across Availability Zones You can specify multiple Availability Zones for your Auto Scaling group, and Amazon EC2 Auto Scaling balances your instances evenly across the Availability Zones as the group scales. This provides high availability and resiliency by protecting your applications from failures in a single location. Multiple instance types and purchase options Within a single Auto Scaling group, you can launch multiple instance types and purchase options (Spot and On-Demand Instances), allowing you to optimize costs through Spot Instance usage. You can also take advantage of Reserved Instance and Savings Plans discounts by using them in conjunction with On-Demand Instances in the group. Automated replacement of Spot Instances If your group includes Spot Instances, Amazon EC2 Auto Scaling can automatically request replacement Spot capacity if your Spot Instances are interrupted. Through Capacity Rebalancing, Amazon EC2 Auto Scaling can also monitor and proactively replace your Spot Instances that are at an elevated risk of interruption. Load balancing You can use Elastic Load Balancing load balancing and health checks to ensure an even distribution of application traffic to your healthy instances. Whenever instances are launched or terminated, Amazon EC2 Auto Scaling automatically registers and deregisters the instances from the load balancer. Scalability Amazon EC2 Auto Scaling also provides several ways for you to scale your Auto Scaling groups. Using auto scaling allows you to maintain application availability and reduce costs by adding capacity to handle peak loads and removing capacity when demand is lower. You can also manually adjust the size of your Auto Scaling group as needed. Instance refresh The instance refresh feature provides a mechanism to update instances in a rolling fashion when you update your AMI or launch template. You can also use a phased approach, known as a canary deployment, to test a new AMI or launch template on a small set of instances before rolling it out to the whole group. Lifecycle hooks Lifecycle hooks are useful for defining custom actions that are invoked as new instances launch or before instances are terminated. This feature is particularly useful for building event-driven architectures, but it also helps you manage instances through their lifecycle. Support for stateful workloads Lifecycle hooks also offer a mechanism for persisting state on shut down. To ensure continuity for stateful applications, you can also use scale-in protection or custom termination policies to prevent instances with long-running processes from terminating early.\n\nFor more information about the benefits of Amazon EC2 Auto Scaling, see Auto Scaling benefits for application architecture .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 3666
        }
      },
      {
        "header": "Pricing for Amazon EC2 Auto Scaling",
        "content": "There are no additional fees with Amazon EC2 Auto Scaling, so it's easy to try it out and see how it can benefit your AWS architecture. You only pay for the AWS resources (for example, EC2 instances, EBS volumes, and CloudWatch alarms) that you use.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 249
        }
      },
      {
        "header": "Get started",
        "content": "To begin, complete the Create your first Auto Scaling group tutorial to create an Auto Scaling group and see how it responds when an instance in that group terminates.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 167
        }
      },
      {
        "header": "Work with Auto Scaling groups",
        "content": "You can create, access, and manage your Auto Scaling groups using any of the following interfaces:\n\nAWS Management Console â Provides a web interface that you can use to access your Auto Scaling groups. If you've signed up for an AWS account, you can access your Auto Scaling groups by signing into the AWS Management Console, using the search box on the navigation bar to search for Auto Scaling groups , and then choosing Auto Scaling groups . AWS Command Line Interface (AWS CLI) â Provides commands for a broad set of AWS services, and is supported on Windows, macOS, and Linux. To get started, see Prepare to use the AWS CLI . For more information, see autoscaling in the AWS CLI Command Reference . AWS Tools for Windows PowerShell â Provides commands for a broad set of AWS products for those who script in the PowerShell environment. To get started, see the AWS Tools for PowerShell User Guide . For more information, see the AWS Tools for PowerShell Cmdlet Reference . AWS SDKs â Provides language-specific API operations and takes care of many of the connection details, such as calculating signatures, handling request retries, and handling errors. For more information, see AWS SDKs . Query API â Provides low-level API actions that you call using HTTPS requests. Using the Query API is the most direct way to access AWS services. However, it requires your application to handle low-level details such as generating the hash to sign the request, and handling errors. For more information, see the Amazon EC2 Auto Scaling API Reference . CloudFormation â Supports creating Auto Scaling groups using CloudFormation templates. For more information, see Create Auto Scaling groups with AWS CloudFormation .\n\nTo connect programmatically to an AWS service, you use an endpoint. For information about endpoints for calls to Amazon EC2 Auto Scaling, see Amazon EC2 Auto Scaling endpoints and quotas in the AWS General Reference .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1945
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Get started with Amazon EC2 Auto Scaling",
    "summary": "To get started with Amazon EC2 Auto Scaling, you can follow tutorials that introduce you to the service. Tutorial: Create your first Auto Scaling group",
    "sections": [
      {
        "header": "",
        "content": "Get started with Amazon EC2 Auto Scaling To get started with Amazon EC2 Auto Scaling, you can follow tutorials that introduce you to the service. Topics Tutorial: Create your first Auto Scaling group Tutorial: Set up a scaled and load-balanced application For additional tutorials that focus on specific tools for managing the lifecycle of instances in an Auto Scaling group, see the following topics: Tutorial: Configure a lifecycle hook that invokes a Lambda function . This tutorial shows you how to use Amazon EventBridge to create rules that invoke Lambda functions based on events that happen to the instances in your Auto Scaling group. Tutorial: Use data script and instance metadata to retrieve lifecycle state . This tutorial shows you how to use the Instance Metadata Service (IMDS) to invoke an action from within the instance itself. Before you create an Auto Scaling group for use with your application, review your application thoroughly as it runs in the AWS Cloud. Consider the following: How many Availability Zones the Auto Scaling group should span. What existing resources can be used, such as security groups or Amazon Machine Images (AMIs). Whether you want to scale to increase or decrease capacity, or if you just want to ensure that a specific number of servers are always running. Keep in mind that Amazon EC2 Auto Scaling can do both simultaneously. What metrics have the most relevance to your application's performance. How long it takes to launch and provision a server. The better you understand your application, the more effective you can make your Auto Scaling architecture. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Set up Tutorial: Create your first Auto Scaling group",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 16,
          "content_length": 1876
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/get-started-with-ec2-auto-scaling.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is Amazon Simple Queue Service?",
    "summary": "Amazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Amazon SQS offers common constructs such as dead-letter queues and cost allocation tags . It provides a generic web services API that you can access using any programming language that the AWS SDK supports. Security â You control who can send messages to and receive messages from an Amazon SQS queue. You can choose to transmit sensitive data by protecting the contents of messages in queues by using default Amazon SQS managed server-side encryption (SSE), or by using custom SSE keys managed in AWS Key Management Service (AWS KMS).",
    "sections": [
      {
        "header": "Benefits of using Amazon SQS",
        "content": "Security â You control who can send messages to and receive messages from an Amazon SQS queue. You can choose to transmit sensitive data by protecting the contents of messages in queues by using default Amazon SQS managed server-side encryption (SSE), or by using custom SSE keys managed in AWS Key Management Service (AWS KMS). Durability â For the safety of your messages, Amazon SQS stores them on multiple servers. Standard queues support at-least-once message delivery , and FIFO queues support exactly-once message processing and high-throughput mode. Availability â Amazon SQS uses redundant infrastructure to provide highly-concurrent access to messages and high availability for producing and consuming messages. Scalability â Amazon SQS can process each buffered request independently, scaling transparently to handle any load increases or spikes without any provisioning instructions. Reliability â Amazon SQS locks your messages during processing, so that multiple producers can send and multiple consumers can receive messages at the same time. Customization â Your queues don't have to be exactly alikeâfor example, you can set a default delay on a queue . You can store the contents of messages larger than 1 MiB using Amazon Simple Storage Service (Amazon S3) or Amazon DynamoDB, with Amazon SQS holding a pointer to the Amazon S3 object, or you can split a large message into smaller messages.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 0,
          "content_length": 1424
        }
      },
      {
        "header": "Basic Amazon SQS architecture",
        "content": "This section describes the components of a distributed messaging system and explains the lifecycle of an Amazon SQS message.\n\nDistributed queues\n\nThere are three main parts in a distributed messaging system: the components of your distributed system , your queue (distributed on Amazon SQS servers), and the messages in the queue .\n\nIn the following scenario, your system has several producers (components that send messages to the queue) and consumers (components that receive messages from the queue). The queue (which holds messages A through E) redundantly stores the messages across multiple Amazon SQS servers.\n\nMessage lifecycle\n\nThe following scenario describes the lifecycle of an Amazon SQS message in a queue, from creation to deletion.\n\nA producer (Component 1) sends message A to a queue, and the message is distributed across the Amazon SQS servers redundantly.\n\nWhen a consumer (Component 2) is ready to process messages, it consumes messages from the queue, and message A is returned. While message A is being processed, it remains in the queue and isn't returned to subsequent receive requests for the duration of the visibility timeout .\n\nThe consumer (Component 2) deletes message A from the queue to prevent the message from being received and processed again when the visibility timeout expires.\n\nNote Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 7,
          "content_length": 1635
        }
      },
      {
        "header": "Distributed queues",
        "content": "There are three main parts in a distributed messaging system: the components of your distributed system , your queue (distributed on Amazon SQS servers), and the messages in the queue .\n\nIn the following scenario, your system has several producers (components that send messages to the queue) and consumers (components that receive messages from the queue). The queue (which holds messages A through E) redundantly stores the messages across multiple Amazon SQS servers.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 470
        }
      },
      {
        "header": "Message lifecycle",
        "content": "The following scenario describes the lifecycle of an Amazon SQS message in a queue, from creation to deletion.\n\nA producer (Component 1) sends message A to a queue, and the message is distributed across the Amazon SQS servers redundantly.\n\nWhen a consumer (Component 2) is ready to process messages, it consumes messages from the queue, and message A is returned. While message A is being processed, it remains in the queue and isn't returned to subsequent receive requests for the duration of the visibility timeout .\n\nThe consumer (Component 2) deletes message A from the queue to prevent the message from being received and processed again when the visibility timeout expires.\n\nNote Amazon SQS automatically deletes messages that have been in a queue for more than the maximum message retention period. The default message retention period is 4 days. However, you can set the message retention period to a value from 60 seconds to 1,209,600 seconds (14 days) using the SetQueueAttributes action.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 998
        }
      },
      {
        "header": "Differences between Amazon SQS, Amazon MQ, and Amazon SNS",
        "content": "Amazon SQS, Amazon SNS , and Amazon MQ offer highly scalable and easy-to-use managed messaging services, each designed for specific roles within distributed systems. Here's an enhanced overview of the differences between these services:\n\nAmazon SQS decouples and scales distributed software systems and components as a queue service. It processes messages through a single subscriber typically, ideal for workflows where order and loss prevention are critical. For wider distribution, integrating Amazon SQS with Amazon SNS enables a fanout messaging pattern , effectively pushing messages to multiple subscribers at once.\n\nAmazon SNS allows publishers to send messages to multiple subscribers through topics, which serve as communication channels. Subscribers receive published messages using a supported endpoint type, such as Amazon Data Firehose , Amazon SQS , Lambda , HTTP, email, mobile push notifications, and mobile text messages (SMS). This service is ideal for scenarios requiring immediate notifications, such as real-time user engagement or alarm systems. To prevent message loss when subscribers are offline, integrating Amazon SNS with Amazon SQS queue messages ensures consistent delivery.\n\nAmazon MQ fits best with enterprises looking to migrate from traditional message brokers, supporting standard messaging protocols like AMQP and MQTT, along with Apache ActiveMQ and RabbitMQ . It offers compatibility with legacy systems needing stable, reliable messaging without significant reconfiguration.\n\nThe following chart provides an overview of each services' resource type:\n\nResource type Amazon SNS Amazon SQS Amazon MQ Synchronous No No Yes Asynchronous Yes Yes Yes Queues No Yes Yes Publisher-subscriber messaging Yes No Yes Message brokers No No Yes\n\nBoth Amazon SQS and Amazon SNS are recommended for new applications that can benefit from nearly unlimited scalability and simple APIs. They generally offer more cost-effective solutions for high-volume applications with their pay-as-you-go pricing. We recommend Amazon MQ for migrating applications from existing message brokers that rely on compatibility with APIs such as JMS or protocols such as Advanced Message Queuing Protocol (AMQP), MQTT, OpenWire, and Simple Text Oriented Message Protocol (STOMP).",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 2279
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html",
    "doc_type": "aws",
    "total_sections": 5
  },
  {
    "title": "Getting started with Amazon SQS",
    "summary": "This topic guides you through using the Amazon SQS console to create and manage standard queues and FIFO queues . You'll learn how to navigate the console, view queue attributes, and distinguish between queue types. Key tasks include sending, receiving, and configuring messages, adjusting parameters such as visibility timeout and message retention, and managing queue access through policies. Setting up",
    "sections": [
      {
        "header": "",
        "content": "Getting started with Amazon SQS This topic guides you through using the Amazon SQS console to create and manage standard queues and FIFO queues . You'll learn how to navigate the console, view queue attributes, and distinguish between queue types. Key tasks include sending, receiving, and configuring messages, adjusting parameters such as visibility timeout and message retention, and managing queue access through policies. Topics Setting up Understanding the Amazon SQS console Queue types Creating a standard queue Creating a FIFO queue Common tasks Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions What is Amazon SQS? Setting up",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 6,
          "content_length": 798
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-getting-started.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is Amazon SNS?",
    "summary": "Amazon Simple Notification Service (Amazon SNS) is a fully managed service that provides message delivery from publishers (producers) to subscribers (consumers). Publishers communicate asynchronously with subscribers by sending messages to a topic , which is a logical access point and communication channel. In SNS, publishers send messages to a topic, which acts as a communication channel. The topic acts as a logical access point, ensuring messages are delivered to multiple subscribers across different platforms.",
    "sections": [
      {
        "header": "How it works",
        "content": "In SNS, publishers send messages to a topic, which acts as a communication channel. The topic acts as a logical access point, ensuring messages are delivered to multiple subscribers across different platforms.\n\nSubscribers to an SNS topic can receive messages through different endpoints, depending on their use case, such as:\n\nAmazon SQS Lambda HTTP(S) endpoints Email Mobile push notifications Mobile text messages (SMS) Amazon Data Firehose Service providers (For example, Datadog, MongoDB, Splunk)\n\nSNS supports both Application-to-Application (A2A) and Application-to-Person (A2P) messaging, giving flexibility to send messages between different applications or directly to mobile phones, email addresses, and more.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 720
        }
      },
      {
        "header": "Accessing Amazon SNS",
        "content": "You can access and manage Amazon SNS through the console, AWS CLI, or AWS SDKs, depending on your preferred method of interaction. The console offers a graphical interface for basic tasks, while the AWS CLI and SDKs provide advanced configuration and automation capabilities for more complex use cases.\n\nThe Amazon SNS console provides a convenient user interface for creating topics and subscriptions, sending and receiving messages, and monitoring events and logs. The AWS Command Line Interface (AWS CLI) gives you direct access to the Amazon SNS API for advanced configuration and automation use cases. For more information, see Using Amazon SNS with the AWS CLI . AWS provides SDKs in various languages. For more information, see SDKs and Toolkits .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 754
        }
      },
      {
        "header": "Common Amazon SNS scenarios",
        "content": "Use these common Amazon SNS scenarios to implement scalable, event-driven architectures and ensure reliable, real-time communication between applications and users.\n\nApplication integration\n\nThe Fanout scenario is when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Firehose delivery streams, Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing.\n\nFor example, you can develop an application that publishes a message to an SNS topic whenever an order is placed for a product. Then, SQS queues that are subscribed to the SNS topic receive identical notifications for the new order. An Amazon Elastic Compute Cloud (Amazon EC2) server instance attached to one of the SQS queues can handle the processing or fulfillment of the order. And you can attach another Amazon EC2 server instance to a data warehouse for analysis of all orders received.\n\nYou can also use fanout to replicate data sent to your production environment with your test environment. Expanding upon the previous example, you can subscribe another SQS queue to the same SNS topic for new incoming orders. Then, by attaching this new SQS queue to your test environment, you can continue to improve and test your application using data received from your production environment.\n\nImportant Make sure that you consider data privacy and security before you send any production data to your test environment.\n\nFor more information, see the following resources:\n\nFanout to Firehose delivery streams Fanout Amazon SNS notifications to Lambda functions for automated processing Fanout Amazon SNS notifications to Amazon SQS queues for asynchronous processing Fanout Amazon SNS notifications to HTTPS endpoints Event-Driven Computing with Amazon SNS and AWS Compute, Storage, Database, and Networking Services\n\nApplication alerts\n\nApplication and system alerts are notifications that are triggered by predefined thresholds. Amazon SNS can send these notifications to specified users via SMS and email. For example, you can receive immediate notification when an event occurs, such as a specific change to your Amazon EC2 Auto Scaling group, a new file uploaded to an Amazon S3 bucket, or a metric threshold breached in Amazon CloudWatch. For more information, see Setting up Amazon SNS notifications in the Amazon CloudWatch User Guide .\n\nUser notifications\n\nAmazon SNS can send push email messages and text messages (SMS messages) to individuals or groups. For example, you could send e-commerce order confirmations as user notifications. For more information about using Amazon SNS to send SMS messages, see Mobile text messaging with Amazon SNS .\n\nMobile push notifications\n\nMobile push notifications enable you to send messages directly to mobile apps. For example, you can use Amazon SNS to send update notifications to an app. The notification message can include a link to download and install the update. For more information about using Amazon SNS to send push notification messages, see Sending mobile push notifications with Amazon SNS .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 3104
        }
      },
      {
        "header": "Application integration",
        "content": "The Fanout scenario is when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Firehose delivery streams, Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing.\n\nFor example, you can develop an application that publishes a message to an SNS topic whenever an order is placed for a product. Then, SQS queues that are subscribed to the SNS topic receive identical notifications for the new order. An Amazon Elastic Compute Cloud (Amazon EC2) server instance attached to one of the SQS queues can handle the processing or fulfillment of the order. And you can attach another Amazon EC2 server instance to a data warehouse for analysis of all orders received.\n\nYou can also use fanout to replicate data sent to your production environment with your test environment. Expanding upon the previous example, you can subscribe another SQS queue to the same SNS topic for new incoming orders. Then, by attaching this new SQS queue to your test environment, you can continue to improve and test your application using data received from your production environment.\n\nImportant Make sure that you consider data privacy and security before you send any production data to your test environment.\n\nFor more information, see the following resources:\n\nFanout to Firehose delivery streams Fanout Amazon SNS notifications to Lambda functions for automated processing Fanout Amazon SNS notifications to Amazon SQS queues for asynchronous processing Fanout Amazon SNS notifications to HTTPS endpoints Event-Driven Computing with Amazon SNS and AWS Compute, Storage, Database, and Networking Services",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1674
        }
      },
      {
        "header": "Application alerts",
        "content": "Application and system alerts are notifications that are triggered by predefined thresholds. Amazon SNS can send these notifications to specified users via SMS and email. For example, you can receive immediate notification when an event occurs, such as a specific change to your Amazon EC2 Auto Scaling group, a new file uploaded to an Amazon S3 bucket, or a metric threshold breached in Amazon CloudWatch. For more information, see Setting up Amazon SNS notifications in the Amazon CloudWatch User Guide .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 506
        }
      },
      {
        "header": "User notifications",
        "content": "Amazon SNS can send push email messages and text messages (SMS messages) to individuals or groups. For example, you could send e-commerce order confirmations as user notifications. For more information about using Amazon SNS to send SMS messages, see Mobile text messaging with Amazon SNS .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 290
        }
      },
      {
        "header": "Mobile push notifications",
        "content": "Mobile push notifications enable you to send messages directly to mobile apps. For example, you can use Amazon SNS to send update notifications to an app. The notification message can include a link to download and install the update. For more information about using Amazon SNS to send push notification messages, see Sending mobile push notifications with Amazon SNS .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 370
        }
      },
      {
        "header": "Pricing for Amazon SNS",
        "content": "Amazon SNS has no upfront costs. You pay based on the number of messages that you publish, the number of notifications that you deliver, and any additional API calls for managing topics and subscriptions. Delivery pricing varies by endpoint type. You can get started for free with the Amazon SNS free tier. For information, see Worldwide SMS Pricing .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 351
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/sns/latest/dg/welcome.html",
    "doc_type": "aws",
    "total_sections": 8
  },
  {
    "title": "Create an Amazon SNS topic and publish messages",
    "summary": "This topic provides the foundational steps for managing Amazon SNS resources, specifically focusing on topics, subscriptions, and message publishing. First, you will set up the necessary access permissions for Amazon SNS, ensuring that you have the correct permissions to create and manage Amazon SNS resources. Next, you will create a new Amazon SNS topic, which serves as the central hub for managing and delivering messages to subscribers. After creating the topic, you will proceed to create a subscription to this topic, allowing specific endpoints to receive the messages published to it. Once the topic and subscription are in place, you will publish a message to the topic, observing how Amazon SNS efficiently delivers the message to all subscribed endpoints. Finally, you will learn how to delete both the subscription and the topic, completing the lifecycle of the Amazon SNS resources youâve managed. This approach gives you a clear understanding of the fundamental operations in Amazon SNS, equipping you with the practical skills needed to manage messaging workflows using the Amazon SNS console.",
    "sections": [
      {
        "header": "",
        "content": "Create an Amazon SNS topic and publish messages This topic provides the foundational steps for managing Amazon SNS resources, specifically focusing on topics, subscriptions, and message publishing. First, you will set up the necessary access permissions for Amazon SNS, ensuring that you have the correct permissions to create and manage Amazon SNS resources. Next, you will create a new Amazon SNS topic, which serves as the central hub for managing and delivering messages to subscribers. After creating the topic, you will proceed to create a subscription to this topic, allowing specific endpoints to receive the messages published to it. Once the topic and subscription are in place, you will publish a message to the topic, observing how Amazon SNS efficiently delivers the message to all subscribed endpoints. Finally, you will learn how to delete both the subscription and the topic, completing the lifecycle of the Amazon SNS resources youâve managed. This approach gives you a clear understanding of the fundamental operations in Amazon SNS, equipping you with the practical skills needed to manage messaging workflows using the Amazon SNS console. Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Working with AWS SDKs Setting up",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 10,
          "content_length": 1406
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/sns/latest/dg/sns-getting-started.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is Amazon EKS?",
    "summary": "Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page.",
    "sections": [
      {
        "header": "Amazon EKS: Simplified Kubernetes Management",
        "content": "Amazon Elastic Kubernetes Service (EKS) provides a fully managed Kubernetes service that eliminates the complexity of operating Kubernetes clusters. With EKS, you can:\n\nDeploy applications faster with less operational overhead Scale seamlessly to meet changing workload demands Improve security through AWS integration and automated updates Choose between standard EKS or fully automated EKS Auto Mode\n\nAmazon Elastic Kubernetes Service (Amazon EKS) is the premiere platform for running Kubernetes clusters, both in the Amazon Web Services (AWS) cloud and in your own data centers ( EKS Anywhere and Amazon EKS Hybrid Nodes ).\n\nAmazon EKS simplifies building, securing, and maintaining Kubernetes clusters. It can be more cost effective at providing enough resources to meet peak demand than maintaining your own data centers. Two of the main approaches to using Amazon EKS are as follows:\n\nEKS standard : AWS manages the Kubernetes control plane when you create a cluster with EKS. Components that manage nodes, schedule workloads, integrate with the AWS cloud, and store and scale control plane information to keep your clusters up and running, are handled for you automatically. EKS Auto Mode : Using the EKS Auto Mode feature, EKS extends its control to manage Nodes (Kubernetes data plane) as well. It simplifies Kubernetes management by automatically provisioning infrastructure, selecting optimal compute instances, dynamically scaling resources, continuously optimizing costs, patching operating systems, and integrating with AWS security services.\n\nThe following diagram illustrates how Amazon EKS integrates your Kubernetes clusters with the AWS cloud, depending on which method of cluster creation you choose:\n\nAmazon EKS helps you accelerate time to production, improve performance, availability and resiliency, and enhance system security. For more information, see Amazon Elastic Kubernetes Service .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 1914
        }
      },
      {
        "header": "Features of Amazon EKS",
        "content": "Amazon EKS provides the following high-level features:\n\nManagement interfaces EKS offers multiple interfaces to provision, manage, and maintain clusters, including AWS Management Console, Amazon EKS API/SDKs, CDK, AWS CLI, eksctl CLI, AWS CloudFormation, and Terraform. For more information, see Get started with Amazon EKS and Amazon EKS cluster lifecycle and configuration . Access control tools EKS relies on both Kubernetes and AWS Identity and Access Management (AWS IAM) features to manage access from users and workloads. For more information, see Grant IAM users and roles access to Kubernetes APIs and Grant Kubernetes workloads access to AWS using Kubernetes Service Accounts . Compute resources For compute resources , EKS allows the full range of Amazon EC2 instance types and AWS innovations such as Nitro and Graviton with Amazon EKS for you to optimize the compute for your workloads. For more information, see Manage compute resources by using nodes . Storage EKS Auto Mode automatically creates storage classes using EBS volumes . Using Container Storage Interface (CSI) drivers, you can also use Amazon S3, Amazon EFS, Amazon FSX, and Amazon File Cache for your application storage needs. For more information, see Use application data storage for your cluster . Security The shared responsibility model is employed as it relates to Security in Amazon EKS . For more information, see Security best practices , Infrastructure security , and Kubernetes security . Monitoring tools Use the observability dashboard to monitor Amazon EKS clusters. Monitoring tools include Prometheus , CloudWatch , Cloudtrail , and ADOT Operator . For more information on dashboards, metrics servers, and other tools, see EKS cluster costs and Kubernetes Metrics Server . Kubernetes compatibility and support Amazon EKS is certified Kubernetes-conformant, so you can deploy Kubernetes-compatible applications without refactoring and use Kubernetes community tooling and plugins. EKS offers both standard support and extended support for Kubernetes. For more information, see Understand the Kubernetes version lifecycle on EKS .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 2124
        }
      },
      {
        "header": "Related services",
        "content": "Services to use with Amazon EKS\n\nYou can use other AWS services with the clusters that you deploy using Amazon EKS:\n\nAmazon EC2 Obtain on-demand, scalable compute capacity with Amazon EC2 . Amazon EBS Attach scalable, high-performance block storage resources with Amazon EBS . Amazon ECR Store container images securely with Amazon ECR . Amazon CloudWatch Monitor AWS resources and applications in real time with Amazon CloudWatch . Amazon Prometheus Track metrics for containerized applications with Amazon Managed Service for Prometheus . Elastic Load Balancing Distribute incoming traffic across multiple targets with Elastic Load Balancing . Amazon GuardDuty Detect threats to EKS clusters with Amazon GuardDuty . AWS Resilience Hub Assess EKS cluster resiliency with AWS Resilience Hub .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 792
        }
      },
      {
        "header": "Amazon EKS Pricing",
        "content": "Amazon EKS has per cluster pricing based on Kubernetes cluster version support, pricing for Amazon EKS Auto Mode, and per vCPU pricing for Amazon EKS Hybrid Nodes.\n\nWhen using Amazon EKS, you pay separately for the AWS resources you use to run your applications on Kubernetes worker nodes. For example, if you are running Kubernetes worker nodes as Amazon EC2 instances with Amazon EBS volumes and public IPv4 addresses, you are charged for the instance capacity through Amazon EC2, the volume capacity through Amazon EBS, and the IPv4 address through Amazon VPC.\n\nVisit the respective pricing pages of the AWS services you are using with your Kubernetes applications for detailed pricing information.\n\nFor Amazon EKS cluster, Amazon EKS Auto Mode, and Amazon EKS Hybrid Nodes pricing, see Amazon EKS Pricing . For Amazon EC2 pricing, see Amazon EC2 On-Demand Pricing and Amazon EC2 Spot Pricing . For AWS Fargate pricing, see AWS Fargate Pricing . You can use your savings plans for compute used in Amazon EKS clusters. For more information, see Pricing with Savings Plans .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 1075
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Get started with Amazon EKS",
    "summary": "Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page.",
    "sections": [
      {
        "header": "",
        "content": "Help improve this page To contribute to this user guide, choose the Edit this page on GitHub link that is located in the right pane of every page. Get started with Amazon EKS Make sure that you are set up to use Amazon EKS before going through the getting started guides. For more information, see Set up to use Amazon EKS . There are two getting started guides available for creating a new Kubernetes cluster with nodes in Amazon EKS: Get started with Amazon EKS â eksctl â This getting started guide helps you to install all of the required resources to get started with Amazon EKS using eksctl , a simple command line utility for creating and managing Kubernetes clusters on Amazon EKS. At the end of the tutorial, you will have a running Amazon EKS cluster that you can deploy applications to. This is the fastest and simplest way to get started with Amazon EKS. Get started with Amazon EKS â AWS Management Console and AWS CLI â This getting started guide helps you to create all of the required resources to get started with Amazon EKS using the AWS Management Console and AWS CLI. At the end of the tutorial, you will have a running Amazon EKS cluster that you can deploy applications to. In this guide, you manually create each resource required for an Amazon EKS cluster. The procedures give you visibility into how each resource is created and how they interact with each other. We also offer the following references: For a collection of hands-on tutorials, see EKS Cluster Setup on AWS Community . For code examples, see Code examples for Amazon EKS using AWS SDKs . Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Learn Amazon EKS Create cluster (EKS Auto Mode)",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 15,
          "content_length": 1847
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is Amazon CloudFront?",
    "summary": "Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance. If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.",
    "sections": [
      {
        "header": "How you set up CloudFront to deliver content",
        "content": "You create a CloudFront distribution to tell CloudFront where you want content to be delivered from, and the details about how to track and manage content delivery. Then CloudFront uses computersâedge serversâthat are close to your viewers to deliver that content quickly when someone wants to see it or use it.\n\nHow you configure CloudFront to deliver your content You specify origin servers , like an Amazon S3 bucket or your own HTTP server, from which CloudFront gets your files which will then be distributed from CloudFront edge locations all over the world. An origin server stores the original, definitive version of your objects. If you're serving content over HTTP, your origin server is either an Amazon S3 bucket or an HTTP server, such as a web server. Your HTTP server can run on an Amazon Elastic Compute Cloud (Amazon EC2) instance or on a server that you manage; these servers are also known as custom origins. You upload your files to your origin servers. Your files, also known as objects , typically include web pages, images, and media files, but can be anything that can be served over HTTP. If you're using an Amazon S3 bucket as an origin server, you can make the objects in your bucket publicly readable, so that anyone who knows the CloudFront URLs for your objects can access them. You also have the option of keeping objects private and controlling who accesses them. See Serve private content with signed URLs and signed cookies . You create a CloudFront distribution , which tells CloudFront which origin servers to get your files from when users request the files through your web site or application. At the same time, you specify details such as whether you want CloudFront to log all requests and whether you want the distribution to be enabled as soon as it's created. CloudFront assigns a domain name to your new distribution that you can see in the CloudFront console, or that is returned in the response to a programmatic request, for example, an API request. If you like, you can add an alternate domain name to use instead. CloudFront sends your distribution's configuration (but not your content) to all of its edge locations or points of presence (POPs)â collections of servers in geographically-dispersed data centers where CloudFront caches copies of your files.\n\nAs you develop your website or application, you use the domain name that CloudFront provides for your URLs. For example, if CloudFront returns d111111abcdef8.cloudfront.net as the domain name for your distribution, the URL for logo.jpg in your Amazon S3 bucket (or in the root directory on an HTTP server) is https://d111111abcdef8.cloudfront.net/logo.jpg .\n\nOr you can set up CloudFront to use your own domain name with your distribution. In that case, the URL might be https://www.example.com/logo.jpg .\n\nOptionally, you can configure your origin server to add headers to the files, to indicate how long you want the files to stay in the cache in CloudFront edge locations. By default, each file stays in an edge location for 24 hours before it expires. The minimum expiration time is 0 seconds; there isn't a maximum expiration time. For more information, see Manage how long content stays in the cache (expiration) .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 3234
        }
      },
      {
        "header": "Choose between standard distribution or multi-tenant distribution",
        "content": "CloudFront offers distribution options for single websites or apps, and for multi-tenant scenarios.\n\nStandard distribution Designed for unique configurations per website or application. Choose this in the following use cases: You need a standalone CloudFront distribution Each site or application requires its own custom settings Most people start with a standard distribution. Multi-tenant distribution and distribution tenants (CloudFront SaaS Manager) Designed specifically for SaaS providers and multi-tenant scenarios. Choose this in the following use cases: You're building a SaaS platform to serve multiple customer websites or applications You need to manage multiple similar distributions efficiently You want centralized control over shared configurations For more information, see Understand how multi-tenant distributions work .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 840
        }
      },
      {
        "header": "Pricing",
        "content": "CloudFront charges for data transfers out from its edge locations, along with HTTP or HTTPS requests. Pricing varies by usage type, geographical region, and feature selection.\n\nThe data transfer from your origin to CloudFront is always free when using AWS origins like Amazon Simple Storage Service (Amazon S3), Elastic Load Balancing, or Amazon API Gateway. You are only billed for the outbound data transfer from CloudFront to the viewer when using AWS origins.\n\nFor more information, see CloudFront pricing and the Billing and Savings Bundle FAQs .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 551
        }
      },
      {
        "header": "CloudFront technical resources",
        "content": "Use the following resources to get answers to technical questions about CloudFront:\n\nAWS re:Post â A community-based question and answer site for developers to discuss technical questions related to CloudFront. Support Center â This site includes information about your recent support cases and results from AWS Trusted Advisor and health checks. It also provides links to discussion forums, technical FAQs, the service health dashboard, and information about Support plans. AWS Premium Support â Learn about AWS Premium Support, a one-on-one, fast-response support channel that helps you build and run applications on AWS. AWS IQ â Get help from AWS certified professionals and experts.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 695
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Get started with CloudFront",
    "summary": "The topics in this section show you how to get started delivering your content with Amazon CloudFront. The Set up your AWS account topic describes prerequisites for the following tutorials, such as creating an AWS account and creating a user with administrative access.",
    "sections": [
      {
        "header": "",
        "content": "Get started with CloudFront The topics in this section show you how to get started delivering your content with Amazon CloudFront. The Set up your AWS account topic describes prerequisites for the following tutorials, such as creating an AWS account and creating a user with administrative access. The basic distribution tutorial shows you how to set up origin access control (OAC) to send authenticated requests to an Amazon S3 origin. The secure static website tutorial shows you how to create a secure static website for your domain name using OAC with an Amazon S3 origin. The tutorial uses an Amazon CloudFront (CloudFront) template for configuration and deployment. Topics Set up your AWS account Get started with a CloudFront standard distribution Get started with a standard distribution (AWS CLI) Get started with a secure static website Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Working with AWS SDKs Set up your AWS account",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 8,
          "content_length": 1105
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is Amazon RouteÂ 53?",
    "summary": "Amazon RouteÂ 53 is a highly available and scalable Domain Name System (DNS) web service. You can use RouteÂ 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking. If you choose to use RouteÂ 53 for all three functions, be sure to follow the order below:",
    "sections": [
      {
        "header": "",
        "content": "What is Amazon RouteÂ 53? Amazon RouteÂ 53 is a highly available and scalable Domain Name System (DNS) web service. You can use RouteÂ 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking. If you choose to use RouteÂ 53 for all three functions, be sure to follow the order below: 1. Register domain names Your website needs a name, such as example.com. RouteÂ 53 lets you register a name for your website or web application, known as a domain name . For an overview, see How domain registration works . For a procedure, see Registering a new domain . For a tutorial that takes you through registering a domain and creating a simple website in an Amazon S3 bucket, see Getting started with Amazon RouteÂ 53 . 2. Route internet traffic to the resources for your domain When a user opens a web browser and enters your domain name (example.com) or subdomain name (acme.example.com) in the address bar, RouteÂ 53 helps connect the browser with your website or web application. For an overview, see How internet traffic is routed to your website or web application . For procedures, see Configuring Amazon RouteÂ 53 as your DNS service . For a procedure on how to route email to Amazon WorkMail, see Routing traffic to Amazon WorkMail . 3. Check the health of your resources RouteÂ 53 sends automated requests over the internet to a resource, such as a web server, to verify that it's reachable, available, and functional. You also can choose to receive notifications when a resource becomes unavailable and choose to route internet traffic away from unhealthy resources. For an overview, see How Amazon RouteÂ 53 checks the health of your resources . For procedures, see Creating Amazon RouteÂ 53 health checks . Other RouteÂ 53 features In addition to being a Domain Name System (DNS) web service, RouteÂ 53 offers the following features: RouteÂ 53 Resolver Get recursive DNS for your Amazon VPCs in AWS Regions, VPCs in AWS Outposts racks, or any other on-premises networks. Create conditional forwarding rules and RouteÂ 53 endpoints to resolve custom names mastered in RouteÂ 53 private hosted zones or in your on-premises DNS servers. For more information , see What is Amazon RouteÂ 53 Resolver? . Amazon Route 53 Resolver on Outposts Connect RouteÂ 53 Resolver on Outpost racks with DNS servers in your on-premises data centers through RouteÂ 53 Resolver endpoints. This enables resolution of DNS queries between the Outposts racks and your other on-premises resources. For more information , see What is Amazon Route 53 on Outposts? . Route 53 Resolver DNS Firewall Protect your recursive DNS queries within the RouteÂ 53 Resolver. Create domain lists and build firewall rules that filter outbound DNS traffic against these rules. For more information , see Using DNS Firewall to filter outbound DNS traffic . Traffic Flow Easy-to-use and cost-effective global traffic management: route end users to the best endpoint for your application based on geoproximity, latency, health, and other considerations. For more information , see Using Traffic Flow to route DNS traffic . Amazon Route 53 Profiles With Route 53 Profiles, you can apply and manage DNS-related RouteÂ 53 configurations across many VPCs and in different AWS account. For more information , see What are Amazon Route 53 Profiles? . Topics How domain registration works How internet traffic is routed to your website or web application How Amazon RouteÂ 53 checks the health of your resources Amazon RouteÂ 53 concepts How to get started with Amazon RouteÂ 53 Accessing Amazon RouteÂ 53 AWS Identity and Access Management Amazon RouteÂ 53 pricing and billing Using RouteÂ 53 with an AWS SDK Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions How domain registration works",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 34,
          "content_length": 3955
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "Getting started with Amazon RouteÂ 53",
    "summary": "Get started with the basic steps by registering a domain with Amazon RouteÂ 53 and configuring RouteÂ 53 to respond to DNS queries that resolve to a static website. The first tutorial hosts a static website in an open Amazon S3 bucket, and the second tutorial uses Amazon CloudFront distribution to serve the website with SSL/TLS. Estimated cost",
    "sections": [
      {
        "header": "",
        "content": "Getting started with Amazon RouteÂ 53 Get started with the basic steps by registering a domain with Amazon RouteÂ 53 and configuring RouteÂ 53 to respond to DNS queries that resolve to a static website. The first tutorial hosts a static website in an open Amazon S3 bucket, and the second tutorial uses Amazon CloudFront distribution to serve the website with SSL/TLS. Estimated cost There's an annual fee to register a domain, ranging from $9 to several hundred dollars, depending on the top-level domain, such as .com. For more information, see RouteÂ 53 Pricing for Domain Registration . This fee is not refundable. When you register a domain, we automatically create a hosted zone that has the same name as the domain. You use the hosted zone to specify where you want RouteÂ 53 to route traffic for your domain. During this tutorial, you create an Amazon S3 bucket and upload a sample web page. If you're a new AWS customer, you can get started with Amazon S3 for free. If you're an existing AWS customer, charges are based on how much data you store, on the number of requests for your data, and on the amount of data transferred. For more information, see Amazon S3 Pricing . CloudFront charges are based on the number of requests for your data, the number of edge locations you use, and on the amount of data transferred. For more information, see CloudFront Pricing . Topics Set up Route DNS traffic to an Amazon S3 static website Route DNS traffic to a CloudFront distribution Javascript is disabled or is unavailable in your browser. To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions. Document Conventions Working with AWS SDKs Set up",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 16,
          "content_length": 1728
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/getting-started.html",
    "doc_type": "aws",
    "total_sections": 1
  },
  {
    "title": "What is AWS Elastic Beanstalk?",
    "summary": "With Elastic Beanstalk you can deploy web applications into the AWS Cloud on a variety of supported platforms. You build and deploy your applications. Elastic Beanstalk provisions Amazon EC2 instances, configures load balancing, sets up health monitoring, and dynamically scales your environment. In addition to web server environments, Elastic Beanstalk also provides worker environments which you can use to process messages from an Amazon SQS queue, useful for asynchronous or long-running tasks. For more information, see Elastic Beanstalk worker environments .",
    "sections": [
      {
        "header": "Supported platforms",
        "content": "Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. Elastic Beanstalk also supports Docker containers, where you can choose your own programming language and application dependencies. When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, in your AWS account to run your application.\n\nYou can interact with Elastic Beanstalk through the Elastic Beanstalk console, the AWS Command Line Interface (AWS CLI), or the EB CLI, a high-level command line tool designed specifically for Elastic Beanstalk.\n\nYou can perform most deployment tasks, such as changing the size of your fleet of Amazon EC2 instances or monitoring your application, directly from the Elastic Beanstalk web interface (console).\n\nTo learn more about how to deploy a sample web application using Elastic Beanstalk, see Learn how to get started with Elastic Beanstalk .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 997
        }
      },
      {
        "header": "Application deploy workflow",
        "content": "To use Elastic Beanstalk, you create an application, then upload your application source bundle to Elastic Beanstalk. Next, you provide information about the application, and Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code.\n\nAfter you create and deploy your application and your environment is launched, you can manage your environment and deploy new application versions. Information about the applicationâincluding metrics, events, and environment statusâis made available through the Elastic Beanstalk console, APIs, and Command Line Interfaces.\n\nThe following diagram illustrates Elastic Beanstalk workflow:",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 3,
          "content_length": 692
        }
      },
      {
        "header": "Pricing",
        "content": "There is no additional charge for Elastic Beanstalk. You pay only for the underlying AWS resources that your application consumes. For details about pricing, see the Elastic Beanstalk service detail page .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 205
        }
      },
      {
        "header": "Next steps",
        "content": "We recommend the tutorial, Getting started tutorial , to start using Elastic Beanstalk. The tutorial steps you through creating, viewing, and updating a sample Elastic Beanstalk application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 190
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html",
    "doc_type": "aws",
    "total_sections": 4
  },
  {
    "title": "Learn how to get started with Elastic Beanstalk",
    "summary": "With Elastic Beanstalk you can deploy, monitor, and scale web applications and services. Typically, you will develop your code locally then deploy it to Amazon EC2 server instances. Theses instances, also called environments , run on platforms that can be upgraded through the AWS console or the command line. To get started, we recommend deploying a pre-built sample application directly from the console. Then, you can learn how to develop locally and deploy from the command line in the QuickStart: Deploy a PHP application to Elastic Beanstalk .",
    "sections": [
      {
        "header": "What you will build",
        "content": "Your first Elastic Beanstalk application will consist of a single Amazon EC2 environment running the PHP sample on a PHP managed platform.\n\nElastic Beanstalk application An Elastic Beanstalk application is a container for Elastic Beanstalk components, including environments where your application code runs on platforms provided and managed by Elastic Beanstalk, or in custom containers that you provide. Environment An Elastic Beanstalk environment is a collection of AWS resources running together including an Amazon EC2 instance. When you create an environment, Elastic Beanstalk provisions the necessary resources into your AWS account. Platform A platform is a combination of an operating system, programming language runtime, web server, application server, and additional Elastic Beanstalk components. Elastic Beanstalk provides manged platforms, or you can provide your own platform in a container.\n\nElastic Beanstalk supports platforms for different programming languages, application servers, and Docker containers. When you create an environment, you must choose the platform. You can upgrade the platform, but you cannot change the platform for an environment.\n\nSwitching platforms If you need to change programming languages, you must create and switch to a new environment on a different platform.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1313
        }
      },
      {
        "header": "Step 1 - Create an application",
        "content": "To create your example application, you'll use the Create application console wizard. It creates an Elastic Beanstalk application and launches an environment within it.\n\nReminder: an environment is a collection of AWS resources required to run your application code.\n\nTo create an application Open the Elastic Beanstalk console . Choose Create application . For Application name enter getting-started-app .\n\nThe console provides a six step process for creating an application and configuring an environment. For this quick start, you'll only need to focus on the first two steps, then you can skip ahead to review and create your application and environment.\n\nTo configure an environment In Environment information , for Environment name enter: gs-app-web-env . For Platform , choose the PHP platform. For Application code and Presets , accept the defaults ( Sample application and Single instance ), then choose Next .\n\nTo configure service access\n\nNext, you need two roles. A service role allows Elastic Beanstalk to monitor your EC2 instances and upgrade you environmentâs platform. An EC2 instance profile role permits tasks such as writing logs and interacting with other services.\n\nTo create or select the Service role If you have previously created a Service role and would like to choose an existing one, select the value from the Service role drop-down and skip the remainder of these steps to create a Service role. If you don't see any values listed for Service role , or you'd like to create a new one, continue with the next steps. For Service role , choose Create role . For Trusted entity type , choose AWS service . For Use case , choose Elastic Beanstalk â Environment . Choose Next . Verify that Permissions policies include the following, then choose Next : AWSElasticBeanstalkEnhancedHealth AWSElasticBeanstalkManagedUpdatesCustomerRolePolicy Choose Create role . Return to the Configure service access tab, refresh the list, then select the newly created service role.\n\nTo create or select an EC2 instance profile If you have previously created an EC2 instance profile and would like to choose an existing one, select the value from the EC2 instance profile drop-down and skip the remainder of these steps to create an EC2 instance profile. If you don't see any values listed for EC2 instance profile , or you'd like to create a new one, continue with the next steps. Choose Create role . For Trusted entity type , choose AWS service . For Use case , choose Elastic Beanstalk â Compute . Choose Next . Verify that Permissions policies include the following, then choose Next : AWSElasticBeanstalkWebTier AWSElasticBeanstalkWorkerTier AWSElasticBeanstalkMulticontainerDocker Choose Create role . Return to the Configure service access tab, refresh the list, then select the newly created EC2 instance profile.\n\nTo finish configuring and creating your application Skip over EC2 key pair . We'll show you other ways to connect to your Amazon EC2 instances through the Console. Choose Skip to Review to move over several optional steps. Optional steps: networking, databases, scaling parameters, advanced configuration for updates, monitoring, and logging. On the Review page which shows a summary of your choices, choose Submit .\n\nCongratulations! You have created an application and configured an environment! Now you need to wait for the resources to deploy.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 3384
        }
      },
      {
        "header": "Step 2 - Deploy your application",
        "content": "When you create an application, Elastic Beanstalk sets up the environments for you. You just need to sit back and wait.\n\nThe initial deploy can take up to five minutes to create the resources. Updates will take less time because only changes will be deployed to your stack.\n\nWhen you create the example application, Elastic Beanstalk creates the following resources:\n\nEC2 instance â An Amazon EC2 virtual machine configured to run web apps on the platform you selected. Every platform runs a different set of software, configuration files, and scripts to support a specific language version, framework, web container, or combination thereof. Most platforms use either Apache or nginx as a reverse proxy to forward web traffic to your web app, serve static assets, and generate access and error logs. You can connect to your Amazon EC2 instances to view configuration and logs. Instance security group â An Amazon EC2 security group will be created to allow incoming requests on port 80, so inbound traffic on a load balancer can reach your web app. Amazon S3 bucket â A storage location for your source code, logs, and other artifacts. Amazon CloudWatch alarms â Two CloudWatch alarms are created to monitor the load on your instances and scale them up or down as needed. CloudFormation stack â Elastic Beanstalk uses CloudFormation to deploy the resources in your environment and make configuration changes. You can view the resource definition template in the CloudFormation console . Domain name â A domain name that routes to your web app in the form : subdomain . region .elasticbeanstalk.com .\n\nElastic Beanstalk creates your application, launches an environment, makes an application version, then deploys your code into the environment. During the process, the console tracks progress and displays event status in the Events tab.\n\nAfter all of the resources are deployed, the environment's health should change to Ok .\n\nYour application is ready! After you see your application health change to Ok , you can browse to your web application's website.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2069
        }
      },
      {
        "header": "Step 3 - Explore the Elastic Beanstalk environment",
        "content": "You'll start exploring your deployed application environment from the Environment overview page in the console.\n\nTo view the environment and your application Open the Elastic Beanstalk console , and in the Regions list, select your AWS Region. In the navigation pane, choose Environments , and then choose the name of your environment from the list. Choose Go to environment to browse your application! (You can also choose the URL link listed for Domain to browse your application.) The connection will be HTTP (not HTTPS), so you might see a warning in your browser.\n\nBack in the Elastic Beanstalk console, the upper portion shows the Environment overview with top level information about your environment, including name, domain URL, current health status, running version, and the platform that the application is running on. The running version and platform are essential for troubleshooting your currently deployed application.\n\nAfter the overview pane, you will see recent environment activity in the Events tab.\n\nWhile Elastic Beanstalk creates your AWS resources and launches your application, the environment is in a Pending state. Status messages about launch events are continuously added to the list of Events .\n\nThe environment's Domain is the URL for your deployed web application. In the left navigation pane, Go to environment also takes you to your domain. Similarly, the left navigation pane has links that correspond to the various tabs.\n\nTake note of the Configuration link in the left navigation pane. which displays a summary of environment configuration option values, grouped by category.\n\nEnvironment configuration settings Take note of the Configuration link in the left navigation pane. You can view and edit detailed environment settings, such as service roles, networking, database, scaling, managed platform updates, memory, health monitoring, rolling deployment, logging, and more!\n\nThe various tabs contain detailed information about your environment:\n\nEvents â View an updating list of information and error messages from the Elastic Beanstalk service and other services for resources in your environment. Health â View status and detailed health information for the Amazon EC2 instances running your application. Logs â Retrieve and download logs from the Amazon EC2 in your environment. You can retrieve full logs or recent activity. The retrieved logs are available for 15 minutes. Monitoring â View statistics for the environment, such as average latency and CPU utilization. Alarms â View and edit alarms that are configured for environment metrics. Managed updates â View information about upcoming and completed managed platform updates and instance replacement. Tags â View and edit key-value pairs that are applied to your environment.\n\nNote Links in the console navigation pane will display the corresponding tab.\n\nTroubleshooting with logs\n\nFor troubleshooting unexpected behaviors or debugging deployments, you might want to check the logs in your environments.\n\nYou can request 100 lines of all the log files under the Logs tab in the Elastic Beanstalk console. Alternatively, you can connect directly to the Amazon EC2 instance and tail the logs in realtime.\n\nTo request the logs (Elastic Beanstalk console) Navigate to your environment in the Elastic Beanstalk console. Choose the Logs tab or left-nav, then choose Request logs . Select Last 100 lines . After the logs are created, choose the Download link to view the logs in the browser.\n\nIn the logs, find the log and note the directory for the nginx access log.\n\nAdd a policy to enable connections to Amazon EC2 Before you can connect, you must add a policy that enables connections to Amazon EC2 with Session Manager. Navigate to the IAM console. Find and select the aws-elasticbeanstalk-ec2-role role. Choose Add permission , then Attach policies . Search for a default policy that begins with the following text: AmazonSSMManagedEC2Instance , then add it to the role.\n\nTo connect to your Amazon EC2 with Session Manager Navigate to the Amazon EC2 console. Choose Instances , then select your gs-app-web-env instance. Choose Connect , then Session Manager . Choose Connect .\n\nAfter connecting to the instance, start a bash shell and tail the logs:\n\nRun the command bash . Run the command cd /var/log/nginx . Run the command tail -f access.log . In your browser, go to the application domain URL. Refresh.\n\nCongratulations, you're connected! You should see log entries in your instance update every time you refresh the page.\n\nConnect button not working? If the connect button is not available, go back to IAM and verify that you added the necessary policy to the role.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 11,
          "content_length": 4687
        }
      },
      {
        "header": "Troubleshooting with logs",
        "content": "For troubleshooting unexpected behaviors or debugging deployments, you might want to check the logs in your environments.\n\nYou can request 100 lines of all the log files under the Logs tab in the Elastic Beanstalk console. Alternatively, you can connect directly to the Amazon EC2 instance and tail the logs in realtime.\n\nTo request the logs (Elastic Beanstalk console) Navigate to your environment in the Elastic Beanstalk console. Choose the Logs tab or left-nav, then choose Request logs . Select Last 100 lines . After the logs are created, choose the Download link to view the logs in the browser.\n\nIn the logs, find the log and note the directory for the nginx access log.\n\nAdd a policy to enable connections to Amazon EC2 Before you can connect, you must add a policy that enables connections to Amazon EC2 with Session Manager. Navigate to the IAM console. Find and select the aws-elasticbeanstalk-ec2-role role. Choose Add permission , then Attach policies . Search for a default policy that begins with the following text: AmazonSSMManagedEC2Instance , then add it to the role.\n\nTo connect to your Amazon EC2 with Session Manager Navigate to the Amazon EC2 console. Choose Instances , then select your gs-app-web-env instance. Choose Connect , then Session Manager . Choose Connect .\n\nAfter connecting to the instance, start a bash shell and tail the logs:\n\nRun the command bash . Run the command cd /var/log/nginx . Run the command tail -f access.log . In your browser, go to the application domain URL. Refresh.\n\nCongratulations, you're connected! You should see log entries in your instance update every time you refresh the page.\n\nConnect button not working? If the connect button is not available, go back to IAM and verify that you added the necessary policy to the role.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1787
        }
      },
      {
        "header": "Step 4 - Update your application",
        "content": "Eventually, you will want to update your application. You can deploy a new version at any time, as long as no other update operations are in progress on your environment.\n\nThe application version that you started this tutorial with is called Sample Application .\n\nTo update your application version Download the following PHP sample application: PHP â php-v2.zip Open the Elastic Beanstalk console , and in the Regions list, select your AWS Region. In the navigation pane, choose Environments , and then choose the name of your environment from the list. On the environment overview page, choose Upload and deploy . Select Choose file , and then upload the sample application source bundle that you downloaded. The console automatically fills in the Version label with a new unique label, automatically incrementing a trailing integer. If you choose your own version label, ensure that it's unique. Choose Deploy .\n\nWhile Elastic Beanstalk deploys your file to your Amazon EC2 instances, you can view the deployment status on the Environment overview page. While the application version is updated, the environment Health status is gray. When the deployment is complete, Elastic Beanstalk performs an application health check. When the application responds to the health check, it's considered healthy and the status returns to green. The environment overview shows the new Running Version âthe name you provided as the Version label .\n\nElastic Beanstalk also uploads your new application version and adds it to the table of application versions. To view the table, choose Application versions under getting-started-app on the navigation pane.\n\nUpdate success! You should see an updated \"v2\" message after refreshing your browser. If you want to edit the source yourself, unzip, edit, then re-zip the source bundle. On macOS, use the following command from inside your php directory with the -X to exclude extra file attributes: zip -X -r ../php-v2.zip .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1958
        }
      },
      {
        "header": "Step 5 - Scale your application",
        "content": "You can configure your environment to better suit your application. For example, if you have a compute-intensive application, you can change the type of Amazon Elastic Compute Cloud (Amazon EC2) instance that is running your application. To apply configuration changes, Elastic Beanstalk performs an environment update.\n\nSome configuration changes are simple and happen quickly. Some changes require deleting and recreating AWS resources, which can take several minutes. When you change configuration settings, Elastic Beanstalk warns you about potential application downtime.\n\nIncrease capacity settings\n\nIn this example of a configuration change, you edit your environment's capacity settings. You configure a load-balanced, scalable environment that has between two and four Amazon EC2 instances in its Auto Scaling group, and then you verify that the change occurred. Elastic Beanstalk creates an additional Amazon EC2 instance, adding to the single instance that it created initially. Then, Elastic Beanstalk associates both instances with the environment's load balancer. As a result, your application's responsiveness is improved and its availability is increased.\n\nTo change your environment's capacity Open the Elastic Beanstalk console , and in the Regions list, select your AWS Region. In the navigation pane, choose Environments , and then choose the name of your environment from the list. In the navigation pane, choose Configuration . In the Instance traffic and scaling configuration category, choose Edit . Collapse the Instances section, so you can more easily see the Capacity section. Under Auto Scaling group change Environment type to Load balanced . In the Instances row, change Min to 2 and Max to 4 . To save the changes choose Apply at the bottom of the page. If you are warned that the update will replace all of your current instances. Choose Confirm .\n\nThe environment update can take a few minutes. You should see several updates in the list of events. Watch for the event Successfully deployed new configuration to environment .\n\nVerify increased capacity\n\nAfter the environment update is complete and the environment is ready, Elastic Beanstalk automatically launched a second instance to meet your new minimum capacity setting.\n\nTo verify the increased capacity Choose Health from either the tab list or left navigation pane. Review the Enhanced instance health section.\n\nYou just scaled up! With two Amazon EC2 instances, your environment capacity has doubled, and it only took a few minutes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 5,
          "content_length": 2526
        }
      },
      {
        "header": "Increase capacity settings",
        "content": "In this example of a configuration change, you edit your environment's capacity settings. You configure a load-balanced, scalable environment that has between two and four Amazon EC2 instances in its Auto Scaling group, and then you verify that the change occurred. Elastic Beanstalk creates an additional Amazon EC2 instance, adding to the single instance that it created initially. Then, Elastic Beanstalk associates both instances with the environment's load balancer. As a result, your application's responsiveness is improved and its availability is increased.\n\nTo change your environment's capacity Open the Elastic Beanstalk console , and in the Regions list, select your AWS Region. In the navigation pane, choose Environments , and then choose the name of your environment from the list. In the navigation pane, choose Configuration . In the Instance traffic and scaling configuration category, choose Edit . Collapse the Instances section, so you can more easily see the Capacity section. Under Auto Scaling group change Environment type to Load balanced . In the Instances row, change Min to 2 and Max to 4 . To save the changes choose Apply at the bottom of the page. If you are warned that the update will replace all of your current instances. Choose Confirm .\n\nThe environment update can take a few minutes. You should see several updates in the list of events. Watch for the event Successfully deployed new configuration to environment .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1453
        }
      },
      {
        "header": "Verify increased capacity",
        "content": "After the environment update is complete and the environment is ready, Elastic Beanstalk automatically launched a second instance to meet your new minimum capacity setting.\n\nTo verify the increased capacity Choose Health from either the tab list or left navigation pane. Review the Enhanced instance health section.\n\nYou just scaled up! With two Amazon EC2 instances, your environment capacity has doubled, and it only took a few minutes.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 438
        }
      },
      {
        "header": "Cleaning up your Elastic Beanstalk environment",
        "content": "To ensure that you're not charged for any services you aren't using, delete all application versions and terminate environments, which also deletes the AWS resources that the environment created for you.\n\nTo delete the application and all associated resources Delete all application versions. Open the Elastic Beanstalk console , and in the Regions list, select your AWS Region. In the navigation pane, choose Applications , and then choose getting-started-app . In the navigation pane, find your application's name and choose Application versions . On the Application versions page, select all application versions that you want to delete. Choose Actions , and then choose Delete . Turn on Delete versions from Amazon S3 . Choose Delete , and then choose Done . Terminate the environment. In the navigation pane, choose getting-started-app , and then choose GettingStartedApp-env in the environment list. Choose Actions , and then choose Terminate Environment . Confirm that you want to terminate GettingStartedApp-env by typing the environment name, and then choose Terminate . Delete the getting-started-app application. In the navigation pane, choose the getting-started-app . Choose Actions , and then choose Delete application . Confirm that you want to delete getting-started-app by typing the application name, and then choose Delete .\n\nCongratulations! You have successfully deployed a sample application to the AWS Cloud, uploaded a new version, modified its configuration to add a second Auto Scaling instance, and cleaned up your AWS resources!",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 1556
        }
      },
      {
        "header": "Next steps",
        "content": "To learn how to use the eb command line tool to automate deploying your code to Elastic Beanstalk, We suggest continuing with the QuickStart: Deploy a PHP application to Elastic Beanstalk .\n\nNext, you might want to review how to set up HTTPS connection, see Configuring HTTPS for your Elastic Beanstalk environment .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 316
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/GettingStarted.html",
    "doc_type": "aws",
    "total_sections": 11
  },
  {
    "title": "What is Amazon Kinesis Data Streams?",
    "summary": "You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real time. You can create data-processing applications, known as Kinesis Data Streams applications . A typical Kinesis Data Streams application reads data from a data stream as data records. These applications can use the Kinesis Client Library, and they can run on Amazon EC2 instances. You can send the processed records to dashboards, use them to generate alerts, dynamically change pricing and advertising strategies, or send data to a variety of other AWS services. For information about Kinesis Data Streams features and pricing, see Amazon Kinesis Data Streams . Kinesis Data Streams is part of the Kinesis streaming data platform, along with Firehose , Kinesis Video Streams , and Managed Service for Apache Flink .",
    "sections": [
      {
        "header": "What can I do with Kinesis Data Streams?",
        "content": "You can use Kinesis Data Streams for rapid and continuous data intake and aggregation. The type of data used can include IT infrastructure log data, application logs, social media, market data feeds, and web clickstream data. Because the response time for the data intake and processing is in real time, the processing is typically lightweight.\n\nThe following are typical scenarios for using Kinesis Data Streams:\n\nAccelerated log and data feed intake and processing You can have producers push data directly into a stream. For example, push system and application logs and they are available for processing in seconds. This prevents the log data from being lost if the front end or application server fails. Kinesis Data Streams provides accelerated data feed intake because you don't batch the data on the servers before you submit it for intake. Real-time metrics and reporting You can use data collected into Kinesis Data Streams for simple data analysis and reporting in real time. For example, your data-processing application can work on metrics and reporting for system and application logs as the data is streaming in, rather than wait to receive batches of data. Real-time data analytics This combines the power of parallel processing with the value of real-time data. For example, process website clickstreams in real time, and then analyze site usability engagement using multiple different Kinesis Data Streams applications running in parallel. Complex stream processing You can create Directed Acyclic Graphs (DAGs) of Kinesis Data Streams applications and data streams. This typically involves putting data from multiple Kinesis Data Streams applications into another stream for downstream processing by a different Kinesis Data Streams application.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 1764
        }
      },
      {
        "header": "Benefits of using Kinesis Data Streams",
        "content": "Although you can use Kinesis Data Streams to solve a variety of streaming data problems, a common use is the real-time aggregation of data followed by loading the aggregate data into a data warehouse or map-reduce cluster.\n\nData is put into Kinesis data streams, which ensures durability and elasticity. The delay between the time a record is put into the stream and the time it can be retrieved (put-to-get delay) is typically less than 1 second. In other words, a Kinesis Data Streams application can start consuming the data from the stream almost immediately after the data is added. The managed service aspect of Kinesis Data Streams relieves you of the operational burden of creating and running a data intake pipeline. You can create streaming map-reduceâtype applications. The elasticity of Kinesis Data Streams enables you to scale the stream up or down, so that you never lose data records before they expire.\n\nMultiple Kinesis Data Streams applications can consume data from a stream, so that multiple actions, like archiving and processing, can take place concurrently and independently. For example, two applications can read data from the same stream. The first application calculates running aggregates and updates an Amazon DynamoDB table, and the second application compresses and archives data to a data store like Amazon Simple Storage Service (Amazon S3). The DynamoDB table with running aggregates is then read by a dashboard for up-to-the-minute reports.\n\nThe Kinesis Client Library enables fault-tolerant consumption of data from streams and provides scaling support for Kinesis Data Streams applications.",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1630
        }
      },
      {
        "header": "Related services",
        "content": "For information about using Amazon EMR clusters to read and process Kinesis data streams directly, see Kinesis Connector .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 122
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/streams/latest/dev/introduction.html",
    "doc_type": "aws",
    "total_sections": 3
  },
  {
    "title": "What is Amazon SageMaker AI?",
    "summary": "Amazon SageMaker AI is a fully managed machine learning (ML) service. With SageMaker AI, data scientists and developers can quickly and confidently build, train, and deploy ML models into a production-ready hosted environment. It provides a UI experience for running ML workflows that makes SageMaker AI ML tools available across multiple integrated development environments (IDEs). With SageMaker AI, you can store and share your data without having to build and manage your own servers. This gives you or your organizations more time to collaboratively build and develop your ML workflow, and do it sooner. SageMaker AI provides managed ML algorithms to run efficiently against extremely large data in a distributed environment. With built-in support for bring-your-own-algorithms and frameworks, SageMaker AI offers flexible distributed training options that adjust to your specific workflows. Within a few steps, you can deploy a model into a secure and scalable environment from the SageMaker AI console.",
    "sections": [
      {
        "header": "Amazon SageMaker AI rename",
        "content": "On December 03, 2024, Amazon SageMaker was renamed to Amazon SageMaker AI. This name change does not apply to any of the existing Amazon SageMaker features.\n\nLegacy namespaces remain the same\n\nThe sagemaker API namespaces, along with the following related namespaces, remain unchanged for backward compatibility purposes.\n\nAWS CLI commands Managed policies containing AmazonSageMaker prefixes Service endpoints containing sagemaker AWS CloudFormation resources containing AWS::SageMaker prefixes Service-linked role containing AWSServiceRoleForSageMaker Console URLs containing sagemaker Documentation URLs containing sagemaker",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 2,
          "content_length": 627
        }
      },
      {
        "header": "Legacy namespaces remain the same",
        "content": "The sagemaker API namespaces, along with the following related namespaces, remain unchanged for backward compatibility purposes.\n\nAWS CLI commands Managed policies containing AmazonSageMaker prefixes Service endpoints containing sagemaker AWS CloudFormation resources containing AWS::SageMaker prefixes Service-linked role containing AWSServiceRoleForSageMaker Console URLs containing sagemaker Documentation URLs containing sagemaker",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h3",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 434
        }
      },
      {
        "header": "Amazon SageMaker and Amazon SageMaker AI",
        "content": "On December 03, 2024, Amazon released the next generation of Amazon SageMaker.\n\nAmazon SageMaker is a unified platform for data, analytics, and AI. Bringing together AWS machine learning and analytics capabilities, the next generation of SageMaker delivers an integrated experience for analytics and AI with unified access to all your data.\n\nAmazon SageMaker includes the following capabilities:\n\nAmazon SageMaker AI (formerly Amazon SageMaker) - Build, train, and deploy ML and foundation models, with fully managed infrastructure, tools, and workflows Amazon SageMaker Lakehouse â Unify data access across Amazon S3 data lakes, Amazon Redshift, and other data sources Amazon SageMaker Data and AI Governance â Discover, govern, and collaborate on data and AI securely with Amazon SageMaker Catalog, built on Amazon DataZone SQL Analytics - Gain insights with the most price-performant SQL engine with Amazon Redshift Amazon SageMaker Data Processing - Analyze, prepare, and integrate data for analytics and AI using open-source frameworks on Amazon Athena, Amazon EMR, and AWS Glue Amazon SageMaker Unified Studio â Build with all your data and tools for analytics and AI in a single development environment Amazon Bedrock - Build and scale generative AI applications\n\nFor more information, refer to Amazon SageMaker .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 4,
          "content_length": 1326
        }
      },
      {
        "header": "Pricing for Amazon SageMaker AI",
        "content": "For information about AWS Free Tier limits and the cost of using SageMaker AI, see Amazon SageMaker AI Pricing .",
        "code_examples": [],
        "usage_examples": [],
        "metadata": {
          "level": "h2",
          "has_code": false,
          "has_usage": false,
          "has_table": false,
          "paragraph_count": 1,
          "content_length": 112
        }
      }
    ],
    "url": "https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html",
    "doc_type": "aws",
    "total_sections": 4
  }
]